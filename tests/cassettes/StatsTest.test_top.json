{
  "http_interactions": [
    {
      "recorded_at": "2016-06-19T20:27:59",
      "request": {
        "body": {
          "encoding": "utf-8",
          "string": "grant_type=password&password=<PASSWORD>&username=<USERNAME>"
        },
        "headers": {
          "Accept": "*/*",
          "Accept-Encoding": "identity",
          "Authorization": "Basic <BASIC_AUTH>",
          "Connection": "keep-alive",
          "Content-Length": "57",
          "Content-Type": "application/x-www-form-urlencoded",
          "User-Agent": "prawtools/0.19 PRAW/4.0.0b4 prawcore/0.0.8"
        },
        "method": "POST",
        "uri": "https://www.reddit.com/api/v1/access_token"
      },
      "response": {
        "body": {
          "encoding": "UTF-8",
          "string": "{\"access_token\": \"YE4ESXZ9lTjIP0ukcopwXM6361g\", \"token_type\": \"bearer\", \"expires_in\": 3600, \"scope\": \"*\"}"
        },
        "headers": {
          "CF-RAY": "2b59b990a3b8141f-LAX",
          "Connection": "keep-alive",
          "Content-Length": "105",
          "Content-Type": "application/json; charset=UTF-8",
          "Date": "Sun, 19 Jun 2016 20:27:59 GMT",
          "Server": "cloudflare-nginx",
          "Set-Cookie": "__cfduid=da3dbedd6bfa050f4f12fea14395b830f1466368079; expires=Mon, 19-Jun-17 20:27:59 GMT; path=/; domain=.reddit.com; HttpOnly",
          "Strict-Transport-Security": "max-age=15552000; includeSubDomains; preload",
          "X-Moose": "majestic",
          "cache-control": "max-age=0, must-revalidate",
          "x-content-type-options": "nosniff",
          "x-frame-options": "SAMEORIGIN",
          "x-xss-protection": "1; mode=block"
        },
        "status": {
          "code": 200,
          "message": "OK"
        },
        "url": "https://www.reddit.com/api/v1/access_token"
      }
    },
    {
      "recorded_at": "2016-06-19T20:28:01",
      "request": {
        "body": {
          "encoding": "utf-8",
          "string": ""
        },
        "headers": {
          "Accept": "*/*",
          "Accept-Encoding": "identity",
          "Authorization": "bearer YE4ESXZ9lTjIP0ukcopwXM6361g",
          "Connection": "keep-alive",
          "Cookie": "loid=SdAZIdDQoUDzjNqfRk; loidcreated=2016-06-19T20%3A27%3A59.507Z; __cfduid=da3dbedd6bfa050f4f12fea14395b830f1466368079",
          "User-Agent": "prawtools/0.19 PRAW/4.0.0b4 prawcore/0.0.8"
        },
        "method": "GET",
        "uri": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&raw_json=1"
      },
      "response": {
        "body": {
          "encoding": "UTF-8",
          "string": "{\"kind\": \"Listing\", \"data\": {\"modhash\": null, \"children\": [{\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ENearly 1 year ago we \\u003Ca href=\\\"http://www.redditblog.com/2014/09/hell-its-about-time-reddit-now-supports.html\\\"\\u003Egave you the ability to view reddit completely over SSL\\u003C/a\\u003E. Now we\\u0026#39;re ready to enforce that everyone use a secure connection with reddit.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EPlease ensure that all of your scripts can perform all of their functions over HTTPS by June 29.\\u003C/strong\\u003E At this time we will begin redirecting all site traffic to be over HTTPS and HTTP will no longer be available.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf this will be a problem for you, please let us know immediately.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT\\u003C/strong\\u003E 2015-08-21: IT IS DONE. You also have HSTS too.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Nearly 1 year ago we [gave you the ability to view reddit completely over SSL](http://www.redditblog.com/2014/09/hell-its-about-time-reddit-now-supports.html). Now we're ready to enforce that everyone use a secure connection with reddit.\\n\\n**Please ensure that all of your scripts can perform all of their functions over HTTPS by June 29.** At this time we will begin redirecting all site traffic to be over HTTPS and HTTP will no longer be available.\\n\\nIf this will be a problem for you, please let us know immediately.\\n\\n**EDIT** 2015-08-21: IT IS DONE. You also have HSTS too.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"39zje0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"rram\", \"media\": null, \"score\": 260, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 121, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1440173665.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/39zje0/reddit_will_soon_only_be_available_over_https/\", \"locked\": false, \"name\": \"t3_39zje0\", \"created\": 1434447340.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/39zje0/reddit_will_soon_only_be_available_over_https/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"reddit will soon only be available over HTTPS\", \"created_utc\": 1434418540.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 260}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAs promised, here is the big dump of voting information that you guys\\n\\u003Ca href=\\\"http://reddit.com/ddz0s\\\"\\u003Edonated to research\\u003C/a\\u003E. Warning: this contains\\nmuch geekery that may \\u003Ca href=\\\"http://www.flickr.com/photos/33809408@N00/1053349944/lightbox/\\\"\\u003Eresult in discomfort for the\\nnerd-challenged\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m trying to use it to build a recommender, and I\\u0026#39;ve got some\\npreliminary source code. I\\u0026#39;m looking for feedback on all of these\\nsteps, since I\\u0026#39;m not experienced at machine learning.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003EHere\\u0026#39;s what I\\u0026#39;ve done\\u003C/h2\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EI dumped all of the raw data that we\\u0026#39;ll need to generate the public\\ndumps. The queries are the comments in the two \\u003Ccode\\u003E.pig\\u003C/code\\u003E files and it\\ntook about 52 minutes to do the dump against production. The result\\nof this raw dump looks like:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E$ wc -l *.dump\\n   13,830,070 reddit_data_link.dump\\n  136,650,300 reddit_linkvote.dump\\n       69,489 reddit_research_ids.dump\\n   13,831,374 reddit_thing_link.dump\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EI filtered the list of votes for the list of users that gave us\\npermission to use their data. For the curious, that\\u0026#39;s 67,059 users:\\n62,763 with \\u0026quot;public votes\\u0026quot; and 6,726 with \\u0026quot;allow my data to be used\\nfor research\\u0026quot;. I\\u0026#39;d really like to see that second category\\nsignificantly increased, and hopefully this project will be what\\ndoes it. This filtering is done by \\u003Ccode\\u003Esrrecs_researchers.pig\\u003C/code\\u003E and took\\n83m55.335s on my laptop.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EI converted data-dumps that were in our DB schema format to a more\\nuseable format using \\u003Ccode\\u003Esrrecs.pig\\u003C/code\\u003E (about 13min)\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EFrom that dump I mapped all of the \\u003Ccode\\u003Eaccount_id\\u003C/code\\u003Es, \\u003Ccode\\u003Elink_id\\u003C/code\\u003Es, and\\n\\u003Ccode\\u003Esr_id\\u003C/code\\u003Es to salted hashes (using \\u003Ccode\\u003Eobscure()\\u003C/code\\u003E in \\u003Ccode\\u003Esrrecs.py\\u003C/code\\u003E with a\\nrandom seed, so even I don\\u0026#39;t know it). This took about 13min on my\\nlaptop. The result of this, \\u003Ccode\\u003Evotes.dump\\u003C/code\\u003E is the file that is\\nactually public. It is a tab-separated file consisting in:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eaccount_id,link_id,sr_id,dir\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThere are 23,091,688 votes from 43,976 users over 3,436,063 links in\\n11,675 reddits. (Interestingly these ~44k users represent almost 17%\\nof our total votes). The dump is 2.2gb uncompressed, 375mb in bz2.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Ch2\\u003EWhat to do with it\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EThe recommendations system that I\\u0026#39;m trying right now turns those votes\\ninto a set of affinities. That is, \\u0026quot;67% of user #223\\u0026#39;s votes on\\n\\u003Ccode\\u003E/r/reddit.com\\u003C/code\\u003E are upvotes and 52% on \\u003Ccode\\u003Eprogramming\\u003C/code\\u003E). To make these\\naffinities (55m45.107s on my laptop):\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E cat votes.dump | ./srrecs.py \\u0026quot;affinities_m()\\u0026quot; | sort -S200m | ./srrecs.py \\u0026quot;affinities_r()\\u0026quot; \\u0026gt; affinities.dump\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThen I turn the affinities into a sparse matrix representing\\nN-dimensional co-ordinates in the vector space of affinities (scaled\\nto -1..1 instead of 0..1), in the format used by R\\u0026#39;s\\n\\u003Ca href=\\\"http://cran.r-project.org/web/packages/skmeans/index.html\\\"\\u003Eskmeans\\u003C/a\\u003E\\npackage (less than a minute on my laptop). Imagine that this matrix\\nlooks like\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E          reddit.com pics       programming horseporn  bacon\\n          ---------- ---------- ----------- ---------  -----\\nketralnis -0.5       (no votes) +0.45       (no votes) +1.0\\njedberg   (no votes) -0.25      +0.95       +1.0       -1.0\\nraldi     +0.75      +0.75      +0.7        (no votes) +1.0\\n...\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EWe build it like:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E# they were already grouped by account_id, so we don\\u0026#39;t have to\\n# sort. changes to the previous step will probably require this\\n# step to have to sort the affinities first\\ncat affinities.dump | ./srrecs.py \\u0026quot;write_matrix(\\u0026#39;affinities.cm\\u0026#39;, \\u0026#39;affinities.clabel\\u0026#39;, \\u0026#39;affinities.rlabel\\u0026#39;)\\u0026quot;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI pass that through an R program \\u003Ccode\\u003Esrrecs.r\\u003C/code\\u003E (if you don\\u0026#39;t have R\\ninstalled, you\\u0026#39;ll need to install that, and the package \\u003Ccode\\u003Eskmeans\\u003C/code\\u003E like\\n\\u003Ccode\\u003Einstall.packages(\\u0026#39;skmeans\\u0026#39;)\\u003C/code\\u003E). This program plots the users in this\\nvector space finding clusters using a sperical kmeans clustering\\nalgorithm (on my laptop, takes about 10 minutes with 15 clusters and\\n16 minutes with 50 clusters, during which R sits at about 220mb of\\nRAM)\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E# looks for the files created by write_matrix in the current directory\\nR -f ./srrecs.r\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThe output of the program is a generated list of cluster-IDs,\\ncorresponding in order to the order of user-IDs in\\n\\u003Ccode\\u003Eaffinities.clabel\\u003C/code\\u003E. The numbers themselves are meaningless, but\\npeople in the same cluster ID have been clustered together.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003EHere are the files\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EThese are torrents of bzip2-compressed files. If you can\\u0026#39;t use the\\ntorrents for some reason it\\u0026#39;s pretty trivial to figure out from the\\nURL how to get to the files directly on S3, but \\u003Cem\\u003Eplease\\u003C/em\\u003E try the\\ntorrents first since it saves us a few bucks. It\\u0026#39;s S3 seeding the\\ntorrents anyway, so it\\u0026#39;s unlikely that direct-downloading is going to\\ngo any faster or be any easier.\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/votes.dump.bz2?torrent\\\"\\u003Evotes.dump.bz2\\u003C/a\\u003E -- A tab-separated list of:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eaccount_id, link_id, sr_id, direction\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EFor your convenience, a tab-separated list of votes already reduced to percent-affinities \\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/affinities.dump.bz2?torrent\\\"\\u003Eaffinities.dump.bz2\\u003C/a\\u003E, formatted:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eaccount_id, sr_id, affinity (scaled 0..1)\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EFor your convenience, \\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/affinities-matrix.tar.bz2?torrent\\\"\\u003Eaffinities-matrix.tar.bz2\\u003C/a\\u003E contains the R CLUTO format matrix files \\u003Ccode\\u003Eaffinities.cm\\u003C/code\\u003E, \\u003Ccode\\u003Eaffinities.clabel\\u003C/code\\u003E, \\u003Ccode\\u003Eaffinities.rlabel\\u003C/code\\u003E\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Ch2\\u003EAnd the code\\u003C/h2\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.pig\\\"\\u003Esrrecs.pig\\u003C/a\\u003E, \\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs_researchers.pig\\\"\\u003Esrrecs_researchers.pig\\u003C/a\\u003E -- what I used to\\ngenerate and format the dumps (you probably won\\u0026#39;t need this)\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/mr_tools.py\\\"\\u003Emr_tools.py\\u003C/a\\u003E, \\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.py\\\"\\u003Esrrecs.py\\u003C/a\\u003E -- what I used to salt/hash the user information and generate the R CLUTO-format matrix files (you probably won\\u0026#39;t need this unless you want different information in the matrix)\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.r\\\"\\u003Esrrecs.r\\u003C/a\\u003E -- the R-code to generate the clusters\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Ch2\\u003EHere\\u0026#39;s what you can experiment with\\u003C/h2\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EThe code isn\\u0026#39;t nearly useable yet. We need to turn the generated\\nclusters into an actual set of recommendations per cluster,\\npreferably ordered by predicted match. We probably need to do some\\nadditional post-processing per user, too. (If they gave us an\\naffinity of 0% to \\u003Ca href=\\\"/r/askreddit\\\"\\u003E/r/askreddit\\u003C/a\\u003E, we shouldn\\u0026#39;t recommend it, even if\\nwe predicted that the rest of their cluster would like it.)\\u003C/li\\u003E\\n\\u003Cli\\u003EWe need a test suite to gauge the accuracy of the results of\\ndifferent approaches. This could be done by dividing the data-set in\\nand using 80% for training and 20% to see if the predictions made by\\nthat 80% match.\\u003C/li\\u003E\\n\\u003Cli\\u003EWe need to get the whole process to less than two hours, because\\nthat\\u0026#39;s how often I want to run the recommender. It\\u0026#39;s okay to use two\\nor three machines to accomplish that and a lot of the steps can be\\ndone in parallel. That said we might just have to accept running it\\nless often. It needs to run end-to-end with no user-intervention,\\nfailing gracefully on error\\u003C/li\\u003E\\n\\u003Cli\\u003EIt would be handy to be able to idenfity the cluster of just a\\nsingle user on-the-fly after generating the clusters in bulk\\u003C/li\\u003E\\n\\u003Cli\\u003EThe results need to be hooked into the reddit UI. If you\\u0026#39;re willing\\nto dive into the codebase, this one will be important as soon as the\\nrest of the process is working and has a lot of room for creativity\\u003C/li\\u003E\\n\\u003Cli\\u003EWe need to find the sweet spot for the number of clusters to\\nuse. Put another way, how many different types of redditors do you\\nthink there are? This could best be done using the aforementioned\\ntest-suite and a good-old-fashioned binary search.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Ch2\\u003ESome notes:\\u003C/h2\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EI\\u0026#39;m not attached to doing this in R (I don\\u0026#39;t even know much R, it\\njust has a handy prebaked skmeans implementation). In fact I\\u0026#39;m not\\nattached to my methods here at all, I just want a good end-result.\\u003C/li\\u003E\\n\\u003Cli\\u003EThis is my weekend fun project, so it\\u0026#39;s likely to move very slowly\\nif we don\\u0026#39;t pick up enough participation here\\u003C/li\\u003E\\n\\u003Cli\\u003EThe final version will run against the whole dataset, not just the\\npublic one. So even though I can\\u0026#39;t release the whole dataset for\\nprivacy reasons, I can run your code and a test-suite against it\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"As promised, here is the big dump of voting information that you guys\\n[donated to research](http://reddit.com/ddz0s). Warning: this contains\\nmuch geekery that may [result in discomfort for the\\nnerd-challenged](http://www.flickr.com/photos/33809408@N00/1053349944/lightbox/).\\n\\nI'm trying to use it to build a recommender, and I've got some\\npreliminary source code. I'm looking for feedback on all of these\\nsteps, since I'm not experienced at machine learning.\\n\\n## Here's what I've done\\n\\n* I dumped all of the raw data that we'll need to generate the public\\n  dumps. The queries are the comments in the two `.pig` files and it\\n  took about 52 minutes to do the dump against production. The result\\n  of this raw dump looks like:\\n\\n      $ wc -l *.dump\\n       13,830,070 reddit_data_link.dump\\n      136,650,300 reddit_linkvote.dump\\n           69,489 reddit_research_ids.dump\\n       13,831,374 reddit_thing_link.dump\\n\\n* I filtered the list of votes for the list of users that gave us\\n  permission to use their data. For the curious, that's 67,059 users:\\n  62,763 with \\\"public votes\\\" and 6,726 with \\\"allow my data to be used\\n  for research\\\". I'd really like to see that second category\\n  significantly increased, and hopefully this project will be what\\n  does it. This filtering is done by `srrecs_researchers.pig` and took\\n  83m55.335s on my laptop.\\n* I converted data-dumps that were in our DB schema format to a more\\n  useable format using `srrecs.pig` (about 13min)\\n* From that dump I mapped all of the `account_id`s, `link_id`s, and\\n  `sr_id`s to salted hashes (using `obscure()` in `srrecs.py` with a\\n  random seed, so even I don't know it). This took about 13min on my\\n  laptop. The result of this, `votes.dump` is the file that is\\n  actually public. It is a tab-separated file consisting in:\\n\\n      account_id,link_id,sr_id,dir\\n\\n  There are 23,091,688 votes from 43,976 users over 3,436,063 links in\\n  11,675 reddits. (Interestingly these ~44k users represent almost 17%\\n  of our total votes). The dump is 2.2gb uncompressed, 375mb in bz2.\\n\\n## What to do with it\\n\\nThe recommendations system that I'm trying right now turns those votes\\ninto a set of affinities. That is, \\\"67% of user #223's votes on\\n`/r/reddit.com` are upvotes and 52% on `programming`). To make these\\naffinities (55m45.107s on my laptop):\\n\\n     cat votes.dump | ./srrecs.py \\\"affinities_m()\\\" | sort -S200m | ./srrecs.py \\\"affinities_r()\\\" \\u003E affinities.dump\\n\\nThen I turn the affinities into a sparse matrix representing\\nN-dimensional co-ordinates in the vector space of affinities (scaled\\nto -1..1 instead of 0..1), in the format used by R's\\n[skmeans](http://cran.r-project.org/web/packages/skmeans/index.html)\\npackage (less than a minute on my laptop). Imagine that this matrix\\nlooks like\\n\\n              reddit.com pics       programming horseporn  bacon\\n              ---------- ---------- ----------- ---------  -----\\n    ketralnis -0.5       (no votes) +0.45       (no votes) +1.0\\n    jedberg   (no votes) -0.25      +0.95       +1.0       -1.0\\n    raldi     +0.75      +0.75      +0.7        (no votes) +1.0\\n    ...\\n\\nWe build it like:\\n\\n    # they were already grouped by account_id, so we don't have to\\n    # sort. changes to the previous step will probably require this\\n    # step to have to sort the affinities first\\n    cat affinities.dump | ./srrecs.py \\\"write_matrix('affinities.cm', 'affinities.clabel', 'affinities.rlabel')\\\"\\n\\nI pass that through an R program `srrecs.r` (if you don't have R\\ninstalled, you'll need to install that, and the package `skmeans` like\\n`install.packages('skmeans')`). This program plots the users in this\\nvector space finding clusters using a sperical kmeans clustering\\nalgorithm (on my laptop, takes about 10 minutes with 15 clusters and\\n16 minutes with 50 clusters, during which R sits at about 220mb of\\nRAM)\\n\\n    # looks for the files created by write_matrix in the current directory\\n    R -f ./srrecs.r\\n\\nThe output of the program is a generated list of cluster-IDs,\\ncorresponding in order to the order of user-IDs in\\n`affinities.clabel`. The numbers themselves are meaningless, but\\npeople in the same cluster ID have been clustered together.\\n\\n## Here are the files\\n\\nThese are torrents of bzip2-compressed files. If you can't use the\\ntorrents for some reason it's pretty trivial to figure out from the\\nURL how to get to the files directly on S3, but *please* try the\\ntorrents first since it saves us a few bucks. It's S3 seeding the\\ntorrents anyway, so it's unlikely that direct-downloading is going to\\ngo any faster or be any easier.\\n\\n* [votes.dump.bz2](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/votes.dump.bz2?torrent) -- A tab-separated list of:\\n\\n      account_id, link_id, sr_id, direction\\n\\n* For your convenience, a tab-separated list of votes already reduced to percent-affinities [affinities.dump.bz2](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/affinities.dump.bz2?torrent), formatted:\\n\\n      account_id, sr_id, affinity (scaled 0..1)\\n\\n* For your convenience, [affinities-matrix.tar.bz2](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/affinities-matrix.tar.bz2?torrent) contains the R CLUTO format matrix files `affinities.cm`, `affinities.clabel`, `affinities.rlabel`\\n\\n## And the code\\n\\n* [srrecs.pig](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.pig), [srrecs_researchers.pig](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs_researchers.pig) -- what I used to\\n  generate and format the dumps (you probably won't need this)\\n* [mr_tools.py](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/mr_tools.py), [srrecs.py](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.py) -- what I used to salt/hash the user information and generate the R CLUTO-format matrix files (you probably won't need this unless you want different information in the matrix)\\n* [srrecs.r](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.r) -- the R-code to generate the clusters\\n\\n## Here's what you can experiment with\\n\\n* The code isn't nearly useable yet. We need to turn the generated\\n  clusters into an actual set of recommendations per cluster,\\n  preferably ordered by predicted match. We probably need to do some\\n  additional post-processing per user, too. (If they gave us an\\n  affinity of 0% to /r/askreddit, we shouldn't recommend it, even if\\n  we predicted that the rest of their cluster would like it.)\\n* We need a test suite to gauge the accuracy of the results of\\n  different approaches. This could be done by dividing the data-set in\\n  and using 80% for training and 20% to see if the predictions made by\\n  that 80% match.\\n* We need to get the whole process to less than two hours, because\\n  that's how often I want to run the recommender. It's okay to use two\\n  or three machines to accomplish that and a lot of the steps can be\\n  done in parallel. That said we might just have to accept running it\\n  less often. It needs to run end-to-end with no user-intervention,\\n  failing gracefully on error\\n* It would be handy to be able to idenfity the cluster of just a\\n  single user on-the-fly after generating the clusters in bulk\\n* The results need to be hooked into the reddit UI. If you're willing\\n  to dive into the codebase, this one will be important as soon as the\\n  rest of the process is working and has a lot of room for creativity\\n* We need to find the sweet spot for the number of clusters to\\n  use. Put another way, how many different types of redditors do you\\n  think there are? This could best be done using the aforementioned\\n  test-suite and a good-old-fashioned binary search.\\n\\n## Some notes:\\n\\n* I'm not attached to doing this in R (I don't even know much R, it\\n  just has a handy prebaked skmeans implementation). In fact I'm not\\n  attached to my methods here at all, I just want a good end-result.\\n* This is my weekend fun project, so it's likely to move very slowly\\n  if we don't pick up enough participation here\\n* The final version will run against the whole dataset, not just the\\n  public one. So even though I can't release the whole dataset for\\n  privacy reasons, I can run your code and a test-suite against it\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"dtg4j\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ketralnis\", \"media\": null, \"score\": 180, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 70, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/dtg4j/want_to_help_reddit_build_a_recommender_a_public/\", \"locked\": false, \"name\": \"t3_dtg4j\", \"created\": 1287542473.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/dtg4j/want_to_help_reddit_build_a_recommender_a_public/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Want to help reddit build a recommender? -- A public dump of voting data that our users have donated for research\", \"created_utc\": 1287513673.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 180}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHola all,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs of right now, if the number of online users that are on a subreddit totals fewer than 100, the metric simply displays the value as \\u0026quot;\\u0026lt;100\\u0026quot;. I purposefully took a very conservative approach to this, as giving a more detailed metric for small count of active users has some potential privacy implications. For example, in a very small subreddit with a limited set of active users, you could do some analysis and an educated guess at when a group of those individuals are on reddit. The less active the subreddit, the more educated the guess. It\\u0026#39;s a bit of a reach, but I decided to err on the side of caution.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESince the feature was rolled out, the general response seems to be that people want minimum display value lowered. Here\\u0026#39;s my proposal on how to execute that, while still minimizing the potential privacy problems.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EJust as it is now, the metric will be accurate for values of 100 or greater. However, if the true count is fewer than 100, a random jitter will be added to fuzz the true value. The jitter will be the largest for very small counts, and exponentially decreases as the true count increases, reaching a jitter of 0 when the true value is 100. For example, a true value of 0 may display anywhere from 0-6, a true value of 40 may display anywere from 40-43.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAdditionally, low values will be cached on the back-end for 5 minutes. This prevents someone from rapidly sampling the fuzzed values to determine the true value.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI also recognize that some subreddits simply want to hide low values. To easily allow for this, I will also be adding a \\u0026quot;fuzzed\\u0026quot; CSS class to any value less than 100. This will allow subreddits to hide the low value fuzzed numbers, while still displaying higher values. Of course, the count can still be hidden entirely via CSS, just as it is now.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease let me know any thoughts or concerns you might have regarding this proposed change.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Echeers,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ealienth\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Etl;dr\\u003C/strong\\u003E Users-online will be display all the way down to zero, but low values will be fuzzed and cached for a period of 5 minutes to protect privacy.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hola all,\\n\\nAs of right now, if the number of online users that are on a subreddit totals fewer than 100, the metric simply displays the value as \\\"\\u003C100\\\". I purposefully took a very conservative approach to this, as giving a more detailed metric for small count of active users has some potential privacy implications. For example, in a very small subreddit with a limited set of active users, you could do some analysis and an educated guess at when a group of those individuals are on reddit. The less active the subreddit, the more educated the guess. It's a bit of a reach, but I decided to err on the side of caution.\\n\\nSince the feature was rolled out, the general response seems to be that people want minimum display value lowered. Here's my proposal on how to execute that, while still minimizing the potential privacy problems.\\n\\nJust as it is now, the metric will be accurate for values of 100 or greater. However, if the true count is fewer than 100, a random jitter will be added to fuzz the true value. The jitter will be the largest for very small counts, and exponentially decreases as the true count increases, reaching a jitter of 0 when the true value is 100. For example, a true value of 0 may display anywhere from 0-6, a true value of 40 may display anywere from 40-43.\\n\\nAdditionally, low values will be cached on the back-end for 5 minutes. This prevents someone from rapidly sampling the fuzzed values to determine the true value.\\n\\nI also recognize that some subreddits simply want to hide low values. To easily allow for this, I will also be adding a \\\"fuzzed\\\" CSS class to any value less than 100. This will allow subreddits to hide the low value fuzzed numbers, while still displaying higher values. Of course, the count can still be hidden entirely via CSS, just as it is now.\\n\\nPlease let me know any thoughts or concerns you might have regarding this proposed change.\\n\\ncheers,\\n\\nalienth\\n\\n**tl;dr** Users-online will be display all the way down to zero, but low values will be fuzzed and cached for a period of 5 minutes to protect privacy.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"yjk55\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"alienth\", \"media\": null, \"score\": 127, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 146, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1345496896.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/yjk55/proposed_change_to_the_users_online_count_for_low/\", \"locked\": false, \"name\": \"t3_yjk55\", \"created\": 1345525092.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/yjk55/proposed_change_to_the_users_online_count_for_low/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Proposed change to the 'users online' count for low values (\\u003C100)\", \"created_utc\": 1345496292.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 127}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Ch3\\u003EUPDATE 2: \\u003Ca href=\\\"http://groups.google.com/group/rrecommender\\\"\\u003EJoin the Google Group to stay on top of progress!\\u003C/a\\u003E \\u003Ca href=\\\"http://groups.google.com/group/rrecommender\\\"\\u003Ehttp://groups.google.com/group/rrecommender\\u003C/a\\u003E\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EI made a post about the Google Group here: \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/mev1j/reddit_recommendor_google_group_to_coordinate/\\\"\\u003Ehttp://www.reddit.com/r/redditdev/comments/mev1j/reddit_recommendor_google_group_to_coordinate/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EUPDATE: User \\u003Ca href=\\\"http://www.reddit.com/user/killerstorm\\\"\\u003Ekillerstorm\\u003C/a\\u003E \\u0026amp; \\u003Ca href=\\\"/user/qubey\\\"\\u003Equbey\\u003C/a\\u003E are doing the coordination on the project. For admin assistance, please contact \\u003Ca href=\\\"http://www.reddit.com/user/chromakode\\\"\\u003Echromakode\\u003C/a\\u003E.\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EUpdate 3: My original suggestion and overview of the problem a year ago: \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/d95ad/request_we_need_to_work_on_a_solution_to_the/\\\"\\u003Ehttp://www.reddit.com/r/redditdev/comments/d95ad/request_we_need_to_work_on_a_solution_to_the/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EHello Reddit developers! \\u003Ca href=\\\"http://www.reddit.com/user/ketralnis\\\"\\u003Eketralnis\\u003C/a\\u003E \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/dtg4j/want_to_help_reddit_build_a_recommender_a_public/\\\"\\u003Ewrote the following message below a year ago\\u003C/a\\u003E. After leaving reddit, the project got put on the back burner. We still have the same problem introducing people to new reddits and the \\u003Ca href=\\\"http://www.reddit.com/reddits\\\"\\u003Ereddit search\\u003C/a\\u003E is still terrible. After reading the recent reddit blog post about introducing more reddits into the default set, this project needs to be revived more than ever. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am hoping you can help revive this project and make it a reality! Together, we can make reddit a better place. \\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EAs promised, here is the big dump of voting information that you guys\\n\\u003Ca href=\\\"http://reddit.com/ddz0s\\\"\\u003Edonated to research\\u003C/a\\u003E. Warning: this contains\\nmuch geekery that may \\u003Ca href=\\\"http://www.flickr.com/photos/33809408@N00/1053349944/lightbox/\\\"\\u003Eresult in discomfort for the\\nnerd-challenged\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m trying to use it to build a recommender, and I\\u0026#39;ve got some\\npreliminary source code. I\\u0026#39;m looking for feedback on all of these\\nsteps, since I\\u0026#39;m not experienced at machine learning.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003EHere\\u0026#39;s what I\\u0026#39;ve done\\u003C/h2\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EI dumped all of the raw data that we\\u0026#39;ll need to generate the public\\ndumps. The queries are the comments in the two \\u003Ccode\\u003E.pig\\u003C/code\\u003E files and it\\ntook about 52 minutes to do the dump against production. The result\\nof this raw dump looks like:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E$ wc -l *.dump\\n   13,830,070 reddit_data_link.dump\\n  136,650,300 reddit_linkvote.dump\\n       69,489 reddit_research_ids.dump\\n   13,831,374 reddit_thing_link.dump\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EI filtered the list of votes for the list of users that gave us\\npermission to use their data. For the curious, that\\u0026#39;s 67,059 users:\\n62,763 with \\u0026quot;public votes\\u0026quot; and 6,726 with \\u0026quot;allow my data to be used\\nfor research\\u0026quot;. I\\u0026#39;d really like to see that second category\\nsignificantly increased, and hopefully this project will be what\\ndoes it. This filtering is done by \\u003Ccode\\u003Esrrecs_researchers.pig\\u003C/code\\u003E and took\\n83m55.335s on my laptop.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EI converted data-dumps that were in our DB schema format to a more\\nuseable format using \\u003Ccode\\u003Esrrecs.pig\\u003C/code\\u003E (about 13min)\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EFrom that dump I mapped all of the \\u003Ccode\\u003Eaccount_id\\u003C/code\\u003Es, \\u003Ccode\\u003Elink_id\\u003C/code\\u003Es, and\\n\\u003Ccode\\u003Esr_id\\u003C/code\\u003Es to salted hashes (using \\u003Ccode\\u003Eobscure()\\u003C/code\\u003E in \\u003Ccode\\u003Esrrecs.py\\u003C/code\\u003E with a\\nrandom seed, so even I don\\u0026#39;t know it). This took about 13min on my\\nlaptop. The result of this, \\u003Ccode\\u003Evotes.dump\\u003C/code\\u003E is the file that is\\nactually public. It is a tab-separated file consisting in:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eaccount_id,link_id,sr_id,dir\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThere are 23,091,688 votes from 43,976 users over 3,436,063 links in\\n11,675 reddits. (Interestingly these ~44k users represent almost 17%\\nof our total votes). The dump is 2.2gb uncompressed, 375mb in bz2.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Ch2\\u003EWhat to do with it\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EThe recommendations system that I\\u0026#39;m trying right now turns those votes\\ninto a set of affinities. That is, \\u0026quot;67% of user #223\\u0026#39;s votes on\\n\\u003Ccode\\u003E/r/reddit.com\\u003C/code\\u003E are upvotes and 52% on \\u003Ccode\\u003Eprogramming\\u003C/code\\u003E). To make these\\naffinities (55m45.107s on my laptop):\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E cat votes.dump | ./srrecs.py \\u0026quot;affinities_m()\\u0026quot; | sort -S200m | ./srrecs.py \\u0026quot;affinities_r()\\u0026quot; \\u0026gt; affinities.dump\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThen I turn the affinities into a sparse matrix representing\\nN-dimensional co-ordinates in the vector space of affinities (scaled\\nto -1..1 instead of 0..1), in the format used by R\\u0026#39;s\\n\\u003Ca href=\\\"http://cran.r-project.org/web/packages/skmeans/index.html\\\"\\u003Eskmeans\\u003C/a\\u003E\\npackage (less than a minute on my laptop). Imagine that this matrix\\nlooks like\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E          reddit.com pics       programming horseporn  bacon\\n          ---------- ---------- ----------- ---------  -----\\nketralnis -0.5       (no votes) +0.45       (no votes) +1.0\\njedberg   (no votes) -0.25      +0.95       +1.0       -1.0\\nraldi     +0.75      +0.75      +0.7        (no votes) +1.0\\n...\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EWe build it like:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E# they were already grouped by account_id, so we don\\u0026#39;t have to\\n# sort. changes to the previous step will probably require this\\n# step to have to sort the affinities first\\ncat affinities.dump | ./srrecs.py \\u0026quot;write_matrix(\\u0026#39;affinities.cm\\u0026#39;, \\u0026#39;affinities.clabel\\u0026#39;, \\u0026#39;affinities.rlabel\\u0026#39;)\\u0026quot;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI pass that through an R program \\u003Ccode\\u003Esrrecs.r\\u003C/code\\u003E (if you don\\u0026#39;t have R\\ninstalled, you\\u0026#39;ll need to install that, and the package \\u003Ccode\\u003Eskmeans\\u003C/code\\u003E like\\n\\u003Ccode\\u003Einstall.packages(\\u0026#39;skmeans\\u0026#39;)\\u003C/code\\u003E). This program plots the users in this\\nvector space finding clusters using a sperical kmeans clustering\\nalgorithm (on my laptop, takes about 10 minutes with 15 clusters and\\n16 minutes with 50 clusters, during which R sits at about 220mb of\\nRAM)\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E# looks for the files created by write_matrix in the current directory\\nR -f ./srrecs.r\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThe output of the program is a generated list of cluster-IDs,\\ncorresponding in order to the order of user-IDs in\\n\\u003Ccode\\u003Eaffinities.clabel\\u003C/code\\u003E. The numbers themselves are meaningless, but\\npeople in the same cluster ID have been clustered together.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003EHere are the files\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EThese are torrents of bzip2-compressed files. If you can\\u0026#39;t use the\\ntorrents for some reason it\\u0026#39;s pretty trivial to figure out from the\\nURL how to get to the files directly on S3, but \\u003Cem\\u003Eplease\\u003C/em\\u003E try the\\ntorrents first since it saves us a few bucks. It\\u0026#39;s S3 seeding the\\ntorrents anyway, so it\\u0026#39;s unlikely that direct-downloading is going to\\ngo any faster or be any easier.\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/votes.dump.bz2?torrent\\\"\\u003Evotes.dump.bz2\\u003C/a\\u003E -- A tab-separated list of:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eaccount_id, link_id, sr_id, direction\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EFor your convenience, a tab-separated list of votes already reduced to percent-affinities \\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/affinities.dump.bz2?torrent\\\"\\u003Eaffinities.dump.bz2\\u003C/a\\u003E, formatted:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eaccount_id, sr_id, affinity (scaled 0..1)\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EFor your convenience, \\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/affinities-matrix.tar.bz2?torrent\\\"\\u003Eaffinities-matrix.tar.bz2\\u003C/a\\u003E contains the R CLUTO format matrix files \\u003Ccode\\u003Eaffinities.cm\\u003C/code\\u003E, \\u003Ccode\\u003Eaffinities.clabel\\u003C/code\\u003E, \\u003Ccode\\u003Eaffinities.rlabel\\u003C/code\\u003E\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Ch2\\u003EAnd the code\\u003C/h2\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.pig\\\"\\u003Esrrecs.pig\\u003C/a\\u003E, \\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs_researchers.pig\\\"\\u003Esrrecs_researchers.pig\\u003C/a\\u003E -- what I used to\\ngenerate and format the dumps (you probably won\\u0026#39;t need this)\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/mr_tools.py\\\"\\u003Emr_tools.py\\u003C/a\\u003E, \\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.py\\\"\\u003Esrrecs.py\\u003C/a\\u003E -- what I used to salt/hash the user information and generate the R CLUTO-format matrix files (you probably won\\u0026#39;t need this unless you want different information in the matrix)\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.r\\\"\\u003Esrrecs.r\\u003C/a\\u003E -- the R-code to generate the clusters\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Ch2\\u003EHere\\u0026#39;s what you can experiment with\\u003C/h2\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EThe code isn\\u0026#39;t nearly useable yet. We need to turn the generated\\nclusters into an actual set of recommendations per cluster,\\npreferably ordered by predicted match. We probably need to do some\\nadditional post-processing per user, too. (If they gave us an\\naffinity of 0% to \\u003Ca href=\\\"/r/askreddit\\\"\\u003E/r/askreddit\\u003C/a\\u003E, we shouldn\\u0026#39;t recommend it, even if\\nwe predicted that the rest of their cluster would like it.)\\u003C/li\\u003E\\n\\u003Cli\\u003EWe need a test suite to gauge the accuracy of the results of\\ndifferent approaches. This could be done by dividing the data-set in\\nand using 80% for training and 20% to see if the predictions made by\\nthat 80% match.\\u003C/li\\u003E\\n\\u003Cli\\u003EWe need to get the whole process to less than two hours, because\\nthat\\u0026#39;s how often I want to run the recommender. It\\u0026#39;s okay to use two\\nor three machines to accomplish that and a lot of the steps can be\\ndone in parallel. That said we might just have to accept running it\\nless often. It needs to run end-to-end with no user-intervention,\\nfailing gracefully on error\\u003C/li\\u003E\\n\\u003Cli\\u003EIt would be handy to be able to idenfity the cluster of just a\\nsingle user on-the-fly after generating the clusters in bulk\\u003C/li\\u003E\\n\\u003Cli\\u003EThe results need to be hooked into the reddit UI. If you\\u0026#39;re willing\\nto dive into the codebase, this one will be important as soon as the\\nrest of the process is working and has a lot of room for creativity\\u003C/li\\u003E\\n\\u003Cli\\u003EWe need to find the sweet spot for the number of clusters to\\nuse. Put another way, how many different types of redditors do you\\nthink there are? This could best be done using the aforementioned\\ntest-suite and a good-old-fashioned binary search.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Ch2\\u003ESome notes:\\u003C/h2\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EI\\u0026#39;m not attached to doing this in R (I don\\u0026#39;t even know much R, it\\njust has a handy prebaked skmeans implementation). In fact I\\u0026#39;m not\\nattached to my methods here at all, I just want a good end-result.\\u003C/li\\u003E\\n\\u003Cli\\u003EThis is my weekend fun project, so it\\u0026#39;s likely to move very slowly\\nif we don\\u0026#39;t pick up enough participation here\\u003C/li\\u003E\\n\\u003Cli\\u003EThe final version will run against the whole dataset, not just the\\npublic one. So even though I can\\u0026#39;t release the whole dataset for\\nprivacy reasons, I can run your code and a test-suite against it\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"###UPDATE 2: [Join the Google Group to stay on top of progress!](http://groups.google.com/group/rrecommender) http://groups.google.com/group/rrecommender  \\n\\nI made a post about the Google Group here: http://www.reddit.com/r/redditdev/comments/mev1j/reddit_recommendor_google_group_to_coordinate/\\n\\n**UPDATE: User [killerstorm](http://www.reddit.com/user/killerstorm) \\u0026 [qubey](/user/qubey) are doing the coordination on the project. For admin assistance, please contact [chromakode](http://www.reddit.com/user/chromakode).**\\n\\nUpdate 3: My original suggestion and overview of the problem a year ago: http://www.reddit.com/r/redditdev/comments/d95ad/request_we_need_to_work_on_a_solution_to_the/\\n\\n\\n---\\n\\nHello Reddit developers! [ketralnis](http://www.reddit.com/user/ketralnis) [wrote the following message below a year ago](http://www.reddit.com/r/redditdev/comments/dtg4j/want_to_help_reddit_build_a_recommender_a_public/). After leaving reddit, the project got put on the back burner. We still have the same problem introducing people to new reddits and the [reddit search](http://www.reddit.com/reddits) is still terrible. After reading the recent reddit blog post about introducing more reddits into the default set, this project needs to be revived more than ever. \\n\\nI am hoping you can help revive this project and make it a reality! Together, we can make reddit a better place. \\n\\n----\\n\\nAs promised, here is the big dump of voting information that you guys\\n[donated to research](http://reddit.com/ddz0s). Warning: this contains\\nmuch geekery that may [result in discomfort for the\\nnerd-challenged](http://www.flickr.com/photos/33809408@N00/1053349944/lightbox/).\\n\\nI'm trying to use it to build a recommender, and I've got some\\npreliminary source code. I'm looking for feedback on all of these\\nsteps, since I'm not experienced at machine learning.\\n\\n## Here's what I've done\\n\\n* I dumped all of the raw data that we'll need to generate the public\\n  dumps. The queries are the comments in the two `.pig` files and it\\n  took about 52 minutes to do the dump against production. The result\\n  of this raw dump looks like:\\n\\n      $ wc -l *.dump\\n       13,830,070 reddit_data_link.dump\\n      136,650,300 reddit_linkvote.dump\\n           69,489 reddit_research_ids.dump\\n       13,831,374 reddit_thing_link.dump\\n\\n* I filtered the list of votes for the list of users that gave us\\n  permission to use their data. For the curious, that's 67,059 users:\\n  62,763 with \\\"public votes\\\" and 6,726 with \\\"allow my data to be used\\n  for research\\\". I'd really like to see that second category\\n  significantly increased, and hopefully this project will be what\\n  does it. This filtering is done by `srrecs_researchers.pig` and took\\n  83m55.335s on my laptop.\\n* I converted data-dumps that were in our DB schema format to a more\\n  useable format using `srrecs.pig` (about 13min)\\n* From that dump I mapped all of the `account_id`s, `link_id`s, and\\n  `sr_id`s to salted hashes (using `obscure()` in `srrecs.py` with a\\n  random seed, so even I don't know it). This took about 13min on my\\n  laptop. The result of this, `votes.dump` is the file that is\\n  actually public. It is a tab-separated file consisting in:\\n\\n      account_id,link_id,sr_id,dir\\n\\n  There are 23,091,688 votes from 43,976 users over 3,436,063 links in\\n  11,675 reddits. (Interestingly these ~44k users represent almost 17%\\n  of our total votes). The dump is 2.2gb uncompressed, 375mb in bz2.\\n\\n## What to do with it\\n\\nThe recommendations system that I'm trying right now turns those votes\\ninto a set of affinities. That is, \\\"67% of user #223's votes on\\n`/r/reddit.com` are upvotes and 52% on `programming`). To make these\\naffinities (55m45.107s on my laptop):\\n\\n     cat votes.dump | ./srrecs.py \\\"affinities_m()\\\" | sort -S200m | ./srrecs.py \\\"affinities_r()\\\" \\u003E affinities.dump\\n\\nThen I turn the affinities into a sparse matrix representing\\nN-dimensional co-ordinates in the vector space of affinities (scaled\\nto -1..1 instead of 0..1), in the format used by R's\\n[skmeans](http://cran.r-project.org/web/packages/skmeans/index.html)\\npackage (less than a minute on my laptop). Imagine that this matrix\\nlooks like\\n\\n              reddit.com pics       programming horseporn  bacon\\n              ---------- ---------- ----------- ---------  -----\\n    ketralnis -0.5       (no votes) +0.45       (no votes) +1.0\\n    jedberg   (no votes) -0.25      +0.95       +1.0       -1.0\\n    raldi     +0.75      +0.75      +0.7        (no votes) +1.0\\n    ...\\n\\nWe build it like:\\n\\n    # they were already grouped by account_id, so we don't have to\\n    # sort. changes to the previous step will probably require this\\n    # step to have to sort the affinities first\\n    cat affinities.dump | ./srrecs.py \\\"write_matrix('affinities.cm', 'affinities.clabel', 'affinities.rlabel')\\\"\\n\\nI pass that through an R program `srrecs.r` (if you don't have R\\ninstalled, you'll need to install that, and the package `skmeans` like\\n`install.packages('skmeans')`). This program plots the users in this\\nvector space finding clusters using a sperical kmeans clustering\\nalgorithm (on my laptop, takes about 10 minutes with 15 clusters and\\n16 minutes with 50 clusters, during which R sits at about 220mb of\\nRAM)\\n\\n    # looks for the files created by write_matrix in the current directory\\n    R -f ./srrecs.r\\n\\nThe output of the program is a generated list of cluster-IDs,\\ncorresponding in order to the order of user-IDs in\\n`affinities.clabel`. The numbers themselves are meaningless, but\\npeople in the same cluster ID have been clustered together.\\n\\n## Here are the files\\n\\nThese are torrents of bzip2-compressed files. If you can't use the\\ntorrents for some reason it's pretty trivial to figure out from the\\nURL how to get to the files directly on S3, but *please* try the\\ntorrents first since it saves us a few bucks. It's S3 seeding the\\ntorrents anyway, so it's unlikely that direct-downloading is going to\\ngo any faster or be any easier.\\n\\n* [votes.dump.bz2](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/votes.dump.bz2?torrent) -- A tab-separated list of:\\n\\n      account_id, link_id, sr_id, direction\\n\\n* For your convenience, a tab-separated list of votes already reduced to percent-affinities [affinities.dump.bz2](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/affinities.dump.bz2?torrent), formatted:\\n\\n      account_id, sr_id, affinity (scaled 0..1)\\n\\n* For your convenience, [affinities-matrix.tar.bz2](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/affinities-matrix.tar.bz2?torrent) contains the R CLUTO format matrix files `affinities.cm`, `affinities.clabel`, `affinities.rlabel`\\n\\n## And the code\\n\\n* [srrecs.pig](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.pig), [srrecs_researchers.pig](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs_researchers.pig) -- what I used to\\n  generate and format the dumps (you probably won't need this)\\n* [mr_tools.py](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/mr_tools.py), [srrecs.py](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.py) -- what I used to salt/hash the user information and generate the R CLUTO-format matrix files (you probably won't need this unless you want different information in the matrix)\\n* [srrecs.r](http://redditketralnis.s3.amazonaws.com/publicvotes-20101018/srrecs.r) -- the R-code to generate the clusters\\n\\n## Here's what you can experiment with\\n\\n* The code isn't nearly useable yet. We need to turn the generated\\n  clusters into an actual set of recommendations per cluster,\\n  preferably ordered by predicted match. We probably need to do some\\n  additional post-processing per user, too. (If they gave us an\\n  affinity of 0% to /r/askreddit, we shouldn't recommend it, even if\\n  we predicted that the rest of their cluster would like it.)\\n* We need a test suite to gauge the accuracy of the results of\\n  different approaches. This could be done by dividing the data-set in\\n  and using 80% for training and 20% to see if the predictions made by\\n  that 80% match.\\n* We need to get the whole process to less than two hours, because\\n  that's how often I want to run the recommender. It's okay to use two\\n  or three machines to accomplish that and a lot of the steps can be\\n  done in parallel. That said we might just have to accept running it\\n  less often. It needs to run end-to-end with no user-intervention,\\n  failing gracefully on error\\n* It would be handy to be able to idenfity the cluster of just a\\n  single user on-the-fly after generating the clusters in bulk\\n* The results need to be hooked into the reddit UI. If you're willing\\n  to dive into the codebase, this one will be important as soon as the\\n  rest of the process is working and has a lot of room for creativity\\n* We need to find the sweet spot for the number of clusters to\\n  use. Put another way, how many different types of redditors do you\\n  think there are? This could best be done using the aforementioned\\n  test-suite and a good-old-fashioned binary search.\\n\\n## Some notes:\\n\\n* I'm not attached to doing this in R (I don't even know much R, it\\n  just has a handy prebaked skmeans implementation). In fact I'm not\\n  attached to my methods here at all, I just want a good end-result.\\n* This is my weekend fun project, so it's likely to move very slowly\\n  if we don't pick up enough participation here\\n* The final version will run against the whole dataset, not just the\\n  public one. So even though I can't release the whole dataset for\\n  privacy reasons, I can run your code and a test-suite against it\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"lowwf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"TedFromTheFuture\", \"media\": null, \"score\": 116, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 52, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/lowwf/attempt_2_want_to_help_reddit_build_a_recommender/\", \"locked\": false, \"name\": \"t3_lowwf\", \"created\": 1319608063.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/lowwf/attempt_2_want_to_help_reddit_build_a_recommender/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Attempt #2: Want to help reddit build a recommender? -- A public dump of voting data that our users have donated for research\", \"created_utc\": 1319579263.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 116}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESome people have asked for a dump of some voting data, so I made one. You can \\u003Ca href=\\\"http://redditketralnis.s3.amazonaws.com/publicvotes.csv.gz?torrent\\\" title=\\\"download me!\\\"\\u003Edownload it via bittorrent\\u003C/a\\u003E (it\\u0026#39;s hosted and seeded by S3, so don\\u0026#39;t worry about it going away) and have at. The format is\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eusername,link_id,vote\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Ewhere \\u003Ccode\\u003Evote\\u003C/code\\u003E is -1 or 1 (downvote or upvote).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe dump is 29MB gzip compressed and contains 7,405,561 votes from 31,927 users over 2,046,401 links. It contains votes only from users with the preference \\u0026quot;make my votes public\\u0026quot; turned on (which is not the default).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis doesn\\u0026#39;t have the subreddit ID or anything in there, but I\\u0026#39;d be willing to make another dump with more data if anything comes of this one\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Some people have asked for a dump of some voting data, so I made one. You can [download it via bittorrent](http://redditketralnis.s3.amazonaws.com/publicvotes.csv.gz?torrent \\\"download me!\\\") (it's hosted and seeded by S3, so don't worry about it going away) and have at. The format is\\n\\n    username,link_id,vote\\n\\nwhere `vote` is -1 or 1 (downvote or upvote).\\n\\nThe dump is 29MB gzip compressed and contains 7,405,561 votes from 31,927 users over 2,046,401 links. It contains votes only from users with the preference \\\"make my votes public\\\" turned on (which is not the default).\\n\\nThis doesn't have the subreddit ID or anything in there, but I'd be willing to make another dump with more data if anything comes of this one\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"bubhl\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ketralnis\", \"media\": null, \"score\": 118, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 75, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/bubhl/csv_dump_of_reddit_voting_data/\", \"locked\": false, \"name\": \"t3_bubhl\", \"created\": 1271916590.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/bubhl/csv_dump_of_reddit_voting_data/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"CSV dump of reddit voting data\", \"created_utc\": 1271887790.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 118}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"groups.google.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"dqkfz\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ketralnis\", \"media\": null, \"score\": 97, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 49, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/dqkfz/why_is_reddit_so_slow/\", \"locked\": false, \"name\": \"t3_dqkfz\", \"created\": 1286982565.0, \"url\": \"http://groups.google.com/group/reddit-dev/msg/c6988091fda9672d\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"\\\"Why is Reddit so slow?\\\"\", \"created_utc\": 1286953765.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 97}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAnnouncement here: \\u003Ca href=\\\"http://www.reddit.com/r/announcements/comments/28hjga/reddit_changes_individual_updown_vote_counts_no/\\\"\\u003Ehttp://www.reddit.com/r/announcements/comments/28hjga/reddit_changes_individual_updown_vote_counts_no/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Announcement here: http://www.reddit.com/r/announcements/comments/28hjga/reddit_changes_individual_updown_vote_counts_no/\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"28hpop\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"grimtrigger\", \"media\": null, \"score\": 87, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 392, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/28hpop/will_todays_announcement_regarding_visibility_of/\", \"locked\": false, \"name\": \"t3_28hpop\", \"created\": 1403152768.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/28hpop/will_todays_announcement_regarding_visibility_of/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Will todays announcement regarding visibility of up/down votes affect the api?\", \"created_utc\": 1403123968.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 87}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"blog.reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"cg4i3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ketralnis\", \"media\": null, \"score\": 75, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 47, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/cg4i3/weve_opensourced_reddits_official_iphone_app/\", \"locked\": false, \"name\": \"t3_cg4i3\", \"created\": 1276830676.0, \"url\": \"http://blog.reddit.com/2010/06/weve-open-sourced-ireddit.html\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"We've open-sourced reddit's official iPhone app\", \"created_utc\": 1276801876.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 75}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"i.imgur.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3nhx9l\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"susannebitch\", \"media\": null, \"score\": 63, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3nhx9l/for_the_love_of_god_please_remove_this_try_our/\", \"locked\": false, \"name\": \"t3_3nhx9l\", \"created\": 1444023111.0, \"url\": \"http://i.imgur.com/mtaUQ4Z.png\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"For the love of god, please remove this \\\"try our beta\\\" modal. It's been there for 4 months and it's confusing mobile users so much!\", \"created_utc\": 1443994311.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 63}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EToday we are introducing standardized \\u003Ca href=\\\"/wiki/api\\\"\\u003EAPI Terms of Use\\u003C/a\\u003E. You, our community of developers, are important to us, and have been instrumental to the success of the Reddit platform. First and foremost, we want to reaffirm our commitment to providing (and improving!) a public API.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThere are a couple of notable changes to the API terms that I\\u2019d like to highlight. The first is that we are requesting all users of the API to register with us. This provides a point of contact for when we have important updates to share; provides a point of contact for when things go wrong; and helps us prevent abuse.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe are also no longer requiring a special licensing agreement to use our API for commercial purposes. We do request that you seek approval for your monetization model in the registration process.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe have added clarity about the types of things that the API is not intended for\\u2013namely applications that promote illegal activity, disrupt core Reddit functionality, or introduce security risks. But you weren\\u2019t doing any of these things anyway.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe still require users of our API to comply with our User Agreement, Privacy Policy, API Usage Limits, and any other applicable laws or regulations. We will continue to require the use of OAuth2.  We understand moving to OAuth2 can take time, so we are giving developers until March 17th to make this change.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe look forward to working with you more to create great experiences for our communities. There are many wonderful projects built on our API, and we would love to see even more. Thank you for all that you do.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYou can contact the \\u003Ca href=\\\"mailto:api@reddit.com\\\"\\u003Eapi@reddit.com\\u003C/a\\u003E alias to ask questions about the API service.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Today we are introducing standardized [API Terms of Use](/wiki/api). You, our community of developers, are important to us, and have been instrumental to the success of the Reddit platform. First and foremost, we want to reaffirm our commitment to providing (and improving!) a public API.\\n\\nThere are a couple of notable changes to the API terms that I\\u2019d like to highlight. The first is that we are requesting all users of the API to register with us. This provides a point of contact for when we have important updates to share; provides a point of contact for when things go wrong; and helps us prevent abuse.\\n\\nWe are also no longer requiring a special licensing agreement to use our API for commercial purposes. We do request that you seek approval for your monetization model in the registration process.\\n\\nWe have added clarity about the types of things that the API is not intended for\\u2013namely applications that promote illegal activity, disrupt core Reddit functionality, or introduce security risks. But you weren\\u2019t doing any of these things anyway.\\n\\nWe still require users of our API to comply with our User Agreement, Privacy Policy, API Usage Limits, and any other applicable laws or regulations. We will continue to require the use of OAuth2.  We understand moving to OAuth2 can take time, so we are giving developers until March 17th to make this change.\\n\\nWe look forward to working with you more to create great experiences for our communities. There are many wonderful projects built on our API, and we would love to see even more. Thank you for all that you do.\\n\\nYou can contact the api@reddit.com alias to ask questions about the API service.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3xdf11\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"spez\", \"media\": null, \"score\": 62, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 102, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3xdf11/introducing_new_api_terms/\", \"locked\": false, \"name\": \"t3_3xdf11\", \"created\": 1450491519.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3xdf11/introducing_new_api_terms/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Introducing new API terms\", \"created_utc\": 1450462719.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 62}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Cstrong\\u003EIntroduction\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease criticize me! This will be an ongoing thing, I will be coming back to posts and fixing things. I want to know what you think.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo after poking around in the reddit source code like a blind man for a while, I thought that the only way to motivate myself to understand it is to explain it to others, so here it goes.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI will try to explain things at a source code level. I will not be explaining how to deploy the server, or anything like that, there are already tutorials about that. Mainly, I\\u0026#39;ll be referencing the source tree and specific files a lot. I\\u0026#39;ll try to submit diagrams every now and then to illustrate certain points better.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EPylons\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EReddit is a web-app. It uses a Python API called Pylons to deploy the webserver and handle all calls. Pylons basically does everything. When you submit a link, view comments, or just go to reddit.com, what happens is that the relevant information is sent to Pylons which then processes it according to the reddit API.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs \\u003Ca href=\\\"http://www.reddit.com/user/eurleif\\\"\\u003Eeurleif\\u003C/a\\u003E once said about \\u003Ca href=\\\"http://www.omegle.com\\\"\\u003Ehis popular Pylons based website\\u003C/a\\u003E, Pylons has very little documentation and tutorials available. So I\\u0026#39;m going to break it down as simple as possible and then break it down some more.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPylons facilitates using a MVC (Model-View-Controller) architecture to deploy a web application. This means that there are three components of the web-app and they work separate from one another. I\\u0026#39;m not going to go into the particulars of MVC, you can google that yourself. I\\u0026#39;m going to work with them in order of simplest to most complicated.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EController\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe controller is the portion of the code that facilitates calls from the client (you) and the server (reddit). For example, if I would go to \\u0026quot;\\u003Ca href=\\\"http://www.reddit.com/r/gaming?sort=new\\\"\\u003Ehttp://www.reddit.com/r/gaming?sort=new\\u003C/a\\u003E\\u0026quot; the controller takes the \\u0026quot;r/\\u0026quot; portion of the URL and processes it and whatever follows it, doing whatever the code tells it to do. In this case, it strips out every part of the URL after \\u0026#39;/r\\u0026#39; and passes that on to the middleware, which we will discuss later.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIn Pylons, this basically means that when a call is made, the code looks at a map of calls to controllers located at \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/config/routing.py\\\"\\u003E/config/routing.py\\u003C/a\\u003E and instantiates the appropriate controller class located in the files in \\u003Ca href=\\\"https://github.com/reddit/reddit/tree/master/r2/r2/controllers\\\"\\u003E/controllers\\u003C/a\\u003E and calls the mapped function. The function then returns a complete HTML page, which is sent to the client.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPylons generally gets HTML parameters to the controller by having them passed as parameters to the controller function (or \\u0026quot;action\\u0026quot; in Pylons speak), the reddit code base instead makes use of decorators to set method parameters. HTML parameters are passed along using the @validate decorator that you see above the GET functions in the controller. This sets the names of the HTML parameters to the function parameters usually using several decorator functions such as \\u0026quot;nop()\\u0026quot; and \\u0026quot;Validate()\\u0026quot; which I haven\\u0026#39;t figured out yet.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor instance, if I want to search, the following is called, \\u003Ca href=\\\"http://www.reddit.com/search?q=hello\\u0026amp;count=50\\u0026amp;after=t3_fcf41\\\"\\u003Ehttp://www.reddit.com/search?q=hello\\u0026amp;count=50\\u0026amp;after=t3_fcf41\\u003C/a\\u003E (search for \\u0026quot;hello\\u0026quot;, list 50 results, and list them after the result titled \\u0026quot;t3_fcf41\\u0026quot;). This tells Pylons to look at the routing map for something called search. Pylons finds the appropriate mapping:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E mc = map.connect\\n mc(\\u0026#39;/reddits/search\\u0026#39;, controller=\\u0026#39;front\\u0026#39;, action=\\u0026#39;search_reddits\\u0026#39;)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eand passes that along to the controller. Here, the controller being used is \\u0026quot;front\\u0026quot; and the function is \\u0026quot;search_reddits\\u0026quot;. This tells Pylons to get the class \\u0026quot;\\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/controllers/front.py\\\"\\u003EFrontController\\u003C/a\\u003E\\u0026quot;, instantiate a new object from it and call the function \\u0026quot;GET_search_reddits\\u0026quot;. @validate then sets the parameters \\u0026quot;query=q=hello\\u0026quot;, \\u0026quot;count=50\\u0026quot;, \\u0026quot;after=t3_fcf41\\u0026quot;, \\u0026quot;reverse=\\u0026quot;, \\u0026quot;num=\\u0026quot;. Several functions are then called to perform the search and render the page based on the search results. Pylons then takes the fully rendered page and sends it to the client. If you notice, several mappings have a colon before the name. This is a wildcard, which means that it is not mapped to a particular string and the controller can see that string and decide what to do with it. These are passed along to the controller as function parameters. For instance, a mapping that looks like this mc(\\u0026#39;/foo/:bar\\u0026#39;, controller=\\u0026#39;foo\\u0026#39;, action=\\u0026#39;baz\\u0026#39;) will call a function defined like so:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Edef GET_baz(self, bar):\\n      return dostuff\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThe parameter \\u0026quot;bar\\u0026quot; is mapped to the next thing in the URL after \\u0026#39;foo/\\u0026#39;.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAll in all pretty simple. To review: The mapping of calls is stored in \\u0026#39;config/routing.py\\u0026#39; in the form of \\u0026quot;call_name, controller, function\\u0026quot;. The call name is everything past \\u0026quot;reddit.com\\u0026quot; in the URL, the controller is a class with the name \\u0026quot;FooController\\u0026quot; where \\u0026quot;foo\\u0026quot; is the name of the controller in the map, and \\u0026quot;function\\u0026quot; is the aliased name of the method to call in the controller class that has the form of \\u0026quot;GET_function(self, **params)\\u0026quot; where params are a series of variables including the generic request variables and the GET and POST parameters. The page is then created and sent to the client.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EQuestions? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/fewoh/a_beginners_guide_to_the_reddit_source_code_part/\\\"\\u003EPart 2\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"**Introduction**\\n\\nPlease criticize me! This will be an ongoing thing, I will be coming back to posts and fixing things. I want to know what you think.\\n\\nSo after poking around in the reddit source code like a blind man for a while, I thought that the only way to motivate myself to understand it is to explain it to others, so here it goes.\\n\\nI will try to explain things at a source code level. I will not be explaining how to deploy the server, or anything like that, there are already tutorials about that. Mainly, I'll be referencing the source tree and specific files a lot. I'll try to submit diagrams every now and then to illustrate certain points better.\\n\\n**Pylons**\\n\\nReddit is a web-app. It uses a Python API called Pylons to deploy the webserver and handle all calls. Pylons basically does everything. When you submit a link, view comments, or just go to reddit.com, what happens is that the relevant information is sent to Pylons which then processes it according to the reddit API.\\n\\nAs [eurleif](http://www.reddit.com/user/eurleif) once said about [his popular Pylons based website](http://www.omegle.com), Pylons has very little documentation and tutorials available. So I'm going to break it down as simple as possible and then break it down some more.\\n\\nPylons facilitates using a MVC (Model-View-Controller) architecture to deploy a web application. This means that there are three components of the web-app and they work separate from one another. I'm not going to go into the particulars of MVC, you can google that yourself. I'm going to work with them in order of simplest to most complicated.\\n\\n**Controller**\\n\\nThe controller is the portion of the code that facilitates calls from the client (you) and the server (reddit). For example, if I would go to \\\"http://www.reddit.com/r/gaming?sort=new\\\" the controller takes the \\\"r/\\\" portion of the URL and processes it and whatever follows it, doing whatever the code tells it to do. In this case, it strips out every part of the URL after '/r' and passes that on to the middleware, which we will discuss later.\\n\\nIn Pylons, this basically means that when a call is made, the code looks at a map of calls to controllers located at [/config/routing.py](https://github.com/reddit/reddit/blob/master/r2/r2/config/routing.py) and instantiates the appropriate controller class located in the files in [/controllers](https://github.com/reddit/reddit/tree/master/r2/r2/controllers) and calls the mapped function. The function then returns a complete HTML page, which is sent to the client.\\n\\nPylons generally gets HTML parameters to the controller by having them passed as parameters to the controller function (or \\\"action\\\" in Pylons speak), the reddit code base instead makes use of decorators to set method parameters. HTML parameters are passed along using the @validate decorator that you see above the GET functions in the controller. This sets the names of the HTML parameters to the function parameters usually using several decorator functions such as \\\"nop()\\\" and \\\"Validate()\\\" which I haven't figured out yet.\\n\\nFor instance, if I want to search, the following is called, http://www.reddit.com/search?q=hello\\u0026count=50\\u0026after=t3_fcf41 (search for \\\"hello\\\", list 50 results, and list them after the result titled \\\"t3_fcf41\\\"). This tells Pylons to look at the routing map for something called search. Pylons finds the appropriate mapping:\\n\\n     mc = map.connect\\n     mc('/reddits/search', controller='front', action='search_reddits')\\n\\nand passes that along to the controller. Here, the controller being used is \\\"front\\\" and the function is \\\"search_reddits\\\". This tells Pylons to get the class \\\"[FrontController](https://github.com/reddit/reddit/blob/master/r2/r2/controllers/front.py)\\\", instantiate a new object from it and call the function \\\"GET_search_reddits\\\". @validate then sets the parameters \\\"query=q=hello\\\", \\\"count=50\\\", \\\"after=t3_fcf41\\\", \\\"reverse=\\\", \\\"num=\\\". Several functions are then called to perform the search and render the page based on the search results. Pylons then takes the fully rendered page and sends it to the client. If you notice, several mappings have a colon before the name. This is a wildcard, which means that it is not mapped to a particular string and the controller can see that string and decide what to do with it. These are passed along to the controller as function parameters. For instance, a mapping that looks like this mc('/foo/:bar', controller='foo', action='baz') will call a function defined like so:\\n\\n    def GET_baz(self, bar):\\n          return dostuff\\n\\nThe parameter \\\"bar\\\" is mapped to the next thing in the URL after 'foo/'.\\n\\nAll in all pretty simple. To review: The mapping of calls is stored in 'config/routing.py' in the form of \\\"call_name, controller, function\\\". The call name is everything past \\\"reddit.com\\\" in the URL, the controller is a class with the name \\\"FooController\\\" where \\\"foo\\\" is the name of the controller in the map, and \\\"function\\\" is the aliased name of the method to call in the controller class that has the form of \\\"GET_function(self, **params)\\\" where params are a series of variables including the generic request variables and the GET and POST parameters. The page is then created and sent to the client.\\n\\nQuestions? \\n\\n[Part 2](http://www.reddit.com/r/redditdev/comments/fewoh/a_beginners_guide_to_the_reddit_source_code_part/)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"fdhlw\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Yserbius\", \"media\": null, \"score\": 59, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 18, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/fdhlw/a_beginners_guide_to_the_reddit_source_code_part/\", \"locked\": false, \"name\": \"t3_fdhlw\", \"created\": 1296639451.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/fdhlw/a_beginners_guide_to_the_reddit_source_code_part/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"A Beginners Guide to the reddit Source Code: Part 1, Understanding Pylons\", \"created_utc\": 1296610651.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 59}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"28jvod\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Strider96\", \"media\": null, \"score\": 60, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 34, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/28jvod/pull_request_to_bring_back_the_upvotedownvote/\", \"locked\": false, \"name\": \"t3_28jvod\", \"created\": 1403215661.0, \"url\": \"https://github.com/reddit/reddit/pull/1071\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Pull request to bring back the upvote/downvote counter\", \"created_utc\": 1403186861.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 60}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"i.imgur.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"mr3cn\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ubershmekel\", \"media\": null, \"score\": 56, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/mr3cn/im_a_bit_embarrassed_every_time_i_show_reddit_to/\", \"locked\": false, \"name\": \"t3_mr3cn\", \"created\": 1322451942.0, \"url\": \"http://i.imgur.com/Q8sQe.png\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I'm a bit embarrassed every time I show reddit to someone with an RTL browser\", \"created_utc\": 1322423142.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 56}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1bhdpa\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"spladug\", \"media\": null, \"score\": 58, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1bhdpa/the_source_code_for_reddits_april_fools_2013_is/\", \"locked\": false, \"name\": \"t3_1bhdpa\", \"created\": 1364893840.0, \"url\": \"https://github.com/reddit/reddit-plugin-f2p\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"The source code for reddit's April Fools 2013 is now available on GitHub.\", \"created_utc\": 1364865040.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 58}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"apigee.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"jetkj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"spladug\", \"media\": null, \"score\": 56, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/jetkj/new_tool_for_api_developers_the_reddit_api_console/\", \"locked\": false, \"name\": \"t3_jetkj\", \"created\": 1313029229.0, \"url\": \"https://apigee.com/console/reddit\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New tool for API developers: the reddit API console.\", \"created_utc\": 1313000429.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 56}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EGreetings reddit API users,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have two important messages for you all today. The first is about licensing for reddit API clients, and the second is about cookie-authenticated use of reddit\\u0026#39;s API.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003ELicensing\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EWe have filled out our \\u003Ca href=\\\"/wiki/licensing\\\"\\u003Elicensing page\\u003C/a\\u003E with information about what is acceptable and not acceptable for reddit API clients. The two most important pieces is that (1) we\\u0026#39;re asking API clients to not use the word \\u0026quot;reddit\\u0026quot; in their name except in the phrase \\u0026quot;for reddit\\u0026quot;, e.g., \\u0026quot;My cool app for reddit\\u0026quot; and (2) we\\u0026#39;re asking \\u0026quot;commercial\\u0026quot; API consumers to register with us.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs reddit (the company) officially steps into mobile with our AMA app and Alien Blue, we realized that it can be difficult for users to tell when an app is \\u0026quot;by reddit, Inc.\\u0026quot; or simply \\u0026quot;for reddit.\\u0026quot; I know that adding rules and restrictions is not fun, so I want to be the first one to say right here, right now: We\\u2019re not trying to shut down our API and we fully intend to continue supporting 3rd party developers. In fact, hopefully part 2 of this post makes it clear that we\\u0026#39;re trying to be more deliberate in our support of API consumers.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYes, this does mean we will be reaching out to app developers in the coming weeks and asking them to rename or re-license with us as appropriate. We\\u0026#39;re asking for name changes to be completed by \\u003Cstrong\\u003EMarch 30, 2015\\u003C/strong\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERegarding the commercial use clause: Running servers and building out APIs cost money. It\\u0026#39;s not tenable for large, commercial clients to profit off of reddit\\u0026#39;s API without an appropriate cost-sharing mechanism. In the future, we may choose to implement a more methodical cost-sharing program, such as what \\u003Ca href=\\\"https://api.imgur.com/#commercial\\\"\\u003Eimgur does with mashape\\u003C/a\\u003E, but for now, we simply want to keep tabs on commercial use of our API. \\u003C/p\\u003E\\n\\n\\u003Ch2\\u003EDeprecation of cookie authentication for API consumers\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EUse of the API when authenticated via cookies is deprecated and slated for removal. All API clients MUST convert to authenticating to the reddit API via OAuth 2 by \\u003Cstrong\\u003EAugust 3, 2015\\u003C/strong\\u003E. After that date, reddit.com will begin heavily throttling and/or blocking API access that is not authenticated with an OAuth 2 access token*.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E* Yes, this applies to \\u0026quot;logged out\\u0026quot; access to the API. For API access without a reddit user, please use \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/OAuth2#application-only-oauth\\\"\\u003EApplication Only Authentication\\u003C/a\\u003E to get an access token.\\u003C/p\\u003E\\n\\n\\u003Ch3\\u003EWhy are we doing this?\\u003C/h3\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003ETo protect users. Websites and mobile apps that use cookie authentication end up having to directly ask users for their reddit.com password. We want to discourage that practice so that users are not in the habit of being asked for their reddit password unless they are on \\u003Ca href=\\\"http://www.reddit.com\\\"\\u003Ewww.reddit.com\\u003C/a\\u003E. OAuth 2 access tokens are easier for users to revoke and limited in duration. They are also limited in scope - there are some actions, such as resetting passwords and managing your OAuth 2 apps, that 3rd parties have no reason to access.\\u003C/li\\u003E\\n\\u003Cli\\u003ETo more fairly apply rate limiting across 3rd parties.\\u003C/li\\u003E\\n\\u003Cli\\u003ETo allow us to be more deliberate about how we design and build the API, without being tied to how browsers access the reddit website. \\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Ch3\\u003EAww, dangit, OAuth seems like a lot of work. Why should I bother?\\u003C/h3\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003ESee the first answer from above. You should care about not wanting to ask users for their passwords to sites/apps that aren\\u0026#39;t yours.\\u003C/li\\u003E\\n\\u003Cli\\u003EOnly OAuth API consumers (well, and browsers) will be able to access new features. (You\\u0026#39;re already missing out on the trophy endpoint if you\\u0026#39;re not on OAuth!)\\u003C/li\\u003E\\n\\u003Cli\\u003EOAuth clients have had higher rate limits for a while now. The higher rate limit is here to stay, so when you switch, you\\u0026#39;ll be able to ask us for data 2x as often!\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Ch3\\u003EWhat about browser extensions?\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EBrowser extensions have an easier time with cookie-auth, so may get exemptions or extensions on the deadline. I\\u0026#39;ll be working to figure out the best road forward to minimize pain.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso, I (personally) am committed to making this as easy as I can. I\\u0026#39;ve written the code for \\u003Ca href=\\\"https://www.reddit.com/r/redditdev/comments/2owrnn/oauth2_implicit_grants_cors_apponly_oauth2/\\\"\\u003Emany\\u003C/a\\u003E \\u003Ca href=\\\"https://www.reddit.com/r/redditdev/comments/2eatps/oauth2_manual_token_revocation/\\\"\\u003Easpects\\u003C/a\\u003E of reddit\\u0026#39;s OAuth2 \\u003Ca href=\\\"https://www.reddit.com/r/redditdev/comments/2skahz/oauth2_apifriend_is_here/\\\"\\u003Eimplementation\\u003C/a\\u003E over the last year or so, \\u003Ca href=\\\"https://www.reddit.com/r/redditdev/comments/218wd7/oauth_20_you_asked_i_listened_updated_and_more/\\\"\\u003Eupdated documentation\\u003C/a\\u003E and more. I\\u0026#39;ll be here in \\u003Ca href=\\\"/r/redditdev\\\"\\u003E/r/redditdev\\u003C/a\\u003E as often as I can to answer questions, and I do my best to update documentation or implement features to make things easier.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo what happens in August?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECome August, we will begin heavily throttling access to reddit\\u0026#39;s API that is not via OAuth. Over time, we will be more aggressive about locking down API usage that\\u0026#39;s not over OAuth.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETL;DR: Cookie-authentication for API use is deprecated; please convert your clients, scripts and apps to OAuth-authentication within 6 months. Also, licensing for API clients has been clarified slightly - please familiarize yourself with the new terms.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: Added deadline for name changes.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Greetings reddit API users,\\n\\nI have two important messages for you all today. The first is about licensing for reddit API clients, and the second is about cookie-authenticated use of reddit's API.\\n\\n## Licensing\\n\\nWe have filled out our [licensing page](/wiki/licensing) with information about what is acceptable and not acceptable for reddit API clients. The two most important pieces is that (1) we're asking API clients to not use the word \\\"reddit\\\" in their name except in the phrase \\\"for reddit\\\", e.g., \\\"My cool app for reddit\\\" and (2) we're asking \\\"commercial\\\" API consumers to register with us.\\n\\nAs reddit (the company) officially steps into mobile with our AMA app and Alien Blue, we realized that it can be difficult for users to tell when an app is \\\"by reddit, Inc.\\\" or simply \\\"for reddit.\\\" I know that adding rules and restrictions is not fun, so I want to be the first one to say right here, right now: We\\u2019re not trying to shut down our API and we fully intend to continue supporting 3rd party developers. In fact, hopefully part 2 of this post makes it clear that we're trying to be more deliberate in our support of API consumers.\\n\\nYes, this does mean we will be reaching out to app developers in the coming weeks and asking them to rename or re-license with us as appropriate. We're asking for name changes to be completed by **March 30, 2015**.\\n\\nRegarding the commercial use clause: Running servers and building out APIs cost money. It's not tenable for large, commercial clients to profit off of reddit's API without an appropriate cost-sharing mechanism. In the future, we may choose to implement a more methodical cost-sharing program, such as what [imgur does with mashape](https://api.imgur.com/#commercial), but for now, we simply want to keep tabs on commercial use of our API. \\n\\n## Deprecation of cookie authentication for API consumers\\n\\nUse of the API when authenticated via cookies is deprecated and slated for removal. All API clients MUST convert to authenticating to the reddit API via OAuth 2 by **August 3, 2015**. After that date, reddit.com will begin heavily throttling and/or blocking API access that is not authenticated with an OAuth 2 access token*.\\n\\n\\\\* Yes, this applies to \\\"logged out\\\" access to the API. For API access without a reddit user, please use [Application Only Authentication](https://github.com/reddit/reddit/wiki/OAuth2#application-only-oauth) to get an access token.\\n\\n### Why are we doing this?\\n\\n1. To protect users. Websites and mobile apps that use cookie authentication end up having to directly ask users for their reddit.com password. We want to discourage that practice so that users are not in the habit of being asked for their reddit password unless they are on www.reddit.com. OAuth 2 access tokens are easier for users to revoke and limited in duration. They are also limited in scope - there are some actions, such as resetting passwords and managing your OAuth 2 apps, that 3rd parties have no reason to access.\\n1. To more fairly apply rate limiting across 3rd parties.\\n1. To allow us to be more deliberate about how we design and build the API, without being tied to how browsers access the reddit website. \\n\\n### Aww, dangit, OAuth seems like a lot of work. Why should I bother?\\n\\n1. See the first answer from above. You should care about not wanting to ask users for their passwords to sites/apps that aren't yours.\\n1. Only OAuth API consumers (well, and browsers) will be able to access new features. (You're already missing out on the trophy endpoint if you're not on OAuth!)\\n1. OAuth clients have had higher rate limits for a while now. The higher rate limit is here to stay, so when you switch, you'll be able to ask us for data 2x as often!\\n\\n### What about browser extensions?\\n\\nBrowser extensions have an easier time with cookie-auth, so may get exemptions or extensions on the deadline. I'll be working to figure out the best road forward to minimize pain.\\n\\nAlso, I (personally) am committed to making this as easy as I can. I've written the code for [many](https://www.reddit.com/r/redditdev/comments/2owrnn/oauth2_implicit_grants_cors_apponly_oauth2/) [aspects](https://www.reddit.com/r/redditdev/comments/2eatps/oauth2_manual_token_revocation/) of reddit's OAuth2 [implementation](https://www.reddit.com/r/redditdev/comments/2skahz/oauth2_apifriend_is_here/) over the last year or so, [updated documentation](https://www.reddit.com/r/redditdev/comments/218wd7/oauth_20_you_asked_i_listened_updated_and_more/) and more. I'll be here in /r/redditdev as often as I can to answer questions, and I do my best to update documentation or implement features to make things easier.\\n\\nSo what happens in August?\\n\\nCome August, we will begin heavily throttling access to reddit's API that is not via OAuth. Over time, we will be more aggressive about locking down API usage that's not over OAuth.\\n\\nTL;DR: Cookie-authentication for API use is deprecated; please convert your clients, scripts and apps to OAuth-authentication within 6 months. Also, licensing for API clients has been clarified slightly - please familiarize yourself with the new terms.\\n\\nEdit: Added deadline for name changes.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2ujhkr\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 56, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 113, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1422923851.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2ujhkr/important_api_licensing_terms_clarified/\", \"locked\": false, \"name\": \"t3_2ujhkr\", \"created\": 1422930261.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2ujhkr/important_api_licensing_terms_clarified/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Important: API licensing terms clarified; Cookie-authentication deprecation warning\", \"created_utc\": 1422901461.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 56}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIn case you missed it, the username mentions feature is now available for everyone, not just users with gold subscriptions: \\u003Ca href=\\\"http://www.redditblog.com/2015/01/create-your-own-reddit-alien-avatar.html\\\"\\u003Ehttp://www.redditblog.com/2015/01/create-your-own-reddit-alien-avatar.html\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBots can and should monitor \\u003Ca href=\\\"https://www.reddit.com/message/mentions.json\\\"\\u003Ehttps://www.reddit.com/message/mentions.json\\u003C/a\\u003E rather than polling/scraping every comment, whenever possible. You can also monitor /api/v1/me and check the \\u003Ccode\\u003Ehas_mail\\u003C/code\\u003E attribute to see if you need to look up mentions.json (thanks \\u003Ca href=\\\"/u/pkamb\\\"\\u003E/u/pkamb\\u003C/a\\u003E for the suggestion!)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"In case you missed it, the username mentions feature is now available for everyone, not just users with gold subscriptions: http://www.redditblog.com/2015/01/create-your-own-reddit-alien-avatar.html\\n\\nBots can and should monitor https://www.reddit.com/message/mentions.json rather than polling/scraping every comment, whenever possible. You can also monitor /api/v1/me and check the `has_mail` attribute to see if you need to look up mentions.json (thanks /u/pkamb for the suggestion!)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2rnzkd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 51, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 20, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1420665344.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2rnzkd/attn_bot_maintainers_username_mentions_for/\", \"locked\": false, \"name\": \"t3_2rnzkd\", \"created\": 1420692320.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2rnzkd/attn_bot_maintainers_username_mentions_for/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Attn: Bot maintainers: Username mentions for everyone!\", \"created_utc\": 1420663520.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 51}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ENot sure if this is the right place, but I visited \\u003Ca href=\\\"http://www.reddit.com/r/pics/comments/cwmdb/went_to_the_rock_and_roll_hall_of_fame_the_other/\\\"\\u003Ethis\\u003C/a\\u003E link (a couch) and noticed that the other discussions tab indicated there was another page with a duplicate link. I had a \\u003Ca href=\\\"http://www.reddit.com/r/pics/duplicates/cwmdb/went_to_the_rock_and_roll_hall_of_fame_the_other/\\\"\\u003Elook\\u003C/a\\u003E and found something on Imgur, ummm totally different.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe couch leads to \\u003Ca href=\\\"http://i.imgur.com/kF0PI.jpg\\\"\\u003Ehttp://i.imgur.com/kF0PI.jpg\\u003C/a\\u003E (SFW)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe other link is \\u003Ca href=\\\"http://i.imgur.com/Kf0pI.jpg\\\"\\u003Ehttp://i.imgur.com/Kf0pI.jpg\\u003C/a\\u003E (NSFW)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELooks like Imgur is case sensitive with their links. Is Reddit aware of this when working out other pages with the same links?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Not sure if this is the right place, but I visited [this](http://www.reddit.com/r/pics/comments/cwmdb/went_to_the_rock_and_roll_hall_of_fame_the_other/) link (a couch) and noticed that the other discussions tab indicated there was another page with a duplicate link. I had a [look](http://www.reddit.com/r/pics/duplicates/cwmdb/went_to_the_rock_and_roll_hall_of_fame_the_other/) and found something on Imgur, ummm totally different.\\n\\nThe couch leads to http://i.imgur.com/kF0PI.jpg (SFW)\\n\\nThe other link is http://i.imgur.com/Kf0pI.jpg (NSFW)\\n\\nLooks like Imgur is case sensitive with their links. Is Reddit aware of this when working out other pages with the same links?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"de4sf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Bazzr\", \"media\": null, \"score\": 53, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 14, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/de4sf/found_a_problem_with_reddit_imgur/\", \"locked\": false, \"name\": \"t3_de4sf\", \"created\": 1284578687.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/de4sf/found_a_problem_with_reddit_imgur/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Found a problem with Reddit \\u0026 Imgur\", \"created_utc\": 1284549887.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 53}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThere\\u0026#39;s now a Vagrantfile included with the reddit code \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/Vagrantfile\\\"\\u003Ehttps://github.com/reddit/reddit/blob/master/Vagrantfile\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYou\\u0026#39;ll need to install \\u003Ca href=\\\"https://www.vagrantup.com/\\\"\\u003EVagrant\\u003C/a\\u003E and \\u003Ca href=\\\"https://www.virtualbox.org/\\\"\\u003EVirtualBox\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYou can start up a local reddit install by cloning the main reddit repository (\\u003Ca href=\\\"https://github.com/reddit/reddit\\\"\\u003Ehttps://github.com/reddit/reddit\\u003C/a\\u003E) plus any other plugins you want to try out, and then running \\u003Ccode\\u003Evagrant up\\u003C/code\\u003E. The Vagrantfile includes some notes and comments about configuration and options.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you test this out please let me know if you run into any trouble or need anything clarified.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"There's now a Vagrantfile included with the reddit code https://github.com/reddit/reddit/blob/master/Vagrantfile\\n\\nYou'll need to install [Vagrant](https://www.vagrantup.com/) and [VirtualBox](https://www.virtualbox.org/).\\n\\nYou can start up a local reddit install by cloning the main reddit repository (https://github.com/reddit/reddit) plus any other plugins you want to try out, and then running `vagrant up`. The Vagrantfile includes some notes and comments about configuration and options.\\n\\nIf you test this out please let me know if you run into any trouble or need anything clarified.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3vttz9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bsimpson\", \"media\": null, \"score\": 52, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3vttz9/reddit_vagrant/\", \"locked\": false, \"name\": \"t3_3vttz9\", \"created\": 1449545828.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3vttz9/reddit_vagrant/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"reddit vagrant\", \"created_utc\": 1449517028.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 52}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWe deploy our code to the ~170 application servers currently in our infrastructure via SSH and Git.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis may or may not be useful to anyone else but we like to think that there has to be a compelling reason \\u003Cem\\u003Enot\\u003C/em\\u003E to open source code, so here it is in all its glory.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/push\\\"\\u003Ehttps://github.com/reddit/push\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"We deploy our code to the ~170 application servers currently in our infrastructure via SSH and Git.\\n\\nThis may or may not be useful to anyone else but we like to think that there has to be a compelling reason *not* to open source code, so here it is in all its glory.\\n\\nhttps://github.com/reddit/push\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ynoxx\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"spladug\", \"media\": null, \"score\": 50, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ynoxx/reddits_code_deploy_tool_is_now_open_source/\", \"locked\": false, \"name\": \"t3_ynoxx\", \"created\": 1345697580.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ynoxx/reddits_code_deploy_tool_is_now_open_source/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"reddit's code deploy tool is now open source\", \"created_utc\": 1345668780.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 50}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"i.imgur.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"qx651\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"scorpion032\", \"media\": null, \"score\": 50, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/qx651/i_am_across_the_table_from_the_guys_working_on/\", \"locked\": false, \"name\": \"t3_qx651\", \"created\": 1331806736.0, \"url\": \"http://i.imgur.com/7yZF4.jpg\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I am across the table from the guys working on reddit API at #pycon.\", \"created_utc\": 1331777936.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 50}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EEvening all,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve just updated my app \\u0026quot;Sync for reddit\\u0026quot; and I\\u0026#39;m being hit with 429 errors.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI respect the rate limiting headers, include the app version in the header etc. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAm I missing something or is this an issue with the API?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECheers\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELaurence\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Evening all,\\n\\nI've just updated my app \\\"Sync for reddit\\\" and I'm being hit with 429 errors.\\n\\nI respect the rate limiting headers, include the app version in the header etc. \\n\\nAm I missing something or is this an issue with the API?\\n\\nCheers\\n\\nLaurence\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4lr3pp\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"ljdawson\", \"media\": null, \"score\": 49, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4lr3pp/encountering_a_lot_of_429_errors/\", \"locked\": false, \"name\": \"t3_4lr3pp\", \"created\": 1464664495.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4lr3pp/encountering_a_lot_of_429_errors/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Encountering a lot of 429 errors\", \"created_utc\": 1464635695.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 49}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWith much sadness, I\\u0026#39;m here to inform you that \\u003Ca href=\\\"/u/kemitche\\\"\\u003E/u/kemitche\\u003C/a\\u003E has decided to leave team reddit, and move on to explore new opportunities. He has been an integral part to the company and this community in particular for years, and will hopefully keep a presence here in \\u003Ca href=\\\"/r/redditdev\\\"\\u003E/r/redditdev\\u003C/a\\u003E. While we are very happy for him and support his decision wholeheartedly, unfortunately this also means that we\\u0026#39;ll be experiencing a large blow in terms of lost experience and knowledge, so we will be reevaluating some of the projects that he\\u0026#39;s been crucial to. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe biggest project that affects you directly is the \\u003Ca href=\\\"https://www.reddit.com/r/redditdev/comments/2ujhkr/important_api_licensing_terms_clarified/\\\"\\u003Eoauth transition that we had planned for August\\u003C/a\\u003E. We understand that forcing this move without adequate support from our side is not fair to the dev community so until we have time to help ease this transition we will not be forcing the swap. Note that this is still something we will be pursuing and any new features we release will continue to be supported only on oauth.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBoth \\u003Ca href=\\\"/u/drew\\\"\\u003E/u/drew\\u003C/a\\u003E and I will be working on the apis and hoping to pick up the work that \\u003Ca href=\\\"/u/kemitche\\\"\\u003E/u/kemitche\\u003C/a\\u003E has started to continue to build improvements to what we already have. Please be patient with us as we ramp up, but otherwise we\\u0026#39;re happy to answer any questions you have.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"With much sadness, I'm here to inform you that /u/kemitche has decided to leave team reddit, and move on to explore new opportunities. He has been an integral part to the company and this community in particular for years, and will hopefully keep a presence here in /r/redditdev. While we are very happy for him and support his decision wholeheartedly, unfortunately this also means that we'll be experiencing a large blow in terms of lost experience and knowledge, so we will be reevaluating some of the projects that he's been crucial to. \\n\\nThe biggest project that affects you directly is the [oauth transition that we had planned for August](https://www.reddit.com/r/redditdev/comments/2ujhkr/important_api_licensing_terms_clarified/). We understand that forcing this move without adequate support from our side is not fair to the dev community so until we have time to help ease this transition we will not be forcing the swap. Note that this is still something we will be pursuing and any new features we release will continue to be supported only on oauth.\\n\\nBoth /u/drew and I will be working on the apis and hoping to pick up the work that /u/kemitche has started to continue to build improvements to what we already have. Please be patient with us as we ramp up, but otherwise we're happy to answer any questions you have.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"37e2mv\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"thorarakis\", \"media\": null, \"score\": 43, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 39, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/37e2mv/change_in_team_and_timelines/\", \"locked\": false, \"name\": \"t3_37e2mv\", \"created\": 1432711013.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/37e2mv/change_in_team_and_timelines/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Change in team and timelines\", \"created_utc\": 1432682213.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 43}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey Folks!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EJust a heads-up that in the next week or so we\\u2019re going to be adding support for \\u003Ca href=\\\"/r/subreddit\\\"\\u003E/r/subreddit\\u003C/a\\u003E and \\u003Ca href=\\\"/u/user\\\"\\u003E/u/user\\u003C/a\\u003E links with no leading slash like \\u003Ccode\\u003Er/subreddit\\u003C/code\\u003E and \\u003Ccode\\u003Eu/user\\u003C/code\\u003E (\\u003Ca href=\\\"https://www.reddit.com/r/CrazyIdeas/comments/1grb45/after_a_year_of_hearing_hashtag_everywhere_we/\\\"\\u003Ewhich the cool kids are calling slashtags\\u003C/a\\u003E) to our markdown library.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you do anything with Markdown coming from reddit (render it, match on it with AutoModerator, etc) here\\u2019s what you need to know:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003Eold-style \\u003Ccode\\u003E/r/subreddit\\u003C/code\\u003E and \\u003Ccode\\u003E/u/user\\u003C/code\\u003E links should work exactly as they did before\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Er/subreddit\\u003C/code\\u003E should only be autolinked if the character immediately to the left is an ASCII punctuation or space character. This might change to support non-ASCII punctuation and spaces in the future, but our Markdown library\\u2019s lack of Unicode support makes it difficult.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003ESome examples of things that will be autolinked:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Er/subreddit\\u003C/code\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Ea r/subreddit\\u003C/code\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Efoo;r/subreddit\\u003C/code\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003E\\\\r/subreddit\\u003C/code\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003E**bold**r/subreddit\\u003C/code\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003ESome examples of things that will \\u003Cem\\u003Enot\\u003C/em\\u003E be autolinked:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Efoor/subreddit\\u003C/code\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Er//subreddit\\u003C/code\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003E\\u2603r/subreddit\\u003C/code\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Er\\\\/subreddit\\u003C/code\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EA more exhaustive set of examples \\u003Ca href=\\\"https://github.com/reddit/snudown/commit/3498709cb05e8a5724c7e66199ccd13a74f6525b#diff-f6cf234e8ce521a36086ca431340232bR135\\\"\\u003Ecan be found here\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you\\u2019re not rendering markdown, just scanning through markdown for username / subreddit references, you can do something like this python example:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport re\\nimport string\\nsr_mentions = re.findall(r\\u0026quot;(/|(?\\u0026lt;=[\\u0026quot; + re.escape(string.punctuation) + r\\u0026quot;\\\\s])|(?\\u0026lt;=\\\\A))r/([\\\\w\\\\+\\\\-]{2,}|reddit\\\\.com)[/\\\\w\\\\-]*\\u0026quot;, \\u0026quot;comment with a /r/subreddit r/another \\u0026quot;)\\nuser_mentions = re.findall(r\\u0026quot;(/|(?\\u0026lt;=[\\u0026quot; + re.escape(string.punctuation) + r\\u0026quot;\\\\s])|(?\\u0026lt;=\\\\A))u/([\\\\w\\\\-]{2,})[/\\\\w\\\\-]*\\u0026quot;, \\u0026quot;comment with a /u/user u/another\\u0026quot;)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAs always, you can find the \\u003Ca href=\\\"https://github.com/reddit/snudown/commit/3498709cb05e8a5724c7e66199ccd13a74f6525b\\\"\\u003Echanges on GitHub\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey Folks!\\n\\nJust a heads-up that in the next week or so we\\u2019re going to be adding support for /r/subreddit and /u/user links with no leading slash like `r/subreddit` and `u/user` ([which the cool kids are calling slashtags](https://www.reddit.com/r/CrazyIdeas/comments/1grb45/after_a_year_of_hearing_hashtag_everywhere_we/)) to our markdown library.\\n\\nIf you do anything with Markdown coming from reddit (render it, match on it with AutoModerator, etc) here\\u2019s what you need to know:\\n\\n* old-style `/r/subreddit` and `/u/user` links should work exactly as they did before\\n* `r/subreddit` should only be autolinked if the character immediately to the left is an ASCII punctuation or space character. This might change to support non-ASCII punctuation and spaces in the future, but our Markdown library\\u2019s lack of Unicode support makes it difficult.\\n\\nSome examples of things that will be autolinked:\\n\\n* `r/subreddit`\\n* `a r/subreddit`\\n* `foo;r/subreddit`\\n* `\\\\r/subreddit`\\n* `**bold**r/subreddit`\\n\\nSome examples of things that will _not_ be autolinked:\\n\\n* `foor/subreddit`\\n* `r//subreddit`\\n* `\\u2603r/subreddit`\\n* `r\\\\/subreddit`\\n\\nA more exhaustive set of examples [can be found here](https://github.com/reddit/snudown/commit/3498709cb05e8a5724c7e66199ccd13a74f6525b#diff-f6cf234e8ce521a36086ca431340232bR135).\\n\\nIf you\\u2019re not rendering markdown, just scanning through markdown for username / subreddit references, you can do something like this python example:\\n\\n    import re\\n    import string\\n    sr_mentions = re.findall(r\\\"(/|(?\\u003C=[\\\" + re.escape(string.punctuation) + r\\\"\\\\s])|(?\\u003C=\\\\A))r/([\\\\w\\\\+\\\\-]{2,}|reddit\\\\.com)[/\\\\w\\\\-]*\\\", \\\"comment with a /r/subreddit r/another \\\")\\n    user_mentions = re.findall(r\\\"(/|(?\\u003C=[\\\" + re.escape(string.punctuation) + r\\\"\\\\s])|(?\\u003C=\\\\A))u/([\\\\w\\\\-]{2,})[/\\\\w\\\\-]*\\\", \\\"comment with a /u/user u/another\\\")\\n\\nAs always, you can find the [changes on GitHub](https://github.com/reddit/snudown/commit/3498709cb05e8a5724c7e66199ccd13a74f6525b).\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"37nmt7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"largenocream\", \"media\": null, \"score\": 42, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 28, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1432929348.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/37nmt7/upcoming_changes_to_subreddit_and_user_links_in/\", \"locked\": false, \"name\": \"t3_37nmt7\", \"created\": 1432884845.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/37nmt7/upcoming_changes_to_subreddit_and_user_links_in/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Upcoming changes to subreddit and user links in Markdown: support for `r/subreddit` and `u/user` links\", \"created_utc\": 1432856045.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 42}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHeya folks, so we are seeking a developer relations manager here at reddit! \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEveryone here does a ton of reddit development, so I\\u0026#39;m wondering if anyone would be interested. Our job post is here : \\u003Ca href=\\\"https://boards.greenhouse.io/reddit/jobs/193705#.VxAOfhMrLdQ\\\"\\u003EJob Post\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Etl;dr - Reddit needs someone fluent in api\\u0026#39;s to work with the product team and communicate strategy to developers. They will also manage relationships with developers, troubleshoot technical problems, and help us advance ecosystem technologies.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Heya folks, so we are seeking a developer relations manager here at reddit! \\n\\nEveryone here does a ton of reddit development, so I'm wondering if anyone would be interested. Our job post is here : [Job Post](https://boards.greenhouse.io/reddit/jobs/193705#.VxAOfhMrLdQ)\\n\\ntl;dr - Reddit needs someone fluent in api's to work with the product team and communicate strategy to developers. They will also manage relationships with developers, troubleshoot technical problems, and help us advance ecosystem technologies.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4ez6v7\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"xilvar\", \"media\": null, \"score\": 41, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 25, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4ez6v7/reddit_is_hiring_a_developer_relations_manager/\", \"locked\": false, \"name\": \"t3_4ez6v7\", \"created\": 1460788430.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4ez6v7/reddit_is_hiring_a_developer_relations_manager/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit is hiring a developer relations manager!\", \"created_utc\": 1460759630.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 41}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey folks,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EJust a set of small changes:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EYou can now get via the API whether a comment is controversial or not. The field is \\u003Ccode\\u003Econtroversiality\\u003C/code\\u003E: currently it\\u0026#39;s an integer and either 0 or 1. This is an integer to make it forward compatible in case we decide to introduce more levels of controversiality in the future. (I.E. something like a 3 == really, really controversial). No need to worry about that yet though, it\\u0026#39;s just 0 or 1 for now.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003ELong overdue, there\\u0026#39;s now a \\u003Ccode\\u003Escore\\u003C/code\\u003E field on comments, which no longer needs to be derived.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003E\\u003Ccode\\u003Eupvote_ratio\\u003C/code\\u003E is now available on single link posts, as a float between 0.0 and 1.0. This will show up on GETs to a single link: sets of links won\\u0026#39;t have this field.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EYou can see some examples of \\u003Ccode\\u003Econtroversiality\\u003C/code\\u003E \\u0026amp; \\u003Ccode\\u003Escore\\u003C/code\\u003E on comments and \\u003Ccode\\u003Eupvote_ratio\\u003C/code\\u003E on a link here: \\u003Ca href=\\\"http://www.reddit.com/r/pics/comments/29h0zn/was_taking_random_pictures_of_my_mother_and_this.json?sort=controversial\\u0026amp;limit=3\\\"\\u003Ehttp://www.reddit.com/r/pics/comments/29h0zn/was_taking_random_pictures_of_my_mother_and_this.json?sort=controversial\\u0026amp;limit=3\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHope this is useful!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E-umbrae\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey folks,\\n\\nJust a set of small changes:\\n\\n* You can now get via the API whether a comment is controversial or not. The field is `controversiality`: currently it's an integer and either 0 or 1. This is an integer to make it forward compatible in case we decide to introduce more levels of controversiality in the future. (I.E. something like a 3 == really, really controversial). No need to worry about that yet though, it's just 0 or 1 for now.\\n\\n* Long overdue, there's now a `score` field on comments, which no longer needs to be derived.\\n\\n* `upvote_ratio` is now available on single link posts, as a float between 0.0 and 1.0. This will show up on GETs to a single link: sets of links won't have this field.\\n\\nYou can see some examples of `controversiality` \\u0026 `score` on comments and `upvote_ratio` on a link here: http://www.reddit.com/r/pics/comments/29h0zn/was_taking_random_pictures_of_my_mother_and_this.json?sort=controversial\\u0026limit=3\\n\\nHope this is useful!\\n\\n-umbrae\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"29i58s\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"umbrae\", \"media\": null, \"score\": 39, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/29i58s/reddit_change_api_availability_controversiality/\", \"locked\": false, \"name\": \"t3_29i58s\", \"created\": 1404189565.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/29i58s/reddit_change_api_availability_controversiality/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[reddit change] API Availability: controversiality \\u0026 score on comments, upvote_ratio on individual submissions\", \"created_utc\": 1404160765.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 39}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EFirst of all, I\\u0026#39;m not associated with Reddit.  I\\u0026#39;m doing this for the betterment of mankind.  :)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThere has been some talk about implementing comment streams for Reddit.  Well, I\\u0026#39;ve created a comment stream for developers.  I\\u0026#39;ve put a lot of code together to make sure that all of the comments (that Reddit is willing to deliver) are collected, properly cached, sorted and then made available for the stream.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI cache the previous 2.5 million Reddit comments in memory and the previous 5 billion on disk.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you wish to test the stream out, it is available \\u003Ca href=\\\"http://dev.redditanalytics.com/search/stream/\\\"\\u003Ehere\\u003C/a\\u003E  Please be advised that this stream is for applications to ingest json data (one JSON block per comment).  It will do you little good to load this with your browser unless you\\u0026#39;re simply curious or testing that the stream is active. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EUntil I get more funding, I\\u0026#39;m limited to 1 terabyte outgoing per month.  I\\u0026#39;m working on securing 1 petabyte outgoing per month, but it\\u0026#39;s going to cost a bit of money.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease let me know how the stream works for you!  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks so much.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EWhy should I use this stream?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt will save on your API calls to Reddit and alleviate stress on Reddit\\u0026#39;s servers.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EHow accurate is your data?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy stream does not filter any comments.  The comments are formatted exactly as would be delivered from Reddit minus the wrapper.  Each JSON is delimited by a new line.  My ingest script has failover built in and is smart enough to know when there is an issue on Reddit\\u0026#39;s side.  If you see the comment on Reddit, there\\u0026#39;s a 99.9999% that it is in my stream.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EHow current is your stream?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWithin 5 seconds at most.  If it falls further behind, it\\u0026#39;s Reddit\\u0026#39;s fault.  :)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EDoes your stream get comment edits?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENo.  Comment edits are not available on Reddit\\u0026#39;s stream.  There is currently no known way to get comment edits while bound to the rules for Reddit\\u0026#39;s API usage.  The only way to know if a message was edited is after the fact -- when scraping the comment again at some future date and reading the \\u0026quot;edited\\u0026quot; key in the JSON response.  This key should always be false in my stream -- just as it would be when viewing \\u003Ca href=\\\"http://www.reddit.com/comments.json\\\"\\u003Ehttp://www.reddit.com/comments.json\\u003C/a\\u003E \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EWhere can I send suggestions?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESend them to my PM on Reddit if you like.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EWhat is the location of this stream?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://dev.redditanalytics.com/search/stream/\\\"\\u003Ehttp://dev.redditanalytics.com/search/stream/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EHow many connections can I make?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease make only one.  There is no need to make more since you will get the same data from both.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit:  There is now additional logic to check if there is already an open stream for a given IP address.  If so, it will give an error.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EHow many global connections do you support?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHopefully enough until I get a more powerful server.  This server is very old.  :)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EYou mean I could see just the comments come in real-time??\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYep!  If you\\u0026#39;re using linux and have perl installed (most come pre-installed) and assuming you have installed JSON for Perl from CPAN (sudo cpan install JSON) ... you could use this command in bash  ... \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ewget -qO- \\u0026quot;http://dev.redditanalytics.com/search/stream/\\u0026quot; | perl -MJSON -ne \\u0026#39;print from_json($_)-\\u0026gt;{body}.\\u0026quot;\\\\n\\u0026quot;\\u0026#39;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EThat\\u0026#39;s pretty cool, but what else could I do from the command line?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWell, you could figure out which submissions are currently most active by piping in 1,000 comments to this command (it will take 1-2 minutes to collect all of the data)\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ewget -qO- \\u0026quot;http://dev.redditanalytics.com/search/stream/\\u0026quot; | head -n 1000 | perl -MJSON -ne \\u0026#39;print from_json($_)-\\u0026gt;{link_title}.\\u0026quot;\\\\n\\u0026quot;\\u0026#39; | sort | uniq -c | sort -n\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EI found this really useful and would like to repay you in some way\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EA Reddit Gold reward would always be helpful and contribute to helping me scrape comments for their score values!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E** \\u003Cstrong\\u003EUPDATE\\u003C/strong\\u003E **\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAndrewNeo offered an awesome suggestion to help conserve bandwidth.  If you do not need the entire stream, you can filter by subreddit.  This is how you would stream comments only for askreddit:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://dev.redditanalytics.com/search/stream/?subreddit=askreddit\\\"\\u003Ehttp://dev.redditanalytics.com/search/stream/?subreddit=askreddit\\u003C/a\\u003E \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe subreddit name is not case sensitive.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso, if you plan on using this API for the future, plan on having to pass an API key at some point.  Traffic is going up a bit faster than expected.  I may have to implement API keys at some point.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E** \\u003Cstrong\\u003ECOMING SOON\\u003C/strong\\u003E **\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am also going to put a \\u003Cem\\u003Eq\\u003C/em\\u003E parameter so you can filter the stream by keywords and phrases.  You could use this to trigger a bot or send an e-mail to Wil Wheaton when his name comes through the stream \\u003Cem\\u003Egrin\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EQuestions for you the user\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWould filtering by a specific submission help?\\nWhat about filtering by author name?\\nHow can I make this better?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"First of all, I'm not associated with Reddit.  I'm doing this for the betterment of mankind.  :)\\n\\nThere has been some talk about implementing comment streams for Reddit.  Well, I've created a comment stream for developers.  I've put a lot of code together to make sure that all of the comments (that Reddit is willing to deliver) are collected, properly cached, sorted and then made available for the stream.\\n\\nI cache the previous 2.5 million Reddit comments in memory and the previous 5 billion on disk.  \\n\\nIf you wish to test the stream out, it is available [here](http://dev.redditanalytics.com/search/stream/)  Please be advised that this stream is for applications to ingest json data (one JSON block per comment).  It will do you little good to load this with your browser unless you're simply curious or testing that the stream is active. \\n\\nUntil I get more funding, I'm limited to 1 terabyte outgoing per month.  I'm working on securing 1 petabyte outgoing per month, but it's going to cost a bit of money.\\n\\nPlease let me know how the stream works for you!  \\n\\nThanks so much.\\n\\n**Why should I use this stream?**\\n\\nIt will save on your API calls to Reddit and alleviate stress on Reddit's servers.\\n\\n**How accurate is your data?**\\n\\nMy stream does not filter any comments.  The comments are formatted exactly as would be delivered from Reddit minus the wrapper.  Each JSON is delimited by a new line.  My ingest script has failover built in and is smart enough to know when there is an issue on Reddit's side.  If you see the comment on Reddit, there's a 99.9999% that it is in my stream.\\n\\n**How current is your stream?**\\n\\nWithin 5 seconds at most.  If it falls further behind, it's Reddit's fault.  :)\\n\\n**Does your stream get comment edits?**\\n\\nNo.  Comment edits are not available on Reddit's stream.  There is currently no known way to get comment edits while bound to the rules for Reddit's API usage.  The only way to know if a message was edited is after the fact -- when scraping the comment again at some future date and reading the \\\"edited\\\" key in the JSON response.  This key should always be false in my stream -- just as it would be when viewing http://www.reddit.com/comments.json \\n\\n**Where can I send suggestions?**\\n\\nSend them to my PM on Reddit if you like.\\n\\n**What is the location of this stream?**\\n\\nhttp://dev.redditanalytics.com/search/stream/\\n\\n**How many connections can I make?**\\n\\nPlease make only one.  There is no need to make more since you will get the same data from both.\\n\\nEdit:  There is now additional logic to check if there is already an open stream for a given IP address.  If so, it will give an error.  \\n\\n**How many global connections do you support?**\\n\\nHopefully enough until I get a more powerful server.  This server is very old.  :)\\n\\n**You mean I could see just the comments come in real-time??**\\n\\nYep!  If you're using linux and have perl installed (most come pre-installed) and assuming you have installed JSON for Perl from CPAN (sudo cpan install JSON) ... you could use this command in bash  ... \\n\\n    wget -qO- \\\"http://dev.redditanalytics.com/search/stream/\\\" | perl -MJSON -ne 'print from_json($_)-\\u003E{body}.\\\"\\\\n\\\"'\\n\\n\\n**That's pretty cool, but what else could I do from the command line?**\\n\\nWell, you could figure out which submissions are currently most active by piping in 1,000 comments to this command (it will take 1-2 minutes to collect all of the data)\\n\\n    wget -qO- \\\"http://dev.redditanalytics.com/search/stream/\\\" | head -n 1000 | perl -MJSON -ne 'print from_json($_)-\\u003E{link_title}.\\\"\\\\n\\\"' | sort | uniq -c | sort -n\\n\\n**I found this really useful and would like to repay you in some way**\\n\\nA Reddit Gold reward would always be helpful and contribute to helping me scrape comments for their score values!\\n\\n** **UPDATE** **\\n\\nAndrewNeo offered an awesome suggestion to help conserve bandwidth.  If you do not need the entire stream, you can filter by subreddit.  This is how you would stream comments only for askreddit:\\n\\nhttp://dev.redditanalytics.com/search/stream/?subreddit=askreddit \\n\\nThe subreddit name is not case sensitive.  \\n\\nAlso, if you plan on using this API for the future, plan on having to pass an API key at some point.  Traffic is going up a bit faster than expected.  I may have to implement API keys at some point.\\n\\n** **COMING SOON** **\\n\\nI am also going to put a *q* parameter so you can filter the stream by keywords and phrases.  You could use this to trigger a bot or send an e-mail to Wil Wheaton when his name comes through the stream *grin*\\n\\n**Questions for you the user**\\n\\nWould filtering by a specific submission help?\\nWhat about filtering by author name?\\nHow can I make this better?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1g6dds\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 37, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 44, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1371035009.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1g6dds/reddit_comment_streams_beta/\", \"locked\": false, \"name\": \"t3_1g6dds\", \"created\": 1371041183.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1g6dds/reddit_comment_streams_beta/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Comment Streams (BETA)\", \"created_utc\": 1371012383.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 37}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis is what I\\u0026#39;ve learned (so far)while trying to learn how to use reddit\\u0026#39;s source code. Some or all of it might be wrong.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m posting a journal here to help myself and others--hopefully, some of this will help improve the documentation, somebody will hit me with a cluebat if I \\u0026quot;learned\\u0026quot; something that\\u0026#39;s horribly wrong, etc.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EEasy Mode: Use Ubuntu 12.04 Long Term Support\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EThe \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/install-reddit.sh\\\"\\u003Einstall script\\u003C/a\\u003E is Ubuntu 12.04 specific and \\u003Cem\\u003Eawesome\\u003C/em\\u003E.  It apt-get-s dependencies, sets up the databases, and even gets the stack running.  \\u003Cstrong\\u003EHighly recommended\\u003C/strong\\u003E.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you don\\u0026#39;t like Ubuntu Unity, you can ask Ubuntu to install LXDE, XFCE, KDE, etc.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EFix the /etc/hosts file\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EOne thing install-reddit.sh does NOT do is fix your hosts table.  Adding the following line to the /ect/hosts file can help your browser find the local reddit server:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E0.0.0.0                 reddit.local\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENow you should be able to go to reddit.local in your browser, log in, and look around at...a very, very empty reddit clone.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EWhere\\u0026#39;s the create reddit button?\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EBy default, reddit hides the \\u0026quot;create reddit button\\u0026quot; to reduce the number of reddits created by clueless newbies. There are other ways around this, but one shortcut is to go directly to \\u003Ca href=\\\"http://reddit.local/reddits/create\\\"\\u003Ehttp://reddit.local/reddits/create\\u003C/a\\u003E .\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003ETweaking the configuration:\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EThere are some important files for tweaking your local reddit\\u0026#39;s settings.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E/home/reddit/reddit/r2/example.ini\\u003Cbr/\\u003E\\n\\u003Cstrong\\u003EDON\\u0026#39;T EDIT, JUST LOOK\\u003C/strong\\u003E  \\u003Cem\\u003EThis file lets you see all the default settings.\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E/home/reddit/reddit/r2/run.ini\\u003Cbr/\\u003E\\n\\u003Cem\\u003EThis is just a pointer--it defaults to pointing at development.ini\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E/home/reddit/reddit/r2/development.update\\u003Cbr/\\u003E\\n\\u003Cstrong\\u003ENOTE: Change the *.update file, not the *.ini file\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAdd the following line to /home/reddit/reddit/r2/development.update :\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eshort_description = I changed the description!\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThen run:\\n    /home/reddit/reddit/r2$ sudo make\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EReload \\u003Ca href=\\\"http://reddit.local\\\"\\u003Ehttp://reddit.local\\u003C/a\\u003E and boom, the title of the webpage has changed.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EWhere\\u0026#39;s the off switch?\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003ETurning the local server on and off can be done on the comand line:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esudo initctl emit reddit-stop\\nsudo initctl emit reddit-start\\nsudo initctl emit reddit-restart\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This is what I've learned (so far)while trying to learn how to use reddit's source code. Some or all of it might be wrong.  \\n\\nI'm posting a journal here to help myself and others--hopefully, some of this will help improve the documentation, somebody will hit me with a cluebat if I \\\"learned\\\" something that's horribly wrong, etc.\\n\\n----\\n\\n* Easy Mode: Use Ubuntu 12.04 Long Term Support\\n\\nThe [install script](https://github.com/reddit/reddit/blob/master/install-reddit.sh) is Ubuntu 12.04 specific and *awesome*.  It apt-get-s dependencies, sets up the databases, and even gets the stack running.  **Highly recommended**.  \\n\\nIf you don't like Ubuntu Unity, you can ask Ubuntu to install LXDE, XFCE, KDE, etc.\\n\\n----\\n\\n* Fix the /etc/hosts file\\n\\nOne thing install-reddit.sh does NOT do is fix your hosts table.  Adding the following line to the /ect/hosts file can help your browser find the local reddit server:\\n\\n    0.0.0.0                 reddit.local\\nNow you should be able to go to reddit.local in your browser, log in, and look around at...a very, very empty reddit clone.\\n\\n----\\n\\n* Where's the create reddit button?\\n\\nBy default, reddit hides the \\\"create reddit button\\\" to reduce the number of reddits created by clueless newbies. There are other ways around this, but one shortcut is to go directly to http://reddit.local/reddits/create .\\n\\n----\\n\\n* Tweaking the configuration:\\n\\nThere are some important files for tweaking your local reddit's settings.\\n\\n/home/reddit/reddit/r2/example.ini  \\n**DON'T EDIT, JUST LOOK**  *This file lets you see all the default settings.*\\n\\n/home/reddit/reddit/r2/run.ini  \\n*This is just a pointer--it defaults to pointing at development.ini*\\n\\n/home/reddit/reddit/r2/development.update  \\n**NOTE: Change the *.update file, not the *.ini file**\\n\\nAdd the following line to /home/reddit/reddit/r2/development.update :\\n\\n    short_description = I changed the description!\\n\\nThen run:\\n    /home/reddit/reddit/r2$ sudo make\\n\\nReload http://reddit.local and boom, the title of the webpage has changed.\\n\\n----\\n\\n* Where's the off switch?\\n\\nTurning the local server on and off can be done on the comand line:\\n\\n    sudo initctl emit reddit-stop\\n    sudo initctl emit reddit-start\\n    sudo initctl emit reddit-restart\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"19saz0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"foolblog\", \"media\": null, \"score\": 35, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/19saz0/learning_reddits_code_journal_1/\", \"locked\": false, \"name\": \"t3_19saz0\", \"created\": 1362620361.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/19saz0/learning_reddits_code_journal_1/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Learning reddit's code, Journal #1\", \"created_utc\": 1362591561.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 35}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EGreetings, devs!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor your pleasure and enjoyment, I\\u0026#39;ve added ratelimit headers to reddit\\u0026#39;s HTTP responses for API requests. The headers are:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003EX-Ratelimit-Used\\u003C/code\\u003E: Approximate number of requests used in this period\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003EX-Ratelimit-Remaining\\u003C/code\\u003E: Approximate number of requests left to use\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003EX-Ratelimit-Reset\\u003C/code\\u003E: Approximate number of seconds to end of period\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EThe ratelimits are based on our existing API rules (1 request every 2 seconds per IP). They\\u0026#39;re intended as an indicator and as a way for devs to be a bit more bursty over longer windows. The window is currently set to 10 minutes.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs a bonus for OAuth users, we\\u0026#39;re experimenting with allowing OAuth clients to have a higher rate limit. The limit is currently set to 1 request per second, and is per user-client. Abuse of this change will force me to reconsider, so please continue to respect our servers ;)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease note that while the window is 10 minutes, you still need to be reasonable about spacing out your requests. If you hit us for 300 requests right at the end of one window, and 300 requests right at the beginning of the next, we\\u0026#39;re going to have to cut you off.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFinally, requests that are served cached via our CDN will not include headers, and will not count against your total. So for cases where you don\\u0026#39;t need the freshest, absolute-up-to-the-minute data, consider hitting \\u003Ca href=\\\"http://www.reddit.com\\\"\\u003Ehttp://www.reddit.com\\u003C/a\\u003E (logged out, no cookie, no oauth header) with your GET requests.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: For more info on our ratelimiting rules, \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/API\\\"\\u003Echeck the developer wiki\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Greetings, devs!\\n\\nFor your pleasure and enjoyment, I've added ratelimit headers to reddit's HTTP responses for API requests. The headers are:\\n\\n* `X-Ratelimit-Used`: Approximate number of requests used in this period\\n* `X-Ratelimit-Remaining`: Approximate number of requests left to use\\n* `X-Ratelimit-Reset`: Approximate number of seconds to end of period\\n\\nThe ratelimits are based on our existing API rules (1 request every 2 seconds per IP). They're intended as an indicator and as a way for devs to be a bit more bursty over longer windows. The window is currently set to 10 minutes.\\n\\nAs a bonus for OAuth users, we're experimenting with allowing OAuth clients to have a higher rate limit. The limit is currently set to 1 request per second, and is per user-client. Abuse of this change will force me to reconsider, so please continue to respect our servers ;)\\n\\nPlease note that while the window is 10 minutes, you still need to be reasonable about spacing out your requests. If you hit us for 300 requests right at the end of one window, and 300 requests right at the beginning of the next, we're going to have to cut you off.\\n\\nFinally, requests that are served cached via our CDN will not include headers, and will not count against your total. So for cases where you don't need the freshest, absolute-up-to-the-minute data, consider hitting http://www.reddit.com (logged out, no cookie, no oauth header) with your GET requests.\\n\\nEDIT: For more info on our ratelimiting rules, [check the developer wiki](https://github.com/reddit/reddit/wiki/API).\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1yxrp7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 42, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1393374066.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1yxrp7/formal_ratelimiting_headers/\", \"locked\": false, \"name\": \"t3_1yxrp7\", \"created\": 1393399881.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1yxrp7/formal_ratelimiting_headers/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Formal ratelimiting headers\", \"created_utc\": 1393371081.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 42}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey all,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m currently investigating our CAPTCHA implementation and ways to improve it (one thing we are considering is moving to reCAPTCHA, though it has some interesting limitations). CAPTCHAs are one place that have been difficult to work with in the current reddit API, so I\\u0026#39;d like to take this opportunity to improve things. One of the first steps for that is opening a discussion with you. I\\u0026#39;d like to ask 3 questions to get things started:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EHow do you currently handle reddit CAPTCHAs in your client?\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EIf we make a change in the way we implement CAPTCHAs on the site, how much lead time do you need to support it in your client?\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EIs a CAPTCHA that requires opening a web page sufficient for your client, or do you need more granular access to CAPTCHA images?\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EMany of you run reddit bots that make posts to only a single account around the site. We\\u0026#39;d like to eventually have an API key system where we can provide some exempt status to single-account registered apps.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnother important set of apps to consider are mobile. We understand that CAPTCHAs are a big nuisance to mobile users, though they are an important tool for registrations and unproven users. I\\u0026#39;d love to hear your thoughts on question #3 and what would be ideal for you and your users.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey all,\\n\\nI'm currently investigating our CAPTCHA implementation and ways to improve it (one thing we are considering is moving to reCAPTCHA, though it has some interesting limitations). CAPTCHAs are one place that have been difficult to work with in the current reddit API, so I'd like to take this opportunity to improve things. One of the first steps for that is opening a discussion with you. I'd like to ask 3 questions to get things started:\\n\\n1. How do you currently handle reddit CAPTCHAs in your client?\\n\\n2. If we make a change in the way we implement CAPTCHAs on the site, how much lead time do you need to support it in your client?\\n\\n3. Is a CAPTCHA that requires opening a web page sufficient for your client, or do you need more granular access to CAPTCHA images?\\n\\nMany of you run reddit bots that make posts to only a single account around the site. We'd like to eventually have an API key system where we can provide some exempt status to single-account registered apps.\\n\\nAnother important set of apps to consider are mobile. We understand that CAPTCHAs are a big nuisance to mobile users, though they are an important tool for registrations and unproven users. I'd love to hear your thoughts on question #3 and what would be ideal for you and your users.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"mv40x\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"chromakode\", \"media\": null, \"score\": 41, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 18, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/mv40x/hey_reddit_api_developers_lets_talk_about_captchas/\", \"locked\": false, \"name\": \"t3_mv40x\", \"created\": 1322712465.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/mv40x/hey_reddit_api_developers_lets_talk_about_captchas/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Hey reddit API developers, let's talk about CAPTCHAs.\", \"created_utc\": 1322683665.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 41}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"dt1jt\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ketralnis\", \"media\": null, \"score\": 38, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/dt1jt/just_did_a_public_push/\", \"locked\": false, \"name\": \"t3_dt1jt\", \"created\": 1287471483.0, \"url\": \"http://github.com/reddit/reddit/commit/37e2ba9892d1742f78ab496d8fc1d17797731078\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Just did a public push\", \"created_utc\": 1287442683.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 38}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EFrom reddit\\u0026#39;s student contractor, \\u003Ca href=\\\"/u/slyf\\\"\\u003E/u/slyf\\u003C/a\\u003E:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003EWe have added a mobile friendly version of the authorization page for developers using OAuth for mobile applications.  This component to the \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/OAuth2\\\"\\u003EOAuth flow\\u003C/a\\u003E was lacking in our system.  Previously, the authorization page was only available as a desktop page with no mobile version.  Developers who wish to use the existing page need not change anything.  Developers who wish to use the mobile friendly version may use the newly available .compact version of the url (\\u003Ca href=\\\"https://ssl.reddit.com/api/v1/authorize.compact\\\"\\u003Ehttps://ssl.reddit.com/api/v1/authorize.compact\\u003C/a\\u003E).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELogged out users using the endpoint should see a mobile friendly login page \\u003Ca href=\\\"http://i.imgur.com/EQ6jyMB.png\\\"\\u003Elike this\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAfter logging in, the authorization endpoint should look something \\u003Ca href=\\\"http://i.imgur.com/NgCurxJ.png\\\"\\u003Elike this\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003ESee the \\u003Ca href=\\\"https://github.com/reddit/reddit/commit/79ddb15072dbb22c63f455a26d654579683ef210\\\"\\u003Ecode on github\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"From reddit's student contractor, /u/slyf:\\n\\n\\u003E We have added a mobile friendly version of the authorization page for developers using OAuth for mobile applications.  This component to the [OAuth flow](https://github.com/reddit/reddit/wiki/OAuth2) was lacking in our system.  Previously, the authorization page was only available as a desktop page with no mobile version.  Developers who wish to use the existing page need not change anything.  Developers who wish to use the mobile friendly version may use the newly available .compact version of the url (https://ssl.reddit.com/api/v1/authorize.compact).\\n\\u003E \\n\\u003E Logged out users using the endpoint should see a mobile friendly login page [like this](http://i.imgur.com/EQ6jyMB.png)\\n\\u003E \\n\\u003E After logging in, the authorization endpoint should look something [like this](http://i.imgur.com/NgCurxJ.png)\\n\\nSee the [code on github](https://github.com/reddit/reddit/commit/79ddb15072dbb22c63f455a26d654579683ef210)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1un00d\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 37, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1un00d/oauth_mobile_authorization_flow_now_enabled/\", \"locked\": false, \"name\": \"t3_1un00d\", \"created\": 1389146584.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1un00d/oauth_mobile_authorization_flow_now_enabled/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OAuth: Mobile authorization flow now enabled\", \"created_utc\": 1389117784.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 37}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EDue to CSRF technique irresponsibly announced to a group of people tonight, we\\u0026#39;ve had to make a slight tweak to our login API.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPOST requests to \\u003Ca href=\\\"http://www.reddit.com/dev/api#POST_api_login\\\"\\u003E/api/login\\u003C/a\\u003E must now \\u003Cstrong\\u003Enot\\u003C/strong\\u003E include a \\u003Ccode\\u003Ereddit_session\\u003C/code\\u003E cookie along in the request. If a \\u003Ccode\\u003Ereddit_session\\u003C/code\\u003E cookie exists, the request may fail with a 409 status.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis change may cause some apps and API clients to break. Notably, this will affect user switcher features like RES that don\\u0026#39;t clear out their session cookie before issuing the login request. We\\u0026#39;re sorry that we couldn\\u0026#39;t give a warning before breaking these apps. Please disclose any security issues you find in reddit \\u003Ca href=\\\"http://www.reddit.com/wiki/whitehat\\\"\\u003Ediscreetly and responsibly\\u003C/a\\u003E. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Due to CSRF technique irresponsibly announced to a group of people tonight, we've had to make a slight tweak to our login API.\\n\\nPOST requests to [/api/login](http://www.reddit.com/dev/api#POST_api_login) must now **not** include a `reddit_session` cookie along in the request. If a `reddit_session` cookie exists, the request may fail with a 409 status.\\n\\nThis change may cause some apps and API clients to break. Notably, this will affect user switcher features like RES that don't clear out their session cookie before issuing the login request. We're sorry that we couldn't give a warning before breaking these apps. Please disclose any security issues you find in reddit [discreetly and responsibly](http://www.reddit.com/wiki/whitehat). \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"17oer0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"chromakode\", \"media\": null, \"score\": 37, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 18, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/17oer0/api_change_login_requests_containing_a_session/\", \"locked\": false, \"name\": \"t3_17oer0\", \"created\": 1359722736.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/17oer0/api_change_login_requests_containing_a_session/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API Change: login requests containing a session cookie may fail with a 409 status\", \"created_utc\": 1359693936.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 37}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2c2ktw\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"spladug\", \"media\": null, \"score\": 33, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2c2ktw/the_reddit_live_api_is_now_documented_and_oauthd/\", \"locked\": false, \"name\": \"t3_2c2ktw\", \"created\": 1406691417.0, \"url\": \"https://www.reddit.com/dev/api#section_live\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"The reddit live API is now documented and OAuth'd.\", \"created_utc\": 1406662617.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 33}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIn support of the \\u003Ca href=\\\"https://www.reddit.com/r/announcements/comments/3fx2au/content_policy_update/\\\"\\u003Enew content policy that we announced today\\u003C/a\\u003E, content from \\u003Ca href=\\\"https://reddit.zendesk.com/hc/en-us/articles/205701245\\\"\\u003Equarantined\\u003C/a\\u003E subreddits will only be available to authenticated users who have opted-in to view that content on reddit.com. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe opt-in is per subreddit, per user, and is currently only accessible by visiting the quarantined subreddit on reddit.com. Only users who are logged-in and have a \\u003Ca href=\\\"https://www.reddit.com/prefs/update/\\\"\\u003Everified email address\\u003C/a\\u003E will be able to opt-in.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"In support of the [new content policy that we announced today](https://www.reddit.com/r/announcements/comments/3fx2au/content_policy_update/), content from [quarantined](https://reddit.zendesk.com/hc/en-us/articles/205701245) subreddits will only be available to authenticated users who have opted-in to view that content on reddit.com. \\n\\nThe opt-in is per subreddit, per user, and is currently only accessible by visiting the quarantined subreddit on reddit.com. Only users who are logged-in and have a [verified email address](https://www.reddit.com/prefs/update/) will be able to opt-in.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3fx3gt\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"tdohz\", \"media\": null, \"score\": 33, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 34, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3fx3gt/quarantined_content_will_be_unavailable_through/\", \"locked\": false, \"name\": \"t3_3fx3gt\", \"created\": 1438835158.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3fx3gt/quarantined_content_will_be_unavailable_through/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Quarantined content will be unavailable through the API unless opted-in\", \"created_utc\": 1438806358.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 33}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ECurrently, if you load \\u003Ca href=\\\"http://www.reddit.com/comments\\\"\\u003Ehttp://www.reddit.com/comments\\u003C/a\\u003E, you will receive a full listing of all comments being made anywhere on reddit. This isn\\u0026#39;t really consistent with how most other listings on the site work, so I\\u0026#39;m intending to change this in the near future (probably the next few days) so that it only shows comments in the subreddits that the user is subscribed to.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EIf you have a bot/script/etc. using the full comments listing to monitor all subreddits, please switch to using \\u003Ca href=\\\"http://www.reddit.com/r/all/comments\\\"\\u003Ehttp://www.reddit.com/r/all/comments\\u003C/a\\u003E instead.\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve also sent a PM about this to multiple people using this listing that have their username in their UA, but there are a few users that I can\\u0026#39;t identify from their UA. This is a good example of why it\\u0026#39;s best to include your username in there, since it makes it much simpler for us to ensure we minimize the impact of any changes to the API.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Currently, if you load http://www.reddit.com/comments, you will receive a full listing of all comments being made anywhere on reddit. This isn't really consistent with how most other listings on the site work, so I'm intending to change this in the near future (probably the next few days) so that it only shows comments in the subreddits that the user is subscribed to.\\n\\n**If you have a bot/script/etc. using the full comments listing to monitor all subreddits, please switch to using http://www.reddit.com/r/all/comments instead.**\\n\\nI've also sent a PM about this to multiple people using this listing that have their username in their UA, but there are a few users that I can't identify from their UA. This is a good example of why it's best to include your username in there, since it makes it much simpler for us to ensure we minimize the impact of any changes to the API.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1o0dzv\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Deimorz\", \"media\": null, \"score\": 33, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1o0dzv/upcoming_change_to_httpwwwredditcomcomments/\", \"locked\": false, \"name\": \"t3_1o0dzv\", \"created\": 1381293557.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1o0dzv/upcoming_change_to_httpwwwredditcomcomments/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Upcoming change to http://www.reddit.com/comments - please switch to http://www.reddit.com/r/all/comments to keep the same behavior\", \"created_utc\": 1381264757.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 33}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAs a few of you have already noticed, yesterday I pushed the code behind reddit\\u0026#39;s new new \\u0026quot;plugin system\\u0026quot; to GitHub. The purpose of these plugins is pretty nitty-gritty, so unless you\\u0026#39;re a reddit developer, you will probably never notice they\\u0026#39;re there.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe purpose of the new plugin system is to make it possible to extend reddit without adding to the core codebase. This originally came about when developing the new \\u003Ca href=\\\"/about\\\"\\u003Eabout pages\\u003C/a\\u003E, which add a lot of very reddit.com specific templates and static files, but aren\\u0026#39;t something you\\u0026#39;d by default need on a different deployment.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIn general, plugins can be used to:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003Eadd specific functionality to your reddit instance without changing the core\\u003C/li\\u003E\\n\\u003Cli\\u003Erun experimental code before folding it into the main codebase\\u003C/li\\u003E\\n\\u003Cli\\u003Eimplement interchangeable functionality for a specific purpose (for instance, you could have alternate search plugins implementing Solr, IndexTank, or CloudSearch backends)\\u003C/li\\u003E\\n\\u003Cli\\u003Eoverride specific static files or templates\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EOn the practical side, plugins are separate python packages that you install alongside reddit. You can then enable them by adding them to the \\u003Ccode\\u003Eplugins\\u003C/code\\u003E line of your server \\u003Ccode\\u003E.ini\\u003C/code\\u003E file. They also integrate with the main reddit \\u003Ccode\\u003EMakefile\\u003C/code\\u003E to compile static files for production.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor a sample plugin, check out the \\u003Ca href=\\\"https://github.com/reddit/reddit-plugin-about\\\"\\u003Eabout pages codebase\\u003C/a\\u003E. The basic plugin definition is in \\u003Ca href=\\\"https://github.com/reddit/reddit-plugin-about/blob/master/reddit_about/__init__.py\\\"\\u003E\\u003Ccode\\u003E__init__.py\\u003C/code\\u003E\\u003C/a\\u003E. I\\u0026#39;ll be working on releasing a skeleton plugin package in the future with more detailed information.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy hope is that having the option of writing plugins will help keep the reddit core simple while making it easier to try out and integrate small extensions to the site. If you have any questions or ideas please feel free to comment here or hit us up on IRC in #reddit-dev on FreeNode. :)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere\\u0026#39;s the commits on GitHub behind the new plugin system:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/compare/8b4f584f%7E34...8b4f584f\\\"\\u003Ehttps://github.com/reddit/reddit/compare/8b4f584f~34...8b4f584f\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"As a few of you have already noticed, yesterday I pushed the code behind reddit's new new \\\"plugin system\\\" to GitHub. The purpose of these plugins is pretty nitty-gritty, so unless you're a reddit developer, you will probably never notice they're there.\\n\\nThe purpose of the new plugin system is to make it possible to extend reddit without adding to the core codebase. This originally came about when developing the new [about pages](/about), which add a lot of very reddit.com specific templates and static files, but aren't something you'd by default need on a different deployment.\\n\\nIn general, plugins can be used to:\\n\\n * add specific functionality to your reddit instance without changing the core\\n * run experimental code before folding it into the main codebase\\n * implement interchangeable functionality for a specific purpose (for instance, you could have alternate search plugins implementing Solr, IndexTank, or CloudSearch backends)\\n * override specific static files or templates\\n\\nOn the practical side, plugins are separate python packages that you install alongside reddit. You can then enable them by adding them to the `plugins` line of your server `.ini` file. They also integrate with the main reddit `Makefile` to compile static files for production.\\n\\nFor a sample plugin, check out the [about pages codebase](https://github.com/reddit/reddit-plugin-about). The basic plugin definition is in [`__init__.py`](https://github.com/reddit/reddit-plugin-about/blob/master/reddit_about/__init__.py). I'll be working on releasing a skeleton plugin package in the future with more detailed information.\\n\\nMy hope is that having the option of writing plugins will help keep the reddit core simple while making it easier to try out and integrate small extensions to the site. If you have any questions or ideas please feel free to comment here or hit us up on IRC in #reddit-dev on FreeNode. :)\\n\\nHere's the commits on GitHub behind the new plugin system:\\n\\nhttps://github.com/reddit/reddit/compare/8b4f584f~34...8b4f584f\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"v0oa1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"chromakode\", \"media\": null, \"score\": 33, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/v0oa1/on_reddits_new_server_code_plugin_system/\", \"locked\": false, \"name\": \"t3_v0oa1\", \"created\": 1339658071.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/v0oa1/on_reddits_new_server_code_plugin_system/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"On reddit's new server code plugin system\", \"created_utc\": 1339629271.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 33}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis is a \\u003Ca href=\\\"http://vagrantup.com/\\\"\\u003EVagrant\\u003C/a\\u003E box, like the one \\u003Ca href=\\\"/u/kemitche\\\"\\u003E/u/kemitche\\u003C/a\\u003E \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/qnuxp/pycon_prepackaged_vm/\\\"\\u003Eposted\\u003C/a\\u003E before.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDownload it here: \\u003Ca href=\\\"http://aquatica.mit.edu/%7Eichthyos/reddit-clean.box\\\"\\u003Ehttp://aquatica.mit.edu/~ichthyos/reddit-clean.box\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMD5 = 9f986e9e4dfd5a8dde4d528515a99757\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe code it\\u0026#39;s running is current as of May 15, 2012. It is running reddit on port 8000, forwards that port on your host machine to the same on your VM, and has test data from populatedb.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDownload and install \\u003Ca href=\\\"https://www.virtualbox.org/wiki/Downloads\\\"\\u003EVirtualBox 4.1.14\\u003C/a\\u003E and \\u003Ca href=\\\"http://vagrantup.com/\\\"\\u003EVagrant\\u003C/a\\u003E to get this VM up and running:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E$ vagrant box add base reddit-clean.box\\n$ vagrant init\\n$ vagrant up\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAdd a line to your /etc/hosts file on your host machine to make reddit.local resolve to 127.0.0.1. This will make the links on your dev reddit work. Point your web browser to \\u003Ca href=\\\"http://reddit.local:8000\\\"\\u003Ehttp://reddit.local:8000\\u003C/a\\u003E. Voila, you should see an empty reddit!\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EJust for future reference, here are the steps I took to make it.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELines that look like this are run on my host machine:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehost$ command\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ELines that look like this are run on the VM:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Evagrant$ command\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EFirst, download and install \\u003Ca href=\\\"https://www.virtualbox.org/wiki/Downloads\\\"\\u003EVirtualBox 4.1.14\\u003C/a\\u003E and \\u003Ca href=\\\"http://vagrantup.com/\\\"\\u003EVagrant\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EGet a clean ubuntu image, tell Vagrant to forward port 8000, start your VM, and ssh to it.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehost$ vagrant box add base http://dl.dropbox.com/u/7490647/talifun-ubuntu-11.04-server-amd64.box\\nhost$ vagrant init\\nhost$ echo \\u0026quot;config.vm.forward_port 8000, 8000\\u0026quot; \\u0026gt;\\u0026gt; Vagrantfile\\nhost$ vagrant reload\\nhost$ vagrant up\\nhost$ vagrant ssh\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EUpdate Ubuntu packages.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Evagrant$ sudo apt-get update\\nvagrant$ sudo apt-get upgrade\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EUpdate the VirtualBox Guest Additions on the VM so that they are compatible with your newer VirtualBox.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Evagrant$ sudo aptitude install dkms\\nvagrant$ wget -c http://download.virtualbox.org/virtualbox/4.1.14/VBoxGuestAdditions_4.1.14.iso\\nvagrant$ sudo mount VBoxGuestAdditions_4.1.14.iso -o loop /mnt\\nvagrant$ sudo sh /mnt/VBoxLinuxAdditions.run\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EPer \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu\\\"\\u003Ethese instructions\\u003C/a\\u003E, get \\u003Ca href=\\\"https://gist.github.com/922144\\\"\\u003Ethe latest version of the reddit install script for Ubuntu\\u003C/a\\u003E and run it. You\\u0026#39;ll probably need to change the URL in the first line.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Evagrant$ wget https://raw.github.com/gist/922144/20655c40920185a376a21cfd0975596547927c30/install-reddit.sh\\nvagrant$ chmod +x install-reddit.sh\\nvagrant$ sudo ./install-reddit.sh\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EStill following those instructions, populate your reddit with test data.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Evagrant$ cd ~reddit/reddit/r2/\\nvagrant$ sudo -u reddit paster shell run.ini\\n\\u0026gt;\\u0026gt;\\u0026gt; from r2.models import populatedb\\n\\u0026gt;\\u0026gt;\\u0026gt; populatedb.populate()\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT:\\u003C/strong\\u003E Oops, I included the commands I ran to get reddit running on the VM but not the one I ran to package it up at the end (including the port 8000 forwarding directive in the Vagrantfile config). Here it is:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehost$ vagrant package --output=reddit-clean.box --vagrantfile Vagrantfile\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT 2:\\u003C/strong\\u003E Added some more instructions and the VM\\u0026#39;s MD5 hash.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This is a [Vagrant](http://vagrantup.com/) box, like the one /u/kemitche [posted](http://www.reddit.com/r/redditdev/comments/qnuxp/pycon_prepackaged_vm/) before.\\n\\nDownload it here: http://aquatica.mit.edu/~ichthyos/reddit-clean.box\\n\\nMD5 = 9f986e9e4dfd5a8dde4d528515a99757\\n\\nThe code it's running is current as of May 15, 2012. It is running reddit on port 8000, forwards that port on your host machine to the same on your VM, and has test data from populatedb.\\n\\nDownload and install [VirtualBox 4.1.14](https://www.virtualbox.org/wiki/Downloads) and [Vagrant](http://vagrantup.com/) to get this VM up and running:\\n\\n    $ vagrant box add base reddit-clean.box\\n    $ vagrant init\\n    $ vagrant up\\n\\nAdd a line to your /etc/hosts file on your host machine to make reddit.local resolve to 127.0.0.1. This will make the links on your dev reddit work. Point your web browser to http://reddit.local:8000. Voila, you should see an empty reddit!\\n\\n----\\n\\nJust for future reference, here are the steps I took to make it.\\n\\nLines that look like this are run on my host machine:\\n\\n    host$ command\\n\\nLines that look like this are run on the VM:\\n\\n    vagrant$ command\\n\\nFirst, download and install [VirtualBox 4.1.14](https://www.virtualbox.org/wiki/Downloads) and [Vagrant](http://vagrantup.com/).\\n\\nGet a clean ubuntu image, tell Vagrant to forward port 8000, start your VM, and ssh to it.\\n\\n    host$ vagrant box add base http://dl.dropbox.com/u/7490647/talifun-ubuntu-11.04-server-amd64.box\\n    host$ vagrant init\\n    host$ echo \\\"config.vm.forward_port 8000, 8000\\\" \\u003E\\u003E Vagrantfile\\n    host$ vagrant reload\\n    host$ vagrant up\\n    host$ vagrant ssh\\n\\nUpdate Ubuntu packages.\\n\\n    vagrant$ sudo apt-get update\\n    vagrant$ sudo apt-get upgrade\\n\\nUpdate the VirtualBox Guest Additions on the VM so that they are compatible with your newer VirtualBox.\\n\\n    vagrant$ sudo aptitude install dkms\\n    vagrant$ wget -c http://download.virtualbox.org/virtualbox/4.1.14/VBoxGuestAdditions_4.1.14.iso\\n    vagrant$ sudo mount VBoxGuestAdditions_4.1.14.iso -o loop /mnt\\n    vagrant$ sudo sh /mnt/VBoxLinuxAdditions.run\\n\\nPer [these instructions](https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu), get [the latest version of the reddit install script for Ubuntu](https://gist.github.com/922144) and run it. You'll probably need to change the URL in the first line.\\n\\n    vagrant$ wget https://raw.github.com/gist/922144/20655c40920185a376a21cfd0975596547927c30/install-reddit.sh\\n    vagrant$ chmod +x install-reddit.sh\\n    vagrant$ sudo ./install-reddit.sh\\n\\nStill following those instructions, populate your reddit with test data.\\n\\n    vagrant$ cd ~reddit/reddit/r2/\\n    vagrant$ sudo -u reddit paster shell run.ini\\n    \\u003E\\u003E\\u003E from r2.models import populatedb\\n    \\u003E\\u003E\\u003E populatedb.populate()\\n\\n**EDIT:** Oops, I included the commands I ran to get reddit running on the VM but not the one I ran to package it up at the end (including the port 8000 forwarding directive in the Vagrantfile config). Here it is:\\n\\n    host$ vagrant package --output=reddit-clean.box --vagrantfile Vagrantfile\\n\\n**EDIT 2:** Added some more instructions and the VM's MD5 hash.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"tps8u\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ichthyos\", \"media\": null, \"score\": 31, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 16, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1337342776.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/tps8u/having_trouble_getting_a_dev_instance_of_reddit/\", \"locked\": false, \"name\": \"t3_tps8u\", \"created\": 1337195001.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/tps8u/having_trouble_getting_a_dev_instance_of_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Having trouble getting a dev instance of reddit up and running? Here's an updated reddit VM (as of 2012-05-15) and instructions I followed to make it.\", \"created_utc\": 1337166201.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 31}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe search syntax has been updated as we\\u0026#39;ve moved off of indextank (they were \\u003Ca href=\\\"http://blog.indextank.com/1221/indextank-linkedin-acquires-indextank/\\\"\\u003Ebought by LinkedIn\\u003C/a\\u003E several months ago). If you use a reddit client that interacts with search in any way, it probably needs an update.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor details on the new syntax, see the \\u003Ca href=\\\"/help/search\\\"\\u003Esearch help page\\u003C/a\\u003E. Please feel free to ask questions here (and point out any major oddities/problems with search results). The key points are:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EText fields must be enclosed in quotes: author:\\u0026#39;kemitche\\u0026#39;\\u003C/li\\u003E\\n\\u003Cli\\u003EAND and OR based queries become more lisp-like: (and author:\\u0026#39;kemitche\\u0026#39; subreddit:\\u0026#39;redditdev\\u0026#39;)\\u003C/li\\u003E\\n\\u003Cli\\u003Eis_self and over18 are now integer fields: (and reddit:\\u0026#39;blog\\u0026#39; is_self:1)\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EThere is one known issue where certain older links with low votes and numbers of comments are incorrectly being sorted to the top when searching and sorted by \\u0026quot;relevance\\u0026quot;. This is a data issue that I\\u0026#39;ll be trying to resolve in the next day or so.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT\\u003C/strong\\u003E: For those asking, we\\u0026#39;re on \\u003Ca href=\\\"http://aws.amazon.com/cloudsearch/\\\"\\u003EAmazon Cloudsearch\\u003C/a\\u003E now.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The search syntax has been updated as we've moved off of indextank (they were [bought by LinkedIn](http://blog.indextank.com/1221/indextank-linkedin-acquires-indextank/) several months ago). If you use a reddit client that interacts with search in any way, it probably needs an update.\\n\\nFor details on the new syntax, see the [search help page](/help/search). Please feel free to ask questions here (and point out any major oddities/problems with search results). The key points are:\\n\\n* Text fields must be enclosed in quotes: author:'kemitche'\\n* AND and OR based queries become more lisp-like: (and author:'kemitche' subreddit:'redditdev')\\n* is_self and over18 are now integer fields: (and reddit:'blog' is_self:1)\\n\\nThere is one known issue where certain older links with low votes and numbers of comments are incorrectly being sorted to the top when searching and sorted by \\\"relevance\\\". This is a data issue that I'll be trying to resolve in the next day or so.\\n\\n**EDIT**: For those asking, we're on [Amazon Cloudsearch](http://aws.amazon.com/cloudsearch/) now.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"s3vcj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 31, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 27, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/s3vcj/search_syntax_has_changed_if_your_client_uses_it/\", \"locked\": false, \"name\": \"t3_s3vcj\", \"created\": 1334148869.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/s3vcj/search_syntax_has_changed_if_your_client_uses_it/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Search syntax has changed - if your client uses it, please read\", \"created_utc\": 1334120069.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 31}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ERejoice; for I render unto you an API-accessible list of default subreddits. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/subreddits/default.json\\\"\\u003Ehttps://www.reddit.com/subreddits/default.json\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Rejoice; for I render unto you an API-accessible list of default subreddits. \\n\\nhttps://www.reddit.com/subreddits/default.json\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"35w2di\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ajacksified\", \"media\": null, \"score\": 31, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/35w2di/new_feature_subredditsdefaultjson/\", \"locked\": false, \"name\": \"t3_35w2di\", \"created\": 1431590718.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/35w2di/new_feature_subredditsdefaultjson/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[New Feature] /subreddits/default.json\", \"created_utc\": 1431561918.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 31}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"218wd7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 32, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/218wd7/oauth_20_you_asked_i_listened_updated_and_more/\", \"locked\": false, \"name\": \"t3_218wd7\", \"created\": 1395712135.0, \"url\": \"https://github.com/reddit/reddit/wiki/OAuth2\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[OAuth 2.0] You asked, I listened. Updated and more complete OAuth 2.0 documentation. Feedback desired!\", \"created_utc\": 1395683335.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 32}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1lyirl\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"karangoeluw\", \"media\": null, \"score\": 33, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1lyirl/nobodydoesthis_scans_3_dayold_rdoesanybodyelse/\", \"locked\": false, \"name\": \"t3_1lyirl\", \"created\": 1378649228.0, \"url\": \"https://github.com/thekarangoel/NobodyDoesThis\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"NobodyDoesThis - Scans 3 day-old /r/DoesAnybodyElse posts with 0 score and then comments \\\"Nope, it's just you.\\\"\", \"created_utc\": 1378620428.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 33}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis is the \\u003Ca href=\\\"http://venus.xelio.info\\\"\\u003EReddit Search Engine\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m opening up the backend to developers.  Currently it accepts requests and returns a JSON object.  10,000 calls per day with up to 200 results per call.  I can handle billions of calls (yes, billions) -- so if you need more, let me know.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESuper fast, super fun and super delicious.  Oh yeah, did I also mention you can submit links and find out which submissions had those links submitted (even denormalized urls?).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat say you, front-end developers?  Let\\u0026#39;s get this party started!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit:  Oops.  Forgot to mention that if you are interested, just shoot me a message on here.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This is the [Reddit Search Engine](http://venus.xelio.info).\\n\\nI'm opening up the backend to developers.  Currently it accepts requests and returns a JSON object.  10,000 calls per day with up to 200 results per call.  I can handle billions of calls (yes, billions) -- so if you need more, let me know.  \\n\\nSuper fast, super fun and super delicious.  Oh yeah, did I also mention you can submit links and find out which submissions had those links submitted (even denormalized urls?).\\n\\nWhat say you, front-end developers?  Let's get this party started!\\n\\nEdit:  Oops.  Forgot to mention that if you are interested, just shoot me a message on here.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"vb83r\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"aphexcoil\", \"media\": null, \"score\": 32, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 21, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/vb83r/im_opening_up_my_reddit_search_engine_to_frontend/\", \"locked\": false, \"name\": \"t3_vb83r\", \"created\": 1340192605.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/vb83r/im_opening_up_my_reddit_search_engine_to_frontend/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I'm opening up my Reddit Search engine to front-end developers.  Now's your chance to be a legend.\", \"created_utc\": 1340163805.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 32}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am interested in developing a reddit-like website and I was thinking in doing it in Lisp. What are the disadvantages of this approach? Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am interested in developing a reddit-like website and I was thinking in doing it in Lisp. What are the disadvantages of this approach? Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"o7xgw\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"jsibelius\", \"media\": null, \"score\": 31, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/o7xgw/originally_reddit_was_written_in_lisp_can_someone/\", \"locked\": false, \"name\": \"t3_o7xgw\", \"created\": 1326058091.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/o7xgw/originally_reddit_was_written_in_lisp_can_someone/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Originally Reddit was written in Lisp. Can someone tell me why it had to be rewritten in Python?\", \"created_utc\": 1326029291.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 31}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHere\\u0026#39;s a little write-up of how I work through the reddit code to make a contribution to the source. My background is teaching myself C when I was a teen to develop a MUD, then 2 years of Computer Science (C++, Java, database stuff) and PHP/Python on the side as I worked on some side projects and now working at a local web business. Python is my least familiar language and I have no experience with Pylons/Cassandra, nor am I familiar with the reddit code. \\u003Ca href=\\\"http://blog.reddit.com/2010/11/thanks-hackers-in-both-senses-of-word.html\\\"\\u003EHere\\u0026#39;s a blog post of my first contribution\\u003C/a\\u003E but I\\u0026#39;m going over my \\u003Ca href=\\\"https://github.com/reddit/reddit/pull/18\\\"\\u003E\\u0026quot;mark nsfw\\u0026quot; contribution here\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe first thing I had to do was get the button in, otherwise I can\\u0026#39;t even test the code I\\u0026#39;ll be doing. So I did a grep for one of the buttons, in this case \\u0026quot;approve\\u0026quot;. The first thing I got was \\u003Cstrong\\u003EPOST_approve\\u003C/strong\\u003E in api.py, which I figured was the actual code to handle approving the submission. I\\u0026#39;d come to that later. Then a number of results appeared unrelated, so I narrowed it down by doing a grep for \\u003Cstrong\\u003E\\\\\\u0026quot;approve\\\\\\u0026quot;\\u003C/strong\\u003E and sure enough I found data/templates/printablebuttons.html.py. A quick glance at it and it seems to be a \\u0026quot;compiled\\u0026quot; version, but grep also listed r2/templates/printablebuttons.html which looks to be the right place for buttons. A quick glance at the template code and it looks simple enough. I decide to use the yes/no confirmation button which report uses:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E%elif thing.show_report:\\n  \\u0026lt;li\\u0026gt;\\n    ${ynbutton(_(\\u0026quot;report\\u0026quot;), _(\\u0026quot;reported\\u0026quot;), \\u0026quot;report\\u0026quot;, \\u0026quot;hide_thing\\u0026quot;)}\\n  \\u0026lt;/li\\u0026gt;\\n%endif\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAnd I create this code based on the report code in the same file:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E%if thing.show_marknsfw:\\n  \\u0026lt;li\\u0026gt;${ynbutton(_(\\u0026quot;mark NSFW\\u0026quot;), _(\\u0026quot;marked NSFW\\u0026quot;), \\u0026quot;marknsfw\\u0026quot;)}\\u0026lt;/li\\u0026gt;\\n%endif\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAlong with the bool show_marknsfw, the last argument is clearly an identifier of some sort (since one of the buttons was \\u0026quot;hide_thing\\u0026quot;), so I have to find where that is defined now and create marknsfw and unmarknsfw. Again I\\u0026#39;ll use grep to search for an existing identifier; I picked \\u0026quot;indict\\u0026quot;. Oh and I used grep like this \\u003Cstrong\\u003Efind . -exec grep -l \\u0026quot;indict\\u0026quot; {} \\\\;\\u003C/strong\\u003E I came across a number of strings and the sort (which is what I\\u0026#39;m not looking for) and then found POST_indict in api.py and figured that must be it. An example of what I was looking over:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E  @noresponse(VUser(),\\n              VModhash(),\\n              thing = VByNameIfAuthor(\\u0026#39;id\\u0026#39;))\\n  def POST_del(self, thing):\\n      if not thing: return\\n      \\u0026#39;\\u0026#39;\\u0026#39;for deleting all sorts of things\\u0026#39;\\u0026#39;\\u0026#39;\\n      thing._deleted = True\\n      if (getattr(thing, \\u0026quot;promoted\\u0026quot;, None) is not None and\\n          not promote.is_promoted(thing)):\\n          promote.reject_promotion(thing)\\n      thing._commit()\\n\\n      # flag search indexer that something has changed\\n      changed(thing)\\n\\n      #expire the item from the sr cache\\n      if isinstance(thing, Link):\\n          sr = thing.subreddit_slow\\n          expire_hot(sr)\\n          queries.delete_links(thing)\\n\\n      #comments have special delete tasks\\n      elif isinstance(thing, Comment):\\n          thing._delete()\\n          delete_comment(thing)\\n          queries.new_comment(thing, None)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI understood the Python in the code, but things like the noresponse() parts wasn\\u0026#39;t I knew offhand. Regardless I looked over a number of the functions and found that I needed to \\u003Cstrong\\u003Ething._commit()\\u003C/strong\\u003E and \\u003Cstrong\\u003Echanged(thing)\\u003C/strong\\u003E (found in POST_del) when making a change.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENext up I needed to find how to mark something NSFW. (I spent a bit working on appending the title of the submission with \\u0026quot;(NSFW)\\u0026quot; into the database, but later removed that to append it in the template). So once again I did a grep for \\u0026quot;nsfw\\u0026quot; (case insensitive) and found a line like this \\u003Cstrong\\u003Eitem.nsfw = item.over_18 and user.pref_label_nsfw\\u003C/strong\\u003E, so it looks like bool values in a struct or the sort (originally I ended up setting just item.nsfw to true and that didn\\u0026#39;t work). So back in \\u003Cstrong\\u003Eapi.py\\u003C/strong\\u003E, I create this code basing it off the other functions in the file:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E  @noresponse(VUser(),\\n              VModhash(),\\n              thing = VByName(\\u0026#39;id\\u0026#39;))\\n  def POST_marknsfw(self, thing):\\n      thing.nsfw = True\\n      thing.over_18 = True\\n      thing._commit()\\n\\n      # flag search indexer that something has changed\\n      changed(thing)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENext up is to figure out where to define show_marknsfw that defines when it\\u0026#39;ll show up. It looks like a \\u0026quot;method\\u0026quot; of thing that returns a bool.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAgain, I used grep to look for show_report and found some assignments of it in \\u003Cstrong\\u003Er2/lib/pages/things.py\\u003C/strong\\u003E and found that it\\u0026#39;s not so much a method, but a bool variable in the PrintableButtons class. There\\u0026#39;s a number of things done the file, such as initializing and determining the final value etc. Here are the changes I made:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Edef __init__(self, style, thing,\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E....\\n    -            show_distinguish = False,\\n    +            show_distinguish = False, show_marknsfw = True,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E...\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E                 show_distinguish = show_distinguish,\\n+                show_marknsfw = show_marknsfw,\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EBasically following how the code is for similar vars. Next up is to set the actual bool to determine True/False. First was just simple logic to decide who could mark submissions NSFW. I picked mods, the submitter and admins. Did some looking around (like on the \\u0026quot;remove\\u0026quot; code that mods use) and came up with this code:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E    if (c.user_is_admin or thing.can_ban or is_author) and not thing.nsfw:\\n        show_marknsfw = True\\n    else:\\n        show_marknsfw = False\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENote: I also wrote code to unmark something NSFW, but didn\\u0026#39;t include it here because I didn\\u0026#39;t add the code until months later (just now).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI think that\\u0026#39;s it, unless I forgot anything. It took me a number of days going through this and testing it. Maybe an admin can give the technical details of exactly what is going on here (like noresponse) but I found most of it self explanatory. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Here's a little write-up of how I work through the reddit code to make a contribution to the source. My background is teaching myself C when I was a teen to develop a MUD, then 2 years of Computer Science (C++, Java, database stuff) and PHP/Python on the side as I worked on some side projects and now working at a local web business. Python is my least familiar language and I have no experience with Pylons/Cassandra, nor am I familiar with the reddit code. [Here's a blog post of my first contribution](http://blog.reddit.com/2010/11/thanks-hackers-in-both-senses-of-word.html) but I'm going over my [\\\"mark nsfw\\\" contribution here](https://github.com/reddit/reddit/pull/18).\\n\\nThe first thing I had to do was get the button in, otherwise I can't even test the code I'll be doing. So I did a grep for one of the buttons, in this case \\\"approve\\\". The first thing I got was **POST_approve** in api.py, which I figured was the actual code to handle approving the submission. I'd come to that later. Then a number of results appeared unrelated, so I narrowed it down by doing a grep for **\\\\\\\"approve\\\\\\\"** and sure enough I found data/templates/printablebuttons.html.py. A quick glance at it and it seems to be a \\\"compiled\\\" version, but grep also listed r2/templates/printablebuttons.html which looks to be the right place for buttons. A quick glance at the template code and it looks simple enough. I decide to use the yes/no confirmation button which report uses:\\n\\n    %elif thing.show_report:\\n      \\u003Cli\\u003E\\n        ${ynbutton(_(\\\"report\\\"), _(\\\"reported\\\"), \\\"report\\\", \\\"hide_thing\\\")}\\n      \\u003C/li\\u003E\\n    %endif\\n\\nAnd I create this code based on the report code in the same file:\\n\\n\\n    %if thing.show_marknsfw:\\n      \\u003Cli\\u003E${ynbutton(_(\\\"mark NSFW\\\"), _(\\\"marked NSFW\\\"), \\\"marknsfw\\\")}\\u003C/li\\u003E\\n    %endif\\n\\nAlong with the bool show_marknsfw, the last argument is clearly an identifier of some sort (since one of the buttons was \\\"hide_thing\\\"), so I have to find where that is defined now and create marknsfw and unmarknsfw. Again I'll use grep to search for an existing identifier; I picked \\\"indict\\\". Oh and I used grep like this **find . -exec grep -l \\\"indict\\\" {} \\\\;** I came across a number of strings and the sort (which is what I'm not looking for) and then found POST_indict in api.py and figured that must be it. An example of what I was looking over:\\n\\n      @noresponse(VUser(),\\n                  VModhash(),\\n                  thing = VByNameIfAuthor('id'))\\n      def POST_del(self, thing):\\n          if not thing: return\\n          '''for deleting all sorts of things'''\\n          thing._deleted = True\\n          if (getattr(thing, \\\"promoted\\\", None) is not None and\\n              not promote.is_promoted(thing)):\\n              promote.reject_promotion(thing)\\n          thing._commit()\\n\\n          # flag search indexer that something has changed\\n          changed(thing)\\n\\n          #expire the item from the sr cache\\n          if isinstance(thing, Link):\\n              sr = thing.subreddit_slow\\n              expire_hot(sr)\\n              queries.delete_links(thing)\\n\\n          #comments have special delete tasks\\n          elif isinstance(thing, Comment):\\n              thing._delete()\\n              delete_comment(thing)\\n              queries.new_comment(thing, None)\\n\\n I understood the Python in the code, but things like the noresponse() parts wasn't I knew offhand. Regardless I looked over a number of the functions and found that I needed to **thing._commit()** and **changed(thing)** (found in POST_del) when making a change.\\n\\nNext up I needed to find how to mark something NSFW. (I spent a bit working on appending the title of the submission with \\\"(NSFW)\\\" into the database, but later removed that to append it in the template). So once again I did a grep for \\\"nsfw\\\" (case insensitive) and found a line like this **item.nsfw = item.over_18 and user.pref_label_nsfw**, so it looks like bool values in a struct or the sort (originally I ended up setting just item.nsfw to true and that didn't work). So back in **api.py**, I create this code basing it off the other functions in the file:\\n\\n      @noresponse(VUser(),\\n                  VModhash(),\\n                  thing = VByName('id'))\\n      def POST_marknsfw(self, thing):\\n          thing.nsfw = True\\n          thing.over_18 = True\\n          thing._commit()\\n\\n          # flag search indexer that something has changed\\n          changed(thing)\\n\\nNext up is to figure out where to define show_marknsfw that defines when it'll show up. It looks like a \\\"method\\\" of thing that returns a bool.\\n\\nAgain, I used grep to look for show_report and found some assignments of it in **r2/lib/pages/things.py** and found that it's not so much a method, but a bool variable in the PrintableButtons class. There's a number of things done the file, such as initializing and determining the final value etc. Here are the changes I made:\\n\\n    def __init__(self, style, thing,\\n....\\n    -            show_distinguish = False,\\n    +            show_distinguish = False, show_marknsfw = True,\\n\\n...\\n\\n                     show_distinguish = show_distinguish,\\n    +                show_marknsfw = show_marknsfw,\\n\\nBasically following how the code is for similar vars. Next up is to set the actual bool to determine True/False. First was just simple logic to decide who could mark submissions NSFW. I picked mods, the submitter and admins. Did some looking around (like on the \\\"remove\\\" code that mods use) and came up with this code:\\n\\n        if (c.user_is_admin or thing.can_ban or is_author) and not thing.nsfw:\\n            show_marknsfw = True\\n        else:\\n            show_marknsfw = False\\n\\nNote: I also wrote code to unmark something NSFW, but didn't include it here because I didn't add the code until months later (just now).\\n\\nI think that's it, unless I forgot anything. It took me a number of days going through this and testing it. Maybe an admin can give the technical details of exactly what is going on here (like noresponse) but I found most of it self explanatory. \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"gx3lg\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"reseph\", \"media\": null, \"score\": 30, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/gx3lg/a_day_in_the_life_of_hacking_the_code/\", \"locked\": false, \"name\": \"t3_gx3lg\", \"created\": 1303780833.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/gx3lg/a_day_in_the_life_of_hacking_the_code/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"A day in the life of hacking the code.\", \"created_utc\": 1303752033.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 30}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIf I click on a link that is a picture, \\u003Ca href=\\\"http://i.imgur.com/LsXzq.png\\\"\\u003Ethis bar\\u003C/a\\u003E destroys most pictures for me. And if you zoom the site to hit that way too small x, the whole page reloads. My suggestion would be to minimize the bar by omitting the number of points and comments (who cares anyway?) and to decrease the opacity.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"If I click on a link that is a picture, [this bar](http://i.imgur.com/LsXzq.png) destroys most pictures for me. And if you zoom the site to hit that way too small x, the whole page reloads. My suggestion would be to minimize the bar by omitting the number of points and comments (who cares anyway?) and to decrease the opacity.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"eopdx\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"KETTENSAEGENBENZIN\", \"media\": null, \"score\": 32, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/eopdx/hey_reddit_please_fix_your_mobile_page/\", \"locked\": false, \"name\": \"t3_eopdx\", \"created\": 1292869031.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/eopdx/hey_reddit_please_fix_your_mobile_page/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Hey reddit, please fix your mobile page\", \"created_utc\": 1292840231.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 32}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ccode\\u003Edistinguished\\u003C/code\\u003E has been added to the JSON properties on submissions and comments, to allow determining whether they have been distinguished by moderators/admins.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPossible values are:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Enull\\u003C/code\\u003E - not distinguished\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Emoderator\\u003C/code\\u003E - the green [M]\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Eadmin\\u003C/code\\u003E - the red [A]\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Especial\\u003C/code\\u003E - various other special distinguishes (most commonly seen as the darker red [\\u0394] \\u0026quot;admin emeritus\\u0026quot; - \\u003Ca href=\\\"http://www.reddit.com/r/bestof/comments/175prt/alilarter_connects_with_a_user_who_has_a/c82tlns\\\"\\u003Eexample\\u003C/a\\u003E)\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/commit/c870dde2760f147bfc58c43a8b3e4da71d793df0\\\"\\u003Esee the code on github\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"`distinguished` has been added to the JSON properties on submissions and comments, to allow determining whether they have been distinguished by moderators/admins.\\n\\nPossible values are:\\n\\n* `null` - not distinguished\\n* `moderator` - the green [M]\\n* `admin` - the red [A]\\n* `special` - various other special distinguishes (most commonly seen as the darker red [\\u0394] \\\"admin emeritus\\\" - [example](http://www.reddit.com/r/bestof/comments/175prt/alilarter_connects_with_a_user_who_has_a/c82tlns))\\n\\n[see the code on github](https://github.com/reddit/reddit/commit/c870dde2760f147bfc58c43a8b3e4da71d793df0)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"19ak1b\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Deimorz\", \"media\": null, \"score\": 32, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1361924199.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/19ak1b/api_change_distinguished_is_now_available_in_the/\", \"locked\": false, \"name\": \"t3_19ak1b\", \"created\": 1361952487.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/19ak1b/api_change_distinguished_is_now_available_in_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API change: \\\"distinguished\\\" is now available in the JSON properties for submissions and comments\", \"created_utc\": 1361923687.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 32}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"redditlet.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"17sjc6\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"jtdaugh\", \"media\": null, \"score\": 32, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/17sjc6/hey_guys_check_out_a_bookmarklet_i_made_over_the/\", \"locked\": false, \"name\": \"t3_17sjc6\", \"created\": 1359895288.0, \"url\": \"http://redditlet.com\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Hey guys, check out a bookmarklet I made over the last week. It lets you see what Reddit has to say about anything on the internet without leaving the current page.\", \"created_utc\": 1359866488.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 32}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"gumrm\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"NorwegianMonkey\", \"media\": null, \"score\": 31, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/gumrm/could_someone_please_update_the_socialite_firefox/\", \"locked\": false, \"name\": \"t3_gumrm\", \"created\": 1303349834.0, \"url\": \"http://www.reddit.com/socialite/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Could someone please update the socialite firefox extention so that it works with firefox 4?\", \"created_utc\": 1303321034.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 31}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHere\\u0026#39;s the new links:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Ca href=\\\"http://sp.reddit.com/reddit-vm-vmware.tar.gz?torrent\\\"\\u003EVMWare\\u003C/a\\u003E -- MD5: 237f88bae195816b2c2b5645469dd35d\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ca href=\\\"http://sp.reddit.com/reddit-vm-ovf.tar.gz?torrent\\\"\\u003EOVF/VirtualBox\\u003C/a\\u003E -- MD5: 1da91c51f623bdec340f06430ba90bf9\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve updated the links in the blog post to point to both.  These are about 40% smaller than the previous one.  A huge amount of thanks to \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/c625v/questions_and_discussion_about_the_new_reddit_vm/c0qd7kt\\\"\\u003Egrotgrot\\u003C/a\\u003E for the how-to on cleaning it. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Here's the new links:\\n\\n * [VMWare](http://sp.reddit.com/reddit-vm-vmware.tar.gz?torrent) -- MD5: 237f88bae195816b2c2b5645469dd35d\\n * [OVF/VirtualBox](http://sp.reddit.com/reddit-vm-ovf.tar.gz?torrent) -- MD5: 1da91c51f623bdec340f06430ba90bf9\\n\\nI've updated the links in the blog post to point to both.  These are about 40% smaller than the previous one.  A huge amount of thanks to [grotgrot](http://www.reddit.com/r/redditdev/comments/c625v/questions_and_discussion_about_the_new_reddit_vm/c0qd7kt) for the how-to on cleaning it. \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"c6gf9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"KeyserSosa\", \"media\": null, \"score\": 32, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 22, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/c6gf9/updated_vms_are_now_available_they_fix_the/\", \"locked\": false, \"name\": \"t3_c6gf9\", \"created\": 1274418525.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/c6gf9/updated_vms_are_now_available_they_fix_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Updated VMs are now available.  They fix the networking issues (I hope) and one uses OVF for use with VirtualBox\", \"created_utc\": 1274389725.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 32}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ETo my knowledge right now, there is no way to determine if a sub is banned or quarantined, likewise private or gold only. The first two have \\u003Ccode\\u003E{\\u0026quot;error\\u0026quot;:404}\\u003C/code\\u003E in their json, the others have the same but 403.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhy can\\u0026#39;t we have some proper info as to what is what, so in an app, we can throw back appropriate errors, such as, \\u0026quot;Sorry, this subbreddit has been banned\\u0026quot; or \\u0026quot;Sorry, this subreddit has been quarantined. To view it, you must be logged in, have a verified email, and accept to view it at \\u0026lt;link\\u0026gt; before returning to \\u0026lt;name of app\\u0026gt;, and likewise \\u0026quot;Sorry, this subreddit is private\\u0026quot; or \\u0026quot;Sorry, this subreddit is gold only and requires \\u003Ca href=\\\"/about/gold\\\"\\u003Ereddit gold\\u003C/a\\u003E\\u0026quot;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI can somewhat understand the issue for a quarantined sub, but for the rest (and I feel, even for a quarantined sub) we should be able to handle such issues.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: spelling and grammar\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit2: Also apparently Not Safe For Work subs have the same issue, error 404 on the json.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI wouldn\\u0026#39;t even mind if all that is differed is that they each have a distinct number, (at least for the five main types, NSFW, Quarantined, Banned, Private, Gold Only, if not the two other types that \\u003Ca href=\\\"/u/GoldenSights\\\"\\u003E/u/GoldenSights\\u003C/a\\u003E informed me of of Gold Restricted (gold needed to post, an identifier only needed to restrict posting via the API for apps, if and only \\u003Ca href=\\\"/r/goldbenefits\\\"\\u003E/r/goldbenefits\\u003C/a\\u003E isn\\u0026#39;t the only sub like this) and Employees Only).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMaybe even keep the error but just add something to each JSON like\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E{restriction:\\u0026quot;None\\u0026quot;} - Public\\n{restriction:\\u0026quot;Restricted\\u0026quot;} - Restricted\\n{restriction:\\u0026quot;Private\\u0026quot;} - Private\\n{restriction:\\u0026quot;Gold_Restricted\\u0026quot;} - Gold Restricted (only necessary if there is more than one total sub, ie /r/goldbenefits)\\n{restriction:\\u0026quot;Gold_Only\\u0026quot;} - Gold Only\\n{restriction:\\u0026quot;NSFW\\u0026quot;} - NSFW\\n{restriction:\\u0026quot;Quarantined\\u0026quot;} - Quarantined\\n{restriction:\\u0026quot;Full\\u0026quot;} - Banned\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENote: I have no idea how NSFW subs that are also Restricted to Private would work on this, maybe instead \\u003Ccode\\u003E{restriction:$restriction, NSFW:\\u0026quot;0\\u0026quot;}\\u003C/code\\u003E (or 1 for True)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit 3: Post has been up for around a day now. I\\u0026#39;m guessing no one is commenting since they don\\u0026#39;t have a workaround, and so far I\\u0026#39;ve no \\u0026quot;official\\u0026quot; (if that\\u0026#39;s even the proper term for these matters) response either as to if anything would change.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"To my knowledge right now, there is no way to determine if a sub is banned or quarantined, likewise private or gold only. The first two have `{\\\"error\\\":404}` in their json, the others have the same but 403.\\n\\nWhy can't we have some proper info as to what is what, so in an app, we can throw back appropriate errors, such as, \\\"Sorry, this subbreddit has been banned\\\" or \\\"Sorry, this subreddit has been quarantined. To view it, you must be logged in, have a verified email, and accept to view it at \\u003Clink\\u003E before returning to \\u003Cname of app\\u003E, and likewise \\\"Sorry, this subreddit is private\\\" or \\\"Sorry, this subreddit is gold only and requires [reddit gold](/about/gold)\\\"\\n\\nI can somewhat understand the issue for a quarantined sub, but for the rest (and I feel, even for a quarantined sub) we should be able to handle such issues.\\n\\nEdit: spelling and grammar\\n\\nEdit2: Also apparently Not Safe For Work subs have the same issue, error 404 on the json.\\n\\nI wouldn't even mind if all that is differed is that they each have a distinct number, (at least for the five main types, NSFW, Quarantined, Banned, Private, Gold Only, if not the two other types that /u/GoldenSights informed me of of Gold Restricted (gold needed to post, an identifier only needed to restrict posting via the API for apps, if and only /r/goldbenefits isn't the only sub like this) and Employees Only).\\n\\nMaybe even keep the error but just add something to each JSON like\\n\\n    {restriction:\\\"None\\\"} - Public\\n    {restriction:\\\"Restricted\\\"} - Restricted\\n    {restriction:\\\"Private\\\"} - Private\\n    {restriction:\\\"Gold_Restricted\\\"} - Gold Restricted (only necessary if there is more than one total sub, ie /r/goldbenefits)\\n    {restriction:\\\"Gold_Only\\\"} - Gold Only\\n    {restriction:\\\"NSFW\\\"} - NSFW\\n    {restriction:\\\"Quarantined\\\"} - Quarantined\\n    {restriction:\\\"Full\\\"} - Banned\\n\\nNote: I have no idea how NSFW subs that are also Restricted to Private would work on this, maybe instead `{restriction:$restriction, NSFW:\\\"0\\\"}` (or 1 for True)\\n\\nEdit 3: Post has been up for around a day now. I'm guessing no one is commenting since they don't have a workaround, and so far I've no \\\"official\\\" (if that's even the proper term for these matters) response either as to if anything would change.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3g3hu2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"13steinj\", \"media\": null, \"score\": 30, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1439010268.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3g3hu2/is_there_any_way_to_accurately_determine_whether/\", \"locked\": false, \"name\": \"t3_3g3hu2\", \"created\": 1438953155.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3g3hu2/is_there_any_way_to_accurately_determine_whether/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there any way to accurately determine whether the sub is banned, quarantined, private, or gold only via the API?\", \"created_utc\": 1438924355.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 30}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWe\\u0026#39;ve upgraded our version of jQuery to the latest (1.11), and in doing so, we\\u0026#39;ve found at least one old plugin that relies on reddit\\u0026#39;s jQuery to be available. (It was last updated several years ago.)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you create your own extensions, please package your own version of jQuery to avoid brokenness when we do upgrades in the future.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIn addition, if you notice brokenness, let me know! (Or if you\\u0026#39;ve noticed that things are remarkably fast and smooth.)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"We've upgraded our version of jQuery to the latest (1.11), and in doing so, we've found at least one old plugin that relies on reddit's jQuery to be available. (It was last updated several years ago.)\\n\\nIf you create your own extensions, please package your own version of jQuery to avoid brokenness when we do upgrades in the future.\\n\\nIn addition, if you notice brokenness, let me know! (Or if you've noticed that things are remarkably fast and smooth.)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"268g9y\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ajacksified\", \"media\": null, \"score\": 31, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 31, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/268g9y/jquery_upgrade_psa/\", \"locked\": false, \"name\": \"t3_268g9y\", \"created\": 1400815146.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/268g9y/jquery_upgrade_psa/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"jQuery Upgrade PSA\", \"created_utc\": 1400786346.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 31}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EFiresheep has reminded everyone about the risks, and a lot of websites are moving to https only (for example, github moved yesterday.) \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat is stopping reddit from moving to https for logged in users? Is it a just a lack of time, performance issue, or a problem with how the CDN is setup?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Firesheep has reminded everyone about the risks, and a lot of websites are moving to https only (for example, github moved yesterday.) \\n\\nWhat is stopping reddit from moving to https for logged in users? Is it a just a lack of time, performance issue, or a problem with how the CDN is setup?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"e1glq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"phire\", \"media\": null, \"score\": 26, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 18, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/e1glq/so_what_is_actually_stopping_reddit_from_going_to/\", \"locked\": false, \"name\": \"t3_e1glq\", \"created\": 1288946231.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/e1glq/so_what_is_actually_stopping_reddit_from_going_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"So what is actually stopping reddit from going to https?\", \"created_utc\": 1288917431.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 26}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ETo go with \\u003Ca href=\\\"http://www.reddit.com/r/modnews/comments/1dd0xw/moderators_new_subreddit_feature_comment_scores/\\\"\\u003Ethe new ability for subreddits to hide comment scores initially\\u003C/a\\u003E, I\\u0026#39;ve made a couple of updates today that will allow API clients and browser addons to support it properly (instead of just showing 1 upvote and 0 downvotes while hidden):\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Escore_hidden\\u003C/code\\u003E is now available through the API for comments\\u003C/li\\u003E\\n\\u003Cli\\u003EIn the HTML, a comment with a hidden score will have the class \\u003Ccode\\u003Escore-hidden\\u003C/code\\u003E, and both \\u003Ccode\\u003Edata-ups\\u003C/code\\u003E and \\u003Ccode\\u003Edata-downs\\u003C/code\\u003E will be zero.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/commit/8eac5bf1e6141cb7cebd38876f61479e2bde0ed8\\\"\\u003ESee the code on github\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"To go with [the new ability for subreddits to hide comment scores initially](http://www.reddit.com/r/modnews/comments/1dd0xw/moderators_new_subreddit_feature_comment_scores/), I've made a couple of updates today that will allow API clients and browser addons to support it properly (instead of just showing 1 upvote and 0 downvotes while hidden):\\n\\n* `score_hidden` is now available through the API for comments\\n* In the HTML, a comment with a hidden score will have the class `score-hidden`, and both `data-ups` and `data-downs` will be zero.\\n\\n[See the code on github](https://github.com/reddit/reddit/commit/8eac5bf1e6141cb7cebd38876f61479e2bde0ed8)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1dfdpp\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Deimorz\", \"media\": null, \"score\": 31, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1367348844.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1dfdpp/api_change_score_hidden_added_to_comments_and/\", \"locked\": false, \"name\": \"t3_1dfdpp\", \"created\": 1367375793.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1dfdpp/api_change_score_hidden_added_to_comments_and/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API change: \\\"score_hidden\\\" added to comments (and some HTML changes as well)\", \"created_utc\": 1367346993.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 31}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Ereddit the open source project is flourshing.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIn 2012, we\\u0026#39;ve had \\u003Cstrong\\u003Ealmost 50\\u003C/strong\\u003E distinct contributors\\u003Ca href=\\\"#tooltiptime\\\" title=\\\"note: the reddit.com tech team is only nine people.\\\"\\u003E*\\u003C/a\\u003E to reddit\\u0026#39;s source code. That\\u0026#39;s over 50% more than last year. There\\u0026#39;ve also been over 1,600 total commits to the reddit repository during 2012. Daaaaaamn.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe wanted to take this opportunity at the end of such a great year to say thank you to everyone who has taken part in this, whether it be through patches, wiki documentation, API libraries, client apps, useful bots, answering questions or even asking questions, and everyone else who uses the site. You all make it awesome.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESome yearly stats for fun:\\u003C/p\\u003E\\n\\n\\u003Ctable\\u003E\\u003Cthead\\u003E\\n\\u003Ctr\\u003E\\n\\u003Cth\\u003Eyear\\u003C/th\\u003E\\n\\u003Cth\\u003Edistinct authors\\u003C/th\\u003E\\n\\u003Cth\\u003Etotal commits\\u003C/th\\u003E\\n\\u003C/tr\\u003E\\n\\u003C/thead\\u003E\\u003Ctbody\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd\\u003E2007\\u003C/td\\u003E\\n\\u003Ctd\\u003E3\\u003C/td\\u003E\\n\\u003Ctd\\u003E795\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd\\u003E2008\\u003C/td\\u003E\\n\\u003Ctd\\u003E5\\u003C/td\\u003E\\n\\u003Ctd\\u003E599\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd\\u003E2009\\u003C/td\\u003E\\n\\u003Ctd\\u003E7\\u003C/td\\u003E\\n\\u003Ctd\\u003E435\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd\\u003E2010\\u003C/td\\u003E\\n\\u003Ctd\\u003E14\\u003C/td\\u003E\\n\\u003Ctd\\u003E1,175\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd\\u003E2011\\u003C/td\\u003E\\n\\u003Ctd\\u003E31\\u003C/td\\u003E\\n\\u003Ctd\\u003E885\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd\\u003E2012\\u003C/td\\u003E\\n\\u003Ctd\\u003E48\\u003C/td\\u003E\\n\\u003Ctd\\u003E1,615\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003C/tbody\\u003E\\u003C/table\\u003E\\n\\n\\u003Cp\\u003EThank you for being a part of this.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"reddit the open source project is flourshing.\\n\\nIn 2012, we've had **almost 50** distinct contributors[*](#tooltiptime \\\"note: the reddit.com tech team is only nine people.\\\") to reddit's source code. That's over 50% more than last year. There've also been over 1,600 total commits to the reddit repository during 2012. Daaaaaamn.\\n\\nWe wanted to take this opportunity at the end of such a great year to say thank you to everyone who has taken part in this, whether it be through patches, wiki documentation, API libraries, client apps, useful bots, answering questions or even asking questions, and everyone else who uses the site. You all make it awesome.\\n\\nSome yearly stats for fun:\\n\\nyear|distinct authors|total commits\\n--|--|--\\n2007|3|795\\n2008|5|599\\n2009|7|435\\n2010|14|1,175\\n2011|31|885\\n2012|48|1,615\\n\\nThank you for being a part of this.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"15qtwh\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"spladug\", \"media\": null, \"score\": 30, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/15qtwh/congrats_on_a_banner_year_in_2012_rredditdev/\", \"locked\": false, \"name\": \"t3_15qtwh\", \"created\": 1357021341.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/15qtwh/congrats_on_a_banner_year_in_2012_rredditdev/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Congrats on a banner year in 2012, /r/redditdev!\", \"created_utc\": 1356992541.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 30}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve uploaded a pre-packaged reddit VM for use at the PyCon sprint, but first-timers may find it useful on its own.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://s3.amazonaws.com/kemitche/reddit.box?torrent\\\"\\u003EGet it here (torrent)\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s packaged up as a \\u003Ca href=\\\"http://vagrantup.com\\\"\\u003E\\u0026quot;vagrant\\u0026quot;\\u003C/a\\u003E box, but you can also un-tar it to get to the VMDK disk and OVF file.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: \\u003Ca href=\\\"/u/dakta\\\"\\u003E/u/dakta\\u003C/a\\u003E has pointed out some obviously helpful and much needed info:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003EIf the VM goes into \\u0026quot;Guru Meditation\\u0026quot;, you\\u0026#39;ve probably maxed out your available system memory. Go into the VM settings are tone it down until VirtualBox says it\\u0026#39;s OK.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf the VM doesn\\u0026#39;t boot after that, you need to enable PAE/NX under Settings\\u0026gt;System\\u0026gt;Processor.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFinally, I had to set the VM to open up with a GUI screen to let me select what Ubuntu shit to boot using grub.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor reference, since kemitche didn\\u0026#39;t say so, the default user is \\u0026quot;reddit\\u0026quot; passwd \\u0026quot;reddit\\u0026quot;... I don\\u0026#39;t know how root is really set up, but sudo for \\u0026quot;reddit\\u0026quot; is the Vagrant default of passwordless, so if you really must change root password just sudo passwd root, although I don\\u0026#39;t suggest it.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs expected, reddit is installed already and rearin\\u0026#39; to go. Just point your host machine at localhost:8000 to get to the reddit install. SSH is forwarded on port 2222, so you can ssh reddit@localhost:2222 and connect through that instead.\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EIn addition to manually tweaking the VM, if using vagrant, you can modify the VM with \\u003Ca href=\\\"http://vagrantup.com/docs/config/vm/customize.html\\\"\\u003Econfig.vm.customize\\u003C/a\\u003E commands:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Econfig.vm.customize [\\u0026quot;modifyvm\\u0026quot;, :id, \\u0026quot;--memory\\u0026quot;, \\u0026quot;1024\\u0026quot;] # quotes around the number needed; RAM size in MB\\nconfig.vm.customize [\\u0026quot;modifyvm\\u0026quot;, :id, \\u0026quot;--pae\\u0026quot;, \\u0026quot;on\\u0026quot;]\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've uploaded a pre-packaged reddit VM for use at the PyCon sprint, but first-timers may find it useful on its own.\\n\\n[Get it here (torrent)](http://s3.amazonaws.com/kemitche/reddit.box?torrent)\\n\\nIt's packaged up as a [\\\"vagrant\\\"](http://vagrantup.com) box, but you can also un-tar it to get to the VMDK disk and OVF file.\\n\\nEDIT: /u/dakta has pointed out some obviously helpful and much needed info:\\n\\n\\u003E If the VM goes into \\\"Guru Meditation\\\", you've probably maxed out your available system memory. Go into the VM settings are tone it down until VirtualBox says it's OK.\\n\\n\\u003E If the VM doesn't boot after that, you need to enable PAE/NX under Settings\\u003ESystem\\u003EProcessor.\\n\\n\\u003E Finally, I had to set the VM to open up with a GUI screen to let me select what Ubuntu shit to boot using grub.\\n\\n\\u003E For reference, since kemitche didn't say so, the default user is \\\"reddit\\\" passwd \\\"reddit\\\"... I don't know how root is really set up, but sudo for \\\"reddit\\\" is the Vagrant default of passwordless, so if you really must change root password just sudo passwd root, although I don't suggest it.\\n\\u003E\\n\\u003E As expected, reddit is installed already and rearin' to go. Just point your host machine at localhost:8000 to get to the reddit install. SSH is forwarded on port 2222, so you can ssh reddit@localhost:2222 and connect through that instead.\\n\\nIn addition to manually tweaking the VM, if using vagrant, you can modify the VM with [config.vm.customize](http://vagrantup.com/docs/config/vm/customize.html) commands:\\n\\n    config.vm.customize [\\\"modifyvm\\\", :id, \\\"--memory\\\", \\\"1024\\\"] # quotes around the number needed; RAM size in MB\\n    config.vm.customize [\\\"modifyvm\\\", :id, \\\"--pae\\\", \\\"on\\\"]\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"qnuxp\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 29, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/qnuxp/pycon_prepackaged_vm/\", \"locked\": false, \"name\": \"t3_qnuxp\", \"created\": 1331270156.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/qnuxp/pycon_prepackaged_vm/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"PyCon pre-packaged VM\", \"created_utc\": 1331241356.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 29}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"m83gi\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"chromakode\", \"media\": null, \"score\": 27, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/m83gi/reddit_developers_improved_json_api_access_to/\", \"locked\": false, \"name\": \"t3_m83gi\", \"created\": 1321000146.0, \"url\": \"http://www.reddit.com/r/changelog/comments/m80oy/reddit_change_improved_json_api_access_to_user/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Developers: Improved JSON API access to user lists: friends, subreddit moderators, subreddit contributors, and more \", \"created_utc\": 1320971346.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 27}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ii4q2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 29, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ii4q2/spladug_explains_reddit_comments/\", \"locked\": false, \"name\": \"t3_ii4q2\", \"created\": 1309991873.0, \"url\": \"http://www.reddit.com/r/help/comments/ihpra/how_do_reddit_comments_work_scripting_wise/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"spladug explains reddit comments\", \"created_utc\": 1309963073.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 29}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m unable to change my password. It says \\u0026quot;invalid password\\u0026#39; for the current (to-be-changed) password, even though I\\u0026#39;m perfectly able to log in with it.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI seem not to be authorized to submit a bug report under fixxit. (\\u0026quot;Logged in users \\u003Cem\\u003Ewith sufficient Reddit history\\u003C/em\\u003E can edit all the wiki pages and submit tickets for bug reports\\u0026quot;, emphasis mine.) I searched for tickets containing \\u0026quot;password\\u0026quot;, but all such bug tickets are struck-through.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI tried to use \\u003Ca href=\\\"http://www.reddit.com/feedback/\\\"\\u003Ehttp://www.reddit.com/feedback/\\u003C/a\\u003E but got no reply.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease help! Thanks.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi,\\n\\nI'm unable to change my password. It says \\\"invalid password' for the current (to-be-changed) password, even though I'm perfectly able to log in with it.\\n\\nI seem not to be authorized to submit a bug report under fixxit. (\\\"Logged in users *with sufficient Reddit history* can edit all the wiki pages and submit tickets for bug reports\\\", emphasis mine.) I searched for tickets containing \\\"password\\\", but all such bug tickets are struck-through.\\n\\nI tried to use http://www.reddit.com/feedback/ but got no reply.\\n\\nPlease help! Thanks.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"9o2mm\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"lbzip2\", \"media\": null, \"score\": 29, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/9o2mm/im_unable_to_change_my_password/\", \"locked\": false, \"name\": \"t3_9o2mm\", \"created\": 1253925085.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/9o2mm/im_unable_to_change_my_password/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I'm unable to change my password.\", \"created_utc\": 1253896285.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 29}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Cstrong\\u003ETLDR;\\u003C/strong\\u003E \\u003Ca href=\\\"https://github.com/avinassh/prawoauth2\\\"\\u003EGithub\\u003C/a\\u003E | \\u003Ca href=\\\"https://github.com/avinassh/prawoauth2/tree/master/examples/halflife3-bot\\\"\\u003EExample\\u003C/a\\u003E | \\u003Ca href=\\\"https://pypi.python.org/pypi/prawoauth2/\\\"\\u003Epypi\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EWhat it does?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EIt makes your life super easy for handling OAuth with Praw \\u003C/li\\u003E\\n\\u003Cli\\u003EIt can get you new \\u003Ccode\\u003Eaccess_token\\u003C/code\\u003E and \\u003Ccode\\u003Erefresh_token\\u003C/code\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003EIt can \\u0026#39;refresh\\u0026#39; your praw instance to use new tokens\\u003C/li\\u003E\\n\\u003Cli\\u003EYou don\\u0026#39;t really need to worry about token expiry\\u003C/li\\u003E\\n\\u003Cli\\u003EBest used for bots, which run on Heroku, AWS etc. It will help you run your bot forever! \\u003C/li\\u003E\\n\\u003Cli\\u003EComes with nice documentation and a working example you can play with!\\u003C/li\\u003E\\n\\u003Cli\\u003EIt\\u0026#39;s open source and released under MIT License! (:\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EInstallation:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Epip install prawoauth2\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EInfo:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003Eprawoauth2\\u003C/code\\u003E comes with two components, \\u003Ccode\\u003EPrawOAuth2Mini\\u003C/code\\u003E and \\u003Ccode\\u003EPrawOAuth2Server\\u003C/code\\u003E. \\u003Ccode\\u003EPrawOAuth2Server\\u003C/code\\u003E authorizes your app/script with the Reddit account and gives you access token. \\u003Ccode\\u003EPrawOAuth2Mini\\u003C/code\\u003E uses these tokens for all next transactions with Reddit. Remember, for a bot, you only need \\u003Ccode\\u003Eaccess_token\\u003C/code\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhy it is written like that? If you are writing a bot and running it on a headless server, something like Amazon AWS or Openshift, you cannot authorize your script with the Reddit account, as it tries to open the browser for authorization. This is one time only operation(if you pass the parameter \\u003Ccode\\u003Epermanent\\u003C/code\\u003E). So, I decided to break this into two parts. Run the \\u003Ccode\\u003EPrawOAuth2Server\\u003C/code\\u003E locally on your computer, get the \\u003Ccode\\u003Eaccess_token\\u003C/code\\u003E, \\u003Ccode\\u003Erefresh_token\\u003C/code\\u003E  and save them somewhere. Later, \\u003Ccode\\u003EPrawOAuth2Mini\\u003C/code\\u003E can make use of these tokens for further transactions. And it does not require browser at all, so that it can run in a headless server without any hiccups.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003ETLDR;\\u003C/strong\\u003E \\u003Ccode\\u003EPrawOAuth2Server\\u003C/code\\u003E meant to be run only once locally on your main machine to fetch the first \\u003Ccode\\u003Eaccess_token\\u003C/code\\u003E, \\u003Ccode\\u003Erefresh_token\\u003C/code\\u003E and \\u003Ccode\\u003EPrawOAuth2Mini\\u003C/code\\u003E later and for everything. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"**TLDR;** [Github](https://github.com/avinassh/prawoauth2) | [Example](https://github.com/avinassh/prawoauth2/tree/master/examples/halflife3-bot) | [pypi](https://pypi.python.org/pypi/prawoauth2/)\\n\\n--- \\n\\n**What it does?**\\n\\n- It makes your life super easy for handling OAuth with Praw \\n- It can get you new `access_token` and `refresh_token`\\n- It can 'refresh' your praw instance to use new tokens\\n- You don't really need to worry about token expiry\\n- Best used for bots, which run on Heroku, AWS etc. It will help you run your bot forever! \\n- Comes with nice documentation and a working example you can play with!\\n- It's open source and released under MIT License! (:\\n\\n**Installation:**\\n\\n    pip install prawoauth2\\n\\n**Info:**\\n\\n`prawoauth2` comes with two components, `PrawOAuth2Mini` and `PrawOAuth2Server`. `PrawOAuth2Server` authorizes your app/script with the Reddit account and gives you access token. `PrawOAuth2Mini` uses these tokens for all next transactions with Reddit. Remember, for a bot, you only need `access_token`.\\n\\nWhy it is written like that? If you are writing a bot and running it on a headless server, something like Amazon AWS or Openshift, you cannot authorize your script with the Reddit account, as it tries to open the browser for authorization. This is one time only operation(if you pass the parameter `permanent`). So, I decided to break this into two parts. Run the `PrawOAuth2Server` locally on your computer, get the `access_token`, `refresh_token`  and save them somewhere. Later, `PrawOAuth2Mini` can make use of these tokens for further transactions. And it does not require browser at all, so that it can run in a headless server without any hiccups.\\n\\n**TLDR;** `PrawOAuth2Server` meant to be run only once locally on your main machine to fetch the first `access_token`, `refresh_token` and `PrawOAuth2Mini` later and for everything. \\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ckq5a\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"avinassh\", \"media\": null, \"score\": 30, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 27, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1436378230.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/\", \"locked\": false, \"name\": \"t3_3ckq5a\", \"created\": 1436406570.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Hi, I released a helper module for Praw, which makes writing OAuth bots super easy and fun\", \"created_utc\": 1436377770.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 30}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Ereddit uses the same code for thumbnails (on links) and subreddit images (stylesheets and wiki pages). This was previously hard coded to use Amazon S3. S3 has treated reddit.com really well over the years, and we\\u0026#39;ll be continuing to use it. However, it was also a frequent request from maintainers of sites based on reddit\\u0026#39;s code (and people who want to test stuff locally) for there to be a way to configure what system to use for hosting these images.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve now made this possible through a modular system that we\\u0026#39;ll hopefully be using for other dependencies down the road. New installs of reddit based on the install script will write their images to a local directory and serve them with nginx. Using S3 like reddit.com would require only some configuration file changes. If an alternate system altogether is desired, an implementation can be written in external code and hooked into reddit via the setuptools \\u003Ccode\\u003Eentry_points\\u003C/code\\u003E mechanism.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor more details, check out \\u003Ca href=\\\"https://github.com/reddit/reddit/tree/8f7db4f3da0b0cf03f2a94b7276169d8bdae46af/r2/r2/lib/providers/media\\\"\\u003Ethe media providers section of the code\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"reddit uses the same code for thumbnails (on links) and subreddit images (stylesheets and wiki pages). This was previously hard coded to use Amazon S3. S3 has treated reddit.com really well over the years, and we'll be continuing to use it. However, it was also a frequent request from maintainers of sites based on reddit's code (and people who want to test stuff locally) for there to be a way to configure what system to use for hosting these images.\\n\\nI've now made this possible through a modular system that we'll hopefully be using for other dependencies down the road. New installs of reddit based on the install script will write their images to a local directory and serve them with nginx. Using S3 like reddit.com would require only some configuration file changes. If an alternate system altogether is desired, an implementation can be written in external code and hooked into reddit via the setuptools `entry_points` mechanism.\\n\\nFor more details, check out [the media providers section of the code](https://github.com/reddit/reddit/tree/8f7db4f3da0b0cf03f2a94b7276169d8bdae46af/r2/r2/lib/providers/media).\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ogd6d\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"spladug\", \"media\": null, \"score\": 29, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1381786811.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ogd6d/maintainers_of_redditbased_sites_the/\", \"locked\": false, \"name\": \"t3_1ogd6d\", \"created\": 1381814673.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1ogd6d/maintainers_of_redditbased_sites_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Maintainers of reddit-based sites: the thumbnail/subreddit image system can now use different backends instead of just Amazon S3.\", \"created_utc\": 1381785873.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 29}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAs I type this, there\\u0026#39;s \\u003Ca href=\\\"http://www.reddit.com/r/AskReddit/comments/eaiiv/so_reddit_what_is_one_thing_you_think_everyone/\\\"\\u003Ea topic\\u003C/a\\u003E with 6872 comments on the front page. In the past, things like this have brought reddit to its knees. What\\u0026#39;s changed that is making it work so much better?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"As I type this, there's [a topic](http://www.reddit.com/r/AskReddit/comments/eaiiv/so_reddit_what_is_one_thing_you_think_everyone/) with 6872 comments on the front page. In the past, things like this have brought reddit to its knees. What's changed that is making it work so much better?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"eavt0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"NegativeK\", \"media\": null, \"score\": 27, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/eavt0/why_is_reddit_doing_so_much_better/\", \"locked\": false, \"name\": \"t3_eavt0\", \"created\": 1290596063.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/eavt0/why_is_reddit_doing_so_much_better/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why is reddit doing so much better?\", \"created_utc\": 1290567263.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 27}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWe\\u0026#39;ve made a change today to make it so OAuth requests originating from within places like EC2, Azure, GCE, Rackspace, etc. should be much faster. Previously those were handled with very low priority, as a majority of the traffic from those sources is from spammers or scrapers.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis is hopefully another reason to move towards OAuth and away from cookie authentication for any web developers out there! Please comment if you\\u0026#39;re still seeing slow requests (i.e. 5s when posting to access_token) and we\\u0026#39;ll help you troubleshoot.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"We've made a change today to make it so OAuth requests originating from within places like EC2, Azure, GCE, Rackspace, etc. should be much faster. Previously those were handled with very low priority, as a majority of the traffic from those sources is from spammers or scrapers.\\n\\nThis is hopefully another reason to move towards OAuth and away from cookie authentication for any web developers out there! Please comment if you're still seeing slow requests (i.e. 5s when posting to access_token) and we'll help you troubleshoot.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3f3118\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"gooeyblob\", \"media\": null, \"score\": 27, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 21, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3f3118/oauth_requests_from_within_hosting_providers/\", \"locked\": false, \"name\": \"t3_3f3118\", \"created\": 1438235591.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3f3118/oauth_requests_from_within_hosting_providers/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OAuth requests from within hosting providers should now be much faster\", \"created_utc\": 1438206791.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 27}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"fsf.org\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1twnb2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"csolisr\", \"media\": null, \"score\": 25, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1twnb2/the_fsf_is_trying_to_make_its_librejs_plugin_work/\", \"locked\": false, \"name\": \"t3_1twnb2\", \"created\": 1388302461.0, \"url\": \"https://www.fsf.org/blogs/community/freejs-reddit\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"The FSF is trying to make its LibreJS plugin work with Reddit - it just needs to label the JS files properly and change a few non-freely-licensed libraries. Can you lend us a hand?\", \"created_utc\": 1388273661.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 25}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/commit/071986e0c7bab07f1c39d37335aa70875e0940c4\\\"\\u003EMy commit may be small\\u003C/a\\u003E, but now I know that 2% of every page on Reddit has my code in it. Every bit counts!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"[My commit may be small](https://github.com/reddit/reddit/commit/071986e0c7bab07f1c39d37335aa70875e0940c4), but now I know that 2% of every page on Reddit has my code in it. Every bit counts!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ifjl1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Dominoed\", \"media\": null, \"score\": 26, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ifjl1/today_i_received_the_open_sorcerer_trophy/\", \"locked\": false, \"name\": \"t3_1ifjl1\", \"created\": 1374029738.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1ifjl1/today_i_received_the_open_sorcerer_trophy/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Today I received the 'Open Sorcerer' trophy.\", \"created_utc\": 1374000938.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 26}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1evezm\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"chromakode\", \"media\": null, \"score\": 28, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1evezm/the_code_for_our_new_beta_system_in_use_at/\", \"locked\": false, \"name\": \"t3_1evezm\", \"created\": 1369298311.0, \"url\": \"https://github.com/reddit/reddit-plugin-betamode\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"The code for our new beta system (in use at /r/multibeta) is now available on GitHub.\", \"created_utc\": 1369269511.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 28}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"209.208.27.225\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1eipl7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 26, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1eipl7/i_am_creating_a_reddit_api_backend_that_will/\", \"locked\": false, \"name\": \"t3_1eipl7\", \"created\": 1368830501.0, \"url\": \"http://209.208.27.225/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I am creating a Reddit API backend that will allow for large data dumps including submissions and comments. This is an example of some real-time info for Reddit.\", \"created_utc\": 1368801701.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 26}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/fdhlw/a_beginners_guide_to_the_reddit_source_code_part/\\\"\\u003E\\u0026lt;---Part 1\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EIntroduction\\u003C/strong\\u003E \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease criticize and correct!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELast time, we discussed Pylons and the MVC architecture and how the Pylons controllers directs the client to the proper web page. Now we will discuss the \\u0026quot;view\\u0026quot; portion of the MVC, how the web pages are displayed.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EMako\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPylons uses a library called \\u003Ca href=\\\"http://www.makotemplates.org/\\\"\\u003EMako\\u003C/a\\u003E to generate HTML pages. If you are familiar with other server-side HTML renderers, like PHP and JSP, this should be very familiar to you. The basic structure involves writing a plain HTML page with special tags for Python variables and special PHP-like brackets for Python scripts. In Pylons, all of the templates are stored in the \\u003Ca href=\\\"https://github.com/reddit/reddit/tree/master/r2/r2/templates\\\"\\u003E/templates\\u003C/a\\u003E directory.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESome basic Mako structures:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E*Lines that start with a hash (#) are comments.\\u003Cbr/\\u003E\\n*Lines that start with percentage (%) are Python code.\\u003Cbr/\\u003E\\n*Python code can go anywhere between a \\u0026lt;% and a %\\u0026gt;.\\u003Cbr/\\u003E\\n*Include statements are defined with the \\u003Ccode\\u003E\\u0026lt;%include file=\\u0026quot;filename\\u0026quot;\\u0026gt;\\u003C/code\\u003E tag, where \\u0026quot;filename\\u0026quot; is the name of another template.\\u003Cbr/\\u003E\\n*Functions can be defined with\\u003Cbr/\\u003E\\n     \\u0026lt;%def name=\\u0026quot;functionName(params)\\u0026quot;\\u0026gt;\\n     function stuff\\u003Cbr/\\u003E\\n     \\u0026lt;/%def\\u0026gt;\\u003Cbr/\\u003E\\n*A function can then be called with \\u003Ccode\\u003E${functionName(params)}\\u003C/code\\u003E\\u003Cbr/\\u003E\\n*Variables can be defined with \\u003Ccode\\u003E${variableName}\\u003C/code\\u003E and can be set in the Python code before the template is rendered (more on that later).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe basic method for rendering templates is the (drumroll...) \\u003Ccode\\u003Etemplate.render()\\u003C/code\\u003E call. Whenever this is called it returns a template rendered as plain HTML. The render function takes in the form of the variables set in the template. You set the variables by naming them explicitly in the function, i.e. \\u003Ccode\\u003Erender(var1=\\u0026quot;Hello\\u0026quot;, var2=\\u0026quot;world\\u0026quot;)\\u003C/code\\u003E. In general, the standard way to call the render function is to pass it as a string to the Templater constructor, like so:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E Template(\\u0026quot;\\u0026lt;html\\u0026gt;\\u0026lt;head\\u0026gt;\\u0026lt;title\\u0026gt;${title}\\u0026lt;/title\\u0026gt;\\u0026lt;/head\\u0026gt;\\u0026lt;body\\u0026gt;${someText}  \\n\\u0026lt;/body\\u0026gt;\\u0026lt;/html\\u0026gt;\\u0026quot;).render(title=\\u0026quot;Hello Templates!\\u0026quot;, someText=\\u0026quot;Blah de blah\\u0026quot;)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThis outputs a  simple HTML page:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026lt;html\\u0026gt;\\n    \\u0026lt;head\\u0026gt;\\u0026lt;title\\u0026gt;Hello Templates!\\u0026lt;/title\\u0026gt;\\u0026lt;/head\\u0026gt;\\n    \\u0026lt;body\\u0026gt;Blah de blah\\u0026lt;/body\\u0026gt;\\n\\u0026lt;/html\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAll pretty straight forward.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EPylons and Mako\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIn Pylons, a \\u0026quot;render\\u0026quot; function is included in the controller files. This function works just like the standard Mako renderer, only the first parameter is the name of the template file (located in /templates, but that can be changed.) Another difference is the \\u003Ccode\\u003Etmpl_context\\u003C/code\\u003E class, generally imported as \\u0026quot;c\\u0026quot;. This allows a cleaner way to access the template variables. All the variables found in a template are members of \\u0026quot;c\\u0026quot;. So in our example, we would do the following (assuming the template is named \\u0026quot;hello.html\\u0026quot;:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E  def GET_index(self):\\n         c.title = \\u0026quot;Hello Templates!\\u0026quot;\\n         c.someText = \\u0026quot;Blah de blah\\u0026quot;\\n         render(\\u0026#39;/hello.html\\u0026#39;)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EHow reddit connects\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe reddit codebase has  a series of classes that inherit from the reddits own home brewed \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/lib/wrapped.pyx\\\"\\u003ETemplated\\u003C/a\\u003E. These are in the directory \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/lib/pages\\\"\\u003E/lib/pages\\u003C/a\\u003E. The main file is pages.py. This contains the class Reddit which is a child class of Templated which is a parent class to most of the pages that reddit renders. These are the content renderer classes. If you look at the templates, such as the one for the \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/templates/reddit.html\\\"\\u003Emain page\\u003C/a\\u003E you don\\u0026#39;t see any content. All of the content is set by these content renderer classes. Most of what the class does is set a whole lot of variables and functions that will be used by the templates. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EA level of abstraction above are the \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/lib/wrapped.pyx\\\"\\u003EWrapped\\u003C/a\\u003E classes. These are the classes that define most of the HTML.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELets look at the reddit footer for an example, since it\\u0026#39;s static and all the information is self contained in two files. (The footer is the thing at the bottom with all those outside links, that nobody ever looks at). First there is the \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/templates/redditfooter.html\\\"\\u003Etemplate\\u003C/a\\u003E. Look at this bit of code:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E %for toolbar in thing.nav:\\n      \\u0026lt;div class=\\u0026quot;col\\u0026quot;\\u0026gt;\\n         ${toolbar}\\n     \\u0026lt;/div\\u0026gt;\\n %endfor\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThat\\u0026#39;s about 95% of the footer, right there. What it does is cycle through every item defined in thing.nav and prints it out. Looking back at the \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/lib/pages/pages.py\\\"\\u003Erenderer code\\u003C/a\\u003E (look for RedditFooter) you see that it is made up of a whole bunch of smaller pieces, each of them HTML renderers in their own right. These smaller items are defined in \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/lib/menus.py\\\"\\u003E/lib/menus.py\\u003C/a\\u003E. These are subclasses for the class \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/lib/wrapped.pyx\\\"\\u003EStyled\\u003C/a\\u003E which is a Templated type of class. When the footer render method is called, the template cycles through all of the Styled typed classes and calls the \\u0026quot;render()\\u0026quot; function on each one, shown here as \\u003Ccode\\u003E${toolbar}\\u003C/code\\u003E. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EQuestions?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"[\\u003C---Part 1](http://www.reddit.com/r/redditdev/comments/fdhlw/a_beginners_guide_to_the_reddit_source_code_part/)\\n\\n**Introduction** \\n\\nPlease criticize and correct!\\n\\nLast time, we discussed Pylons and the MVC architecture and how the Pylons controllers directs the client to the proper web page. Now we will discuss the \\\"view\\\" portion of the MVC, how the web pages are displayed.\\n\\n**Mako**\\n\\nPylons uses a library called [Mako](http://www.makotemplates.org/) to generate HTML pages. If you are familiar with other server-side HTML renderers, like PHP and JSP, this should be very familiar to you. The basic structure involves writing a plain HTML page with special tags for Python variables and special PHP-like brackets for Python scripts. In Pylons, all of the templates are stored in the [/templates](https://github.com/reddit/reddit/tree/master/r2/r2/templates) directory.\\n\\nSome basic Mako structures:\\n\\n*Lines that start with a hash (#) are comments.    \\n*Lines that start with percentage (%) are Python code.    \\n*Python code can go anywhere between a \\u003C% and a %\\u003E.     \\n*Include statements are defined with the `\\u003C%include file=\\\"filename\\\"\\u003E` tag, where \\\"filename\\\" is the name of another template.     \\n*Functions can be defined with    \\n     \\u003C%def name=\\\"functionName(params)\\\"\\u003E\\n     function stuff    \\n     \\u003C/%def\\u003E    \\n*A function can then be called with `${functionName(params)}`     \\n*Variables can be defined with `${variableName}` and can be set in the Python code before the template is rendered (more on that later).\\n\\nThe basic method for rendering templates is the (drumroll...) `template.render()` call. Whenever this is called it returns a template rendered as plain HTML. The render function takes in the form of the variables set in the template. You set the variables by naming them explicitly in the function, i.e. `render(var1=\\\"Hello\\\", var2=\\\"world\\\")`. In general, the standard way to call the render function is to pass it as a string to the Templater constructor, like so:\\n\\n     Template(\\\"\\u003Chtml\\u003E\\u003Chead\\u003E\\u003Ctitle\\u003E${title}\\u003C/title\\u003E\\u003C/head\\u003E\\u003Cbody\\u003E${someText}  \\n    \\u003C/body\\u003E\\u003C/html\\u003E\\\").render(title=\\\"Hello Templates!\\\", someText=\\\"Blah de blah\\\")\\n\\nThis outputs a  simple HTML page:\\n\\n    \\u003Chtml\\u003E\\n        \\u003Chead\\u003E\\u003Ctitle\\u003EHello Templates!\\u003C/title\\u003E\\u003C/head\\u003E\\n        \\u003Cbody\\u003EBlah de blah\\u003C/body\\u003E\\n    \\u003C/html\\u003E\\n\\nAll pretty straight forward.\\n\\n**Pylons and Mako**\\n\\nIn Pylons, a \\\"render\\\" function is included in the controller files. This function works just like the standard Mako renderer, only the first parameter is the name of the template file (located in /templates, but that can be changed.) Another difference is the `tmpl_context` class, generally imported as \\\"c\\\". This allows a cleaner way to access the template variables. All the variables found in a template are members of \\\"c\\\". So in our example, we would do the following (assuming the template is named \\\"hello.html\\\":\\n\\n      def GET_index(self):\\n             c.title = \\\"Hello Templates!\\\"\\n             c.someText = \\\"Blah de blah\\\"\\n             render('/hello.html')\\n\\n**How reddit connects**\\n\\nThe reddit codebase has  a series of classes that inherit from the reddits own home brewed [Templated](https://github.com/reddit/reddit/blob/master/r2/r2/lib/wrapped.pyx). These are in the directory [/lib/pages](https://github.com/reddit/reddit/blob/master/r2/r2/lib/pages). The main file is pages.py. This contains the class Reddit which is a child class of Templated which is a parent class to most of the pages that reddit renders. These are the content renderer classes. If you look at the templates, such as the one for the [main page](https://github.com/reddit/reddit/blob/master/r2/r2/templates/reddit.html) you don't see any content. All of the content is set by these content renderer classes. Most of what the class does is set a whole lot of variables and functions that will be used by the templates. \\n\\nA level of abstraction above are the [Wrapped](https://github.com/reddit/reddit/blob/master/r2/r2/lib/wrapped.pyx) classes. These are the classes that define most of the HTML.\\n\\nLets look at the reddit footer for an example, since it's static and all the information is self contained in two files. (The footer is the thing at the bottom with all those outside links, that nobody ever looks at). First there is the [template](https://github.com/reddit/reddit/blob/master/r2/r2/templates/redditfooter.html). Look at this bit of code:\\n\\n     %for toolbar in thing.nav:\\n          \\u003Cdiv class=\\\"col\\\"\\u003E\\n             ${toolbar}\\n         \\u003C/div\\u003E\\n     %endfor\\n\\nThat's about 95% of the footer, right there. What it does is cycle through every item defined in thing.nav and prints it out. Looking back at the [renderer code](https://github.com/reddit/reddit/blob/master/r2/r2/lib/pages/pages.py) (look for RedditFooter) you see that it is made up of a whole bunch of smaller pieces, each of them HTML renderers in their own right. These smaller items are defined in [/lib/menus.py](https://github.com/reddit/reddit/blob/master/r2/r2/lib/menus.py). These are subclasses for the class [Styled](https://github.com/reddit/reddit/blob/master/r2/r2/lib/wrapped.pyx) which is a Templated type of class. When the footer render method is called, the template cycles through all of the Styled typed classes and calls the \\\"render()\\\" function on each one, shown here as `${toolbar}`. \\n\\nQuestions?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"fewoh\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Yserbius\", \"media\": null, \"score\": 28, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/fewoh/a_beginners_guide_to_the_reddit_source_code_part/\", \"locked\": false, \"name\": \"t3_fewoh\", \"created\": 1296813294.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/fewoh/a_beginners_guide_to_the_reddit_source_code_part/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"A Beginners Guide to the reddit Source Code: Part 2, The View, Templates and Mako\", \"created_utc\": 1296784494.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 28}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESince we\\u0026#39;ve never actually distributed or maintained a VM before, I thought it best to pre-emptively start a discussion thread. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://sp.reddit.com/reddit-vm.tar.gz?torrent\\\"\\u003EHere\\u0026#39;s a link to the torrent file.\\u003C/a\\u003E  It runs Ubuntu 10.04 and was built on VMWare 3.0.2 on Mac OS which seems to store the vm as a bundle.  Since I can see the vmx file in the bundle, I\\u0026#39;m hoping that there are no issues getting it working on linux and windows.  If there are, we\\u0026#39;ll just have to tweak it. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT\\u003C/strong\\u003E (T+1hour): 26 Seeders and 55 leechers at the moment.  Whoo-hoo!  Please let me know if you manage to get the VM running, too. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Since we've never actually distributed or maintained a VM before, I thought it best to pre-emptively start a discussion thread. \\n\\n[Here's a link to the torrent file.](http://sp.reddit.com/reddit-vm.tar.gz?torrent)  It runs Ubuntu 10.04 and was built on VMWare 3.0.2 on Mac OS which seems to store the vm as a bundle.  Since I can see the vmx file in the bundle, I'm hoping that there are no issues getting it working on linux and windows.  If there are, we'll just have to tweak it. \\n\\n**EDIT** (T+1hour): 26 Seeders and 55 leechers at the moment.  Whoo-hoo!  Please let me know if you manage to get the VM running, too. \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"c625v\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"KeyserSosa\", \"media\": null, \"score\": 26, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 63, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/c625v/questions_and_discussion_about_the_new_reddit_vm/\", \"locked\": false, \"name\": \"t3_c625v\", \"created\": 1274331166.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/c625v/questions_and_discussion_about_the_new_reddit_vm/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Questions and discussion about the new reddit VM. \", \"created_utc\": 1274302366.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 26}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EToday we released a new feature: \\u003Ca href=\\\"https://www.reddit.com/r/announcements/comments/3sbrro/account_suspensions_a_transparent_alternative_to/\\\"\\u003Eaccount suspensions\\u003C/a\\u003E.  These are a form of account restriction that can be applied to user accounts that will limit the actions that account can perform.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAttempting to performing a \\u003Ca href=\\\"https://reddit.zendesk.com/hc/en-us/articles/205687686\\\"\\u003Eforbidden action\\u003C/a\\u003E as a suspended user via the API will return a 403/forbidden error.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe\\u2019ve also added an \\u003Ccode\\u003Eis_suspended\\u003C/code\\u003E attribute to user objects, that provides information about the user\\u2019s account status:\\u003C/p\\u003E\\n\\n\\u003Ctable\\u003E\\u003Cthead\\u003E\\n\\u003Ctr\\u003E\\n\\u003Cth\\u003Eviewer\\u003C/th\\u003E\\n\\u003Cth\\u003Eviewed account status\\u003C/th\\u003E\\n\\u003Cth\\u003EJSON\\u003C/th\\u003E\\n\\u003C/tr\\u003E\\n\\u003C/thead\\u003E\\u003Ctbody\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd\\u003ESuspended account\\u003C/td\\u003E\\n\\u003Ctd\\u003Epermanent suspension\\u003C/td\\u003E\\n\\u003Ctd\\u003E\\u003Ccode\\u003Eis_suspended: true, suspension_expiration_utc: null\\u003C/code\\u003E\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd\\u003ESuspended account\\u003C/td\\u003E\\n\\u003Ctd\\u003Etemporary suspension\\u003C/td\\u003E\\n\\u003Ctd\\u003E\\u003Ccode\\u003Eis_suspended: true, suspension_expiration_utc: timestamp\\u003C/code\\u003E\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd\\u003EAny other account\\u003C/td\\u003E\\n\\u003Ctd\\u003Epermanent suspension\\u003C/td\\u003E\\n\\u003Ctd\\u003E\\u003Ccode\\u003Eis_suspended: true\\u003C/code\\u003E\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd\\u003EAny other account\\u003C/td\\u003E\\n\\u003Ctd\\u003Etemporary suspension\\u003C/td\\u003E\\n\\u003Ctd\\u003E\\u003Ccode\\u003Eno indication\\u003C/code\\u003E\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003C/tbody\\u003E\\u003C/table\\u003E\\n\\n\\u003Cp\\u003Eedit: We\\u0026#39;ve updated the attribute names to reflect the feature name.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Today we released a new feature: [account suspensions](https://www.reddit.com/r/announcements/comments/3sbrro/account_suspensions_a_transparent_alternative_to/).  These are a form of account restriction that can be applied to user accounts that will limit the actions that account can perform.\\n\\nAttempting to performing a [forbidden action](https://reddit.zendesk.com/hc/en-us/articles/205687686) as a suspended user via the API will return a 403/forbidden error.\\n\\nWe\\u2019ve also added an `is_suspended` attribute to user objects, that provides information about the user\\u2019s account status:\\n\\nviewer | viewed account status | JSON\\n---------|----------|----------\\nSuspended account | permanent suspension | `is_suspended: true, suspension_expiration_utc: null`\\nSuspended account | temporary suspension | `is_suspended: true, suspension_expiration_utc: timestamp`\\nAny other account | permanent suspension | `is_suspended: true`\\nAny other account | temporary suspension | `no indication`\\n\\n\\nedit: We've updated the attribute names to reflect the feature name.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3sbs31\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"powerlanguage\", \"media\": null, \"score\": 26, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1447197465.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3sbs31/api_support_for_account_suspensions/\", \"locked\": false, \"name\": \"t3_3sbs31\", \"created\": 1447220540.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3sbs31/api_support_for_account_suspensions/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API support for account suspensions\", \"created_utc\": 1447191740.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 26}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESome important API changes are going to happen soon.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFirst: The \\u003Ccode\\u003E/about/banned\\u003C/code\\u003E, \\u003Ccode\\u003E/about/contributors\\u003C/code\\u003E, \\u003Ccode\\u003E/about/wikibanned\\u003C/code\\u003E, and \\u003Ccode\\u003E/about/wikicontributors\\u003C/code\\u003E are switching to a paginated list.  The structure will look like:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E{\\u0026quot;kind\\u0026quot;: \\u0026quot;Listing\\u0026quot;, \\u0026quot;data\\u0026quot;: {\\u0026quot;modhash\\u0026quot;: \\u0026quot;reddit\\u0026quot;, \\u0026quot;children\\u0026quot;: [], \\u0026quot;after\\u0026quot;: null, \\u0026quot;before\\u0026quot;: null}}\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EThis change is necessary as currently some subreddits are timing out while loading these lists if they have many users on them.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESecond: \\u003Ccode\\u003E/prefs/friends.json\\u003C/code\\u003E will no longer contain blocked users.  Blocked users will be moving to \\u003Ccode\\u003E/prefs/blocked.json\\u003C/code\\u003E.  The json for \\u003Ccode\\u003Eblocked.json\\u003C/code\\u003E will be switching to a structure of:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E{\\u0026quot;kind\\u0026quot;: \\u0026quot;UserList\\u0026quot;, \\u0026quot;data\\u0026quot;: {\\u0026quot;children\\u0026quot;: [{\\u0026quot;name\\u0026quot;: \\u0026quot;reddit\\u0026quot;, \\u0026quot;id\\u0026quot;: \\u0026quot;t2_1\\u0026quot;}]}}\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EThe format of \\u003Ccode\\u003Efriends.json\\u003C/code\\u003E is staying the same, except the banned list will be empty.  We did this to avoid existing applications crashing until they can switch over to the new endpoint.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Some important API changes are going to happen soon.\\n\\n\\nFirst: The `/about/banned`, `/about/contributors`, `/about/wikibanned`, and `/about/wikicontributors` are switching to a paginated list.  The structure will look like:\\n\\n\\u003E {\\\"kind\\\": \\\"Listing\\\", \\\"data\\\": {\\\"modhash\\\": \\\"reddit\\\", \\\"children\\\": [], \\\"after\\\": null, \\\"before\\\": null}}\\n\\nThis change is necessary as currently some subreddits are timing out while loading these lists if they have many users on them.\\n\\nSecond: `/prefs/friends.json` will no longer contain blocked users.  Blocked users will be moving to `/prefs/blocked.json`.  The json for `blocked.json` will be switching to a structure of:\\n\\n\\u003E {\\\"kind\\\": \\\"UserList\\\", \\\"data\\\": {\\\"children\\\": [{\\\"name\\\": \\\"reddit\\\", \\\"id\\\": \\\"t2_1\\\"}]}}\\n\\nThe format of `friends.json` is staying the same, except the banned list will be empty.  We did this to avoid existing applications crashing until they can switch over to the new endpoint.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1x83az\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"slyf\", \"media\": null, \"score\": 26, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1x83az/heads_up_important_api_changes/\", \"locked\": false, \"name\": \"t3_1x83az\", \"created\": 1391758458.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1x83az/heads_up_important_api_changes/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Heads up! Important API changes\", \"created_utc\": 1391729658.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 26}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EFYI: The default search syntax will be reverting to the old \\u0026quot;lucene\\u0026quot; style before too long. If your library, script, or program uses or interacts with search, you will want to prepare for this change.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYou will have two options:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003ERevert your stuff to use the old \\u0026quot;lucene\\u0026quot; syntax\\u003C/li\\u003E\\n\\u003Cli\\u003EContinue using \\u0026quot;cloudsearch\\u0026quot; syntax, and add \\u0026quot;syntax=cloudsearch\\u0026quot; to your query params.\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EThe nitty gritty is that I\\u0026#39;ve thrown together a query parser/rewriter that will rewrite lucene queries into matching cloudsearch. This means that (a) there may be bugs if using lucene syntax, and (b) lucene queries may not have access to the full range of expressions allowed - for example because cloudsearch doesn\\u0026#39;t support it, or because the rewriter doesn\\u0026#39;t support it. (\\u003Ca href=\\\"https://github.com/kemitche/l2cs\\\"\\u003EHere\\u0026#39;s the rewriter code\\u003C/a\\u003E, if you\\u0026#39;d like to see it).\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"FYI: The default search syntax will be reverting to the old \\\"lucene\\\" style before too long. If your library, script, or program uses or interacts with search, you will want to prepare for this change.\\n\\nYou will have two options:\\n\\n1. Revert your stuff to use the old \\\"lucene\\\" syntax\\n2. Continue using \\\"cloudsearch\\\" syntax, and add \\\"syntax=cloudsearch\\\" to your query params.\\n\\nThe nitty gritty is that I've thrown together a query parser/rewriter that will rewrite lucene queries into matching cloudsearch. This means that (a) there may be bugs if using lucene syntax, and (b) lucene queries may not have access to the full range of expressions allowed - for example because cloudsearch doesn't support it, or because the rewriter doesn't support it. ([Here's the rewriter code](https://github.com/kemitche/l2cs), if you'd like to see it).\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"txuic\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 28, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1337624738.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/txuic/search_syntax_update_returning_to_lucene_by/\", \"locked\": false, \"name\": \"t3_txuic\", \"created\": 1337653326.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/txuic/search_syntax_update_returning_to_lucene_by/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Search syntax update: returning to lucene by default soon\", \"created_utc\": 1337624526.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 28}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"d6r8v\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"mellort\", \"media\": null, \"score\": 26, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/d6r8v/hey_rredditdev_i_made_a_python_wrapper_for_the/\", \"locked\": false, \"name\": \"t3_d6r8v\", \"created\": 1283084279.0, \"url\": \"http://github.com/mellort/reddit_api\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Hey r/redditdev, I made a Python wrapper for the Reddit API; check it out! [x-post from r/prog]\", \"created_utc\": 1283055479.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 26}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3hldv2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"alkorin\", \"media\": null, \"score\": 26, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 22, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3hldv2/i_got_really_fed_up_with_reddits_saved_interface/\", \"locked\": false, \"name\": \"t3_3hldv2\", \"created\": 1440026416.0, \"url\": \"https://github.com/alkorin452/save-to-subreddit\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I got really fed up with Reddit's \\\"saved\\\" interface so I wrote a python script that scans the first page of your saved, posts them to a personal subreddit (that you need to make prior) and then deletes that page of saved\", \"created_utc\": 1439997616.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 26}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EDealing with \\u003Ca href=\\\"http://i.imgur.com/mp6o3ug.png\\\"\\u003Ethis\\u003C/a\\u003E right now, where another bot is duplicating links mine posts in another comment below. I approached the developer via PM about having it maybe ignore my bot\\u0026#39;s comments to avoid clutter, and this is the response I get.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EReddit\\u0026#39;s API is powerful, and you should responsibly implement your usage of it. I believe taking the approach of \\u0026quot;they can ban me if they don\\u0026#39;t like it\\u0026quot; is absurd, let alone responding to as militantly as this.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERemember to follow \\u003Ca href=\\\"https://www.reddit.com/wiki/bottiquette\\\"\\u003EReddit Bottiquette\\u003C/a\\u003E! (thanks \\u003Ca href=\\\"/u/redtaboo\\\"\\u003E/u/redtaboo\\u003C/a\\u003E)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Dealing with [this](http://i.imgur.com/mp6o3ug.png) right now, where another bot is duplicating links mine posts in another comment below. I approached the developer via PM about having it maybe ignore my bot's comments to avoid clutter, and this is the response I get.\\n\\nReddit's API is powerful, and you should responsibly implement your usage of it. I believe taking the approach of \\\"they can ban me if they don't like it\\\" is absurd, let alone responding to as militantly as this.\\n\\nRemember to follow [Reddit Bottiquette](https://www.reddit.com/wiki/bottiquette)! (thanks /u/redtaboo)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2r4ipi\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"pandanomic\", \"media\": null, \"score\": 27, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 18, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1420226813.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2r4ipi/reddit_bot_owners_please_remember_to_be/\", \"locked\": false, \"name\": \"t3_2r4ipi\", \"created\": 1420254451.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2r4ipi/reddit_bot_owners_please_remember_to_be/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit bot owners, please remember to be responsible.\", \"created_utc\": 1420225651.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 27}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo I\\u0026#39;ve been trying to understand how reddit interacts with its many databases and was hoping some clever people could help confirm/correct my understanding.  To illustrate my understanding (or lack thereof) I\\u0026#39;m going to construct an example of two users, Alice and Bob, reading a subreddit.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBob is already reading the subreddit and is happily clicking on up/down voting buttons as he sees fit.  When he clicks on a voting arrow, some javascript is executed that changes the visibility of the up/down arrow and the dispaly with the number of votes for a link (a purely cosmetic change visible only to Bob) and ajax sends a POST request to the appropriate voting action of the API controller.  This bit of code then adds an entry to the rabbitmq link_vote queue, which says that Bob voted up/down/null on link l.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAt some point, the link_vote_q process, which handles the link_vote queue, decides to do something with these votes.  It takes all the votes and updates the postgresql database to reflect this new information.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMeanwhile, Alice has decided she might like to look at the same page as Bob.  She sends a request to the appropriate GET method of the listing controller, which gets the appropriate mako template and renders it by fetching data from the postgresql database.  If rabbitmq has gotten around to commiting Bob\\u0026#39;s votes at this point she will see them, if not, she won\\u0026#39;t.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHowever, rendering each mako template by fetching data from the main db each time a user requests a page is time consuming.  This is where cassandra comes in.  Cassandra stores the rendered html for each page (or at least for the commonly accessed ones) and can give them to the user instead of rendering everything from the sql db.  This works great so long as nothing changes, but of course Bob is voting on things so the html in cassandra needs to be updated.  How does this happen?  I would guess that when link_process_q commits stuff to the sql db it also submits something to cassandra saying the pages that depend on this vote need updating as of \\u0026quot;current time\\u0026quot;.  Then when Alice comes to view the page, cassandra knows the rendered html in its cache is too old and goes off to the mako template and the sql db and renders a fresh version.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBut wait, there\\u0026#39;s more!  Even fetching stuff from cassandra is annoying, because it requires accessing the hard disc.  To minimize this, memcache keeps the most commonly accessed bits of html served by cassandra in memory, so they can accessed super quickly.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESorry that was a bit long, but the reddit db system is a bit complicated so it kind of had to be.  If anyone could help out and tell me how far off I am, that would be great.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Etl;dr Bob and Alice have a fun time on reddit.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So I've been trying to understand how reddit interacts with its many databases and was hoping some clever people could help confirm/correct my understanding.  To illustrate my understanding (or lack thereof) I'm going to construct an example of two users, Alice and Bob, reading a subreddit.\\n\\nBob is already reading the subreddit and is happily clicking on up/down voting buttons as he sees fit.  When he clicks on a voting arrow, some javascript is executed that changes the visibility of the up/down arrow and the dispaly with the number of votes for a link (a purely cosmetic change visible only to Bob) and ajax sends a POST request to the appropriate voting action of the API controller.  This bit of code then adds an entry to the rabbitmq link_vote queue, which says that Bob voted up/down/null on link l.\\n\\nAt some point, the link_vote_q process, which handles the link_vote queue, decides to do something with these votes.  It takes all the votes and updates the postgresql database to reflect this new information.\\n\\nMeanwhile, Alice has decided she might like to look at the same page as Bob.  She sends a request to the appropriate GET method of the listing controller, which gets the appropriate mako template and renders it by fetching data from the postgresql database.  If rabbitmq has gotten around to commiting Bob's votes at this point she will see them, if not, she won't.\\n\\nHowever, rendering each mako template by fetching data from the main db each time a user requests a page is time consuming.  This is where cassandra comes in.  Cassandra stores the rendered html for each page (or at least for the commonly accessed ones) and can give them to the user instead of rendering everything from the sql db.  This works great so long as nothing changes, but of course Bob is voting on things so the html in cassandra needs to be updated.  How does this happen?  I would guess that when link_process_q commits stuff to the sql db it also submits something to cassandra saying the pages that depend on this vote need updating as of \\\"current time\\\".  Then when Alice comes to view the page, cassandra knows the rendered html in its cache is too old and goes off to the mako template and the sql db and renders a fresh version.\\n\\nBut wait, there's more!  Even fetching stuff from cassandra is annoying, because it requires accessing the hard disc.  To minimize this, memcache keeps the most commonly accessed bits of html served by cassandra in memory, so they can accessed super quickly.\\n\\nSorry that was a bit long, but the reddit db system is a bit complicated so it kind of had to be.  If anyone could help out and tell me how far off I am, that would be great.\\n\\ntl;dr Bob and Alice have a fun time on reddit.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"oapba\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MDY\", \"media\": null, \"score\": 26, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/oapba/understanding_the_reddit_db/\", \"locked\": false, \"name\": \"t3_oapba\", \"created\": 1326221469.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/oapba/understanding_the_reddit_db/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Understanding the reddit DB\", \"created_utc\": 1326192669.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 26}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"75j29\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"mrpeenut24\", \"media\": null, \"score\": 25, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/75j29/sticky_for_self_posts_allowing_the_initial/\", \"locked\": false, \"name\": \"t3_75j29\", \"created\": 1223342214.0, \"url\": \"http://www.reddit.com/r/AskReddit/comments/75iqo/dear_reddit_should_the_submitters_comment_should/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Sticky for 'self' posts allowing the initial comment to remain at the top\", \"created_utc\": 1223313414.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 25}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve just finished coding \\u003Ca href=\\\"http://github.com/gabrieldain/StackBot\\\"\\u003EStackBot\\u003C/a\\u003E, a StackOverflow bot for reddit. At this point, it trawls through the comment stream searching for stackoverflow.com links, and replies to the comment with the top/accepted/linked answer, depending on the situation. It borrows heavily from \\u003Ca href=\\\"http://www.reddit.com/u/autowikibot\\\"\\u003Eautowikibot\\u003C/a\\u003E (conceptually), but it is all fresh code.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease see example below, in the comments section, or \\u003Ca href=\\\"http://www.reddit.com/r/StackBot/comments/1vlke7/example/cetftap\\\"\\u003Ehere\\u003C/a\\u003E for an example thread at \\u003Ca href=\\\"http://www.reddit.com/r/StackBot\\\"\\u003E/r/StackBot\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI haven\\u0026#39;t implemented auto-deletion yet, but that would be the next step.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m looking for feedback, namely if you think this would be useful, or if you would find it annoying/spammy.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENote: At the moment it\\u0026#39;s only trawling through the \\u003Ca href=\\\"/r/StackBot\\\"\\u003E/r/StackBot\\u003C/a\\u003E comment stream, so you can play around with it there if you want.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EEDIT: auto-remover functionality has been added, and StackBot is now trawling the \\u003Ca href=\\\"/r/all\\\"\\u003E/r/all\\u003C/a\\u003E comment_stream. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt will delete its own comment if it has a score below zero, and the comment age is older than the average length of time between comments in the relevant subreddit. That should create some balance between comments in super-busy subs, where comments should be removed swiftly once they are downvoted, and comments in low-traffic subs, where the comments will be given a better chance of getting upvotes before self-deleting.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've just finished coding [StackBot](http://github.com/gabrieldain/StackBot), a StackOverflow bot for reddit. At this point, it trawls through the comment stream searching for stackoverflow.com links, and replies to the comment with the top/accepted/linked answer, depending on the situation. It borrows heavily from [autowikibot](http://www.reddit.com/u/autowikibot) (conceptually), but it is all fresh code.\\n\\nPlease see example below, in the comments section, or [here](http://www.reddit.com/r/StackBot/comments/1vlke7/example/cetftap) for an example thread at [/r/StackBot](http://www.reddit.com/r/StackBot).\\n\\nI haven't implemented auto-deletion yet, but that would be the next step.\\n\\nI'm looking for feedback, namely if you think this would be useful, or if you would find it annoying/spammy.\\n\\nNote: At the moment it's only trawling through the /r/StackBot comment stream, so you can play around with it there if you want.\\n\\n-----------------------\\n\\nEDIT: auto-remover functionality has been added, and StackBot is now trawling the /r/all comment_stream. \\n\\nIt will delete its own comment if it has a score below zero, and the comment age is older than the average length of time between comments in the relevant subreddit. That should create some balance between comments in super-busy subs, where comments should be removed swiftly once they are downvoted, and comments in low-traffic subs, where the comments will be given a better chance of getting upvotes before self-deleting.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1vlkoq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"gabrieldain\", \"media\": null, \"score\": 25, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 21, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1390213903.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1vlkoq/new_stackoverflow_reddit_bot_tell_me_if_you_think/\", \"locked\": false, \"name\": \"t3_1vlkoq\", \"created\": 1390177362.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1vlkoq/new_stackoverflow_reddit_bot_tell_me_if_you_think/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New StackOverflow reddit bot, tell me if you think this would be helpful or annoying/spammy.\", \"created_utc\": 1390148562.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 25}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIf you are signed in to a moderator account for a subreddit, when fetching the JSON representation of a link or comment, the \\u003Ccode\\u003Ebanned_by\\u003C/code\\u003E and \\u003Ccode\\u003Eapproved_by\\u003C/code\\u003E properties reflect the name of the moderator who banned or approved the submission.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThese properties had two problems:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EIf a submission had been banned and then approved, both \\u003Ccode\\u003Ebanned_by\\u003C/code\\u003E and \\u003Ccode\\u003Eapproved_by\\u003C/code\\u003E would be set, making it difficult to determine the current state of the submission.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EIf a submission had been banned by a non-moderator (admin / admin script / etc), the \\u003Ccode\\u003Ebanned_by\\u003C/code\\u003E value is supposed to be \\u003Ccode\\u003Etrue\\u003C/code\\u003E. Due to a bug in this code, \\u003Ccode\\u003Ebanned_by\\u003C/code\\u003E would be set to \\u003Ccode\\u003Etrue\\u003C/code\\u003E even when a submission was not banned.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EThese api properties have now been tweaked to be mutually exclusive. If a post is banned, \\u003Ccode\\u003Ebanned_by\\u003C/code\\u003E will be set, and \\u003Ccode\\u003Eapproved_by\\u003C/code\\u003E will be \\u003Ccode\\u003Enull\\u003C/code\\u003E (and vice-versa). In addition, \\u003Ccode\\u003Ebanned_by\\u003C/code\\u003E will now only be non-null if the submission is actually banned.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/commit/fcedf701ee1587e4de153b983ae9cca5ea5d7451\\\"\\u003Esee the code on github\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"If you are signed in to a moderator account for a subreddit, when fetching the JSON representation of a link or comment, the `banned_by` and `approved_by` properties reflect the name of the moderator who banned or approved the submission.\\n\\nThese properties had two problems:\\n\\n * If a submission had been banned and then approved, both `banned_by` and `approved_by` would be set, making it difficult to determine the current state of the submission.\\n\\n * If a submission had been banned by a non-moderator (admin / admin script / etc), the `banned_by` value is supposed to be `true`. Due to a bug in this code, `banned_by` would be set to `true` even when a submission was not banned.\\n\\nThese api properties have now been tweaked to be mutually exclusive. If a post is banned, `banned_by` will be set, and `approved_by` will be `null` (and vice-versa). In addition, `banned_by` will now only be non-null if the submission is actually banned.\\n\\n[see the code on github](https://github.com/reddit/reddit/commit/fcedf701ee1587e4de153b983ae9cca5ea5d7451)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"16pusk\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"chromakode\", \"media\": null, \"score\": 22, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/16pusk/tweaks_and_fixes_to_the_banned_by_approved_by_api/\", \"locked\": false, \"name\": \"t3_16pusk\", \"created\": 1358410110.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/16pusk/tweaks_and_fixes_to_the_banned_by_approved_by_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Tweaks and fixes to the \\\"banned_by\\\" / \\\"approved_by\\\" API properties\", \"created_utc\": 1358381310.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 22}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EPRAW users,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve been working on-and-off for the last few weeks (with significant help from \\u003Ca href=\\\"/u/_Daimon_\\\"\\u003E/u/_Daimon_\\u003C/a\\u003E) on extending the initial OAuth2 support added to PRAW by \\u003Ca href=\\\"/u/intortus\\\"\\u003E/u/intortus\\u003C/a\\u003E. My original plan was for addition-only changes thus resulting in a backward-compatible 1.1 release. However, properly handling reddit\\u0026#39;s OAuth2 scopes necessitated a number of backwards incompatible namespace changes (which in turn prompted numerous other namespace changes) thus I am bumping the major version number.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo minimize the amount of code from breaking that has a PRAW dependency, I wanted to give everyone a little time to update their projects before I add PRAW 2.0 to the cheeseshop (pypi). If you have code that depends on PRAW and you don\\u0026#39;t want it to break for other users of your projects you have two primary options:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe first option is to simply update your \\u003Ccode\\u003Esetup.py\\u003C/code\\u003E or \\u003Ccode\\u003Erequirements.txt\\u003C/code\\u003E file to depend on any PRAW version less than 2.0 (1.0.16 will be the last 1.0 version). If your package installation does not automatically handle dependencies then you can inform your users to run \\u003Ccode\\u003Epip install praw==1.0.16\\u003C/code\\u003E to get an appropriate version, or point them to \\u003Ca href=\\\"https://github.com/praw-dev/praw/archive/praw-1.0.16.tar.gz\\\"\\u003Ethis source tarball\\u003C/a\\u003E or \\u003Ca href=\\\"https://github.com/praw-dev/praw/archive/praw-1.0.16.zip\\\"\\u003Ethis source zipfile\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe second option, of course, is to update your package to work with PRAW versions \\u0026gt;= 2.0. To help you get started you\\u0026#39;ll want to checkout the PRAW 2.0 \\u003Ca href=\\\"https://github.com/praw-dev/praw/wiki/Changelog\\\"\\u003EChange Log\\u003C/a\\u003E. Until I actually release the package on pypi, you\\u0026#39;ll need to clone the github repository (\\u003Ccode\\u003Egit clone git://github.com/praw-dev/praw.git\\u003C/code\\u003E), or manually download the source (\\u003Ca href=\\\"https://github.com/praw-dev/praw/archive/master.zip\\\"\\u003Ezipfile\\u003C/a\\u003E).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m happy to answer any questions you have as replies to this submission. Assuming no major issues are discovered, I will release PRAW 2.0 Wednesday evening (PST).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: I should add that if you are interested in using PRAW via OAuth2, check out the \\u003Ca href=\\\"https://github.com/praw-dev/praw/wiki/OAuth\\\"\\u003EPRAW OAuth wiki page\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"PRAW users,\\n\\nI've been working on-and-off for the last few weeks (with significant help from /u/_Daimon_) on extending the initial OAuth2 support added to PRAW by /u/intortus. My original plan was for addition-only changes thus resulting in a backward-compatible 1.1 release. However, properly handling reddit's OAuth2 scopes necessitated a number of backwards incompatible namespace changes (which in turn prompted numerous other namespace changes) thus I am bumping the major version number.\\n\\nTo minimize the amount of code from breaking that has a PRAW dependency, I wanted to give everyone a little time to update their projects before I add PRAW 2.0 to the cheeseshop (pypi). If you have code that depends on PRAW and you don't want it to break for other users of your projects you have two primary options:\\n\\nThe first option is to simply update your `setup.py` or `requirements.txt` file to depend on any PRAW version less than 2.0 (1.0.16 will be the last 1.0 version). If your package installation does not automatically handle dependencies then you can inform your users to run `pip install praw==1.0.16` to get an appropriate version, or point them to [this source tarball](https://github.com/praw-dev/praw/archive/praw-1.0.16.tar.gz) or [this source zipfile](https://github.com/praw-dev/praw/archive/praw-1.0.16.zip).\\n\\nThe second option, of course, is to update your package to work with PRAW versions \\u003E= 2.0. To help you get started you'll want to checkout the PRAW 2.0 [Change Log](https://github.com/praw-dev/praw/wiki/Changelog). Until I actually release the package on pypi, you'll need to clone the github repository (`git clone git://github.com/praw-dev/praw.git`), or manually download the source ([zipfile](https://github.com/praw-dev/praw/archive/master.zip)).\\n\\nI'm happy to answer any questions you have as replies to this submission. Assuming no major issues are discovered, I will release PRAW 2.0 Wednesday evening (PST).\\n\\nEdit: I should add that if you are interested in using PRAW via OAuth2, check out the [PRAW OAuth wiki page](https://github.com/praw-dev/praw/wiki/OAuth).\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"16m0uu\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bboe\", \"media\": null, \"score\": 24, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1358245810.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/16m0uu/praw_20_is_coming_release_in_2_days/\", \"locked\": false, \"name\": \"t3_16m0uu\", \"created\": 1358273822.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/16m0uu/praw_20_is_coming_release_in_2_days/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"PRAW 2.0 is Coming (release in ~2 days)\", \"created_utc\": 1358245022.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 24}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EYesterday, \\u003Ca href=\\\"https://github.com/reddit/reddit/commit/3a5eabc7cccc7b5136c5e638a9bb4a7441e1e38f\\\"\\u003Ewe fixed\\u003C/a\\u003E a long-standing issue in the JSON responses from our API.  The issue was that the author of a thing would always have their name returned with that thing, even if their account was now deleted. This fix made it so that the \\u003Ccode\\u003Eauthor\\u003C/code\\u003E field in the JSON response would be \\u003Ccode\\u003Enull\\u003C/code\\u003E if the author\\u0026#39;s account was deleted.  Unfortunately, some popular clients were unhappy with this value being \\u003Ccode\\u003Enull\\u003C/code\\u003E (understandably, as it\\u0026#39;d never been null before). \\u003Ca href=\\\"/u/chromakode\\\"\\u003E/u/chromakode\\u003C/a\\u003E made \\u003Ca href=\\\"https://github.com/reddit/reddit/commit/d566d91d84327a9233da9c0c9c534c3547dc2e5f\\\"\\u003Ea patch\\u003C/a\\u003E that changed it to make the author field \\u003Ccode\\u003E\\u0026quot;[deleted]\\u0026quot;\\u003C/code\\u003E instead of \\u003Ccode\\u003Enull\\u003C/code\\u003E to prevent client issues.  We do feel that \\u003Ccode\\u003Enull\\u003C/code\\u003E was the more appropriate value to indicate the account\\u0026#39;s deletion so this is considered temporary and will return to \\u003Ccode\\u003Enull\\u003C/code\\u003E, no earlier than Dec 31, 2012.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Yesterday, [we fixed](https://github.com/reddit/reddit/commit/3a5eabc7cccc7b5136c5e638a9bb4a7441e1e38f) a long-standing issue in the JSON responses from our API.  The issue was that the author of a thing would always have their name returned with that thing, even if their account was now deleted. This fix made it so that the `author` field in the JSON response would be `null` if the author's account was deleted.  Unfortunately, some popular clients were unhappy with this value being `null` (understandably, as it'd never been null before). /u/chromakode made [a patch](https://github.com/reddit/reddit/commit/d566d91d84327a9233da9c0c9c534c3547dc2e5f) that changed it to make the author field `\\\"[deleted]\\\"` instead of `null` to prevent client issues.  We do feel that `null` was the more appropriate value to indicate the account's deletion so this is considered temporary and will return to `null`, no earlier than Dec 31, 2012.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"14adms\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"spladug\", \"media\": null, \"score\": 25, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/14adms/api_announcement_deleted_authors_in_json_responses/\", \"locked\": false, \"name\": \"t3_14adms\", \"created\": 1354690927.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/14adms/api_announcement_deleted_authors_in_json_responses/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API Announcement: Deleted authors in JSON responses\", \"created_utc\": 1354662127.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 25}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"t1m8x\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"larryng\", \"media\": null, \"score\": 25, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/t1m8x/as_suggested_i_open_sourced_my_riama_bot_example/\", \"locked\": false, \"name\": \"t3_t1m8x\", \"created\": 1335916379.0, \"url\": \"https://github.com/larryng/reddit-iama-bot\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"As suggested, I open sourced my /r/IAMA bot.  Example usage of narwal, Heroku, MongoLab, and spending no $.\", \"created_utc\": 1335887579.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 25}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"6nxb3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"7oby\", \"media\": null, \"score\": 26, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 22, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/6nxb3/reddit_developer_discussion_thread_for_june_18/\", \"locked\": false, \"name\": \"t3_6nxb3\", \"created\": 1213835946.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/6nxb3/reddit_developer_discussion_thread_for_june_18/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit developer discussion thread for June 18, 2008\", \"created_utc\": 1213807146.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 26}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"36j94j\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"go1dfish\", \"media\": null, \"score\": 23, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/36j94j/upublicmodlogs_is_not_a_bot_it_is_a_clever_feed/\", \"locked\": false, \"name\": \"t3_36j94j\", \"created\": 1432094953.0, \"url\": \"https://www.reddit.com/user/publicmodlogs\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"/u/publicmodlogs is not a bot, it is a clever feed hack that makes it trivially easy to make a moderation log fully and provably transparent without any backend but reddit (and a third party CORS proxy)\", \"created_utc\": 1432066153.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 23}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EApologies that this isn\\u0026#39;t related to reddit development but there are a lot of bots on reddit using v2 of the Youtube API so I feel it\\u0026#39;s important.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ev3 is also painfully slow to update(20 to 40 minutes) so if you have a bot that posts videos when they are uploaded to a specific channel, it\\u0026#39;s going to get a lot slower.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Apologies that this isn't related to reddit development but there are a lot of bots on reddit using v2 of the Youtube API so I feel it's important.\\n\\nv3 is also painfully slow to update(20 to 40 minutes) so if you have a bot that posts videos when they are uploaded to a specific channel, it's going to get a lot slower.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"31qbe5\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MichealKenny\", \"media\": null, \"score\": 21, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/31qbe5/do_you_run_a_bot_that_uses_the_youtube_api_are/\", \"locked\": false, \"name\": \"t3_31qbe5\", \"created\": 1428427598.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/31qbe5/do_you_run_a_bot_that_uses_the_youtube_api_are/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Do you run a bot that uses the Youtube API? are you using version 2 of the API? if so, it's shutting down April 20, 2015, so migrate to v3.\", \"created_utc\": 1428398798.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 21}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EFrom reddit\\u0026#39;s student contractor \\u003Ca href=\\\"/u/slyf\\\"\\u003E/u/slyf\\u003C/a\\u003E:\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EModerator permissions are now available in the \\u003Ca href=\\\"http://www.reddit.com/subreddits/mine/moderator/.json\\\"\\u003Ehttp://www.reddit.com/subreddits/mine/moderator/.json\\u003C/a\\u003E view with a key of \\u0026quot;mod_permissions\\u0026quot;. The value should be a list of permissions the current user has in that subreddit by name or [\\u0026quot;all\\u0026quot;].\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECurrent available permissions are: access, config, flair, mail, posts, wiki, and all. However, more could be added or changed in the future.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/commit/8c4732418dbe09e06f3fb0f7c911fd16386d7944\\\"\\u003EView the code for this change on github\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"From reddit's student contractor /u/slyf:\\n\\n---\\n\\nModerator permissions are now available in the http://www.reddit.com/subreddits/mine/moderator/.json view with a key of \\\"mod_permissions\\\". The value should be a list of permissions the current user has in that subreddit by name or [\\\"all\\\"].\\n\\nCurrent available permissions are: access, config, flair, mail, posts, wiki, and all. However, more could be added or changed in the future.\\n\\n---\\n\\n[View the code for this change on github](https://github.com/reddit/reddit/commit/8c4732418dbe09e06f3fb0f7c911fd16386d7944)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1iuged\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Deimorz\", \"media\": null, \"score\": 23, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1iuged/api_change_your_moderator_permissions_are_now/\", \"locked\": false, \"name\": \"t3_1iuged\", \"created\": 1374562528.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1iuged/api_change_your_moderator_permissions_are_now/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API change: Your moderator permissions are now available through /subreddits/mine/moderator/.json\", \"created_utc\": 1374533728.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 23}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIn the interest in helping out both Reddit\\u0026#39;s servers and to provide developers with an easy to use source for all Reddit submissions, I am creating an online archive of all Reddit submissions in JSON format.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEach T3 object (a submission that was not spam) is represented as one line of JSON in a file.  Each file is an entire day\\u0026#39;s worth of submissions to Reddit and compressed using gzip.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI will also include Python files to serve as an example of how to process each file.  You can take the information and put it into a database or a flat file or run your own analysis on the data.  I am working backwards from the present and will add approximately a month\\u0026#39;s worth of submissions each day.  Within a month or two, all Reddit submissions will be available in this archive.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease use these archive files instead of using Reddit\\u0026#39;s API if you simply want submission data.  It will spare the Reddit servers a bit of load \\u003Cem\\u003Eand\\u003C/em\\u003E you can get data far more quickly than making requests to the API every two seconds.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe URL for the archive is:  \\u003Ca href=\\\"http://redditfiles.4shared.com\\\"\\u003Ehttp://redditfiles.4shared.com\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you are familiar with JSON data, you should have no problem using this data for your projects.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am creating a search engine for Reddit that is (I hope) superior to what is currently available.  I\\u0026#39;d love to work with anyone who is good at front-end development -- so please send me a message if you are interested.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks and enjoy!  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003ESample Python Parser:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cem\\u003Egzip -cd reddit_2013-04-10.json.gz | thisPythonScript.py\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThere are a lot more variables you can pull from the JSON objects. Just open up the JSON and look at all the variables you have access to and just add them to this script using the same format as I used for the variables already in this script.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E#!/usr/bin/python\\n# --------------------------------------------------------------------------------------------- \\n# This Python script will read from STDIN a piped file containing Reddit JSON data\\n# Example: gzip -cd reddit.json.gz | thisScript.py\\n# --------------------------------------------------------------------------------------------- \\n\\nimport sys\\nimport json\\nimport datetime\\n\\nfor line in iter(sys.stdin.readline, \\u0026quot;\\u0026quot;):\\n    decode = json.loads(line)\\n    subreddit = decode[\\u0026#39;subreddit\\u0026#39;]\\n    url = decode[\\u0026#39;url\\u0026#39;]\\n    id = decode[\\u0026#39;id\\u0026#39;]\\n    title = decode[\\u0026#39;title\\u0026#39;]\\n    score = decode[\\u0026#39;score\\u0026#39;]\\n    time = datetime.datetime.fromtimestamp(int(decode[\\u0026#39;created_utc\\u0026#39;])).strftime(\\u0026#39;%Y-%m-%d %H:%M:%S\\u0026#39;)\\n    link_flare = decode[\\u0026#39;link_flair_css_class\\u0026#39;]\\n    num_comments = decode[\\u0026#39;num_comments\\u0026#39;]\\n    num_reports = decode[\\u0026#39;num_reports\\u0026#39;]\\n    print id, subreddit, link_flare, time, score, num_comments, title.encode(\\u0026#39;utf-8\\u0026#39;)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThere is probably a more elegant way to write this script but I hacked it out quickly for you guys to play with.  If you\\u0026#39;re running linux, you have all the tools you need to start playing with raw Reddit data!  (I just started writing Python code a few weeks ago after a lifetime of writing Perl)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"In the interest in helping out both Reddit's servers and to provide developers with an easy to use source for all Reddit submissions, I am creating an online archive of all Reddit submissions in JSON format.  \\n\\nEach T3 object (a submission that was not spam) is represented as one line of JSON in a file.  Each file is an entire day's worth of submissions to Reddit and compressed using gzip.  \\n\\nI will also include Python files to serve as an example of how to process each file.  You can take the information and put it into a database or a flat file or run your own analysis on the data.  I am working backwards from the present and will add approximately a month's worth of submissions each day.  Within a month or two, all Reddit submissions will be available in this archive.\\n\\nPlease use these archive files instead of using Reddit's API if you simply want submission data.  It will spare the Reddit servers a bit of load *and* you can get data far more quickly than making requests to the API every two seconds.\\n\\nThe URL for the archive is:  http://redditfiles.4shared.com\\n\\nIf you are familiar with JSON data, you should have no problem using this data for your projects.  \\n\\nI am creating a search engine for Reddit that is (I hope) superior to what is currently available.  I'd love to work with anyone who is good at front-end development -- so please send me a message if you are interested.  \\n\\nThanks and enjoy!  \\n\\n**Sample Python Parser:**\\n\\n*gzip -cd reddit_2013-04-10.json.gz | thisPythonScript.py*\\n\\nThere are a lot more variables you can pull from the JSON objects. Just open up the JSON and look at all the variables you have access to and just add them to this script using the same format as I used for the variables already in this script.\\n\\n    #!/usr/bin/python\\n    # --------------------------------------------------------------------------------------------- \\n    # This Python script will read from STDIN a piped file containing Reddit JSON data\\n    # Example: gzip -cd reddit.json.gz | thisScript.py\\n    # --------------------------------------------------------------------------------------------- \\n\\n    import sys\\n    import json\\n    import datetime\\n\\n    for line in iter(sys.stdin.readline, \\\"\\\"):\\n        decode = json.loads(line)\\n        subreddit = decode['subreddit']\\n        url = decode['url']\\n        id = decode['id']\\n        title = decode['title']\\n        score = decode['score']\\n        time = datetime.datetime.fromtimestamp(int(decode['created_utc'])).strftime('%Y-%m-%d %H:%M:%S')\\n        link_flare = decode['link_flair_css_class']\\n        num_comments = decode['num_comments']\\n        num_reports = decode['num_reports']\\n        print id, subreddit, link_flare, time, score, num_comments, title.encode('utf-8')\\n\\nThere is probably a more elegant way to write this script but I hacked it out quickly for you guys to play with.  If you're running linux, you have all the tools you need to start playing with raw Reddit data!  (I just started writing Python code a few weeks ago after a lifetime of writing Perl)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1c4i8q\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"aphexcoil\", \"media\": null, \"score\": 22, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 23, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1365759439.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1c4i8q/attention_reddit_developers_i_am_creating_a/\", \"locked\": false, \"name\": \"t3_1c4i8q\", \"created\": 1365699479.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1c4i8q/attention_reddit_developers_i_am_creating_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Attention Reddit Developers -- I am creating a massive archive of Reddit submissions for developers to use.\", \"created_utc\": 1365670679.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 22}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reditr.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"18n3t5\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kortank\", \"media\": null, \"score\": 25, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/18n3t5/made_a_reddit_client_a_few_months_ago_still_has/\", \"locked\": false, \"name\": \"t3_18n3t5\", \"created\": 1361060090.0, \"url\": \"http://reditr.com\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Made a Reddit Client a few months ago, still has some work to do, but what do you guys think?\", \"created_utc\": 1361031290.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 25}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EJust wondering if anyone has actually succeeded in installing reddit on a server.  I\\u0026#39;m trying to understand how I\\u0026#39;d install it on a VPS server I\\u0026#39;m paying for.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve tried reading the wiki but I think I\\u0026#39;m underestimating the task, so how easy is it to do this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Just wondering if anyone has actually succeeded in installing reddit on a server.  I'm trying to understand how I'd install it on a VPS server I'm paying for.\\n\\nI've tried reading the wiki but I think I'm underestimating the task, so how easy is it to do this?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"mox66\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"sukotu\", \"media\": null, \"score\": 24, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/mox66/does_anyone_actually_have_a_working_reddit_on/\", \"locked\": false, \"name\": \"t3_mox66\", \"created\": 1322262690.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/mox66/does_anyone_actually_have_a_working_reddit_on/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Does anyone actually have a working reddit on their own site?\", \"created_utc\": 1322233890.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 24}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe thread from \\u003Ca href=\\\"/r/IdeasForTheAdmins\\\"\\u003E/r/IdeasForTheAdmins\\u003C/a\\u003E is \\u003Ca href=\\\"http://www.reddit.com/r/ideasfortheadmins/comments/enuzm/colour_blind_redditors_are_having_trouble_with/\\\"\\u003Ehere\\u003C/a\\u003E as is a new shape replacement for the \\u0026quot;You\\u0026#39;ve Got Mail\\u0026quot; icon c/o user \\u003Ca href=\\\"http://www.reddit.com/r/ideasfortheadmins/comments/enuzm/colour_blind_redditors_are_having_trouble_with/c19jooi\\\"\\u003ETsadiq\\u003C/a\\u003E.   \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EUser SquireCD in that thread is the colour blind redditor if you\\u0026#39;re seeking target user feedback. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The thread from /r/IdeasForTheAdmins is [here](http://www.reddit.com/r/ideasfortheadmins/comments/enuzm/colour_blind_redditors_are_having_trouble_with/) as is a new shape replacement for the \\\"You've Got Mail\\\" icon c/o user [Tsadiq](http://www.reddit.com/r/ideasfortheadmins/comments/enuzm/colour_blind_redditors_are_having_trouble_with/c19jooi).   \\n  \\nUser SquireCD in that thread is the colour blind redditor if you're seeking target user feedback. \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"eo2p0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"defrost\", \"media\": null, \"score\": 23, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/eo2p0/fyi_raldi_has_promised_a_props_post_for_polishing/\", \"locked\": false, \"name\": \"t3_eo2p0\", \"created\": 1292740631.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/eo2p0/fyi_raldi_has_promised_a_props_post_for_polishing/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"FYI - Raldi has promised a props post for polishing up a CSS theme for the colour blind :)\", \"created_utc\": 1292711831.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 23}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have done Live CD\\u0026#39;s of Ubuntu and other Linux distros before.See \\u003Ca href=\\\"http://pubserver.co.nr\\\"\\u003Ehttp://pubserver.co.nr\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBoth of these had the option of installation. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHow many of you would be interested in a live CD distro from the VM of Reddit? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: sorry guys reddit doesnt want live CD\\u0026#39;s of the system. Thanks for the support!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have done Live CD's of Ubuntu and other Linux distros before.See http://pubserver.co.nr\\n\\nBoth of these had the option of installation. \\n\\n\\nHow many of you would be interested in a live CD distro from the VM of Reddit? \\n\\nEDIT: sorry guys reddit doesnt want live CD's of the system. Thanks for the support!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"dfbh8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"mesamunefire\", \"media\": null, \"score\": 20, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 17, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/dfbh8/how_many_of_you_would_be_interested_in_a_live_cd/\", \"locked\": false, \"name\": \"t3_dfbh8\", \"created\": 1284776671.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/dfbh8/how_many_of_you_would_be_interested_in_a_live_cd/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How many of you would be interested in a live CD of reddit?\", \"created_utc\": 1284747871.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 20}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"chrome.google.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"cfrxf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"chronoBG\", \"media\": null, \"score\": 23, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/cfrxf/hi_guys_i_made_a_reddit_chrome_plugin_i_think/\", \"locked\": false, \"name\": \"t3_cfrxf\", \"created\": 1276754253.0, \"url\": \"https://chrome.google.com/extensions/detail/cnlggdhmllfmmonecfeabjekpblkocma?hl=en\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Hi guys, i made a reddit chrome plugin. I think this is a nice place to post this; Hope it could be useful to you.\", \"created_utc\": 1276725453.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 23}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"i.imgur.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"45m4qw\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"lecherous_hump\", \"media\": null, \"score\": 22, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/45m4qw/when_you_think_your_script_is_broken_because_the/\", \"locked\": false, \"name\": \"t3_45m4qw\", \"created\": 1455420232.0, \"url\": \"http://i.imgur.com/VBSZ0A5.png\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"When you think your script is broken because the very first result is this\", \"created_utc\": 1455391432.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 22}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWe\\u0026#39;re currently \\u003Ca href=\\\"https://www.reddit.com/r/ModSupport/comments/3lzi4c/limited_beta_lock_a_post/\\\"\\u003Ebeta-testing a feature called lock a post\\u003C/a\\u003E that lets moderators prevent a post from receiving any new comments. There are two API updates related to this that you should be aware of, one of which is in effect now, and one which will go into effect later, when we release this feature to everyone.\\u003C/p\\u003E\\n\\n\\u003Ch4\\u003EIn effect now\\u003C/h4\\u003E\\n\\n\\u003Cp\\u003EAttempting to comment on a locked post via the API will result in the following error:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E{\\n    \\u0026quot;json\\u0026quot;: {\\n        \\u0026quot;errors\\u0026quot;: [\\n            [\\n                \\u0026quot;THREAD_LOCKED\\u0026quot;,\\n                \\u0026quot;Comments are locked.\\u0026quot;,\\n                \\u0026quot;parent\\u0026quot;\\n            ]\\n        ]\\n    }\\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch4\\u003EIn effect later\\u003C/h4\\u003E\\n\\n\\u003Cp\\u003ELink objects will have a new boolean attribute, \\u003Ccode\\u003E\\u0026quot;locked\\u0026quot;\\u003C/code\\u003E. If \\u003Ccode\\u003E\\u0026quot;locked\\u0026quot;\\u003C/code\\u003E is \\u003Ccode\\u003Etrue\\u003C/code\\u003E, then new comments can\\u0026#39;t be added. This will be in effect when we release lock a post to everyone within the next couple of weeks.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"We're currently [beta-testing a feature called lock a post](https://www.reddit.com/r/ModSupport/comments/3lzi4c/limited_beta_lock_a_post/) that lets moderators prevent a post from receiving any new comments. There are two API updates related to this that you should be aware of, one of which is in effect now, and one which will go into effect later, when we release this feature to everyone.\\n\\n#### In effect now\\nAttempting to comment on a locked post via the API will result in the following error:\\n\\n    {\\n        \\\"json\\\": {\\n            \\\"errors\\\": [\\n                [\\n                    \\\"THREAD_LOCKED\\\",\\n                    \\\"Comments are locked.\\\",\\n                    \\\"parent\\\"\\n                ]\\n            ]\\n        }\\n    }\\n\\n#### In effect later\\nLink objects will have a new boolean attribute, `\\\"locked\\\"`. If `\\\"locked\\\"` is `true`, then new comments can't be added. This will be in effect when we release lock a post to everyone within the next couple of weeks.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3m35z8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"tdohz\", \"media\": null, \"score\": 23, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1443033895.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3m35z8/api_updates_for_lock_a_post/\", \"locked\": false, \"name\": \"t3_3m35z8\", \"created\": 1443056855.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3m35z8/api_updates_for_lock_a_post/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API updates for lock a post\", \"created_utc\": 1443028055.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 23}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Ereddit has provided thumbnails for posts for quite some time.  However, these thumbnails are quite small, and forced into a square, whether or not that size is appropriate for the client.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe\\u0026#39;d now like to introduce to you a new system we\\u0026#39;ve been working on to provide multiple-resolution preview images for posts.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThere is now an additional field, \\u003Ccode\\u003Epreview\\u003C/code\\u003E, in \\u003Ca href=\\\"https://www.reddit.com/r/HumanPorn/comments/37h28o/monks_release_glowing_lanterns_into_the_night_sky/.json\\\"\\u003Ethe json\\u003C/a\\u003E output describing a post.  In there, you\\u0026#39;ll find a link to the full-size image as well as a variety of other sizes.  The resized versions will have their aspect ratio smart-cropped to a 1:2 width-to-height ratio if necessary.  You\\u0026#39;ll also find \\u003Ca href=\\\"https://www.reddit.com/r/webdings/comments/388wr5/this_isnt_actually_nsfw/.json\\\"\\u003Ea \\u003Ccode\\u003Evariants\\u003C/code\\u003E section\\u003C/a\\u003E in some posts that, for now, contains blurred versions of NSFW images.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso, in a departure from the thumbnail system, self-posts may also have preview images; we extract any urls from the text and scrape those the same way we do with link posts.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you\\u0026#39;d like to see it in action, we\\u0026#39;re using it on \\u003Ca href=\\\"https://m.reddit.com/\\\"\\u003Ethe mobile web beta\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERemember that you can always send us suggestions and bug reports via \\u003Ca href=\\\"/r/IdeasForTheAdmins\\\"\\u003E/r/IdeasForTheAdmins\\u003C/a\\u003E and \\u003Ca href=\\\"/r/bugs\\\"\\u003E/r/bugs\\u003C/a\\u003E, respectively.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHappy developing!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/compare/2f7837e826cd0be34eabbb1975578545e5f5631c...df671653f2f6b781c08659372be27d765e7598f1\\\"\\u003E\\u003Cstrong\\u003ESee the code behind this change on GitHub.\\u003C/strong\\u003E\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"reddit has provided thumbnails for posts for quite some time.  However, these thumbnails are quite small, and forced into a square, whether or not that size is appropriate for the client.\\n\\nWe'd now like to introduce to you a new system we've been working on to provide multiple-resolution preview images for posts.\\n\\nThere is now an additional field, `preview`, in [the json](https://www.reddit.com/r/HumanPorn/comments/37h28o/monks_release_glowing_lanterns_into_the_night_sky/.json) output describing a post.  In there, you'll find a link to the full-size image as well as a variety of other sizes.  The resized versions will have their aspect ratio smart-cropped to a 1:2 width-to-height ratio if necessary.  You'll also find [a `variants` section](https://www.reddit.com/r/webdings/comments/388wr5/this_isnt_actually_nsfw/.json) in some posts that, for now, contains blurred versions of NSFW images.\\n\\nAlso, in a departure from the thumbnail system, self-posts may also have preview images; we extract any urls from the text and scrape those the same way we do with link posts.\\n\\nIf you'd like to see it in action, we're using it on [the mobile web beta](https://m.reddit.com/).\\n\\nRemember that you can always send us suggestions and bug reports via /r/IdeasForTheAdmins and /r/bugs, respectively.\\n\\nHappy developing!\\n\\n[**See the code behind this change on GitHub.**](https://github.com/reddit/reddit/compare/2f7837e826cd0be34eabbb1975578545e5f5631c...df671653f2f6b781c08659372be27d765e7598f1)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"39yr53\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"xiong_as_admin\", \"media\": null, \"score\": 23, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/39yr53/reddit_change_new_preview_images_available_for/\", \"locked\": false, \"name\": \"t3_39yr53\", \"created\": 1434433779.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/39yr53/reddit_change_new_preview_images_available_for/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[reddit change] New preview images available for posts\", \"created_utc\": 1434404979.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 23}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"35j9no\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ljdawson\", \"media\": null, \"score\": 22, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/35j9no/for_gold_users_is_there_anyway_to_return/\", \"locked\": false, \"name\": \"t3_35j9no\", \"created\": 1431325914.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/35j9no/for_gold_users_is_there_anyway_to_return/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"For gold users, is there anyway to return highlighted new comments in the API?\", \"created_utc\": 1431297114.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 22}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve added 10 new attributes to the data returned in a subreddit\\u0026#39;s \\u003Ccode\\u003Eabout.json\\u003C/code\\u003E (ex. \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/about.json\\\"\\u003Ehttp://www.reddit.com/r/redditdev/about.json\\u003C/a\\u003E ):\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Ecomment_score_hide_mins\\u003C/code\\u003E - the number of minutes that subreddit initially hides comment scores (0 if they don\\u0026#39;t)\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Epublic_traffic\\u003C/code\\u003E - whether the subreddit\\u0026#39;s traffic page is publicly-accessible\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Esubmission_type\\u003C/code\\u003E - which type of submissions the subreddit allows (value is one of \\u003Ccode\\u003Eany\\u003C/code\\u003E, \\u003Ccode\\u003Elink\\u003C/code\\u003E, or \\u003Ccode\\u003Eself\\u003C/code\\u003E)\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Esubmit_link_label\\u003C/code\\u003E - the subreddit\\u0026#39;s custom label for the submit link button, if any\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Esubmit_text_label\\u003C/code\\u003E - the subreddit\\u0026#39;s custom label for the submit text button, if any\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Esubreddit_type\\u003C/code\\u003E - the subreddit\\u0026#39;s type (value is one of \\u003Ccode\\u003Epublic\\u003C/code\\u003E, \\u003Ccode\\u003Eprivate\\u003C/code\\u003E, \\u003Ccode\\u003Erestricted\\u003C/code\\u003E, or in very special cases \\u003Ccode\\u003Egold_restricted\\u003C/code\\u003E or \\u003Ccode\\u003Earchived\\u003C/code\\u003E)\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Euser_is_banned\\u003C/code\\u003E - whether the logged-in user is banned from the subreddit\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Euser_is_contributor\\u003C/code\\u003E - whether the logged-in user is an approved submitter in the subreddit\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Euser_is_moderator\\u003C/code\\u003E - whether the logged-in user is a moderator of the subreddit\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Euser_is_subscriber\\u003C/code\\u003E - whether the logged-in user is subscribed to the subreddit\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EHopefully these should be useful to various people, let me know if you have any questions or anything.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/commit/cc5d31630cff72595a97a35fb999f3d86ddce3f7\\\"\\u003EView the code for this change on github\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Ch3\\u003EEdit: \\u003Ccode\\u003Etype\\u003C/code\\u003E has now been renamed to \\u003Ccode\\u003Esubreddit_type\\u003C/code\\u003E due to the original causing issues in BaconReader.\\u003C/h3\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've added 10 new attributes to the data returned in a subreddit's `about.json` (ex. http://www.reddit.com/r/redditdev/about.json ):\\n\\n* `comment_score_hide_mins` - the number of minutes that subreddit initially hides comment scores (0 if they don't)\\n* `public_traffic` - whether the subreddit's traffic page is publicly-accessible\\n* `submission_type` - which type of submissions the subreddit allows (value is one of `any`, `link`, or `self`)\\n* `submit_link_label` - the subreddit's custom label for the submit link button, if any\\n* `submit_text_label` - the subreddit's custom label for the submit text button, if any\\n* `subreddit_type` - the subreddit's type (value is one of `public`, `private`, `restricted`, or in very special cases `gold_restricted` or `archived`)\\n* `user_is_banned` - whether the logged-in user is banned from the subreddit\\n* `user_is_contributor` - whether the logged-in user is an approved submitter in the subreddit\\n* `user_is_moderator` - whether the logged-in user is a moderator of the subreddit\\n* `user_is_subscriber` - whether the logged-in user is subscribed to the subreddit\\n\\nHopefully these should be useful to various people, let me know if you have any questions or anything.\\n\\n[View the code for this change on github](https://github.com/reddit/reddit/commit/cc5d31630cff72595a97a35fb999f3d86ddce3f7)\\n\\n### Edit: `type` has now been renamed to `subreddit_type` due to the original causing issues in BaconReader.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1kthy1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Deimorz\", \"media\": null, \"score\": 22, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 23, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1377114412.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1kthy1/api_change_10_new_attributes_available_on/\", \"locked\": false, \"name\": \"t3_1kthy1\", \"created\": 1377137762.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1kthy1/api_change_10_new_attributes_available_on/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API change: 10 new attributes available on subreddits\", \"created_utc\": 1377108962.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 22}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"redditanalytics.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1h1wqu\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 23, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1h1wqu/anonymous_ftp_access_for_reddit_comment_data_is/\", \"locked\": false, \"name\": \"t3_1h1wqu\", \"created\": 1372213336.0, \"url\": \"ftp://redditanalytics.com\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Anonymous FTP access for Reddit comment data is now available\", \"created_utc\": 1372184536.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 23}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi! RedditDev community\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe (Simon and Gilles) created a website using the Reddit API.\\nYou will be able to see every video, post or image from any subreddit without opening any link, using an infinite scroll.\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit-roll.com\\\"\\u003Ewww.reddit-roll.com\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EThe website is not finished yet. We would like your feedback! \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat improvements could we do ? What features can you think of ? Did you encounter any bug? Any suggestions?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease visit the website and tell us what\\u0026#39;s on your mind using the feedback button on the left. We appreciate every feedback and the time you spend helping us and the community.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESimon and Gilles out!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi! RedditDev community\\n\\nWe (Simon and Gilles) created a website using the Reddit API.\\nYou will be able to see every video, post or image from any subreddit without opening any link, using an infinite scroll.\\n\\n\\u003E  www.reddit-roll.com\\n\\nThe website is not finished yet. We would like your feedback! \\n\\nWhat improvements could we do ? What features can you think of ? Did you encounter any bug? Any suggestions?\\n\\nPlease visit the website and tell us what's on your mind using the feedback button on the left. We appreciate every feedback and the time you spend helping us and the community.\\n\\nSimon and Gilles out!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"18ivbo\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"simuse\", \"media\": null, \"score\": 21, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 18, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/18ivbo/i_built_a_website_using_the_reddit_api_please/\", \"locked\": false, \"name\": \"t3_18ivbo\", \"created\": 1360891100.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/18ivbo/i_built_a_website_using_the_reddit_api_please/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I built a website using the Reddit API, please visit the Beta and share thoughts\", \"created_utc\": 1360862300.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 21}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have a little obsession of checking \\u003Ca href=\\\"/r/gamedeals\\\"\\u003E/r/gamedeals\\u003C/a\\u003E frequently for new game deals. Just wrote my first useful python script to make my life a little easier. The script checks for new posts made to that sub-reddit and pushes a Growl notification. The Growl notification looks like - \\u003Ca href=\\\"http://i.imgur.com/ET9vL.png\\\"\\u003Ehttp://i.imgur.com/ET9vL.png\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E#!/usr/bin/env python\\n#\\n# This script looks up /r/gamedeals/new every 120 seconds and pushes the notification\\n# for new posts to Growl app on Mac OS\\n#\\n# Uses PRAW - https://github.com/praw-dev/praw (for easy access to Reddit API)\\n# and GNTP - https://github.com/kfdm/gntp (for pushing Growl notification)\\n#\\n\\nimport gntp.notifier\\nimport praw\\nimport time\\n\\n# icon for use with growl notification\\nICON_URL = \\u0026quot;http://cdn2.iconfinder.com/data/icons/crystalproject/128x128/apps/package_games.png\\u0026quot;\\nUSER_AGENT = \\u0026#39;new /r/gamedeals notifier by /u/mpheus\\u0026#39;\\n# reddit doesn\\u0026#39;t shows new posts made to a subreddit without logging in\\n# REDDIT_ID = \\u0026#39;\\u0026#39; # not required\\n# REDDIT_PASS = \\u0026#39;\\u0026#39; # not required\\n\\ngrowl = gntp.notifier.GrowlNotifier(\\n    applicationName = \\u0026quot;/r/gamedeals notifier\\u0026quot;,\\n    notifications = [\\u0026quot;New Deal\\u0026quot;],\\n    defaultNotifications = [\\u0026quot;New Deal\\u0026quot;],\\n    # hostname = \\u0026quot;computer.example.com\\u0026quot;, # Defaults to localhost\\n    # password = \\u0026quot;abc123\\u0026quot; # Defaults to a blank password\\n)\\ngrowl.register()\\n\\nr = praw.Reddit(user_agent=USER_AGENT)\\n# r.login(REDDIT_ID, REDDIT_PASS)\\n\\nalready_done = [] # for storing the uids of posts already notified\\n\\nwhile True:\\n    data = r.get_subreddit(\\u0026#39;gamedeals\\u0026#39;).get_new_by_date(limit=10)\\n    for x in data:\\n        if x.id not in already_done:\\n            already_done.append(x.id)\\n            growl.notify(\\n                noteType = \\u0026quot;New Deal\\u0026quot;,\\n                title = x.domain,\\n                description = x.title,\\n                icon = ICON_URL,\\n                sticky = True, # so that notification remains on the screen until closed\\n                priority = 1,\\n                callback = x.permalink\\n            )\\n            print \\u0026quot;Notified about\\u0026quot;, x.title, \\u0026quot;at\\u0026quot;, time.strftime(\\u0026quot;%d %b - %I:%M:%S %p\\u0026quot;)\\n\\n    print \\u0026quot;Last checked for game deals at\\u0026quot;, time.strftime(\\u0026quot;%d %b - %I:%M:%S %p\\u0026quot;)\\n    time.sleep(120)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI just launch this script in a terminal window and leave it running. I\\u0026#39;m learning python right now and plan on improving the script by storing the uids of posts that already have been notified about in a database and leave the script running on Raspberry Pi that runs 24/7. Growl can be set to receive notification pushed from remote computer (in this case, RPi) as well.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks to reddit dev and PRAW dev for all their amazing work!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: Removed login call and replaced \\u003Ccode\\u003Eget_new\\u003C/code\\u003E function with \\u003Ccode\\u003Eget_new_by_date\\u003C/code\\u003E.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have a little obsession of checking /r/gamedeals frequently for new game deals. Just wrote my first useful python script to make my life a little easier. The script checks for new posts made to that sub-reddit and pushes a Growl notification. The Growl notification looks like - http://i.imgur.com/ET9vL.png\\n\\n\\t#!/usr/bin/env python\\n\\t#\\n\\t# This script looks up /r/gamedeals/new every 120 seconds and pushes the notification\\n\\t# for new posts to Growl app on Mac OS\\n\\t#\\n\\t# Uses PRAW - https://github.com/praw-dev/praw (for easy access to Reddit API)\\n\\t# and GNTP - https://github.com/kfdm/gntp (for pushing Growl notification)\\n\\t#\\n\\t\\n\\timport gntp.notifier\\n\\timport praw\\n\\timport time\\n\\t\\n\\t# icon for use with growl notification\\n\\tICON_URL = \\\"http://cdn2.iconfinder.com/data/icons/crystalproject/128x128/apps/package_games.png\\\"\\n\\tUSER_AGENT = 'new /r/gamedeals notifier by /u/mpheus'\\n\\t# reddit doesn't shows new posts made to a subreddit without logging in\\n\\t# REDDIT_ID = '' # not required\\n\\t# REDDIT_PASS = '' # not required\\n\\t\\n\\tgrowl = gntp.notifier.GrowlNotifier(\\n\\t\\tapplicationName = \\\"/r/gamedeals notifier\\\",\\n\\t\\tnotifications = [\\\"New Deal\\\"],\\n\\t\\tdefaultNotifications = [\\\"New Deal\\\"],\\n\\t\\t# hostname = \\\"computer.example.com\\\", # Defaults to localhost\\n\\t\\t# password = \\\"abc123\\\" # Defaults to a blank password\\n\\t)\\n\\tgrowl.register()\\n\\t\\n\\tr = praw.Reddit(user_agent=USER_AGENT)\\n\\t# r.login(REDDIT_ID, REDDIT_PASS)\\n\\t\\n\\talready_done = [] # for storing the uids of posts already notified\\n\\t\\n\\twhile True:\\n\\t\\tdata = r.get_subreddit('gamedeals').get_new_by_date(limit=10)\\n\\t\\tfor x in data:\\n\\t\\t\\tif x.id not in already_done:\\n\\t\\t\\t\\talready_done.append(x.id)\\n\\t\\t\\t\\tgrowl.notify(\\n\\t\\t\\t\\t\\tnoteType = \\\"New Deal\\\",\\n\\t\\t\\t\\t\\ttitle = x.domain,\\n\\t\\t\\t\\t\\tdescription = x.title,\\n\\t\\t\\t\\t\\ticon = ICON_URL,\\n\\t\\t\\t\\t\\tsticky = True, # so that notification remains on the screen until closed\\n\\t\\t\\t\\t\\tpriority = 1,\\n\\t\\t\\t\\t\\tcallback = x.permalink\\n\\t\\t\\t\\t)\\n\\t\\t\\t\\tprint \\\"Notified about\\\", x.title, \\\"at\\\", time.strftime(\\\"%d %b - %I:%M:%S %p\\\")\\n\\t\\t\\t\\t\\n\\t\\tprint \\\"Last checked for game deals at\\\", time.strftime(\\\"%d %b - %I:%M:%S %p\\\")\\n\\t\\ttime.sleep(120)\\n\\nI just launch this script in a terminal window and leave it running. I'm learning python right now and plan on improving the script by storing the uids of posts that already have been notified about in a database and leave the script running on Raspberry Pi that runs 24/7. Growl can be set to receive notification pushed from remote computer (in this case, RPi) as well.\\n\\nThanks to reddit dev and PRAW dev for all their amazing work!\\n\\nEDIT: Removed login call and replaced `get_new` function with `get_new_by_date`.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"16bh8j\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"mpheus\", \"media\": null, \"score\": 24, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1357899575.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/16bh8j/wrote_my_first_python_script_using_praw_to_check/\", \"locked\": false, \"name\": \"t3_16bh8j\", \"created\": 1357862524.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/16bh8j/wrote_my_first_python_script_using_praw_to_check/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Wrote my first python script using PRAW to check /r/gamedeals for new posts and notify me\", \"created_utc\": 1357833724.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 24}}], \"after\": \"t3_16bh8j\", \"before\": null}}"
        },
        "headers": {
          "CF-RAY": "2b59b994904420c6-LAX",
          "Connection": "keep-alive",
          "Content-Type": "application/json; charset=UTF-8",
          "Date": "Sun, 19 Jun 2016 20:28:00 GMT",
          "Server": "cloudflare-nginx",
          "Strict-Transport-Security": "max-age=15552000; includeSubDomains; preload",
          "Transfer-Encoding": "chunked",
          "Vary": "accept-encoding",
          "X-Moose": "majestic",
          "cache-control": "private, s-maxage=0, max-age=0, must-revalidate",
          "expires": "-1",
          "x-content-type-options": "nosniff",
          "x-frame-options": "SAMEORIGIN",
          "x-ratelimit-remaining": "586.0",
          "x-ratelimit-reset": "120",
          "x-ratelimit-used": "14",
          "x-reddit-tracking": "https://pixel.redditmedia.com/pixel/of_destiny.png?v=Z6%2BBbUzLCxWSEcfO1wkzUXjAxotZa9yJtlqfR2qIh%2BNB8vb4fuQynihbL1JGrzFFRylo15fPWDeUx3NOK7yfOYHlblFRJQfd",
          "x-ua-compatible": "IE=edge",
          "x-xss-protection": "1; mode=block"
        },
        "status": {
          "code": 200,
          "message": "OK"
        },
        "url": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&raw_json=1"
      }
    },
    {
      "recorded_at": "2016-06-19T20:28:02",
      "request": {
        "body": {
          "encoding": "utf-8",
          "string": ""
        },
        "headers": {
          "Accept": "*/*",
          "Accept-Encoding": "identity",
          "Authorization": "bearer YE4ESXZ9lTjIP0ukcopwXM6361g",
          "Connection": "keep-alive",
          "Cookie": "loid=SdAZIdDQoUDzjNqfRk; loidcreated=2016-06-19T20%3A27%3A59.507Z; __cfduid=da3dbedd6bfa050f4f12fea14395b830f1466368079",
          "User-Agent": "prawtools/0.19 PRAW/4.0.0b4 prawcore/0.0.8"
        },
        "method": "GET",
        "uri": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_16bh8j&raw_json=1"
      },
      "response": {
        "body": {
          "encoding": "UTF-8",
          "string": "{\"kind\": \"Listing\", \"data\": {\"modhash\": null, \"children\": [{\"kind\": \"t3\", \"data\": {\"domain\": \"cs229.stanford.edu\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"167m6w\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"cantcopy\", \"media\": null, \"score\": 20, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/167m6w/reddit_recommendation_system_cs_229_machine/\", \"locked\": false, \"name\": \"t3_167m6w\", \"created\": 1357714613.0, \"url\": \"http://cs229.stanford.edu/proj2011/PoonWuZhang-RedditRecommendationSystem.pdf\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Recommendation System - CS 229: Machine Learning [pdf]\", \"created_utc\": 1357685813.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 20}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m just curious, as I now see tons of annoying console log messages in Chrome about event.layerX and event.layerY being deprecated, and it turns out they\\u0026#39;re coming from jQuery...\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHardly the end of the world (although it does affect performance) -- but once webkit does deprecate those, some reddit functionality that relies on jQuery may (maybe?) break....\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm just curious, as I now see tons of annoying console log messages in Chrome about event.layerX and event.layerY being deprecated, and it turns out they're coming from jQuery...\\n\\nHardly the end of the world (although it does affect performance) -- but once webkit does deprecate those, some reddit functionality that relies on jQuery may (maybe?) break....\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"neabt\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"honestbleeps\", \"media\": null, \"score\": 21, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/neabt/will_reddit_be_updating_to_jquery_17_any_time_soon/\", \"locked\": false, \"name\": \"t3_neabt\", \"created\": 1324013791.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/neabt/will_reddit_be_updating_to_jquery_17_any_time_soon/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Will Reddit be updating to jQuery 1.7 any time soon?\", \"created_utc\": 1323984991.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 21}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"blog.reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"a0aff\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ketralnis\", \"media\": null, \"score\": 22, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/a0aff/well_no_longer_be_showing_the_edited_star_for/\", \"locked\": false, \"name\": \"t3_a0aff\", \"created\": 1257226964.0, \"url\": \"http://blog.reddit.com/2009/11/no-stars-for-early.html\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"We'll no longer be showing the \\\"edited\\\" star for comments edited in the first minute of their lifetime\", \"created_utc\": 1257198164.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 22}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWe\\u0026#39;ve released the \\u003Ca href=\\\"https://www.reddit.com/r/redditdev/comments/3m35z8/api_updates_for_lock_a_post/\\\"\\u003Elock a post\\u003C/a\\u003E feature that we previewed last month to all subreddits. You can find more details about the feature on \\u003Ca href=\\\"https://www.reddit.com/r/modnews/comments/3qguqv/moderators_lock_a_post/\\\"\\u003Ethe r/modnews post\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere\\u0026#39;s a rundown of all of the API changes in support of this feature:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003ENew error message on comments\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAttempting to comment on a locked post via the API will result in the following error:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E{\\n    \\u0026quot;json\\u0026quot;: {\\n        \\u0026quot;errors\\u0026quot;: [\\n            [\\n                \\u0026quot;THREAD_LOCKED\\u0026quot;,\\n                \\u0026quot;Comments are locked.\\u0026quot;,\\n                \\u0026quot;parent\\u0026quot;\\n            ]\\n        ]\\n    }\\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003ENew attribute on Link objects\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELink objects have a new boolean attribute, \\u003Ccode\\u003E\\u0026quot;locked\\u0026quot;\\u003C/code\\u003E, that indicate if a post is locked. If \\u003Ccode\\u003E\\u0026quot;locked\\u0026quot;\\u003C/code\\u003E is \\u003Ccode\\u003Etrue\\u003C/code\\u003E, then new comments can\\u0026#39;t be added. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003ENew endpoint for locking/unlocking posts\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThere are two new endpoints to support locking \\u0026amp; unlocking posts from the API: \\u003Ccode\\u003E/api/lock\\u003C/code\\u003E and \\u003Ccode\\u003E/api/unlock\\u003C/code\\u003E. Like most other mod actions, these endpoints require a \\u003Ca href=\\\"https://www.reddit.com/dev/api#fullnames\\\"\\u003Efullname\\u003C/a\\u003E and a \\u003Ca href=\\\"https://www.reddit.com/dev/api#modhashes\\\"\\u003Emodhash\\u003C/a\\u003E. See the \\u003Ca href=\\\"https://www.reddit.com/dev/api#POST_api_lock\\\"\\u003Edocumentation\\u003C/a\\u003E for more information.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"We've released the [lock a post](https://www.reddit.com/r/redditdev/comments/3m35z8/api_updates_for_lock_a_post/) feature that we previewed last month to all subreddits. You can find more details about the feature on [the r/modnews post](https://www.reddit.com/r/modnews/comments/3qguqv/moderators_lock_a_post/).\\n\\nHere's a rundown of all of the API changes in support of this feature:\\n\\n**New error message on comments**\\n\\nAttempting to comment on a locked post via the API will result in the following error:\\n\\n    {\\n        \\\"json\\\": {\\n            \\\"errors\\\": [\\n                [\\n                    \\\"THREAD_LOCKED\\\",\\n                    \\\"Comments are locked.\\\",\\n                    \\\"parent\\\"\\n                ]\\n            ]\\n        }\\n    }\\n\\n**New attribute on Link objects**\\n\\nLink objects have a new boolean attribute, `\\\"locked\\\"`, that indicate if a post is locked. If `\\\"locked\\\"` is `true`, then new comments can't be added. \\n\\n**New endpoint for locking/unlocking posts**\\n\\nThere are two new endpoints to support locking \\u0026 unlocking posts from the API: `/api/lock` and `/api/unlock`. Like most other mod actions, these endpoints require a [fullname](https://www.reddit.com/dev/api#fullnames) and a [modhash](https://www.reddit.com/dev/api#modhashes). See the [documentation](https://www.reddit.com/dev/api#POST_api_lock) for more information.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3qgwvu\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"tdohz\", \"media\": null, \"score\": 19, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3qgwvu/api_support_for_lock_a_post/\", \"locked\": false, \"name\": \"t3_3qgwvu\", \"created\": 1446007823.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3qgwvu/api_support_for_lock_a_post/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API support for lock a post\", \"created_utc\": 1445979023.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 19}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello guys! I recently developed a single-page client for Reddit using \\u003Ccode\\u003EBackbone.js\\u003C/code\\u003E, as a hobby project. \\u003Cem\\u003EReddit Reader\\u003C/em\\u003E allows its users to: \\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003ELogin to Reddit using OAuth 2.0 - Implicit Grant Flow (\\u003Cem\\u003EReddit Reader\\u003C/em\\u003E will never ask you for your password)\\u003C/li\\u003E\\n\\u003Cli\\u003EView subscribed, default and popular subreddits (and even jump to any subreddit you like)\\u003C/li\\u003E\\n\\u003Cli\\u003EView post listings along with the ability to sort them by \\u0026#39;Hot\\u0026#39;, \\u0026#39;New\\u0026#39;, \\u0026#39;Rising\\u0026#39;, \\u0026#39;Controversial\\u0026#39; and \\u0026#39;Top\\u0026#39;\\u003C/li\\u003E\\n\\u003Cli\\u003EView embedded post content (images and videos) by simply clicking on a post thumbnail\\u003C/li\\u003E\\n\\u003Cli\\u003EView comments for any post along with the ability to sort them and jump between threads \\u003C/li\\u003E\\n\\u003Cli\\u003EVote on posts and comments\\u003C/li\\u003E\\n\\u003Cli\\u003ESeamlessly switch between light and dark themes\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EHere\\u0026#39;s the link to the app: \\u003Ca href=\\\"http://enthusiast94.github.io/reddit-reader-backbone/\\\"\\u003Ehttp://enthusiast94.github.io/reddit-reader-backbone/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe source can be found here: \\u003Ca href=\\\"https://github.com/enthusiast94/reddit-reader-backbone\\\"\\u003Ehttps://github.com/enthusiast94/reddit-reader-backbone\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt would be really awesome if you could provide me with some feedback. Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello guys! I recently developed a single-page client for Reddit using `Backbone.js`, as a hobby project. *Reddit Reader* allows its users to: \\n\\n- Login to Reddit using OAuth 2.0 - Implicit Grant Flow (*Reddit Reader* will never ask you for your password)\\n- View subscribed, default and popular subreddits (and even jump to any subreddit you like)\\n- View post listings along with the ability to sort them by 'Hot', 'New', 'Rising', 'Controversial' and 'Top'\\n- View embedded post content (images and videos) by simply clicking on a post thumbnail\\n- View comments for any post along with the ability to sort them and jump between threads \\n- Vote on posts and comments\\n- Seamlessly switch between light and dark themes\\n\\nHere's the link to the app: http://enthusiast94.github.io/reddit-reader-backbone/\\n\\nThe source can be found here: https://github.com/enthusiast94/reddit-reader-backbone\\n\\nIt would be really awesome if you could provide me with some feedback. Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ejbmb\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"enthusiast_94\", \"media\": null, \"score\": 19, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 21, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ejbmb/reddit_reader_a_minimalist_singlepage_client_for/\", \"locked\": false, \"name\": \"t3_3ejbmb\", \"created\": 1437834212.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ejbmb/reddit_reader_a_minimalist_singlepage_client_for/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Reader - A minimalist single-page client for Reddit with an intuitive and responsive UI.\", \"created_utc\": 1437805412.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 19}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI will preface this by saying that I recently had the chance to interview to be Reddit\\u0026#39;s DBA and during the interview process I was giving a vivid account of the poor state of reddit\\u0026#39;s database infrastructure. While I can provide little information about how to speed up Cassandra, I believe I have some insights as to how to make Postgres perform better.\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EThe tables in reddit\\u0026#39;s database are very, very tall. Billions of lines tall from the sound of it. This will cause many issues like poor index performance, and a slow auto vacuum process. The best way to counteract this is to is to partition the tables so that each partition contains less data. The best tool I\\u0026#39;ve found to use this is \\u003Ca href=\\\"https://github.com/keithf4/pg_partman\\\"\\u003Epgpartman\\u003C/a\\u003E. It will run autonomously in the background managing partition creation, and making sure there is no data in the parent table.\\u003C/li\\u003E\\n\\u003Cli\\u003EWith such large tables the auto vacuum process takes a very long time to run. This will cause transaction wraparound warnings, and slow down database performance dramatically. Instead of using the auto vacuum process, something like \\u003Ca href=\\\"https://reorg.github.io/pg_repack/\\\"\\u003Epg_reorg\\u003C/a\\u003E will take care of that much more effeciently for you. It\\u0026#39;s an extension that will work in the background, with very minimal locking, to vacuum the table and rebuild the indexes. Be warned though, it does take a lot of disk space up for big tables, as it creates an exact copy of it in a separate tablespace.\\u003C/li\\u003E\\n\\u003Cli\\u003EYou guys should look into a real partitioning scheme for your data. Moving whole tables out of one DB cluster and putting them in another isn\\u0026#39;t a long term solution in any way. If you don\\u0026#39;t want to invest in building your own there are utilities out there like \\u003Ca href=\\\"https://github.com/citusdata/pg_shard\\\"\\u003Epg_shard\\u003C/a\\u003E that may work, but I have not used it personally. Postgres \\u003Ca href=\\\"https://wiki.postgresql.org/wiki/BDR_User_Guide\\\"\\u003EBi-Directional Replication\\u003C/a\\u003E is coming soon, and I believe it would be a good fit for your usecase.\\u003C/li\\u003E\\n\\u003Cli\\u003EInvestigate using a graph database to store your comment trees. From the sounds of things they are one of the biggest pain points for reddit\\u0026#39;s performance, and moving them into a graph data format should increase this performance. I don\\u0026#39;t have much expertise with graph databases yet, so further investigation would be required.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EIf anyone has database questions, please feel free to pass them along to me.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I will preface this by saying that I recently had the chance to interview to be Reddit's DBA and during the interview process I was giving a vivid account of the poor state of reddit's database infrastructure. While I can provide little information about how to speed up Cassandra, I believe I have some insights as to how to make Postgres perform better.\\n\\n* The tables in reddit's database are very, very tall. Billions of lines tall from the sound of it. This will cause many issues like poor index performance, and a slow auto vacuum process. The best way to counteract this is to is to partition the tables so that each partition contains less data. The best tool I've found to use this is [pgpartman](https://github.com/keithf4/pg_partman). It will run autonomously in the background managing partition creation, and making sure there is no data in the parent table.\\n* With such large tables the auto vacuum process takes a very long time to run. This will cause transaction wraparound warnings, and slow down database performance dramatically. Instead of using the auto vacuum process, something like [pg_reorg](https://reorg.github.io/pg_repack/) will take care of that much more effeciently for you. It's an extension that will work in the background, with very minimal locking, to vacuum the table and rebuild the indexes. Be warned though, it does take a lot of disk space up for big tables, as it creates an exact copy of it in a separate tablespace.\\n* You guys should look into a real partitioning scheme for your data. Moving whole tables out of one DB cluster and putting them in another isn't a long term solution in any way. If you don't want to invest in building your own there are utilities out there like [pg_shard](https://github.com/citusdata/pg_shard) that may work, but I have not used it personally. Postgres [Bi-Directional Replication](https://wiki.postgresql.org/wiki/BDR_User_Guide) is coming soon, and I believe it would be a good fit for your usecase.\\n* Investigate using a graph database to store your comment trees. From the sounds of things they are one of the biggest pain points for reddit's performance, and moving them into a graph data format should increase this performance. I don't have much expertise with graph databases yet, so further investigation would be required.\\n\\nIf anyone has database questions, please feel free to pass them along to me.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2zs2zw\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"mr_mustash\", \"media\": null, \"score\": 21, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2zs2zw/ideas_to_speed_up_reddits_databases/\", \"locked\": false, \"name\": \"t3_2zs2zw\", \"created\": 1426942800.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2zs2zw/ideas_to_speed_up_reddits_databases/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Ideas to speed up Reddit's databases.\", \"created_utc\": 1426914000.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 21}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"imgur.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2izcwj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"geeky2015\", \"media\": null, \"score\": 21, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2izcwj/rworldnews_is_now_scanned_by_google_news/\", \"locked\": false, \"name\": \"t3_2izcwj\", \"created\": 1413096097.0, \"url\": \"http://imgur.com/hwE8lun\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"R/worldnews is now scanned by Google News?\", \"created_utc\": 1413067297.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 21}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis is a friendly reminder that API requests via OAuth should be done over TLS (https) and on the \\u003Ccode\\u003Eoauth.reddit.com\\u003C/code\\u003E domain.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI pushed out a change today that has caught a few scripts/libraries off-guard, as they were using \\u003Ccode\\u003Essl.reddit.com\\u003C/code\\u003E for OAuth requests. While you \\u003Cem\\u003Eshould\\u003C/em\\u003E use \\u003Ccode\\u003Ehttps://ssl.reddit.com\\u003C/code\\u003E for \\u003Cem\\u003Eauthorization\\u003C/em\\u003E and to retrieve your access token, API requests \\u003Cem\\u003Eusing\\u003C/em\\u003E the token need to be against the oauth domain.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIn summary (\\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/OAuth2#wiki-other-important-information\\\"\\u003Efrom the reddit github wiki\\u003C/a\\u003E):\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Ehttps://ssl.reddit.com\\u003C/code\\u003E - for authorization and access_token fetching\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Ehttps://oauth.reddit.com\\u003C/code\\u003E - for API calls with an access token\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This is a friendly reminder that API requests via OAuth should be done over TLS (https) and on the `oauth.reddit.com` domain.\\n\\nI pushed out a change today that has caught a few scripts/libraries off-guard, as they were using `ssl.reddit.com` for OAuth requests. While you *should* use `https://ssl.reddit.com` for *authorization* and to retrieve your access token, API requests *using* the token need to be against the oauth domain.\\n\\nIn summary ([from the reddit github wiki](https://github.com/reddit/reddit/wiki/OAuth2#wiki-other-important-information)):\\n\\n* `https://ssl.reddit.com` - for authorization and access_token fetching\\n* `https://oauth.reddit.com` - for API calls with an access token\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ya5f9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 21, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ya5f9/reminder_use_httpsoauthredditcom_for_api_requests/\", \"locked\": false, \"name\": \"t3_1ya5f9\", \"created\": 1392790303.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1ya5f9/reminder_use_httpsoauthredditcom_for_api_requests/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reminder: Use https://oauth.reddit.com for API requests using OAuth tokens\", \"created_utc\": 1392761503.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 21}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EReddit has to be creating \\u0026#39;things\\u0026#39; at least a hundred times a second. How do they go about creating unique id\\u0026#39;s like \\u003Cstrong\\u003E14kmg6\\u003C/strong\\u003E that don\\u0026#39;t overlap?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Reddit has to be creating 'things' at least a hundred times a second. How do they go about creating unique id's like **14kmg6** that don't overlap?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"14yppc\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bobbonew\", \"media\": null, \"score\": 21, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/14yppc/how_does_reddit_come_up_with_unique_ids_for_things/\", \"locked\": false, \"name\": \"t3_14yppc\", \"created\": 1355729712.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/14yppc/how_does_reddit_come_up_with_unique_ids_for_things/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How does reddit come up with unique IDs for 'things'?\", \"created_utc\": 1355700912.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 21}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"rf15g\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"chromakode\", \"media\": null, \"score\": 22, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/rf15g/new_api_documentation_a_project_from_the_pycon/\", \"locked\": false, \"name\": \"t3_rf15g\", \"created\": 1332835522.0, \"url\": \"http://www.reddit.com/r/changelog/comments/rf0y4/reddit_change_new_api_documentation_a_project/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New API documentation (a project from the PyCon sprints) [xpost from /r/changelog]\", \"created_utc\": 1332806722.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 22}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI run karmawhores.net.  It has been down for a while, I believe because my bot got blocked.  I have attempted to contact the admins numerous times without any response.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI think it is a useful site that people appreciate, so I am asking for help reaching someone who can tell me what to do.  I\\u0026#39;ve paid for it out of my pocket, never once asked for donations or shown ads.  I\\u0026#39;ve offered to reduce the crawl interval down to whatever the admins feel is reasonable (once a day, once a month, I don\\u0026#39;t care).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am hoping that by posting here I can get the attention of someone in a position to help get the site working again.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ekarmawhores /var/www/karmawhores.net: curl --head http://www.reddit.com/user/jontas/about.json\\nHTTP/1.1 403 Forbidden\\nContent-Type: text/html\\nVary: Accept-Encoding\\nCache-Control: no-cache\\nDate: Fri, 02 Dec 2011 22:40:25 GMT\\nConnection: keep-alive\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I run karmawhores.net.  It has been down for a while, I believe because my bot got blocked.  I have attempted to contact the admins numerous times without any response.\\n\\nI think it is a useful site that people appreciate, so I am asking for help reaching someone who can tell me what to do.  I've paid for it out of my pocket, never once asked for donations or shown ads.  I've offered to reduce the crawl interval down to whatever the admins feel is reasonable (once a day, once a month, I don't care).\\n\\nI am hoping that by posting here I can get the attention of someone in a position to help get the site working again.\\n\\nThanks!\\n\\n    karmawhores /var/www/karmawhores.net: curl --head http://www.reddit.com/user/jontas/about.json\\n    HTTP/1.1 403 Forbidden\\n    Content-Type: text/html\\n    Vary: Accept-Encoding\\n    Cache-Control: no-cache\\n    Date: Fri, 02 Dec 2011 22:40:25 GMT\\n    Connection: keep-alive\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"obd0c\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"jontas\", \"media\": null, \"score\": 19, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 14, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/obd0c/karmawhoresnet_needs_some_help/\", \"locked\": false, \"name\": \"t3_obd0c\", \"created\": 1326260525.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/obd0c/karmawhoresnet_needs_some_help/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"karmawhores.net needs some help\", \"created_utc\": 1326231725.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 19}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"bwggv\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ketralnis\", \"media\": null, \"score\": 20, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/bwggv/fork_our_github_repository/\", \"locked\": false, \"name\": \"t3_bwggv\", \"created\": 1272353564.0, \"url\": \"http://github.com/reddit/reddit\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Fork our github repository!\", \"created_utc\": 1272324764.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 20}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhat would be required to use Reddit\\u0026#39;s open source software to replace the PHPBB forums for a company? Do we just follow the directions given on \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/LICENSE\\\"\\u003Ehttps://github.com/reddit/reddit/blob/master/LICENSE\\u003C/a\\u003E 6.3 Derivative Works, and replace the terms \\u0026quot;reddit\\u0026quot;, \\u0026quot;CPAL\\u0026quot;, and make it clear that the new license isn\\u0026#39;t meant for reddit but rather is the basis for a forum system for a video game company\\u0026#39;s website? We\\u0026#39;d like our own \\u0026quot;front page\\u0026quot; on our website, and our own subreddits related to various game topics, etc.  Why not use Reddit itself? Because the company wants more control than that and I\\u0026#39;m advocating the use of Reddit-style forums rather than the more traditional PHPBB-based forums.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"What would be required to use Reddit's open source software to replace the PHPBB forums for a company? Do we just follow the directions given on https://github.com/reddit/reddit/blob/master/LICENSE 6.3 Derivative Works, and replace the terms \\\"reddit\\\", \\\"CPAL\\\", and make it clear that the new license isn't meant for reddit but rather is the basis for a forum system for a video game company's website? We'd like our own \\\"front page\\\" on our website, and our own subreddits related to various game topics, etc.  Why not use Reddit itself? Because the company wants more control than that and I'm advocating the use of Reddit-style forums rather than the more traditional PHPBB-based forums.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4amaoy\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"KJ6BWB\", \"media\": null, \"score\": 19, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4amaoy/reddit_is_open_source_right/\", \"locked\": false, \"name\": \"t3_4amaoy\", \"created\": 1458132442.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4amaoy/reddit_is_open_source_right/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit is open source, right?\", \"created_utc\": 1458103642.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 19}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EA few people have asked about how API clients should deal with the HTTPS \\nuser preference, so here\\u0026#39;s a short guide with everything you should \\nneed to know:\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003EHow can I tell if a user has the preference enabled?\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EA new field has been added to the \\u003Ccode\\u003E/api/login\\u003C/code\\u003E response: \\u003Ccode\\u003Eneed_https\\u003C/code\\u003E. \\nPretty self explanatory, if \\u003Ccode\\u003Etrue\\u003C/code\\u003E you should always make requests to the \\nHTTPS version of the site, if \\u003Ccode\\u003Efalse\\u003C/code\\u003E you aren\\u0026#39;t required to.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003EDoes this affect users of the OAuth API?\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003ENope! OAuth users have been using HTTPS since day one. You\\u0026#39;re beautiful, \\nnever change.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003EWhat happens if a client erroneously requests something over HTTP?\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EIn most cases the behaviour of your app shouldn\\u0026#39;t change. All requests \\nwith a \\u003Ccode\\u003E.json\\u003C/code\\u003E extension or with a hostname of \\u003Ccode\\u003Eapi.reddit.com\\u003C/code\\u003E don\\u0026#39;t currently get \\ntheir responses changed in any way, the User-Agent just gets logged as a \\nclient that didn\\u0026#39;t respect HTTPS preferences. In the future, requests \\nerroneously made over HTTP may be dropped to incentivize proper support for the \\npreference.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf there was no extension and the request wasn\\u0026#39;t via \\u003Ccode\\u003Eapi.reddit.com\\u003C/code\\u003E, \\nthe request gets handled as if it was from a browser, and you\\u0026#39;ll get a \\n307 redirect to the secure version. If your HTTP library doesn\\u0026#39;t \\ntransparently handle 307 redirects, this might cause issues for you.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003EDo JSONP / CORS / .embed / etc requests need to be made via HTTPS?\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003ENot at the moment, however, CORS might cause some issues with HSTS in modern browsers, see below.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003EMy javascript app pulls from the API using AJAX, but I get a CORS failure when the user has \\u0026quot;force HTTPS\\u0026quot; enabled. What do?\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EIt looks like this is the result of bugs in both \\u003Ca href=\\\"https://bugzilla.mozilla.org/show_bug.cgi?id=881830\\\"\\u003EFirefox\\u003C/a\\u003E and \\u003Ca href=\\\"https://code.google.com/p/chromium/issues/detail?id=387198\\\"\\u003EChrome\\u003C/a\\u003E where CORS requests that get redirected via HSTS always fail. A good workaround until those are fixed is to use HTTPS for \\u003Cem\\u003Eall\\u003C/em\\u003E CORS requests.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003ETL;DR\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EThe easiest thing to do is make \\u003Cem\\u003Eall\\u003C/em\\u003E non-OAuth requests via \\u003Ccode\\u003Ehttps://api.reddit.com/\\u003C/code\\u003E, \\nbut if you don\\u0026#39;t want to for some reason, you still need to respect the user\\u0026#39;s \\nHTTPS preferences when they\\u0026#39;re logged in. If the \\u003Ccode\\u003Eneed_https\\u003C/code\\u003E parameter in the \\u003Ccode\\u003E/api/login\\u003C/code\\u003E \\nresponse is \\u003Ccode\\u003Etrue\\u003C/code\\u003E, you need to make all subsequent requests via HTTPS or Bad Things\\u2122 might happen.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"A few people have asked about how API clients should deal with the HTTPS \\nuser preference, so here's a short guide with everything you should \\nneed to know:\\n\\n## How can I tell if a user has the preference enabled?\\n\\nA new field has been added to the `/api/login` response: `need_https`. \\nPretty self explanatory, if `true` you should always make requests to the \\nHTTPS version of the site, if `false` you aren't required to.\\n\\n## Does this affect users of the OAuth API?\\n\\nNope! OAuth users have been using HTTPS since day one. You're beautiful, \\nnever change.\\n\\n## What happens if a client erroneously requests something over HTTP?\\n\\nIn most cases the behaviour of your app shouldn't change. All requests \\nwith a `.json` extension or with a hostname of `api.reddit.com` don't currently get \\ntheir responses changed in any way, the User-Agent just gets logged as a \\nclient that didn't respect HTTPS preferences. In the future, requests \\nerroneously made over HTTP may be dropped to incentivize proper support for the \\npreference.\\n\\nIf there was no extension and the request wasn't via `api.reddit.com`, \\nthe request gets handled as if it was from a browser, and you'll get a \\n307 redirect to the secure version. If your HTTP library doesn't \\ntransparently handle 307 redirects, this might cause issues for you.\\n\\n## Do JSONP / CORS / .embed / etc requests need to be made via HTTPS?\\n\\nNot at the moment, however, CORS might cause some issues with HSTS in modern browsers, see below.\\n\\n## My javascript app pulls from the API using AJAX, but I get a CORS failure when the user has \\\"force HTTPS\\\" enabled. What do?\\n\\nIt looks like this is the result of bugs in both [Firefox](https://bugzilla.mozilla.org/show_bug.cgi?id=881830) and [Chrome](https://code.google.com/p/chromium/issues/detail?id=387198) where CORS requests that get redirected via HSTS always fail. A good workaround until those are fixed is to use HTTPS for *all* CORS requests.\\n\\n## TL;DR\\n\\nThe easiest thing to do is make *all* non-OAuth requests via `https://api.reddit.com/`, \\nbut if you don't want to for some reason, you still need to respect the user's \\nHTTPS preferences when they're logged in. If the `need_https` parameter in the `/api/login` \\nresponse is `true`, you need to make all subsequent requests via HTTPS or Bad Things\\u2122 might happen.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2g2mu2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"largenocream\", \"media\": null, \"score\": 21, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1412277654.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2g2mu2/looking_to_implement_support_for_the_https/\", \"locked\": false, \"name\": \"t3_2g2mu2\", \"created\": 1410436078.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2g2mu2/looking_to_implement_support_for_the_https/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Looking to implement support for the HTTPS preference in your app? Look here!\", \"created_utc\": 1410407278.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 21}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1fnuvk\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"spladug\", \"media\": null, \"score\": 21, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1fnuvk/new_reddit_plugin_meatspace_the_code_for_the_new/\", \"locked\": false, \"name\": \"t3_1fnuvk\", \"created\": 1370394958.0, \"url\": \"https://github.com/reddit/reddit-plugin-meatspace\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New reddit plugin: meatspace. The code for the new meetup QR code system.\", \"created_utc\": 1370366158.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 21}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"16u2lh\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MoederPoeder\", \"media\": null, \"score\": 20, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/16u2lh/i_created_a_basic_reddit_oauth_php_library_check/\", \"locked\": false, \"name\": \"t3_16u2lh\", \"created\": 1358568498.0, \"url\": \"https://github.com/jariz/RedditOAuth\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I created a basic Reddit OAuth PHP library, check it out!\", \"created_utc\": 1358539698.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 20}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"scribd.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"14ts77\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"TeamNarnia\", \"media\": null, \"score\": 20, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/14ts77/reddit_architecture_and_extension_proposal/\", \"locked\": false, \"name\": \"t3_14ts77\", \"created\": 1355488246.0, \"url\": \"http://www.scribd.com/doc/116782750/Reddit-Architecture-and-Extension-Proposal#fullscreen\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Architecture and Extension Proposal\", \"created_utc\": 1355459446.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 20}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"tszxg\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"swiftjr\", \"media\": null, \"score\": 19, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/tszxg/trying_to_figure_out_the_reddit_code_is_there/\", \"locked\": false, \"name\": \"t3_tszxg\", \"created\": 1337356089.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/tszxg/trying_to_figure_out_the_reddit_code_is_there/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Trying to figure out the Reddit code... Is there some sort of docs/diagrams I can look at? Where do I start?\", \"created_utc\": 1337327289.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 19}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve started a new site \\u003Ca href=\\\"http://sciteit.com\\\"\\u003Esciteit.com\\u003C/a\\u003E based on the reddit source code.  The idea is basically to be a place for scientists and others involved in academia to share links to papers, comment on them, with features/structure tailored towards this purpose.  You can read my full rationalization \\u003Ca href=\\\"http://dobby.dyndns.biz/constantamateur/2012/01/11/introducing-sciteit/\\\"\\u003Ehere\\u003C/a\\u003E if you\\u0026#39;re interested.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s still very much \\u0026quot;work in progress\\u0026quot;, but I wanted to say thank you to everyone here and in the IRC channel who has helped me understand the reddit source well enough to get this far.  Special thanks to spladug and bboe.  I fully expect to have many future dumb questions to ask as things break and I hope you will be as patient and indulging as you have been thus far.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIn fact, I already have a question.  My cassandra instance crashed this morning, with error message \\u0026quot;java.lang.RuntimeException: javax.management.InstanceAlreadyExistsException\\u0026quot;.  At this point I don\\u0026#39;t actually use cassandra to do anything as far as I\\u0026#39;m aware (use_query_cache=write_query_queue=False), but it still needs to be running for the site to be live.  I\\u0026#39;m running on a virtual private server with only 500mb of ram, is it possible that this is the problem?  The max heap size seems to be set correctly (to 256mb).\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've started a new site [sciteit.com](http://sciteit.com) based on the reddit source code.  The idea is basically to be a place for scientists and others involved in academia to share links to papers, comment on them, with features/structure tailored towards this purpose.  You can read my full rationalization [here](http://dobby.dyndns.biz/constantamateur/2012/01/11/introducing-sciteit/) if you're interested.\\n\\nIt's still very much \\\"work in progress\\\", but I wanted to say thank you to everyone here and in the IRC channel who has helped me understand the reddit source well enough to get this far.  Special thanks to spladug and bboe.  I fully expect to have many future dumb questions to ask as things break and I hope you will be as patient and indulging as you have been thus far.\\n\\nIn fact, I already have a question.  My cassandra instance crashed this morning, with error message \\\"java.lang.RuntimeException: javax.management.InstanceAlreadyExistsException\\\".  At this point I don't actually use cassandra to do anything as far as I'm aware (use_query_cache=write_query_queue=False), but it still needs to be running for the site to be live.  I'm running on a virtual private server with only 500mb of ram, is it possible that this is the problem?  The max heap size seems to be set correctly (to 256mb).\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"odqll\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MDY\", \"media\": null, \"score\": 20, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/odqll/my_new_site_based_on_the_reddit_source_code/\", \"locked\": false, \"name\": \"t3_odqll\", \"created\": 1326389185.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/odqll/my_new_site_based_on_the_reddit_source_code/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"My new site based on the reddit source code...\", \"created_utc\": 1326360385.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 20}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m a big proponent of OpenID, and I\\u0026#39;d like to see reddit accept it as a means of logging in. I\\u0026#39;d be willing to put some time into writing a good OpenID implementation for reddit, but I wanted to make sure that it won\\u0026#39;t be rejected on philosophical grounds first. If there are objections to my implementation I understand if it ends up being rejected, but before I put my time into it I want to make sure it won\\u0026#39;t be rejected on principle.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm a big proponent of OpenID, and I'd like to see reddit accept it as a means of logging in. I'd be willing to put some time into writing a good OpenID implementation for reddit, but I wanted to make sure that it won't be rejected on philosophical grounds first. If there are objections to my implementation I understand if it ends up being rejected, but before I put my time into it I want to make sure it won't be rejected on principle.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"bb9b3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"AusIV\", \"media\": null, \"score\": 21, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/bb9b3/any_chance_of_accepting_an_openid_patch/\", \"locked\": false, \"name\": \"t3_bb9b3\", \"created\": 1268190511.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/bb9b3/any_chance_of_accepting_an_openid_patch/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Any chance of accepting an OpenID patch?\", \"created_utc\": 1268161711.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 21}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWould this feature be practical?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESubmitter can choose a thumbnail image for a link. The list of possible thumbnails would be generated from available images on the page. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf this creates too much load on the servers, maybe the current thumbnail algorithm can be used to present a list of only three thumbnails at a time, sorted by judged relevancy. By using \\u0026quot;prev\\u0026quot; and \\u0026quot;next\\u0026quot; buttons the submitter can browse the list of images, three at a time. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis can even be made optional by using a \\u0026quot;select thumbnail image\\u0026quot; button on the submission page. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Would this feature be practical?\\n\\nSubmitter can choose a thumbnail image for a link. The list of possible thumbnails would be generated from available images on the page. \\n\\nIf this creates too much load on the servers, maybe the current thumbnail algorithm can be used to present a list of only three thumbnails at a time, sorted by judged relevancy. By using \\\"prev\\\" and \\\"next\\\" buttons the submitter can browse the list of images, three at a time. \\n\\nThis can even be made optional by using a \\\"select thumbnail image\\\" button on the submission page. \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ah9v7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"sigurdur\", \"media\": null, \"score\": 18, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ah9v7/feature_request_allow_submitter_to_choose/\", \"locked\": false, \"name\": \"t3_ah9v7\", \"created\": 1261475309.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ah9v7/feature_request_allow_submitter_to_choose/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Feature request: Allow submitter to choose thumbnail images from the submitted webpage?\", \"created_utc\": 1261446509.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 18}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi everyone,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s been a long time coming, but I am finally ready to receive some feedback on the latest beta version of PRAW4.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003EBreaking Changes\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EPRAW4 is completely backwards incompatible. OAuth was a bolt-on addition to PRAW and as part of stripping out the cookie-based authentication I\\u0026#39;ve decided to make significant changes to how one interacts with PRAW.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo ensure your existing script won\\u0026#39;t break due to the eventual release of PRAW4 I recommend you restrict the allowable version to less than version 4.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIn setup.py that looks like:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esetup(...,\\n      install_requires=[\\u0026#39;praw \\u0026lt;4\\u0026#39;],\\n      ...)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIn requirements.txt that looks like:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Epraw \\u0026lt;4\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch2\\u003EWhy Upgrade\\u003C/h2\\u003E\\n\\n\\u003Ch3\\u003ESpeed\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EThe most compelling reason to upgrade is that PRAW 4 dynamically rate limits using HTTP response headers. This allows bursts in speed when requests are not consistently issued. More importantly, because PRAW \\u0026lt; 4 assumed 1 request every 2 seconds, and reddit via OAuth supports 1 request every 1 second, this should result in a 2x speed up for most clients.\\u003C/p\\u003E\\n\\n\\u003Ch3\\u003ENew Features\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EA lot of the refactoring I\\u0026#39;ve done should make it easier for people to contribute to PRAW. As a result I have no intention to support any older version of PRAW. Of course if someone wants that responsibility, I am happy to share it. In essence new features will very likely only be added to PRAW4+.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003EInstalling PRAW4\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003ETo install PRAW4 I recommend using the praw4 branch zip file via:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Epip install --upgrade https://github.com/praw-dev/praw/archive/praw4.zip\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch2\\u003EPRAW4 Usage\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EIn the time I\\u0026#39;ve had to work on the project, I\\u0026#39;ve focused on refactoring the code, and building out code coverage for the refactored parts. This means there are pieces missing, and I have yet to update the documentation. Consider this the documentation for the time being.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe following hopefully is enough to get you started. I\\u0026#39;ll answer questions as I can, and I encourage people to dig in and help others out as well. Thanks!\\u003C/p\\u003E\\n\\n\\u003Ch3\\u003EInitializing a Reddit instance\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EThe process of creating a \\u003Ccode\\u003EReddit instance\\u003C/code\\u003E is similar, however, two additional parameters are required.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport praw\\nreddit = praw.Reddit(user_agent=\\u0026#39;your agent\\u0026#39;,\\n                     client_id=\\u0026#39;your app client id\\u0026#39;,\\n                     client_secret=\\u0026#39;your app client secret\\u0026#39;)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAt this point you can use the \\u003Ccode\\u003Ereddit\\u003C/code\\u003E instance in \\u003Cem\\u003Eread only\\u003C/em\\u003E mode, however, read only mode will only get you so far.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E# Obtain the top reddit submissions\\nfor submission in reddit.front.hot(limit=10):\\n        print(submission.title)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAt present, PRAW4 only supports the script-based OAuth flow where you provide your username and password and are given access to all scopes for your user account. In order to use script-auth you will need to provide two additional arguments to \\u003Ccode\\u003EReddit\\u003C/code\\u003E:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ereddit = praw.Reddit(user_agent=\\u0026#39;your agent\\u0026#39;,\\n                     client_id=\\u0026#39;your app client id\\u0026#39;,\\n                     client_secret=\\u0026#39;your app client secret\\u0026#39;,\\n                     username=\\u0026#39;your username\\u0026#39;,\\n                     password=\\u0026#39;your password\\u0026#39;)\\n\\n# Output all the information about the authenticated user\\nimport pprint\\npprint.pprint(reddit.user.me())\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIf you want to switch to read only mode when providing a username and password execute:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ereddit.read_only = True\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAnd to switch back to authenticated mode:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ereddit.read_only = False\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch2\\u003EProviding Configuration Options\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EIf you look at the \\u003Ca href=\\\"https://github.com/praw-dev/praw/blob/a00b3e5227a2e89835119be9f0ae4193d78cd2db/praw/reddit.py#L56\\\"\\u003Esource\\u003C/a\\u003E you may notice that the \\u003Ccode\\u003EReddit\\u003C/code\\u003E constructor doesn\\u0026#39;t directly ask for any of these parameters. They are all passed when creating a Config instance.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe \\u003Ca href=\\\"https://github.com/praw-dev/praw/blob/a00b3e5227a2e89835119be9f0ae4193d78cd2db/praw/config.py#L34\\\"\\u003EConfig\\u003C/a\\u003E class looks for these settings in three locations in the following order:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E1) a praw.ini file under the \\u003Ccode\\u003ESITE_NAME\\u003C/code\\u003E section (default: reddit).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E2) An environment variable of the setting name prefixed with \\u003Ccode\\u003Epraw_\\u003C/code\\u003E. For instance to pass your username and password via environment variable rather than directly in the script you may start your program via:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Epraw_username=bboe praw_password=not_my_password python my_script.py\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E3) Directly provided as an argument to \\u003Ccode\\u003EReddit\\u003C/code\\u003E (e.g., as shown above)\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003EExamples\\u003C/h2\\u003E\\n\\n\\u003Ch3\\u003EFront Page\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EAll of the listings pertaining to the front page can be accessed through the \\u003Ccode\\u003Efront\\u003C/code\\u003E object:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ereddit.front.controversial()\\nreddit.front.gilded()\\nreddit.front.hot()\\nreddit.front.new()\\nreddit.front.rising()\\nreddit.front.top()\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAll of these methods return a ListingGenerator object, which means no request is made until you ask for one of the items. Most of these methods are defined in \\u003Ca href=\\\"https://github.com/praw-dev/praw/blob/a00b3e5227a2e89835119be9f0ae4193d78cd2db/praw/models/listing/mixins/base.py#L16\\\"\\u003Emixins/base.py\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Ch3\\u003ESubreddit\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EIndividual subreddits have all the same listings. A subreddit is obtained via:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esubreddit = reddit.subreddit(\\u0026#39;redditdev\\u0026#39;)\\n# Get author of newest listing\\nprint(next(subreddit.new()).author)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch3\\u003ESubmissions\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003ESubmissions objects are accessed via:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esubmission = reddit.submission(id=\\u0026#39;39zje0\\u0026#39;)\\n# or\\nsubmission = reddit.submission(url=\\u0026#39;https://www.reddit.com/r/redditdev/comments/39zje0/reddit_will_soon_only_be_available_over_https/\\u0026#39;)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch3\\u003ERedditors\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EGet info about a specific Redditor via:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eredditor = reddit.redditor(\\u0026#39;bboe\\u0026#39;)\\nprint(redditor.link_karma)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch3\\u003EInbox Methods\\u003C/h3\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ereddit.inbox.all()\\nreddit.inbox.comment_replies()\\nreddit.inbox.mark_read(items)\\nreddit.inbox.mark_unread(items)\\nreddit.inbox.messages()\\nreddit.inbox.sent()\\nreddit.inbox.submission_replies()\\nreddit.inbox.unread()\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch3\\u003ESubmission comments\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003ESubmissions have a \\u003Ccode\\u003Ecomments\\u003C/code\\u003E attribute that is a CommentForest instance. That instance is iterable and represents the top-level comments.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Etop_level = list(submission.comments)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIf you instead want to iterate over all comments you can get a list of comments via:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ecomments = submission.comments.list()\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAs you may be aware there will periodically be \\u003Ccode\\u003EMoreComments\\u003C/code\\u003E instances scattered throughout the forest. Replace those at any time by doing:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esubmission.comments.replace_more()\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch3\\u003EOther\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EThat\\u0026#39;s all I have time to write about now. Consult the integration tests for other features currently supported:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/praw-dev/praw/tree/praw4/tests/integration\\\"\\u003Ehttps://github.com/praw-dev/praw/tree/praw4/tests/integration\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs I mentioned before I appreciate any comments or concerns you might have. Thanks!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you are interested in helping out some of the things that I would love help with are:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003ERe-writing the documentation\\u003C/li\\u003E\\n\\u003Cli\\u003EPorting over the remaining functionality from praw 3.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EIf you\\u0026#39;re interested in either of these please let me know how you\\u0026#39;d like to help.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFinally it would be awesome if you update your existing tools to use PRAW4 so that we can flush out and discuss any issues you encounter. Then we can rebuild the list of PRAW-based tools in order to provide excellent examples.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi everyone,\\n\\nIt's been a long time coming, but I am finally ready to receive some feedback on the latest beta version of PRAW4.\\n\\n## Breaking Changes\\n\\nPRAW4 is completely backwards incompatible. OAuth was a bolt-on addition to PRAW and as part of stripping out the cookie-based authentication I've decided to make significant changes to how one interacts with PRAW.\\n\\nTo ensure your existing script won't break due to the eventual release of PRAW4 I recommend you restrict the allowable version to less than version 4.\\n\\nIn setup.py that looks like:\\n\\n    setup(...,\\n          install_requires=['praw \\u003C4'],\\n          ...)\\n\\n\\nIn requirements.txt that looks like:\\n\\n    praw \\u003C4\\n\\n## Why Upgrade\\n\\n### Speed\\n\\nThe most compelling reason to upgrade is that PRAW 4 dynamically rate limits using HTTP response headers. This allows bursts in speed when requests are not consistently issued. More importantly, because PRAW \\u003C 4 assumed 1 request every 2 seconds, and reddit via OAuth supports 1 request every 1 second, this should result in a 2x speed up for most clients.\\n\\n### New Features\\n\\nA lot of the refactoring I've done should make it easier for people to contribute to PRAW. As a result I have no intention to support any older version of PRAW. Of course if someone wants that responsibility, I am happy to share it. In essence new features will very likely only be added to PRAW4+.\\n\\n## Installing PRAW4\\n\\nTo install PRAW4 I recommend using the praw4 branch zip file via:\\n\\n    pip install --upgrade https://github.com/praw-dev/praw/archive/praw4.zip\\n\\n## PRAW4 Usage\\n\\nIn the time I've had to work on the project, I've focused on refactoring the code, and building out code coverage for the refactored parts. This means there are pieces missing, and I have yet to update the documentation. Consider this the documentation for the time being.\\n\\nThe following hopefully is enough to get you started. I'll answer questions as I can, and I encourage people to dig in and help others out as well. Thanks!\\n\\n### Initializing a Reddit instance\\n\\nThe process of creating a `Reddit instance` is similar, however, two additional parameters are required.\\n\\n    import praw\\n    reddit = praw.Reddit(user_agent='your agent',\\n                         client_id='your app client id',\\n                         client_secret='your app client secret')\\n\\nAt this point you can use the `reddit` instance in _read only_ mode, however, read only mode will only get you so far.\\n\\n    # Obtain the top reddit submissions\\n    for submission in reddit.front.hot(limit=10):\\n            print(submission.title)\\n\\nAt present, PRAW4 only supports the script-based OAuth flow where you provide your username and password and are given access to all scopes for your user account. In order to use script-auth you will need to provide two additional arguments to `Reddit`:\\n\\n    reddit = praw.Reddit(user_agent='your agent',\\n                         client_id='your app client id',\\n                         client_secret='your app client secret',\\n                         username='your username',\\n                         password='your password')\\n\\n    # Output all the information about the authenticated user\\n    import pprint\\n    pprint.pprint(reddit.user.me())\\n\\nIf you want to switch to read only mode when providing a username and password execute:\\n\\n    reddit.read_only = True\\n\\nAnd to switch back to authenticated mode:\\n\\n    reddit.read_only = False\\n\\n## Providing Configuration Options\\n\\nIf you look at the [source](https://github.com/praw-dev/praw/blob/a00b3e5227a2e89835119be9f0ae4193d78cd2db/praw/reddit.py#L56) you may notice that the `Reddit` constructor doesn't directly ask for any of these parameters. They are all passed when creating a Config instance.\\n\\nThe [Config](https://github.com/praw-dev/praw/blob/a00b3e5227a2e89835119be9f0ae4193d78cd2db/praw/config.py#L34) class looks for these settings in three locations in the following order:\\n\\n1) a praw.ini file under the `SITE_NAME` section (default: reddit).\\n\\n2) An environment variable of the setting name prefixed with `praw_`. For instance to pass your username and password via environment variable rather than directly in the script you may start your program via:\\n\\n    praw_username=bboe praw_password=not_my_password python my_script.py\\n\\n3) Directly provided as an argument to `Reddit` (e.g., as shown above)\\n\\n## Examples\\n\\n### Front Page\\nAll of the listings pertaining to the front page can be accessed through the `front` object:\\n\\n    reddit.front.controversial()\\n    reddit.front.gilded()\\n    reddit.front.hot()\\n    reddit.front.new()\\n    reddit.front.rising()\\n    reddit.front.top()\\n\\nAll of these methods return a ListingGenerator object, which means no request is made until you ask for one of the items. Most of these methods are defined in [mixins/base.py](https://github.com/praw-dev/praw/blob/a00b3e5227a2e89835119be9f0ae4193d78cd2db/praw/models/listing/mixins/base.py#L16).\\n\\n### Subreddit\\n\\nIndividual subreddits have all the same listings. A subreddit is obtained via:\\n\\n    subreddit = reddit.subreddit('redditdev')\\n    # Get author of newest listing\\n    print(next(subreddit.new()).author)\\n\\n### Submissions\\n\\nSubmissions objects are accessed via:\\n\\n    submission = reddit.submission(id='39zje0')\\n    # or\\n    submission = reddit.submission(url='https://www.reddit.com/r/redditdev/comments/39zje0/reddit_will_soon_only_be_available_over_https/')\\n\\n\\n### Redditors\\n\\nGet info about a specific Redditor via:\\n\\n    redditor = reddit.redditor('bboe')\\n    print(redditor.link_karma)\\n\\n### Inbox Methods\\n\\n    reddit.inbox.all()\\n    reddit.inbox.comment_replies()\\n    reddit.inbox.mark_read(items)\\n    reddit.inbox.mark_unread(items)\\n    reddit.inbox.messages()\\n    reddit.inbox.sent()\\n    reddit.inbox.submission_replies()\\n    reddit.inbox.unread()\\n\\n### Submission comments\\n\\nSubmissions have a `comments` attribute that is a CommentForest instance. That instance is iterable and represents the top-level comments.\\n\\n    top_level = list(submission.comments)\\n\\nIf you instead want to iterate over all comments you can get a list of comments via:\\n\\n    comments = submission.comments.list()\\n\\nAs you may be aware there will periodically be `MoreComments` instances scattered throughout the forest. Replace those at any time by doing:\\n\\n    submission.comments.replace_more()\\n\\n### Other\\n\\nThat's all I have time to write about now. Consult the integration tests for other features currently supported:\\n\\nhttps://github.com/praw-dev/praw/tree/praw4/tests/integration\\n\\nAs I mentioned before I appreciate any comments or concerns you might have. Thanks!\\n\\nIf you are interested in helping out some of the things that I would love help with are:\\n\\n* Re-writing the documentation\\n* Porting over the remaining functionality from praw 3.\\n\\nIf you're interested in either of these please let me know how you'd like to help.\\n\\nFinally it would be awesome if you update your existing tools to use PRAW4 so that we can flush out and discuss any issues you encounter. Then we can rebuild the list of PRAW-based tools in order to provide excellent examples.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4bvp73\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"bboe\", \"media\": null, \"score\": 20, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 33, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1459196712.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4bvp73/praw_4_beta_feedback_desired/\", \"locked\": false, \"name\": \"t3_4bvp73\", \"created\": 1458920805.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4bvp73/praw_4_beta_feedback_desired/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"PRAW 4 Beta: Feedback Desired\", \"created_utc\": 1458892005.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 20}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey folks, I am the author of \\u003Ccode\\u003Eprawoauth2\\u003C/code\\u003E, a library which makes writing Reddit bots/apps using OAuth2 super easy and simple. Lately I have been receiving private messages, seeking help in migration, so I thought I would write a tutorial and redirect them here next time. Most of the text below is copied from the documentation.\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003ELink to \\u003Ca href=\\\"https://github.com/avinassh/prawoauth2\\\"\\u003EGithub\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003ELink to \\u003Ca href=\\\"http://prawoauth2.readthedocs.org/\\\"\\u003EDocumentation\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003ETLDR version: Remove every references of \\u003Ccode\\u003Eusername\\u003C/code\\u003E, \\u003Ccode\\u003Epassword\\u003C/code\\u003E and \\u003Ccode\\u003Epraw.login\\u003C/code\\u003E in your code. \\u003Ca href=\\\"https://www.reddit.com/prefs/apps/\\\"\\u003ERegister\\u003C/a\\u003E your bot/app in Reddit. Create an instance of \\u003Ccode\\u003EPrawOAuth2Mini\\u003C/code\\u003E with valid params. So you have to remove one line and add another in your main code. That\\u0026#39;s all ;)\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EInstallation:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Epip install prawoauth2\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EStop using \\u003Ccode\\u003Epraw.login\\u003C/code\\u003E. Your current code probably uses Reddit account (your\\u0026#39;s or your bot\\u0026#39;s) username and password with \\u003Ccode\\u003Epraw.login\\u003C/code\\u003E and you have to remove that. With OAuth2, there should be NO references to Reddit username and password in your code:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ereddit_client = praw.Reddit(user_agent=user_agent)\\n# you gotta remove the following line\\nreddit_client.login(reddit_username, reddit_password)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EFigure out what all \\u003Ccode\\u003Escopes\\u003C/code\\u003E you need. Scopes specify what all permissions your app (or bot script) needs from user\\u0026#39;s Reddit account(or your bot account), like read private messages, spend gold credits etc. You can read about different scopes on praw\\u0026#39;s \\u003Ca href=\\\"https://praw.readthedocs.org/en/stable/pages/oauth.html#oauth-scopes\\\"\\u003Eofficial documentation\\u003C/a\\u003E. For example, if your bot replies to comments and also responds to private messages, then it will need atleast these scopes:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Escopes = [\\u0026#39;identity\\u0026#39;, \\u0026#39;read\\u0026#39;, \\u0026#39;submit\\u0026#39;, \\u0026#39;privatemessages\\u0026#39;]\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EYou need to register your bot/app on Reddit. The praw documentation already has a nice overview about \\u003Ca href=\\\"https://praw.readthedocs.org/en/stable/pages/oauth.html#a-step-by-step-oauth-guide\\\"\\u003Ehow\\u003C/a\\u003E. Go \\u003Ca href=\\\"https://www.reddit.com/prefs/apps/\\\"\\u003Ehere\\u003C/a\\u003E and here\\u0026#39;s what I recommend for a bot:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://i.imgur.com/qiBMIl1.png\\\"\\u003Eregistering\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EJust make sure you are setting \\u003Ccode\\u003Eredirect uri\\u003C/code\\u003E to \\u003Ccode\\u003Ehttp://127.0.0.1:65010/authorize_callback\\u003C/code\\u003E. Rest doesn\\u0026#39;t matter. Once you have created the app, you will get \\u003Ccode\\u003Eapp_key\\u003C/code\\u003E and \\u003Ccode\\u003Eapp_secret\\u003C/code\\u003E:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://i.imgur.com/qxEKyOe.png\\\"\\u003Etokens\\u003C/a\\u003E\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003E\\u003Ccode\\u003Eprawoauth2\\u003C/code\\u003E comes with two components, \\u003Ccode\\u003EPrawOAuth2Mini\\u003C/code\\u003E and \\u003Ccode\\u003EPrawOAuth2Server\\u003C/code\\u003E. \\u003Ccode\\u003EPrawOAuth2Server\\u003C/code\\u003E authorizes your app/script with the Reddit account and gives you access token. This is one time only operation. Let\\u0026#39;s call this script as \\u003Ccode\\u003Eonetime.py\\u003C/code\\u003E. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003EPrawOAuth2Mini\\u003C/code\\u003E uses these tokens for all next transactions with Reddit. Remember, for a bot, you only need a valid \\u003Ccode\\u003Erefresh_token\\u003C/code\\u003E so it can \\u003Cem\\u003Erefresh\\u003C/em\\u003E the expired \\u003Ccode\\u003Eaccess_token\\u003C/code\\u003E.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003ESo lets first build \\u003Ccode\\u003Eonetime.py\\u003C/code\\u003E. As name suggests, you need to run this script only once for the first time. You should run this script locally, on your computer since it requires browser access. Import the required modules and create a \\u003Ccode\\u003Epraw\\u003C/code\\u003E instance:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport praw\\nfrom prawoauth2 import PrawOAuth2Server        \\nuser_agent = \\u0026#39;some string that uniquely identifies my bot\\u0026#39;\\nreddit_client = praw.Reddit(user_agent=user_agent)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EPass the \\u003Ccode\\u003Eapp_key\\u003C/code\\u003E and \\u003Ccode\\u003Eapp_secret\\u003C/code\\u003E of your app, along with the praw instance to the \\u003Ccode\\u003EPrawOAuth2Server\\u003C/code\\u003E\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eoauthserver = PrawOAuth2Server(reddit_client, app_key, app_secret,\\n                               state=user_agent, scopes=scopes)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003ENow, you need to start the oauth client server, which runs internally. \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eoauthserver.start()\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThe moment you start it, it opens the default web browser. If you are not logged in, log in with your bot account credentials and authorize the script (i.e. clicking on \\u003Ccode\\u003Eaccept\\u003C/code\\u003E).\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EOnce it is successful, you can get the tokens by calling \\u003Ccode\\u003Eget_access_codes\\u003C/code\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Etokens = oauthserver.get_access_codes()\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThe \\u003Ccode\\u003Etokens\\u003C/code\\u003E is a \\u003Ccode\\u003Edict\\u003C/code\\u003E type:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026gt;\\u0026gt;\\u0026gt; tokens\\n{\\u0026#39;access_token\\u0026#39;: \\u0026#39;2...U\\u0026#39;, \\u0026#39;scope\\u0026#39;: set([\\u0026#39;identity\\u0026#39;, \\u0026#39;read\\u0026#39;, \\u0026#39;submit\\u0026#39;]), \\u0026#39;refresh_token\\u0026#39;: u\\u0026#39;2...s\\u0026#39;}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003ENow in your main script, create an instance of \\u003Ccode\\u003EPrawOAuth2Mini\\u003C/code\\u003E with all the required parameters:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ereddit_client = praw.Reddit(user_agent=user_agent)\\noauth_helper = PrawOAuth2Mini(reddit_client, app_key=app_key,\\n                              app_secret=app_secret,\\n                              access_token=access_token,\\n                              refresh_token=refresh_token,\\n                              scopes=scopes)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EThat\\u0026#39;s all! Now rest of your code would require no changes and it will work as usual.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EHere\\u0026#39;s a working example of \\u003Ccode\\u003Eonetime.py\\u003C/code\\u003E - \\u003Ca href=\\\"https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/onetime.py\\\"\\u003Elink\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003EAnd here\\u0026#39;s an example of bot which uses PRAW OAuth2 - \\u003Ca href=\\\"https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/bot.py\\\"\\u003Elink\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003EDetailed user guide of \\u003Ccode\\u003Eprawoauth2\\u003C/code\\u003E - \\u003Ca href=\\\"http://prawoauth2.readthedocs.org/usage_guide.html\\\"\\u003Elink\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003EWhen to use refresh - \\u003Ca href=\\\"http://prawoauth2.readthedocs.org/tips_and_more.html#when-to-use-refresh\\\"\\u003Elink\\u003C/a\\u003E.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey folks, I am the author of `prawoauth2`, a library which makes writing Reddit bots/apps using OAuth2 super easy and simple. Lately I have been receiving private messages, seeking help in migration, so I thought I would write a tutorial and redirect them here next time. Most of the text below is copied from the documentation.\\n\\n - Link to [Github](https://github.com/avinassh/prawoauth2)\\n - Link to [Documentation](http://prawoauth2.readthedocs.org/)\\n\\n---\\n\\nTLDR version: Remove every references of `username`, `password` and `praw.login` in your code. [Register](https://www.reddit.com/prefs/apps/) your bot/app in Reddit. Create an instance of `PrawOAuth2Mini` with valid params. So you have to remove one line and add another in your main code. That's all ;)\\n\\n---\\nInstallation:\\n\\n    pip install prawoauth2\\n\\n---\\n\\n1. Stop using `praw.login`. Your current code probably uses Reddit account (your's or your bot's) username and password with `praw.login` and you have to remove that. With OAuth2, there should be NO references to Reddit username and password in your code:\\n\\n        reddit_client = praw.Reddit(user_agent=user_agent)\\n        # you gotta remove the following line\\n        reddit_client.login(reddit_username, reddit_password)\\n\\n2. Figure out what all `scopes` you need. Scopes specify what all permissions your app (or bot script) needs from user's Reddit account(or your bot account), like read private messages, spend gold credits etc. You can read about different scopes on praw's [official documentation](https://praw.readthedocs.org/en/stable/pages/oauth.html#oauth-scopes). For example, if your bot replies to comments and also responds to private messages, then it will need atleast these scopes:\\n\\n        scopes = ['identity', 'read', 'submit', 'privatemessages']\\n\\n3. You need to register your bot/app on Reddit. The praw documentation already has a nice overview about [how](https://praw.readthedocs.org/en/stable/pages/oauth.html#a-step-by-step-oauth-guide). Go [here](https://www.reddit.com/prefs/apps/) and here's what I recommend for a bot:\\n    \\n    [registering](http://i.imgur.com/qiBMIl1.png)\\n    \\n    Just make sure you are setting `redirect uri` to `http://127.0.0.1:65010/authorize_callback`. Rest doesn't matter. Once you have created the app, you will get `app_key` and `app_secret`:\\n\\n    [tokens](http://i.imgur.com/qxEKyOe.png)\\n\\n4. `prawoauth2` comes with two components, `PrawOAuth2Mini` and `PrawOAuth2Server`. `PrawOAuth2Server` authorizes your app/script with the Reddit account and gives you access token. This is one time only operation. Let's call this script as `onetime.py`. \\n\\n    `PrawOAuth2Mini` uses these tokens for all next transactions with Reddit. Remember, for a bot, you only need a valid `refresh_token` so it can *refresh* the expired `access_token`.\\n\\n5. So lets first build `onetime.py`. As name suggests, you need to run this script only once for the first time. You should run this script locally, on your computer since it requires browser access. Import the required modules and create a `praw` instance:\\n    \\n        import praw\\n        from prawoauth2 import PrawOAuth2Server        \\n        user_agent = 'some string that uniquely identifies my bot'\\n        reddit_client = praw.Reddit(user_agent=user_agent)\\n\\n6. Pass the `app_key` and `app_secret` of your app, along with the praw instance to the `PrawOAuth2Server`\\n\\n        oauthserver = PrawOAuth2Server(reddit_client, app_key, app_secret,\\n                                       state=user_agent, scopes=scopes)\\n\\n7. Now, you need to start the oauth client server, which runs internally. \\n\\n        oauthserver.start()\\n\\n    The moment you start it, it opens the default web browser. If you are not logged in, log in with your bot account credentials and authorize the script (i.e. clicking on `accept`).\\n\\n8. Once it is successful, you can get the tokens by calling `get_access_codes`.\\n\\n        tokens = oauthserver.get_access_codes()\\n\\n    The `tokens` is a `dict` type:\\n\\n        \\u003E\\u003E\\u003E tokens\\n        {'access_token': '2...U', 'scope': set(['identity', 'read', 'submit']), 'refresh_token': u'2...s'}\\n\\n9. Now in your main script, create an instance of `PrawOAuth2Mini` with all the required parameters:\\n    \\n        reddit_client = praw.Reddit(user_agent=user_agent)\\n        oauth_helper = PrawOAuth2Mini(reddit_client, app_key=app_key,\\n                                      app_secret=app_secret,\\n                                      access_token=access_token,\\n                                      refresh_token=refresh_token,\\n                                      scopes=scopes)\\n\\nThat's all! Now rest of your code would require no changes and it will work as usual.\\n\\n---\\n\\n- Here's a working example of `onetime.py` - [link](https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/onetime.py)\\n- And here's an example of bot which uses PRAW OAuth2 - [link](https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/bot.py)\\n- Detailed user guide of `prawoauth2` - [link](http://prawoauth2.readthedocs.org/usage_guide.html)\\n- When to use refresh - [link](http://prawoauth2.readthedocs.org/tips_and_more.html#when-to-use-refresh).\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3lotxj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"avinassh\", \"media\": null, \"score\": 18, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 17, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1442816336.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/\", \"locked\": false, \"name\": \"t3_3lotxj\", \"created\": 1442795461.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Tutorial] How to migrate your existing bots from HTTP to OAuth2 (Python-PRAW)\", \"created_utc\": 1442766661.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 18}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am in the (much drawn-out) process of trying to create a special-interest online community.  I would like to include functionality similar to reddit\\u0026#39;s.  Since I am already familiar with python/pylons/pyramid, I\\u0026#39;m wondering if the reddit codebase itself might be a good place for me to start.  I don\\u0026#39;t yet know the codebase well enough to know if it will need significant alteration to suit my needs.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs this permitted under the license?  If I choose to do so, what kinds of challenges might I expect to encounter?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHow challenging is it to set up a reddit instance?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am in the (much drawn-out) process of trying to create a special-interest online community.  I would like to include functionality similar to reddit's.  Since I am already familiar with python/pylons/pyramid, I'm wondering if the reddit codebase itself might be a good place for me to start.  I don't yet know the codebase well enough to know if it will need significant alteration to suit my needs.\\n\\nIs this permitted under the license?  If I choose to do so, what kinds of challenges might I expect to encounter?\\n\\nHow challenging is it to set up a reddit instance?\\n\\nThanks!\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3dea1z\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"midlife_science\", \"media\": null, \"score\": 19, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3dea1z/am_i_permitted_to_use_reddit_code_to_host_my_own/\", \"locked\": false, \"name\": \"t3_3dea1z\", \"created\": 1437006215.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3dea1z/am_i_permitted_to_use_reddit_code_to_host_my_own/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Am I permitted to use reddit code to host my own community site?\", \"created_utc\": 1436977415.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 19}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Ehide/unhide endpoints will now accept a comma separated list of thing ids much like some of the other endpoints. The current limit is 50 ids per request.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAdditionally if you pass in a bad thing id the endpoint will now return an error rather then silently failing and happily responding with a 200 regardless of success.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"hide/unhide endpoints will now accept a comma separated list of thing ids much like some of the other endpoints. The current limit is 50 ids per request.\\n\\nAdditionally if you pass in a bad thing id the endpoint will now return an error rather then silently failing and happily responding with a 200 regardless of success.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"384w9i\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"thorarakis\", \"media\": null, \"score\": 19, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/384w9i/hide_and_unhide_endpoints_now_support_comma/\", \"locked\": false, \"name\": \"t3_384w9i\", \"created\": 1433223903.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/384w9i/hide_and_unhide_endpoints_now_support_comma/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"hide and unhide endpoints now support comma separated thing ids and errors\", \"created_utc\": 1433195103.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 19}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/r/changelog/comments/36t36j/reddit_change_all_remaining_uses_of_liked_and/\\\"\\u003EAs announced in /r/changelog\\u003C/a\\u003E, the remaining uses of \\u0026quot;liked\\u0026quot; and \\u0026quot;disliked\\u0026quot; have been replaced with \\u0026quot;upvoted\\u0026quot; and \\u0026quot;downvoted\\u0026quot;. This includes changing the urls for a user\\u0026#39;s liked and disliked pages from /user/\\u0026lt;username\\u0026gt;/liked to /user/\\u0026lt;username\\u0026gt;/upvoted and /user/\\u0026lt;username\\u0026gt;/disliked to /user/\\u0026lt;username\\u0026gt;/downvoted. To try to avoid causing any issues with API clients, the old urls are still functional right now in the API (but redirect on the site itself), but please try to move over to the new ones as soon as you can.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"[As announced in /r/changelog](https://www.reddit.com/r/changelog/comments/36t36j/reddit_change_all_remaining_uses_of_liked_and/), the remaining uses of \\\"liked\\\" and \\\"disliked\\\" have been replaced with \\\"upvoted\\\" and \\\"downvoted\\\". This includes changing the urls for a user's liked and disliked pages from /user/\\u003Cusername\\u003E/liked to /user/\\u003Cusername\\u003E/upvoted and /user/\\u003Cusername\\u003E/disliked to /user/\\u003Cusername\\u003E/downvoted. To try to avoid causing any issues with API clients, the old urls are still functional right now in the API (but redirect on the site itself), but please try to move over to the new ones as soon as you can.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"36t3ya\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Deimorz\", \"media\": null, \"score\": 21, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/36t3ya/reddit_change_user_liked_and_disliked_paths_are/\", \"locked\": false, \"name\": \"t3_36t3ya\", \"created\": 1432275405.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/36t3ya/reddit_change_user_liked_and_disliked_paths_are/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[reddit change] User /liked and /disliked paths are now /upvoted and /downvoted respectively (the old paths still work for API clients for now)\", \"created_utc\": 1432246605.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 21}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThere are many existing companion sites like redditlist, metareddit, subreddits.org, etc, that contain the word \\u0026quot;reddit\\u0026quot; in their domain names, is this in violation of the new rules?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m asking because I have long planned to make another companion site (although haven\\u0026#39;t found the time to actually code it), and have already registered a domain name containing \\u0026quot;reddit\\u0026quot;. Do I have to go for another domain name or is it fine if the website\\u0026#39;s name doesn\\u0026#39;t contain \\u0026quot;reddit\\u0026quot;, while its domain name does?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"There are many existing companion sites like redditlist, metareddit, subreddits.org, etc, that contain the word \\\"reddit\\\" in their domain names, is this in violation of the new rules?\\n\\nI'm asking because I have long planned to make another companion site (although haven't found the time to actually code it), and have already registered a domain name containing \\\"reddit\\\". Do I have to go for another domain name or is it fine if the website's name doesn't contain \\\"reddit\\\", while its domain name does?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"334522\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"AnAppAMonth\", \"media\": null, \"score\": 18, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1429443635.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/334522/does_the_no_reddit_in_name_rule_apply_to_domain/\", \"locked\": false, \"name\": \"t3_334522\", \"created\": 1429467352.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/334522/does_the_no_reddit_in_name_rule_apply_to_domain/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Does the \\\"no reddit in name\\\" rule apply to domain names?\", \"created_utc\": 1429438552.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 18}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESmall API update today: Link \\u0026amp; Comment objects now have an \\u0026quot;archived\\u0026quot; attribute on them. If \\u0026quot;true\\u0026quot;, that Link or Comment is too old to be voted on or replied to.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe commenting/voting endpoints already rejected attempts to comment/vote on those items, but now you can check ahead of time!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Small API update today: Link \\u0026 Comment objects now have an \\\"archived\\\" attribute on them. If \\\"true\\\", that Link or Comment is too old to be voted on or replied to.\\n\\nThe commenting/voting endpoints already rejected attempts to comment/vote on those items, but now you can check ahead of time!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2utf6w\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 20, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2utf6w/new_attribute_on_linkscomments_archived/\", \"locked\": false, \"name\": \"t3_2utf6w\", \"created\": 1423125389.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2utf6w/new_attribute_on_linkscomments_archived/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New attribute on links/comments: archived\", \"created_utc\": 1423096589.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 20}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EGreetings!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ereddit now supports the OAuth2 \\u003Ca href=\\\"https://tools.ietf.org/html/rfc6749#section-4.2\\\"\\u003Eimplicit grant flow\\u003C/a\\u003E, which means you should now be able to create front-end only, JavaScript web apps that access reddit\\u0026#39;s APIs. The \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/OAuth2#authorization-implicit-grant-flow\\\"\\u003Ereddit OAuth2 docs\\u003C/a\\u003E have been updated with information on the flow (and, of course, please provide suggestions for documentation improvements here).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003ENote\\u003C/strong\\u003E: \\u003Cem\\u003EOnly apps \\u003Ca href=\\\"/prefs/apps\\\"\\u003Ecreated as\\u003C/a\\u003E \\u0026quot;installed\\u0026quot; type apps may use the implicit flow. \\u0026quot;web\\u0026quot; and \\u0026quot;script\\u0026quot; type apps are considered \\u0026quot;confidential\\u0026quot; (i.e., they have secrets). Since you cannot safely send a secret via the implicit flow, we have elected to disallow implicit access to apps with secrets.\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECORS restrictions on OAuth2 requests have been loosened to allow for this. Non-oauth2 CORS restrictions are unchanged.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso, reddit now supports 2 methods for accessing OAuth2-only APIs without actually logging in as a user: We\\u0026#39;ve implemented the \\u0026quot;client_credentials\\u0026quot; grant (for confidential clients) and created a similar extension grant (for non-confidential clients). Again, the \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/OAuth2#application-only-oauth\\\"\\u003Ereddit OAuth2 docs\\u003C/a\\u003E have been updated with more info.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe two primary advantages of application-only OAuth2 access to the reddit API are:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003EUser-less access to OAuth2 only APIs, such as \\u003Ca href=\\\"https://www.reddit.com/dev/api#GET_api_v1_user_%7Busername%7D_trophies\\\"\\u003Etrophies\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003ESimplification of your application code - all your standard API requests can go to the same domain, oauth.reddit.com, always using an \\u003Ccode\\u003EAuthorization\\u003C/code\\u003E header.\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Greetings!\\n\\nreddit now supports the OAuth2 [implicit grant flow](https://tools.ietf.org/html/rfc6749#section-4.2), which means you should now be able to create front-end only, JavaScript web apps that access reddit's APIs. The [reddit OAuth2 docs](https://github.com/reddit/reddit/wiki/OAuth2#authorization-implicit-grant-flow) have been updated with information on the flow (and, of course, please provide suggestions for documentation improvements here).\\n\\n**Note**: *Only apps [created as](/prefs/apps) \\\"installed\\\" type apps may use the implicit flow. \\\"web\\\" and \\\"script\\\" type apps are considered \\\"confidential\\\" (i.e., they have secrets). Since you cannot safely send a secret via the implicit flow, we have elected to disallow implicit access to apps with secrets.*\\n\\nCORS restrictions on OAuth2 requests have been loosened to allow for this. Non-oauth2 CORS restrictions are unchanged.\\n\\nAlso, reddit now supports 2 methods for accessing OAuth2-only APIs without actually logging in as a user: We've implemented the \\\"client_credentials\\\" grant (for confidential clients) and created a similar extension grant (for non-confidential clients). Again, the [reddit OAuth2 docs](https://github.com/reddit/reddit/wiki/OAuth2#application-only-oauth) have been updated with more info.\\n\\nThe two primary advantages of application-only OAuth2 access to the reddit API are:\\n\\n1. User-less access to OAuth2 only APIs, such as [trophies](https://www.reddit.com/dev/api#GET_api_v1_user_{username}_trophies)\\n1. Simplification of your application code - all your standard API requests can go to the same domain, oauth.reddit.com, always using an `Authorization` header.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2owrnn\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 19, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 32, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1419009561.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2owrnn/oauth2_implicit_grants_cors_apponly_oauth2/\", \"locked\": false, \"name\": \"t3_2owrnn\", \"created\": 1418278960.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2owrnn/oauth2_implicit_grants_cors_apponly_oauth2/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[OAuth2] Implicit grants, CORS, \\u0026 app-only OAuth2\", \"created_utc\": 1418250160.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 19}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI wanted to take a moment to mention a more powerful search method that I\\u0026#39;ve developed for reddit.  There is a very basic front-end that I have put together, but also a back-end that can be used by developers who wish to roll their own front-end interface.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs an example, let\\u0026#39;s take the recent shooting that took place in California by Elliot Rodger.  There have been a lot of reddit threads on this, but what\\u0026#39;s the best way to find those threads?  Using my API, I\\u0026#39;ll show you!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EFirst, the front end example\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYou can do a search using my very basic front end.  For instance, to find all the recent threads about Elliot Rodger, you could use \\u003Ca href=\\\"http://www.redditanalytics.com/findposts.html?q=%22elliot%20rodger%22\\\"\\u003Ethis link to search for submissions related to Elliot Rodger\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EHow does it work?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy server ingests all comments as they are posted to reddit.  This search will actually look for whatever phrase you provide and find all threads that contain that phrase.  Threads that contain more of that phrase throughout comments will be weighted more heavily.  As you can see from \\u003Ca href=\\\"http://www.redditanalytics.com/findposts.html?q=%22elliot%20rodger%22\\\"\\u003Esearching for the phrase Elliot Rodger\\u003C/a\\u003E, there are a lot of threads that are returned.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EHow is this better than reddit\\u0026#39;s search?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EReddit\\u0026#39;s search does not actually search through comments -- it only finds search terms that are located in the title of the submission or the self-text of a submission.  You will see in the returned results for the search above that many of the returned results don\\u0026#39;t have \\u0026quot;Elliot Rodger\\u0026quot; in the submission title, but they were still found because many comments contained that phrase.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy search API will find results based on comments that have been posted approximately in the past week.  If you need to go back further, you can set the lookback parameter to go back X seconds.  I have been ingesting comments from reddit\\u0026#39;s API since December of last year.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThere is also a back-end API that will allow you to do a search and have the results returned in JSON format.  If you are interested in how to use it, please let me know and I will go into more detail.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EOther examples:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.redditanalytics.com/findposts.html?q=tornadoes\\\"\\u003EInterested in the recent outbreak of tornadoes?\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.redditanalytics.com/findposts.html?q=%22Donald%20Sterling%22\\\"\\u003EInterested in submissions related to Donald Sterling and the Clippers?\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.redditanalytics.com/findposts.html?q=x-men\\\"\\u003EFind out what people are saying about the new X-men movie\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.redditanalytics.com/findposts.html?q=picard\\u0026amp;lookback=2000000\\\"\\u003EPerhaps you are a Star Trek fan and want to find threads where Picard is mentioned frequently?\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.redditanalytics.com/findposts.html?q=unidan\\u0026amp;lookback=8640000\\\"\\u003EWant to scan the past 100 days worth of comments to find threads related to our beloved user Unidan?  No problem!\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.redditanalytics.com/findposts.html?q=Wheeler+FCC\\u0026amp;lookback=8640000\\\"\\u003EHow about submissions over the past 100 days that mention Wheeler and the FCC?\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.redditanalytics.com/findposts.html?q=Net%20Neutrality\\u0026amp;lookback=8640000\\\"\\u003EHere are results from the past 100 days worth of comments that mention Net Neutrality\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EAre you an awesome front-end developer?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you\\u0026#39;re an awesome front-end developer and you\\u0026#39;d like to help in this project, I would love to hear from you.  My design skills for front-end applications is not the best, but I would love to find people who would like to team up and create something amazing.  Please let me know if you are interested.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EDo you need raw data?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you\\u0026#39;re doing research and need a lot of data, please contact me.  I\\u0026#39;d love to work with more researchers and academic institutions that want to explore social media data.  My database has all publicly available reddit submissions and over 400 million reddit comments.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cem\\u003EYou are more than welcome to use the back-end API directly for your own projects!  If your project gets big and you need to use more resources, feel free to PM me and we\\u0026#39;ll talk!  This is a labor of love that I\\u0026#39;ve undertaken for reddit and the reddit community.\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I wanted to take a moment to mention a more powerful search method that I've developed for reddit.  There is a very basic front-end that I have put together, but also a back-end that can be used by developers who wish to roll their own front-end interface.\\n\\nAs an example, let's take the recent shooting that took place in California by Elliot Rodger.  There have been a lot of reddit threads on this, but what's the best way to find those threads?  Using my API, I'll show you!\\n\\n**First, the front end example**\\n\\nYou can do a search using my very basic front end.  For instance, to find all the recent threads about Elliot Rodger, you could use [this link to search for submissions related to Elliot Rodger](http://www.redditanalytics.com/findposts.html?q=%22elliot%20rodger%22)\\n\\n**How does it work?**\\n\\nMy server ingests all comments as they are posted to reddit.  This search will actually look for whatever phrase you provide and find all threads that contain that phrase.  Threads that contain more of that phrase throughout comments will be weighted more heavily.  As you can see from [searching for the phrase Elliot Rodger](http://www.redditanalytics.com/findposts.html?q=%22elliot%20rodger%22), there are a lot of threads that are returned.  \\n\\n**How is this better than reddit's search?**\\n\\nReddit's search does not actually search through comments -- it only finds search terms that are located in the title of the submission or the self-text of a submission.  You will see in the returned results for the search above that many of the returned results don't have \\\"Elliot Rodger\\\" in the submission title, but they were still found because many comments contained that phrase.\\n\\nMy search API will find results based on comments that have been posted approximately in the past week.  If you need to go back further, you can set the lookback parameter to go back X seconds.  I have been ingesting comments from reddit's API since December of last year.  \\n\\nThere is also a back-end API that will allow you to do a search and have the results returned in JSON format.  If you are interested in how to use it, please let me know and I will go into more detail.\\n\\n**Other examples:**\\n\\n[Interested in the recent outbreak of tornadoes?](http://www.redditanalytics.com/findposts.html?q=tornadoes)\\n\\n[Interested in submissions related to Donald Sterling and the Clippers?](http://www.redditanalytics.com/findposts.html?q=%22Donald%20Sterling%22)\\n\\n[Find out what people are saying about the new X-men movie](http://www.redditanalytics.com/findposts.html?q=x-men)\\n\\n[Perhaps you are a Star Trek fan and want to find threads where Picard is mentioned frequently?](http://www.redditanalytics.com/findposts.html?q=picard\\u0026lookback=2000000)\\n\\n[Want to scan the past 100 days worth of comments to find threads related to our beloved user Unidan?  No problem!](http://www.redditanalytics.com/findposts.html?q=unidan\\u0026lookback=8640000)\\n\\n[How about submissions over the past 100 days that mention Wheeler and the FCC?](http://www.redditanalytics.com/findposts.html?q=Wheeler+FCC\\u0026lookback=8640000)\\n\\n[Here are results from the past 100 days worth of comments that mention Net Neutrality](http://www.redditanalytics.com/findposts.html?q=Net%20Neutrality\\u0026lookback=8640000)\\n\\n**Are you an awesome front-end developer?**\\n\\nIf you're an awesome front-end developer and you'd like to help in this project, I would love to hear from you.  My design skills for front-end applications is not the best, but I would love to find people who would like to team up and create something amazing.  Please let me know if you are interested.\\n\\n**Do you need raw data?**\\n\\nIf you're doing research and need a lot of data, please contact me.  I'd love to work with more researchers and academic institutions that want to explore social media data.  My database has all publicly available reddit submissions and over 400 million reddit comments.\\n\\n*You are more than welcome to use the back-end API directly for your own projects!  If your project gets big and you need to use more resources, feel free to PM me and we'll talk!  This is a labor of love that I've undertaken for reddit and the reddit community.*\\n\\n\\nThanks!\\n\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"26faez\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 19, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1400992207.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/26faez/searching_for_reddit_threads_a_more_powerful/\", \"locked\": false, \"name\": \"t3_26faez\", \"created\": 1401019927.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/26faez/searching_for_reddit_threads_a_more_powerful/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Searching for reddit threads -- a more powerful method is available\", \"created_utc\": 1400991127.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 19}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIf you are a developer and you know how to use JSON, you can search reddit comments with my API.  For example, to show the most recent comments with \\u0026quot;777\\u0026quot; in the comment, you can use:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditanalytics.com/searchRecentComments?q=777\\\"\\u003Ehttp://api.redditanalytics.com/searchRecentComments?q=777\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe search feature can also use logical expressions as well.  If you are interested in using this and have any questions, just ask!  I currently have most of 2014 reddit comments in the database.  Comments are ingested and indexed in real-time.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you can design a cool front-end application using this API, I would very much like to talk with you.  Send me a PM if you\\u0026#39;re interested in helping!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOther examples:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EShow comments with both the words \\u0026quot;captain\\u0026quot; and \\u0026quot;picard\\u0026quot; in the comment: (the two words don\\u0026#39;t have to be next to one another)\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditanalytics.com/searchRecentComments?q=captain+picard\\\"\\u003Ehttp://api.redditanalytics.com/searchRecentComments?q=captain+picard\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EShow comments with the phrase \\u0026quot;captain picard\\u0026quot;: (the words in that exact order):\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditanalytics.com/searchRecentComments?q=%22captain%20picard%22\\\"\\u003Ehttp://api.redditanalytics.com/searchRecentComments?q=%22captain%20picard%22\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EShow comments with the word \\u0026quot;explosion\\u0026quot; but only in the subreddit \\u003Ca href=\\\"/r/news\\\"\\u003E/r/news\\u003C/a\\u003E:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditanalytics.com/searchRecentComments?q=explosion\\u0026amp;subreddit=news\\\"\\u003Ehttp://api.redditanalytics.com/searchRecentComments?q=explosion\\u0026amp;subreddit=news\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EShow comments that contain at least one of these words: plane, 777 or malaysia:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditanalytics.com/searchRecentComments?q=plane%7C777%7Cmalaysia\\\"\\u003Ehttp://api.redditanalytics.com/searchRecentComments?q=plane|777|malaysia\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EShow comments that contain picard AND either riker or data:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditanalytics.com/searchRecentComments?q=picard+(riker%7Cdata)\\\"\\u003Ehttp://api.redditanalytics.com/searchRecentComments?q=picard+(riker|data)\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EShow comments that contain the word music but not metal:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditanalytics.com/searchRecentComments?q=music!metal\\\"\\u003Ehttp://api.redditanalytics.com/searchRecentComments?q=music!metal\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EShow comments related to Facebook stock:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditanalytics.com/searchRecentComments?q=(facebook%7Cfb)+stock\\\"\\u003Ehttp://api.redditanalytics.com/searchRecentComments?q=(facebook|fb)+stock\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT:  I am not affiliated with reddit.  I just love the community.\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"If you are a developer and you know how to use JSON, you can search reddit comments with my API.  For example, to show the most recent comments with \\\"777\\\" in the comment, you can use:\\n\\nhttp://api.redditanalytics.com/searchRecentComments?q=777\\n\\nThe search feature can also use logical expressions as well.  If you are interested in using this and have any questions, just ask!  I currently have most of 2014 reddit comments in the database.  Comments are ingested and indexed in real-time.\\n\\nIf you can design a cool front-end application using this API, I would very much like to talk with you.  Send me a PM if you're interested in helping!\\n\\nThanks!\\n\\nOther examples:\\n\\n**Show comments with both the words \\\"captain\\\" and \\\"picard\\\" in the comment: (the two words don't have to be next to one another)**\\n\\nhttp://api.redditanalytics.com/searchRecentComments?q=captain+picard\\n\\n**Show comments with the phrase \\\"captain picard\\\": (the words in that exact order):**\\n\\nhttp://api.redditanalytics.com/searchRecentComments?q=%22captain%20picard%22\\n\\n**Show comments with the word \\\"explosion\\\" but only in the subreddit /r/news:**\\n\\nhttp://api.redditanalytics.com/searchRecentComments?q=explosion\\u0026subreddit=news\\n\\n**Show comments that contain at least one of these words: plane, 777 or malaysia:**\\n\\nhttp://api.redditanalytics.com/searchRecentComments?q=plane|777|malaysia\\n\\n**Show comments that contain picard AND either riker or data:**\\n\\nhttp://api.redditanalytics.com/searchRecentComments?q=picard+(riker|data)\\n\\n**Show comments that contain the word music but not metal:**\\n\\nhttp://api.redditanalytics.com/searchRecentComments?q=music!metal\\n\\n**Show comments related to Facebook stock:**\\n\\nhttp://api.redditanalytics.com/searchRecentComments?q=(facebook|fb)+stock\\n\\n**EDIT:  I am not affiliated with reddit.  I just love the community.**\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"22s6vp\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 19, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1397249779.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/22s6vp/searching_reddit_comments_is_now_possible/\", \"locked\": false, \"name\": \"t3_22s6vp\", \"created\": 1397257701.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/22s6vp/searching_reddit_comments_is_now_possible/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Searching Reddit Comments is now possible\", \"created_utc\": 1397228901.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 19}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis bot grabs 1000 submissions from \\u003Ca href=\\\"/r/gifs\\\"\\u003E/r/gifs\\u003C/a\\u003E. It then filters out the submissions that are from \\u0026#39;i.imgur.com\\u0026#39; and adds them to a list. After that it cycles through the list and embeds one image into a HTML file as a background-image.  The webpage then refreshes every X amount of seconds to display the new image. The bot also grabs the submission title and overlays it at the bottom. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI had an old monitor I wasn\\u0026#39;t using and was looking for a project to work on with my RPi. I figured I would just use the monitor as a sort of digital picture frame. Then I thought, lets make it more interesting by using gifs instead of images. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you have any suggestions on making it more efficient, I\\u0026#39;d be glad to hear them. This is my first time working with Python and PRAW, so I\\u0026#39;m sure there are better ways of doing this.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport praw\\nimport time\\n\\nr=praw.Reddit(user_agent=\\u0026#39;Dynamic gif Slideshow by /u/xiggy\\u0026#39;)\\nsubreddit = r.get_subreddit(\\u0026#39;gifs\\u0026#39;)\\nsubmissions = subreddit.get_hot(limit=1000)\\n\\nimglinks = []\\nused = []\\nc = 0\\n\\nwhile True:\\n\\n    for submission in submissions:\\n        if submission.id not in used:\\n            if submission.domain == \\u0026#39;i.imgur.com\\u0026#39;:\\n                tit = submission.title #tit... lol\\n                imglinks.append(submission.url)\\n                used.append(submission.id)\\n\\n                html_file = open(\\u0026quot;style.css\\u0026quot;, \\u0026quot;w\\u0026quot;)\\n                html_file.write(\\u0026quot;body {  margin:0px; padding: 0px;width:100%;  height:100%;  background:url(\\\\\\u0026#39;\\u0026quot; + imglinks[c] + \\u0026quot;\\\\\\u0026#39;) center center no-repeat;  background-size:contain;  overflow:hidden;  background-color:#121211;  position:relative;  font-family: sans-serif; } #title {  position:absolute;  width:100%;  min-height:60px; text-align:center; color:#FFF;   bottom:0px;v  left:0px;   background:rgba(0,0,0,0.6);  line-height:60px; align:middle; font-weight: bold; font-size: 125%; }\\u0026quot;)\\n                html_file.close()\\n\\n                html_file = open(\\u0026quot;bg-test.html\\u0026quot;, \\u0026quot;w\\u0026quot;)\\n                html_file.write(\\u0026quot;\\u0026lt;html\\u0026gt;\\u0026lt;head\\u0026gt;\\u0026lt;link rel=\\\\\\u0026quot;stylesheet\\\\\\u0026quot; type=\\\\\\u0026quot;text/css\\\\\\u0026quot; href=\\\\\\u0026quot;style.css\\\\\\u0026quot; /\\u0026gt;\\u0026lt;meta http-equiv=\\\\\\u0026quot;refresh\\\\\\u0026quot; content=\\\\\\u0026quot;65\\\\\\u0026quot;/\\u0026gt;\\u0026lt;/head\\u0026gt;\\u0026lt;body\\u0026gt;\\u0026lt;div id=\\\\\\u0026quot;title\\\\\\u0026quot;\\u0026gt;\\u0026quot; + tit + \\u0026quot;\\u0026lt;/div\\u0026gt;\\u0026lt;/body\\u0026gt;\\u0026lt;/html\\u0026gt;\\u0026quot;)\\n                html_file.close()\\n\\n                print imglinks[c]\\n                c += 1\\n                time.sleep(60)\\n    time.sleep(1300)    \\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This bot grabs 1000 submissions from /r/gifs. It then filters out the submissions that are from 'i.imgur.com' and adds them to a list. After that it cycles through the list and embeds one image into a HTML file as a background-image.  The webpage then refreshes every X amount of seconds to display the new image. The bot also grabs the submission title and overlays it at the bottom. \\n\\nI had an old monitor I wasn't using and was looking for a project to work on with my RPi. I figured I would just use the monitor as a sort of digital picture frame. Then I thought, lets make it more interesting by using gifs instead of images. \\n\\nIf you have any suggestions on making it more efficient, I'd be glad to hear them. This is my first time working with Python and PRAW, so I'm sure there are better ways of doing this.\\n\\n    import praw\\n    import time\\n    \\n    r=praw.Reddit(user_agent='Dynamic gif Slideshow by /u/xiggy')\\n    subreddit = r.get_subreddit('gifs')\\n    submissions = subreddit.get_hot(limit=1000)\\n    \\n    imglinks = []\\n    used = []\\n    c = 0\\n    \\n    while True:\\n    \\t\\n    \\tfor submission in submissions:\\n    \\t\\tif submission.id not in used:\\n    \\t\\t\\tif submission.domain == 'i.imgur.com':\\n    \\t\\t\\t\\ttit = submission.title #tit... lol\\n    \\t\\t\\t\\timglinks.append(submission.url)\\n    \\t\\t\\t\\tused.append(submission.id)\\n    \\n    \\t\\t\\t\\thtml_file = open(\\\"style.css\\\", \\\"w\\\")\\n    \\t\\t\\t\\thtml_file.write(\\\"body {  margin:0px; padding: 0px;width:100%;  height:100%;  background:url(\\\\'\\\" + imglinks[c] + \\\"\\\\') center center no-repeat;  background-size:contain;  overflow:hidden;  background-color:#121211;  position:relative;  font-family: sans-serif; } #title {  position:absolute;  width:100%;  min-height:60px; text-align:center; color:#FFF;   bottom:0px;v  left:0px;   background:rgba(0,0,0,0.6);  line-height:60px; align:middle; font-weight: bold; font-size: 125%; }\\\")\\n    \\t\\t\\t\\thtml_file.close()\\n    \\t\\t\\t\\t\\n    \\t\\t\\t\\thtml_file = open(\\\"bg-test.html\\\", \\\"w\\\")\\n    \\t\\t\\t\\thtml_file.write(\\\"\\u003Chtml\\u003E\\u003Chead\\u003E\\u003Clink rel=\\\\\\\"stylesheet\\\\\\\" type=\\\\\\\"text/css\\\\\\\" href=\\\\\\\"style.css\\\\\\\" /\\u003E\\u003Cmeta http-equiv=\\\\\\\"refresh\\\\\\\" content=\\\\\\\"65\\\\\\\"/\\u003E\\u003C/head\\u003E\\u003Cbody\\u003E\\u003Cdiv id=\\\\\\\"title\\\\\\\"\\u003E\\\" + tit + \\\"\\u003C/div\\u003E\\u003C/body\\u003E\\u003C/html\\u003E\\\")\\n    \\t\\t\\t\\thtml_file.close()\\n    \\t\\t\\t\\t\\n    \\t\\t\\t\\tprint imglinks[c]\\n    \\t\\t\\t\\tc += 1\\n    \\t\\t\\t\\ttime.sleep(60)\\n    \\ttime.sleep(1300)\\t\\n    \\n    \\n    \\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1iredp\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"xiggy\", \"media\": null, \"score\": 19, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/\", \"locked\": false, \"name\": \"t3_1iredp\", \"created\": 1374459139.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"It's not much, but it's my first bot. I made it to run on my Raspberry Pi that's connected to a display.\", \"created_utc\": 1374430339.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 19}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"redditanalytics.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1fsuyo\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"RedditAnalytics\", \"media\": null, \"score\": 18, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1fsuyo/new_reddit_search_engine_is_available_in_alpha/\", \"locked\": false, \"name\": \"t3_1fsuyo\", \"created\": 1370567320.0, \"url\": \"http://www.redditanalytics.com/search/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New Reddit Search Engine is available (in alpha testing). She's buggy and filled with holes but manages to stay afloat!\", \"created_utc\": 1370538520.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 18}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"praw.readthedocs.org\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1d72t8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bboe\", \"media\": null, \"score\": 19, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1d72t8/praw_210_released_with_multiprocessing_support/\", \"locked\": false, \"name\": \"t3_1d72t8\", \"created\": 1367059943.0, \"url\": \"https://praw.readthedocs.org/en/latest/pages/changelog.html#praw-2-1-0\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"PRAW 2.1.0 released with multiprocessing support among other changes\", \"created_utc\": 1367031143.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 19}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWith \\u003Ca href=\\\"https://github.com/reddit/reddit/commit/01146c301e9509f246cf3827c591f29068c82131\\\"\\u003Ethis commit\\u003C/a\\u003E, the ini setting for \\u003Ccode\\u003Elang\\u003C/code\\u003E has changed to \\u003Ccode\\u003Esite_lang\\u003C/code\\u003E. This was necessary to remove an odd reliance on the completely unused r2.mo file in the reddit/reddit repo (translations are found in \\u003Ca href=\\\"https://github.com/reddit/reddit-i18n\\\"\\u003Ehttps://github.com/reddit/reddit-i18n\\u003C/a\\u003E if desired). It\\u0026#39;s possible there\\u0026#39;s a better/cleaner way, and if you know of one, I\\u0026#39;d love to hear it. In the meantime, if you have \\u003Ccode\\u003ELanguageError\\u003C/code\\u003Es on app startup, check your INI setting to make sure that you are NOT setting a \\u003Ccode\\u003Elang\\u003C/code\\u003E variable.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"With [this commit](https://github.com/reddit/reddit/commit/01146c301e9509f246cf3827c591f29068c82131), the ini setting for `lang` has changed to `site_lang`. This was necessary to remove an odd reliance on the completely unused r2.mo file in the reddit/reddit repo (translations are found in https://github.com/reddit/reddit-i18n if desired). It's possible there's a better/cleaner way, and if you know of one, I'd love to hear it. In the meantime, if you have `LanguageError`s on app startup, check your INI setting to make sure that you are NOT setting a `lang` variable.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"y7vpb\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 18, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1345061010.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/y7vpb/ini_setting_change_please_read_if_youre_running_a/\", \"locked\": false, \"name\": \"t3_y7vpb\", \"created\": 1345001251.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/y7vpb/ini_setting_change_please_read_if_youre_running_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"INI setting change - please read if you're running a reddit clone\", \"created_utc\": 1344972451.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 18}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAfter being inspired by \\u003Ca href=\\\"http://www.reddit.com/r/web_design/comments/e0mnq/breaking_news_10_year_old_twins_make_a_website/c14d9le\\\"\\u003Ethis\\u003C/a\\u003E comment, I decided that it would be worthwhile to actually create the horrific thing. So, I present to you a patch that should add blink support to Reddit.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo create a blinking section, you enclose the text between \\u003Ccode\\u003E|\\u003C/code\\u003Es, similar to the way italics currently work with \\u003Ccode\\u003E*\\u003C/code\\u003Es.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EExample:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E|This is a blinking comment|\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EPatch is \\u003Ca href=\\\"http://pastebin.com/QADYkM75\\\"\\u003Ehere\\u003C/a\\u003E, consider it public domain. It is currently untested, so don\\u0026#39;t blame me if it sets fire to your servers or eats you for lunch.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENew patch is \\u003Ca href=\\\"http://pastebin.com/BuYuMK5Z\\\"\\u003Ehere\\u003C/a\\u003E, updated because of the \\u0026lt;blink\\u0026gt; tag\\u0026#39;s removal from HTML5.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"After being inspired by [this](http://www.reddit.com/r/web_design/comments/e0mnq/breaking_news_10_year_old_twins_make_a_website/c14d9le) comment, I decided that it would be worthwhile to actually create the horrific thing. So, I present to you a patch that should add blink support to Reddit.\\n\\nTo create a blinking section, you enclose the text between `|`s, similar to the way italics currently work with `*`s.\\n\\nExample:\\n\\n    |This is a blinking comment|\\n\\nPatch is [here](http://pastebin.com/QADYkM75), consider it public domain. It is currently untested, so don't blame me if it sets fire to your servers or eats you for lunch.\\n\\nEDIT:\\n\\nNew patch is [here](http://pastebin.com/BuYuMK5Z), updated because of the \\u003Cblink\\u003E tag's removal from HTML5.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"e120r\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 21, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/e120r/support_for_the_blink_tag_patch_included/\", \"locked\": false, \"name\": \"t3_e120r\", \"created\": 1288884316.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/e120r/support_for_the_blink_tag_patch_included/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Support for the \\u003Cblink\\u003E tag [patch included]\", \"created_utc\": 1288855516.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 21}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIn a reply to \\u003Ca href=\\\"http://www.reddit.com/r/blog/comments/dqse4/fun_in_the_sidebar/c127h9b\\\"\\u003Ea link about reddit\\u0026#39;s slowness\\u003C/a\\u003E, I suggested to \\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E[...] shift the friend-lookup into the client[.] Some javascript should be able to turn usernames red if they are in a list.\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003Eand ketralnis replied:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003EI\\u0026#39;d recommend you take this discussion to \\u003Ca href=\\\"/r/redditdev\\\"\\u003E/r/redditdev\\u003C/a\\u003E if you\\u0026#39;re really interested, but for this question, the client has to get the data from somewhere too, right?\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EMy idea is, \\u003Ca href=\\\"http://www.reddit.com/r/blog/comments/dqse4/fun_in_the_sidebar/c127t8s\\\"\\u003Esimilar to cdawzrd and AngerMCS\\u003C/a\\u003E, that the client should be able to  cache the data so that reddit only serves the friends-list once. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOne final speculation:\\nIf reddit also serves the list of \\u0026quot;comment upvote ids\\u0026quot; and other stuff seperately, it should be able to serve \\u0026quot;flat comment pages\\u0026quot; to everybody. Then, the comment pages can be cached for a second on the server before reddit has to include the recent  upvotes and comments \\u0026quot;batch-style\\u0026quot; in a single pass.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis should be similar to the wiki architecture where most people just get a plain html file that only gets updated when somebody alters an article.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"In a reply to [a link about reddit's slowness](http://www.reddit.com/r/blog/comments/dqse4/fun_in_the_sidebar/c127h9b), I suggested to \\n\\n\\u003E\\u003E [...] shift the friend-lookup into the client[.] Some javascript should be able to turn usernames red if they are in a list.\\n\\nand ketralnis replied:\\n\\n\\u003EI'd recommend you take this discussion to /r/redditdev if you're really interested, but for this question, the client has to get the data from somewhere too, right?\\n\\nMy idea is, [similar to cdawzrd and AngerMCS](http://www.reddit.com/r/blog/comments/dqse4/fun_in_the_sidebar/c127t8s), that the client should be able to  cache the data so that reddit only serves the friends-list once. \\n\\nOne final speculation:\\nIf reddit also serves the list of \\\"comment upvote ids\\\" and other stuff seperately, it should be able to serve \\\"flat comment pages\\\" to everybody. Then, the comment pages can be cached for a second on the server before reddit has to include the recent  upvotes and comments \\\"batch-style\\\" in a single pass.\\n\\nThis should be similar to the wiki architecture where most people just get a plain html file that only gets updated when somebody alters an article.\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"dqv56\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kleopatra6tilde9\", \"media\": null, \"score\": 18, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/dqv56/turning_reddit_into_wikipedia/\", \"locked\": false, \"name\": \"t3_dqv56\", \"created\": 1287034664.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/dqv56/turning_reddit_into_wikipedia/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Turning Reddit into Wikipedia\", \"created_utc\": 1287005864.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 18}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI recently built a bot in response to \\u003Ca href=\\\"https://www.reddit.com/r/RequestABot/comments/3we9u1/a_bot_that_creates_a_topic_and_increases_a/\\\"\\u003Ethis post\\u003C/a\\u003E in \\u003Ca href=\\\"/u/requestabot\\\"\\u003E/u/requestabot\\u003C/a\\u003E and documented the experience in my blog. I thought maybe someone here might find the tutorial useful.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf anyone\\u0026#39;s interested, here\\u0026#39;s a quick writeup (one of many, I know) that takes you through building a Reddit bot with Python:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://souldeux.com/blog/build-reddit-bot-with-praw/\\\"\\u003Ehttp://souldeux.com/blog/build-reddit-bot-with-praw/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I recently built a bot in response to [this post](https://www.reddit.com/r/RequestABot/comments/3we9u1/a_bot_that_creates_a_topic_and_increases_a/) in /u/requestabot and documented the experience in my blog. I thought maybe someone here might find the tutorial useful.\\n\\nIf anyone's interested, here's a quick writeup (one of many, I know) that takes you through building a Reddit bot with Python:\\n\\nhttp://souldeux.com/blog/build-reddit-bot-with-praw/\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3wpn6e\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"souldeux\", \"media\": null, \"score\": 18, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3wpn6e/quick_buildabot_tutorial/\", \"locked\": false, \"name\": \"t3_3wpn6e\", \"created\": 1450079043.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3wpn6e/quick_buildabot_tutorial/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Quick \\\"build-a-bot\\\" tutorial\", \"created_utc\": 1450050243.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 18}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI sent a license renewal request about a month ago but I haven\\u0026#39;t heard from them yet. Today, I got a rejection mail from Google. The last time I mailed a licensing request, I got a response from their product manager on the same day and everything was done in a couple of days. Anybody experiencing this too? Maybe this is because they are releasing an official android app?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: It\\u0026#39;s been more than a month. Nothing is happening :( I\\u0026#39;ll delete this post when I get my app relicensed.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I sent a license renewal request about a month ago but I haven't heard from them yet. Today, I got a rejection mail from Google. The last time I mailed a licensing request, I got a response from their product manager on the same day and everything was done in a couple of days. Anybody experiencing this too? Maybe this is because they are releasing an official android app?\\n\\nEDIT: It's been more than a month. Nothing is happening :( I'll delete this post when I get my app relicensed.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3g9ofx\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"mightyfrog\", \"media\": null, \"score\": 18, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1442747534.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3g9ofx/reddit_no_longer_gives_a_license_to_android_apps/\", \"locked\": false, \"name\": \"t3_3g9ofx\", \"created\": 1439087914.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3g9ofx/reddit_no_longer_gives_a_license_to_android_apps/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"reddit no longer gives a license to android apps?\", \"created_utc\": 1439059114.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 18}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIf you\\u0026#39;re a web developer, you may be familiar with the \\u003Ca href=\\\"http://www.oembed.com\\\"\\u003EoEmbed specification\\u003C/a\\u003E, which provides a simple format for websites to expose embedded representations of their content for other sites to use. We now support oEmbed for individual comments, which uses the \\u003Ca href=\\\"https://www.reddit.com/r/blog/comments/302347/announcing_embeddable_comment_threads/\\\"\\u003Enewly-minted comment embeds we announced on the blog last month\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe oEmbed endpoint is at \\u003Ca href=\\\"https://www.reddit.com/oembed\\\"\\u003Ehttps://www.reddit.com/oembed\\u003C/a\\u003E, and you can get back a JSON representation of a comment as follows:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/oembed?url=https://www.reddit.com/r/Showerthoughts/comments/2safxv/we_should_start_keeping_giraffes_a_secret_from/cno7zic\\\"\\u003Ehttps://www.reddit.com/oembed?url=https://www.reddit.com/r/Showerthoughts/comments/2safxv/we_should_start_keeping_giraffes_a_secret_from/cno7zic\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe oEmbed endpoint also accepts two non-standard parameters to help make better embeds:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003Eparent - boolean - include the parent in the embed.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003Elive - boolean - allow edits made to this comment to be showed immediately.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EMore documentation on this is available here: \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/oEmbed\\\"\\u003Ehttps://github.com/reddit/reddit/wiki/oEmbed\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/compare/00324fb1ca1cf9099e42de2010cf56d23ee6033d%5E...51f1e00\\\"\\u003ESee the code behind this change on GitHub.\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"If you're a web developer, you may be familiar with the [oEmbed specification](http://www.oembed.com), which provides a simple format for websites to expose embedded representations of their content for other sites to use. We now support oEmbed for individual comments, which uses the [newly-minted comment embeds we announced on the blog last month](https://www.reddit.com/r/blog/comments/302347/announcing_embeddable_comment_threads/).\\n\\nThe oEmbed endpoint is at https://www.reddit.com/oembed, and you can get back a JSON representation of a comment as follows:\\n\\nhttps://www.reddit.com/oembed?url=https://www.reddit.com/r/Showerthoughts/comments/2safxv/we_should_start_keeping_giraffes_a_secret_from/cno7zic\\n\\nThe oEmbed endpoint also accepts two non-standard parameters to help make better embeds:\\n\\n1. parent - boolean - include the parent in the embed.\\n\\n2. live - boolean - allow edits made to this comment to be showed immediately.\\n\\nMore documentation on this is available here: https://github.com/reddit/reddit/wiki/oEmbed\\n\\n[See the code behind this change on GitHub.](https://github.com/reddit/reddit/compare/00324fb1ca1cf9099e42de2010cf56d23ee6033d^...51f1e00)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"34c0pm\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"umbrae\", \"media\": null, \"score\": 20, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 14, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1430398196.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/34c0pm/devs_reddit_now_supports_oembed_for_comment_embeds/\", \"locked\": false, \"name\": \"t3_34c0pm\", \"created\": 1430377389.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/34c0pm/devs_reddit_now_supports_oembed_for_comment_embeds/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Devs: reddit now supports oEmbed for comment embeds\", \"created_utc\": 1430348589.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 20}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {\"content\": \"\\n\\u003Cdiv class=\\\"psuedo-selftext\\\"\\u003E\\n  \\u003Ciframe src=\\\"//www.redditmedia.com/live/ukaeu1ik4sw5/embed?stylesr=redditdev\\\" height=\\\"500\\\"\\u003E\\u003C/iframe\\u003E\\n\\u003C/div\\u003E\\n\", \"width\": 710, \"scrolling\": false, \"height\": 500}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": {\"event_id\": \"ukaeu1ik4sw5\", \"type\": \"liveupdate\"}, \"link_flair_text\": null, \"id\": \"32hbkd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"xiong_as_admin\", \"media\": {\"event_id\": \"ukaeu1ik4sw5\", \"type\": \"liveupdate\"}, \"score\": 19, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {\"content\": \"\\n\\u003Cdiv class=\\\"psuedo-selftext\\\"\\u003E\\n  \\u003Ciframe src=\\\"//www.redditmedia.com/live/ukaeu1ik4sw5/embed?stylesr=redditdev\\\" height=\\\"500\\\"\\u003E\\u003C/iframe\\u003E\\n\\u003C/div\\u003E\\n\", \"width\": 710, \"scrolling\": false, \"height\": 500}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/32hbkd/reddit_change_the_rname_form_of_subreddit_names/\", \"locked\": false, \"name\": \"t3_32hbkd\", \"created\": 1428985111.0, \"url\": \"https://www.reddit.com/live/ukaeu1ik4sw5/updates/29064cfc-e214-11e4-8d95-22000bb24418\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"reddit change: The /r/\\u003Cname\\u003E form of subreddit names should now be accepted everywhere just \\u003Cname\\u003E is\", \"created_utc\": 1428956311.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 19}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESeveral more endpoints are now accessible via reddit\\u0026#39;s OAuth2 implementation:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E/api/friend\\u003C/li\\u003E\\n\\u003Cli\\u003E/api/unfriend\\u003C/li\\u003E\\n\\u003Cli\\u003E/api/leavemoderator\\u003C/li\\u003E\\n\\u003Cli\\u003E/api/leavecontributor\\u003C/li\\u003E\\n\\u003Cli\\u003E/api/accept_moderator_invite\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EThose endpoints come with new scopes:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u0026quot;modcontributors\\u0026quot;: For banning/unbanning, and adding/removed approved submitters from subreddits\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u0026quot;modself\\u0026quot;: For accepting invitations to become a moderator, stepping down from being a moderator, and stepping down from being an approved submitter\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u0026quot;modothers\\u0026quot;: Invite others to moderate subreddits, and remove moderators from subreddits you moderate.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EFor more information, check out the updated information on \\u003Ca href=\\\"/dev/api\\\"\\u003E/dev/api\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAdditionally, you may now specify your OAuth2 scopes as a space-separated string, in compliance with the OAuth2 specification. Comma-separated strings are still supported for backwards-compatibility.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m pleased to announce that with those additions, all API features supported under cookie authentication are accessible via OAuth2! This means you have \\u003Cem\\u003Eno excuse\\u003C/em\\u003E for not switching to OAuth2 API access at this time.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Several more endpoints are now accessible via reddit's OAuth2 implementation:\\n\\n* /api/friend\\n* /api/unfriend\\n* /api/leavemoderator\\n* /api/leavecontributor\\n* /api/accept_moderator_invite\\n\\nThose endpoints come with new scopes:\\n\\n* \\\"modcontributors\\\": For banning/unbanning, and adding/removed approved submitters from subreddits\\n* \\\"modself\\\": For accepting invitations to become a moderator, stepping down from being a moderator, and stepping down from being an approved submitter\\n* \\\"modothers\\\": Invite others to moderate subreddits, and remove moderators from subreddits you moderate.\\n\\nFor more information, check out the updated information on [/dev/api](/dev/api).\\n\\nAdditionally, you may now specify your OAuth2 scopes as a space-separated string, in compliance with the OAuth2 specification. Comma-separated strings are still supported for backwards-compatibility.\\n\\nI'm pleased to announce that with those additions, all API features supported under cookie authentication are accessible via OAuth2! This means you have *no excuse* for not switching to OAuth2 API access at this time.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2skahz\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 18, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2skahz/oauth2_apifriend_is_here/\", \"locked\": false, \"name\": \"t3_2skahz\", \"created\": 1421389277.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2skahz/oauth2_apifriend_is_here/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[OAuth2] /api/friend is here!\", \"created_utc\": 1421360477.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 18}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAnd where would I go about talking to someone or submitting this? Here?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"And where would I go about talking to someone or submitting this? Here?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2cse5d\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"samhollis\", \"media\": null, \"score\": 17, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2cse5d/if_we_notice_things_in_reddits_css_that_can_be/\", \"locked\": false, \"name\": \"t3_2cse5d\", \"created\": 1407363183.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2cse5d/if_we_notice_things_in_reddits_css_that_can_be/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"If we notice things in reddit's CSS that can be optimized for responsiveness.. would reddit care?\", \"created_utc\": 1407334383.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 17}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/naiyt/reddit-replier\\\"\\u003Ehttps://github.com/naiyt/reddit-replier\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESome prior knowledge of Python is expected.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe have PRAW which abstracts the actual HTTP interactions with the Reddit API. Why not something on top of PRAW that simplifies creating reply bots? I\\u0026#39;ve read the source for a lot of Reddit bots, and most weren\\u0026#39;t really designed to lend themselves well to reuse.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eredditreplier is intended to simplify the \\u0026quot;bot\\u0026quot; part of the process, and let you focus on what you actually want your bot to respond to and say. It will use PRAW to continually watch whatever subreddits you specify. It will then pass any new messages to your code, which decides whether it should reply and what it should reply with.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYou\\u0026#39;ll want to go through the README for more details, but you basically just create a class with a \\u003Ccode\\u003Eparse\\u003C/code\\u003E method. redditreplier will pass that any new messages. You then parse that message to decide if you want to respond, and act accordingly. This means creating new bots is a simple process that doesn\\u0026#39;t require you to rewrite all of the PRAW and Reddit interactions every time.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s still in beta, so there are likely some issues. The exception handling definitely needs some work, for example. Feel free to fork the repo and send in a pull request if you have any suggestions or fixes.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI wrote a proof of concept \\u003Ca href=\\\"https://github.com/naiyt/autogithubbot\\\"\\u003Ehere\\u003C/a\\u003E that is basically like AutoWikiBot, but for GitHub links. I\\u0026#39;m not actually running it live anywhere because I didn\\u0026#39;t think anybody would really find it useful. But feel free to peruse that code if you want some ideas.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso, if there is interest in this, \\u003Cstrong\\u003Eplease use it to make interesting, non spammy bots\\u003C/strong\\u003E. Nobody really likes uncalled for bots that respond to every message that contains a specific string. My favorite bots are those that only respond when specifically summoned (with specific keywords that won\\u0026#39;t accidentally be used) and provide the user with interesting information.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf anybody has any questions or comments on this, feel free to PM me or open a GitHub issue.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"https://github.com/naiyt/reddit-replier\\n\\nSome prior knowledge of Python is expected.\\n\\nWe have PRAW which abstracts the actual HTTP interactions with the Reddit API. Why not something on top of PRAW that simplifies creating reply bots? I've read the source for a lot of Reddit bots, and most weren't really designed to lend themselves well to reuse.\\n\\nredditreplier is intended to simplify the \\\"bot\\\" part of the process, and let you focus on what you actually want your bot to respond to and say. It will use PRAW to continually watch whatever subreddits you specify. It will then pass any new messages to your code, which decides whether it should reply and what it should reply with.\\n\\nYou'll want to go through the README for more details, but you basically just create a class with a `parse` method. redditreplier will pass that any new messages. You then parse that message to decide if you want to respond, and act accordingly. This means creating new bots is a simple process that doesn't require you to rewrite all of the PRAW and Reddit interactions every time.\\n\\nIt's still in beta, so there are likely some issues. The exception handling definitely needs some work, for example. Feel free to fork the repo and send in a pull request if you have any suggestions or fixes.\\n\\nI wrote a proof of concept [here](https://github.com/naiyt/autogithubbot) that is basically like AutoWikiBot, but for GitHub links. I'm not actually running it live anywhere because I didn't think anybody would really find it useful. But feel free to peruse that code if you want some ideas.\\n\\nAlso, if there is interest in this, **please use it to make interesting, non spammy bots**. Nobody really likes uncalled for bots that respond to every message that contains a specific string. My favorite bots are those that only respond when specifically summoned (with specific keywords that won't accidentally be used) and provide the user with interesting information.\\n\\nIf anybody has any questions or comments on this, feel free to PM me or open a GitHub issue.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2bo4xn\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"naiyt\", \"media\": null, \"score\": 18, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2bo4xn/i_wrote_a_python_module_to_make_writing_reddit/\", \"locked\": false, \"name\": \"t3_2bo4xn\", \"created\": 1406297471.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2bo4xn/i_wrote_a_python_module_to_make_writing_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I wrote a Python module to make writing Reddit bots simpler (please use for good and not evil)\", \"created_utc\": 1406268671.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 18}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ll be making available new API calls for researchers interested in data mining shortly via \\u003Ca href=\\\"http://api.redditanalytics.com\\\"\\u003Ehttp://api.redditanalytics.com\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you are interested in getting \\u0026quot;virtually unlimited\\u0026quot; access to the API, please send me a PM and discuss your project and I\\u0026#39;ll set you up.  The new API will allow:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAccess to \\u003Cstrong\\u003E500+ million reddit comments\\u003C/strong\\u003E including their scores, etc.  You will be able to ask the API for comments 1,000 at a time with the ability to get comments by time period, subreddit, author flair, score, submission thread, etc.  You will be able to retrieve comments by ID (something reddit currently lacks because of the way their database is structured).  Five API calls per second with bursts allowed.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EAccess to all reddit submissions.\\u003C/strong\\u003E  You can get submission data in blocks of 1,000 at a time based on any type of filter criteria.   If you need every submission ever made to a certain subreddit, you\\u0026#39;ll want to use this API.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EAccess to real-time comment and post streams\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EAccess the comment stream and filter by words, phrases, subreddits, authors, etc.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EAccess to all new submissions made with similar filtering abilities to the comment stream.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EThe current system capability is:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003E5 teraflops of processing power.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003E1,000 requests per second.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003E128 Gigabytes of RAM soon to be upgraded to 1 TB.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003E1 Gps network connection.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003E2 terabytes of SSD storage.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EEurope mirror for European researchers. \\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EI am working on getting 1 petabyte of bandwidth per month.  I\\u0026#39;m trying to find the most cost-effective solution -- but if you\\u0026#39;re a researcher, I\\u0026#39;ll figure out a way to give you basically unlimited access.  I may have to restrict each API key to 100 gigabytes of bandwidth per month for the time-being.  However, with compression, you should be able to get virtually anything you need.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EIf you are interested, please send me a PM with the following information:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EBrief description of your project / research proposal.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EYour university\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EContact E-mail\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'll be making available new API calls for researchers interested in data mining shortly via http://api.redditanalytics.com\\n\\nIf you are interested in getting \\\"virtually unlimited\\\" access to the API, please send me a PM and discuss your project and I'll set you up.  The new API will allow:\\n\\nAccess to **500+ million reddit comments** including their scores, etc.  You will be able to ask the API for comments 1,000 at a time with the ability to get comments by time period, subreddit, author flair, score, submission thread, etc.  You will be able to retrieve comments by ID (something reddit currently lacks because of the way their database is structured).  Five API calls per second with bursts allowed.\\n\\n**Access to all reddit submissions.**  You can get submission data in blocks of 1,000 at a time based on any type of filter criteria.   If you need every submission ever made to a certain subreddit, you'll want to use this API.  \\n\\n**Access to real-time comment and post streams**\\n\\n* Access the comment stream and filter by words, phrases, subreddits, authors, etc.\\n\\n* Access to all new submissions made with similar filtering abilities to the comment stream.\\n\\n**The current system capability is:**\\n\\n* 5 teraflops of processing power.\\n\\n* 1,000 requests per second.\\n\\n* 128 Gigabytes of RAM soon to be upgraded to 1 TB.\\n\\n* 1 Gps network connection.\\n\\n* 2 terabytes of SSD storage.\\n\\n* Europe mirror for European researchers. \\n\\nI am working on getting 1 petabyte of bandwidth per month.  I'm trying to find the most cost-effective solution -- but if you're a researcher, I'll figure out a way to give you basically unlimited access.  I may have to restrict each API key to 100 gigabytes of bandwidth per month for the time-being.  However, with compression, you should be able to get virtually anything you need.\\n\\n**If you are interested, please send me a PM with the following information:**\\n\\n*  Brief description of your project / research proposal.\\n\\n*  Your university\\n\\n*  Contact E-mail\\n\\n\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1twwz2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 19, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1388282954.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1twwz2/new_api_access_for_researchers_academic_students/\", \"locked\": false, \"name\": \"t3_1twwz2\", \"created\": 1388311194.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1twwz2/new_api_access_for_researchers_academic_students/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New API access for researchers / academic students\", \"created_utc\": 1388282394.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 19}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThere ought to be a subreddit-level flag (and perhaps a user-level flag) to instruct bots not to engage on that subreddit (or with that user). Obviously this would only be effective for those bots that respected such a flag, but I believe something like this could reduce the amount the moderators need to fight with bots. There seem to be a lot of subreddits that have a \\u0026quot;we see em, we ban em\\u0026quot; attitude towards reddit bots: why not just modify the ecosystem so the bots already get the message without evening needing the mods to ban them?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere\\u0026#39;s how I imagine this being implemented: \\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EThe option to set this flag needs to be made available in subreddit settings. I think this should default to \\u0026quot;bots are ok here.\\u0026quot;\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EThe main reddit API wrappers (e.g. praw) should be set to respect this flag by default. Overriding this flag should be easy, but this way people just messing around with bots as hobby projects (which I assume is most of the population of praw users) will respect the flag by default.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EEDIT: Some great ideas in here, but I\\u0026#39;m thinking such a feature would be more trouble than it\\u0026#39;s worth and probably actually cause more work for everyone (mods, bot creators, and app developers). There\\u0026#39;s nothing really wrong with the current system of just banning bots when mods find them. Maybe if we work this idea through more we could come up with a better implementation, but I\\u0026#39;m concerned that the options we\\u0026#39;ve considered would make people\\u0026#39;s lives harder instead of easier. Maybe we should bring more mods into the discussion?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"There ought to be a subreddit-level flag (and perhaps a user-level flag) to instruct bots not to engage on that subreddit (or with that user). Obviously this would only be effective for those bots that respected such a flag, but I believe something like this could reduce the amount the moderators need to fight with bots. There seem to be a lot of subreddits that have a \\\"we see em, we ban em\\\" attitude towards reddit bots: why not just modify the ecosystem so the bots already get the message without evening needing the mods to ban them?\\n\\nHere's how I imagine this being implemented: \\n\\n1. The option to set this flag needs to be made available in subreddit settings. I think this should default to \\\"bots are ok here.\\\"\\n\\n2. The main reddit API wrappers (e.g. praw) should be set to respect this flag by default. Overriding this flag should be easy, but this way people just messing around with bots as hobby projects (which I assume is most of the population of praw users) will respect the flag by default.\\n\\nEDIT: Some great ideas in here, but I'm thinking such a feature would be more trouble than it's worth and probably actually cause more work for everyone (mods, bot creators, and app developers). There's nothing really wrong with the current system of just banning bots when mods find them. Maybe if we work this idea through more we could come up with a better implementation, but I'm concerned that the options we've considered would make people's lives harder instead of easier. Maybe we should bring more mods into the discussion?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1e9lwp\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"shaggorama\", \"media\": null, \"score\": 20, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 24, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1368539728.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/\", \"locked\": false, \"name\": \"t3_1e9lwp\", \"created\": 1368505470.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Proposal: robots.txt equivalent for reddit bots.\", \"created_utc\": 1368476670.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 20}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EJust a couple small updates today, but they might be useful for anyone dealing with comment/post replies. \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/1c1nha/api_cant_get_thread_title_of_comment_reply_from/\\\"\\u003EAs requested by /u/ross456 last night\\u003C/a\\u003E, I\\u0026#39;ve added \\u003Ccode\\u003Elikes\\u003C/code\\u003E and \\u003Ccode\\u003Elink_title\\u003C/code\\u003E to \\u0026quot;messages\\u0026quot; that are actually comments in the API. These behave the same as they would for comments elsewhere - \\u003Ccode\\u003Elink_title\\u003C/code\\u003E is the title of the parent submission, and \\u003Ccode\\u003Elikes\\u003C/code\\u003E is whether the logged-in user has upvoted or downvoted that comment.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Just a couple small updates today, but they might be useful for anyone dealing with comment/post replies. [As requested by /u/ross456 last night](http://www.reddit.com/r/redditdev/comments/1c1nha/api_cant_get_thread_title_of_comment_reply_from/), I've added `likes` and `link_title` to \\\"messages\\\" that are actually comments in the API. These behave the same as they would for comments elsewhere - `link_title` is the title of the parent submission, and `likes` is whether the logged-in user has upvoted or downvoted that comment.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1c2uvm\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Deimorz\", \"media\": null, \"score\": 18, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1c2uvm/api_change_added_likes_and_link_title_attributes/\", \"locked\": false, \"name\": \"t3_1c2uvm\", \"created\": 1365647479.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1c2uvm/api_change_added_likes_and_link_title_attributes/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API change: Added 'likes' and 'link_title' attributes for messages\", \"created_utc\": 1365618679.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 18}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1aayha\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 17, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1aayha/reddit_pycon_2013_xpost_rpython/\", \"locked\": false, \"name\": \"t3_1aayha\", \"created\": 1363319888.0, \"url\": \"http://www.reddit.com/r/Python/comments/1aayc4/reddit_pycon_2013/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"reddit @ PyCon 2013 [x-post /r/Python]\", \"created_utc\": 1363291088.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 17}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu\\\"\\u003Ereddit install script for Ubuntu\\u003C/a\\u003E has been overhauled to use a new upstart-based init system for all of reddit. In addition to being cleaner and better mirroring how we run reddit in production these days, it also means that you can now install as a user other than \\u003Ccode\\u003Ereddit\\u003C/code\\u003E and things will work. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs part of cleaning this up, we\\u0026#39;ve removed all the old scripts in \\u003Ccode\\u003Escripts/\\u003C/code\\u003E and daemontools runscripts in \\u003Ccode\\u003Erun/\\u003C/code\\u003E. If you\\u0026#39;re relying on these (particularly if you are symlinking the \\u003Ccode\\u003Esrv\\u003C/code\\u003E directories like the old install script did) you\\u0026#39;ll need to copy these files before merging up to the latest code. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease do give the new install system a try, it\\u0026#39;s \\u003Ca href=\\\"http://youtu.be/I07xDdFMdgw\\\"\\u003Emuch better\\u003C/a\\u003E and make sure to let me know what problems you run into.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The [reddit install script for Ubuntu](https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu) has been overhauled to use a new upstart-based init system for all of reddit. In addition to being cleaner and better mirroring how we run reddit in production these days, it also means that you can now install as a user other than `reddit` and things will work. \\n\\nAs part of cleaning this up, we've removed all the old scripts in `scripts/` and daemontools runscripts in `run/`. If you're relying on these (particularly if you are symlinking the `srv` directories like the old install script did) you'll need to copy these files before merging up to the latest code. \\n\\nPlease do give the new install system a try, it's [much better](http://youtu.be/I07xDdFMdgw) and make sure to let me know what problems you run into.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"v8upy\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"spladug\", \"media\": null, \"score\": 18, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/v8upy/open_sorcerers_install_script_overhauled_and_old/\", \"locked\": false, \"name\": \"t3_v8upy\", \"created\": 1340086461.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/v8upy/open_sorcerers_install_script_overhauled_and_old/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Open Sorcerers: Install script overhauled and old files cleared out\", \"created_utc\": 1340057661.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 18}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe new field is \\u003Ccode\\u003Epublic_description\\u003C/code\\u003E and it is Markdown text. It appears in the Subreddit JSON output and is expected as input to \\u003Ca href=\\\"http://www.reddit.com/dev/api#POST_api_site_admin\\\"\\u003E\\u003Ccode\\u003E/api/site_admin\\u003C/code\\u003E\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/r/changelog/comments/tztot/reddit_change_subreddits_now_have_a_public/\\\"\\u003ESee an explanation over in /r/changelog\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The new field is `public_description` and it is Markdown text. It appears in the Subreddit JSON output and is expected as input to [`/api/site_admin`](http://www.reddit.com/dev/api#POST_api_site_admin).\\n\\n[See an explanation over in /r/changelog](http://www.reddit.com/r/changelog/comments/tztot/reddit_change_subreddits_now_have_a_public/)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"tztpg\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"spladug\", \"media\": null, \"score\": 17, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/tztpg/api_users_new_field_added_to_subreddit_json/\", \"locked\": false, \"name\": \"t3_tztpg\", \"created\": 1337747988.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/tztpg/api_users_new_field_added_to_subreddit_json/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API Users: New field added to subreddit JSON output and /api/site_admin input.\", \"created_utc\": 1337719188.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 17}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"qdxhs\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"WiglyWorm\", \"media\": null, \"score\": 18, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/qdxhs/to_me_cake_days_are_about_giving_back_to_the/\", \"locked\": false, \"name\": \"t3_qdxhs\", \"created\": 1330686389.0, \"url\": \"https://github.com/CSobol/reddit.js\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"To me, cake days are about giving back to the community in whatever way you can. With that in mind I just opensourced my JavaScript API wrapper on Github. Happy forking!\", \"created_utc\": 1330657589.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 18}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am looking to assist with some of the Reddit development (in my spare time, and I have some ideas to kick around).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhile setting up, and running a instance of Reddit is far from difficult, it would be extra time I\\u0026#39;d have to spend (instead of just getting to the code).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWould it be possible for Reddit to officially release an AMI of a Reddit build?  Much like you did with the VM?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve found [ami-56e81c3f]() which is an AMI of a Reddit build, but I don\\u0026#39;t believe it is official (and I\\u0026#39;ve not run it) - so I am unsure of the state.  I\\u0026#39;d feel more comfortable with an officially sanctioned release.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPS - I know you can convert a VM, and build an AMI from there, etc.  But it would be super easy to just spawn up an instance from EC2, and I doubt I am on the only one who would like to run a reddit build out there.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello!\\n\\nI am looking to assist with some of the Reddit development (in my spare time, and I have some ideas to kick around).\\n\\nWhile setting up, and running a instance of Reddit is far from difficult, it would be extra time I'd have to spend (instead of just getting to the code).\\n\\nWould it be possible for Reddit to officially release an AMI of a Reddit build?  Much like you did with the VM?\\n\\nI've found [ami-56e81c3f]() which is an AMI of a Reddit build, but I don't believe it is official (and I've not run it) - so I am unsure of the state.  I'd feel more comfortable with an officially sanctioned release.\\n\\nPS - I know you can convert a VM, and build an AMI from there, etc.  But it would be super easy to just spawn up an instance from EC2, and I doubt I am on the only one who would like to run a reddit build out there.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ejp87\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"wrayjustin\", \"media\": null, \"score\": 17, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ejp87/request_ec2_ami_amazon_machine_image_of_reddit/\", \"locked\": false, \"name\": \"t3_ejp87\", \"created\": 1292037952.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ejp87/request_ec2_ami_amazon_machine_image_of_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Request:  EC2 AMI (Amazon Machine Image) of Reddit\", \"created_utc\": 1292009152.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 17}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello Everyone, \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have been working on this project for a while to quickly get data from the reddit api, and now you can too. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe can get almost anything we want from reddit with 2 lines of code: \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Euser = Reddit::Services::User.new \\u0026quot;username\\u0026quot;, \\u0026quot;password\\u0026quot;, \\u0026quot;script_id\\u0026quot;, \\u0026quot;script_secret\\u0026quot;, \\u0026quot;User Agent Title\\u0026quot;\\nuser_info = Reddit::Services::Account.get_me user\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EReddit-Api provides: \\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EFull Oath Support and token managment (sign-out and extend token life) \\u003C/li\\u003E\\n\\u003Cli\\u003EMultiple Users in a single script \\u003C/li\\u003E\\n\\u003Cli\\u003ERequest Thottling (at any speed)\\u003C/li\\u003E\\n\\u003Cli\\u003EWrappers for all endpoints and methods\\u003C/li\\u003E\\n\\u003Cli\\u003EAutomatic Retry logic for network failures. \\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EFor more examples and usage check out the github: \\n\\u003Ca href=\\\"https://github.com/karl-b/reddit-api/\\\"\\u003Ehttps://github.com/karl-b/reddit-api/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe code is also on ruby-gems: \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Egem install reddit-api\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EI highly recommend using PRY or IRB for auto compelte, here are some hints: \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe namespaces and objects are the exact same names as in the docs defined here: \\u003Ca href=\\\"https://www.reddit.com/dev/api\\\"\\u003Ehttps://www.reddit.com/dev/api\\u003C/a\\u003E \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor example under \\u0026quot;Account\\u0026quot; there is a object GET /api/v1/me will translate to: \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EReddit::Services::Account.get_me \\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAnother Example under \\u0026quot;Listings\\u0026quot; there is a object GET [\\u003Ca href=\\\"/r/subreddit\\\"\\u003E/r/subreddit\\u003C/a\\u003E]/hot will translate to: \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EReddit::Services::Listings.get_hot\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ELet me know if there are any questions or outstanding issues. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello Everyone, \\n\\nI have been working on this project for a while to quickly get data from the reddit api, and now you can too. \\n\\nWe can get almost anything we want from reddit with 2 lines of code: \\n\\n    user = Reddit::Services::User.new \\\"username\\\", \\\"password\\\", \\\"script_id\\\", \\\"script_secret\\\", \\\"User Agent Title\\\"\\n    user_info = Reddit::Services::Account.get_me user\\n\\nReddit-Api provides: \\n\\n* Full Oath Support and token managment (sign-out and extend token life) \\n* Multiple Users in a single script \\n* Request Thottling (at any speed)\\n* Wrappers for all endpoints and methods\\n* Automatic Retry logic for network failures. \\n\\nFor more examples and usage check out the github: \\nhttps://github.com/karl-b/reddit-api/\\n\\n\\nThe code is also on ruby-gems: \\n\\n    gem install reddit-api\\n\\n - - - - - - \\n\\nI highly recommend using PRY or IRB for auto compelte, here are some hints: \\n\\nThe namespaces and objects are the exact same names as in the docs defined here: https://www.reddit.com/dev/api \\n\\nFor example under \\\"Account\\\" there is a object GET /api/v1/me will translate to: \\n\\n    Reddit::Services::Account.get_me \\n\\nAnother Example under \\\"Listings\\\" there is a object GET [/r/subreddit]/hot will translate to: \\n\\n    Reddit::Services::Listings.get_hot\\n\\nLet me know if there are any questions or outstanding issues. \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"487hlr\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"redditdev-karl\", \"media\": null, \"score\": 19, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/487hlr/ruby_new_lightweight_reddit_api_wrapper_with_100/\", \"locked\": false, \"name\": \"t3_487hlr\", \"created\": 1456736218.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/487hlr/ruby_new_lightweight_reddit_api_wrapper_with_100/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Ruby] New lightweight reddit api wrapper with 100% coverage.\", \"created_utc\": 1456707418.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 19}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI know there are a few scripts floating around to download Imgur links from a sub; but I was bored a couple nights back and the ones I found didn\\u0026#39;t work for me.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EInsomnia took me from modifying perl scripts I\\u0026#39;d found that didn\\u0026#39;t really work, to falling back into my comfort zone of C# *Ummm warm fuzzy C#* - and now with mono, executable on mac and linux\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI extended my initial scope (I mentioned the insomnia right?) and added the feature to download multiple subs concurrently on different threads.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOn the off chance this is useful to anyone, or anyone feels like cleaning the code up / Adding functionality etc. etc. I\\u0026#39;ve stuck it on bitbucket (personal preference over the indomitable git hub that everyone seams to use)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://bitbucket.org/w1r3d/redditrip\\\"\\u003Ehttps://bitbucket.org/w1r3d/redditrip\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI used RedditSharp to do handle my API calls - but had to write some pretty awful and hacky code to get more than ~950 subs from a given query. It would set my mind at ease if someone could fix that or even tell me wtf is causing it!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E** Word of warning **\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI left this running against a couple of subs...and I admit it, GW was amongst them...I came back to my PC with ~18GB of images\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I know there are a few scripts floating around to download Imgur links from a sub; but I was bored a couple nights back and the ones I found didn't work for me.\\n\\nInsomnia took me from modifying perl scripts I'd found that didn't really work, to falling back into my comfort zone of C# \\\\*Ummm warm fuzzy C#\\\\* - and now with mono, executable on mac and linux\\n\\nI extended my initial scope (I mentioned the insomnia right?) and added the feature to download multiple subs concurrently on different threads.\\n\\nOn the off chance this is useful to anyone, or anyone feels like cleaning the code up / Adding functionality etc. etc. I've stuck it on bitbucket (personal preference over the indomitable git hub that everyone seams to use)\\n\\nhttps://bitbucket.org/w1r3d/redditrip\\n\\nI used RedditSharp to do handle my API calls - but had to write some pretty awful and hacky code to get more than ~950 subs from a given query. It would set my mind at ease if someone could fix that or even tell me wtf is causing it!\\n\\n** Word of warning **\\n\\nI left this running against a couple of subs...and I admit it, GW was amongst them...I came back to my PC with ~18GB of images\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3sojzl\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"benuk\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1447473849.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3sojzl/redditrip_downloading_all_images_from_subreddit/\", \"locked\": false, \"name\": \"t3_3sojzl\", \"created\": 1447463585.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3sojzl/redditrip_downloading_all_images_from_subreddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"RedditRip - Downloading All Images from Subreddit\", \"created_utc\": 1447434785.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E/api/info now allows you to send a comma separated list of fullnames. The objects can be Comments, Links, or Subreddits. Previously only a single fullname was accepted per request.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Esee:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/dev/api#GET_api_info\\\"\\u003Ehttps://www.reddit.com/dev/api#GET_api_info\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/commit/b497b4f962bc0bdaef370fc0f7c57600302839d2\\\"\\u003Ehttps://github.com/reddit/reddit/commit/b497b4f962bc0bdaef370fc0f7c57600302839d2\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"/api/info now allows you to send a comma separated list of fullnames. The objects can be Comments, Links, or Subreddits. Previously only a single fullname was accepted per request.\\n\\nsee:\\n\\nhttps://www.reddit.com/dev/api#GET_api_info\\n\\nhttps://github.com/reddit/reddit/commit/b497b4f962bc0bdaef370fc0f7c57600302839d2\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2eur0l\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bsimpson\", \"media\": null, \"score\": 17, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2eur0l/api_update_apiinfo_supports_lists_of_fullnames/\", \"locked\": false, \"name\": \"t3_2eur0l\", \"created\": 1409285632.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2eur0l/api_update_apiinfo_supports_lists_of_fullnames/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API update: /api/info supports lists of fullnames\", \"created_utc\": 1409256832.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 17}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAm I the only one who\\u0026#39;s getting that since \\u003Ca href=\\\"http://www.reddit.com/r/announcements/comments/28hjga/reddit_changes_individual_updown_vote_counts_no/\\\"\\u003Ethe latest update\\u003C/a\\u003E?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENot sure about the exact causes yet, but \\u003Ccode\\u003Efor comment in comment_generator:\\u003C/code\\u003E doesn\\u0026#39;t work anymore for example.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Am I the only one who's getting that since [the latest update](http://www.reddit.com/r/announcements/comments/28hjga/reddit_changes_individual_updown_vote_counts_no/)?\\n\\nNot sure about the exact causes yet, but `for comment in comment_generator:` doesn't work anymore for example.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"28houf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"IAmAnAnonymousCoward\", \"media\": null, \"score\": 17, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 307, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/28houf/attributeerror_cant_set_attribute/\", \"locked\": false, \"name\": \"t3_28houf\", \"created\": 1403152278.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/28houf/attributeerror_cant_set_attribute/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"AttributeError: can't set attribute\", \"created_utc\": 1403123478.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 17}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"23kpux\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"sahilmuthoo\", \"media\": null, \"score\": 18, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/23kpux/redditjs_a_browser_based_reddit_api_wrapper/\", \"locked\": false, \"name\": \"t3_23kpux\", \"created\": 1398094583.0, \"url\": \"https://github.com/sahilm/reddit.js\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit.js - a browser based Reddit API Wrapper\", \"created_utc\": 1398065783.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 18}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey everyone! One of my recent goals has been to try and shore up reddit\\u0026#39;s OAuth2 implementation. To that end, I\\u0026#39;d love to hear about whether you\\u0026#39;ve attempted to use OAuth2 to login. If you have, did you decide to use it? Why or why not? If you\\u0026#39;re still using cookie auth, what\\u0026#39;s keeping you from making the switch?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAll login-related feedback is welcome. I want to know what I can be working on to make things easier for you.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EP.S. In case you missed it, OAuth users \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/1yxrp7/formal_ratelimiting_headers/\\\"\\u003Eget slightly higher ratelimits\\u003C/a\\u003E!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey everyone! One of my recent goals has been to try and shore up reddit's OAuth2 implementation. To that end, I'd love to hear about whether you've attempted to use OAuth2 to login. If you have, did you decide to use it? Why or why not? If you're still using cookie auth, what's keeping you from making the switch?\\n\\nAll login-related feedback is welcome. I want to know what I can be working on to make things easier for you.\\n\\nP.S. In case you missed it, OAuth users [get slightly higher ratelimits](http://www.reddit.com/r/redditdev/comments/1yxrp7/formal_ratelimiting_headers/)!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"208ntj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/208ntj/developer_feedback_requested_are_you_using_oauth/\", \"locked\": false, \"name\": \"t3_208ntj\", \"created\": 1394671155.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/208ntj/developer_feedback_requested_are_you_using_oauth/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Developer feedback requested: Are you using OAuth?\", \"created_utc\": 1394642355.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ihb8k\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"azad_hind_fauji\", \"media\": null, \"score\": 18, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ihb8k/dear_redditteam_do_you_guys_make_reddit_data/\", \"locked\": false, \"name\": \"t3_1ihb8k\", \"created\": 1374091322.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1ihb8k/dear_redditteam_do_you_guys_make_reddit_data/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Dear RedditTeam, do you guys make Reddit data dumps publicly available (like stackoverflow.com does)?\", \"created_utc\": 1374062522.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 18}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI noticed a small issue in the API the other day: All \\u003Ccode\\u003Eparent_id\\u003C/code\\u003E values in the various messages pages (inbox, unread, comment replies, etc.) were always starting with the \\u003Ccode\\u003Et4_\\u003C/code\\u003E prefix, which means that the parent is a message. However, in the cases where the message was due to a comment/submission reply, this obviously isn\\u0026#39;t correct.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis has been fixed now, so \\u003Ccode\\u003Eparent_id\\u003C/code\\u003E will have the correct prefix depending on what the message was a reply to. I imagine most people were already disregarding this prefix since it wasn\\u0026#39;t reliable, but just wanted to let you know.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/commit/6211d82f7a5a30dcbb865a726d047c01f791c879\\\"\\u003ESee the code on github\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I noticed a small issue in the API the other day: All `parent_id` values in the various messages pages (inbox, unread, comment replies, etc.) were always starting with the `t4_` prefix, which means that the parent is a message. However, in the cases where the message was due to a comment/submission reply, this obviously isn't correct.\\n\\nThis has been fixed now, so `parent_id` will have the correct prefix depending on what the message was a reply to. I imagine most people were already disregarding this prefix since it wasn't reliable, but just wanted to let you know.\\n\\n[See the code on github](https://github.com/reddit/reddit/commit/6211d82f7a5a30dcbb865a726d047c01f791c879)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1e38x5\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Deimorz\", \"media\": null, \"score\": 19, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1e38x5/minor_api_change_parent_id_values_on_messages/\", \"locked\": false, \"name\": \"t3_1e38x5\", \"created\": 1368246385.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1e38x5/minor_api_change_parent_id_values_on_messages/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Minor API change: parent_id values on messages will now have the correct type prefix\", \"created_utc\": 1368217585.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 19}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Ch2\\u003E\\u003C/h2\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"-\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"161odw\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ButtCrackFTW\", \"media\": null, \"score\": 17, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 29, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1367387705.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/161odw/wrote_a_simple_script_in_python_using_praw_to/\", \"locked\": false, \"name\": \"t3_161odw\", \"created\": 1357477377.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/161odw/wrote_a_simple_script_in_python_using_praw_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Wrote a simple script in python using PRAW to download all my saved reddit links, thought I would share.\", \"created_utc\": 1357448577.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 17}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"imgur.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"a2bh8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"phyzome\", \"media\": null, \"score\": 17, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/a2bh8/how_does_this_even_happen/\", \"locked\": false, \"name\": \"t3_a2bh8\", \"created\": 1257762428.0, \"url\": \"http://imgur.com/CZWPN.png\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How does this even *happen*?\", \"created_utc\": 1257733628.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 17}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"i.imgur.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"48kiyd\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"susannebitch\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/48kiyd/the_captcha_is_not_loading_on_oauth_please_add/\", \"locked\": false, \"name\": \"t3_48kiyd\", \"created\": 1456921798.0, \"url\": \"http://i.imgur.com/WdaoEDS.jpg\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"The CAPTCHA is not loading on oAuth! PLEASE add the full path to the image.\", \"created_utc\": 1456892998.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3wvfkp\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Superseuss\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3wvfkp/why_and_how_does_mredditcom_work_when/\", \"locked\": false, \"name\": \"t3_3wvfkp\", \"created\": 1450180087.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3wvfkp/why_and_how_does_mredditcom_work_when/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why and how does m.reddit.com work when www.reddit.com is down?\", \"created_utc\": 1450151287.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EToday we rolled out a new feature for mods for beta testing: the ability to sticky a comment at the top of a thread. \\u003Ca href=\\\"https://www.reddit.com/r/beta/comments/3vy7zl/new_beta_feature_for_mods_sticky_comments/\\\"\\u003EMore on this change in general over at /r/beta\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis has a few updates to the API that should not be considered final until general release, but I wanted to give you a preview so that you\\u0026#39;re prepared:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EComments will have a new field in their JSON representation, \\u003Ccode\\u003Estickied\\u003C/code\\u003E, which indicates if they are stickied to the top of this comments thread. The stickied comment will still show up as the topmost comment in a post thread\\u0026#39;s JSON, so you won\\u0026#39;t need to worry about specifically reordering them and existing API clients should work just fine without changes. The stickied field will only show up for users in beta during the beta period, to avoid breaking brittle API clients.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EThis provides a new optional boolean parameter, \\u003Ccode\\u003Esticky\\u003C/code\\u003E, to pass to distinguish. If passed and set to \\u003Ccode\\u003Etrue\\u003C/code\\u003E, the selected comment will be stickied for this post. If set to \\u003Ccode\\u003Efalse\\u003C/code\\u003E or omitted, if will be unstickied if it is presently stickied. This will be documented in the API docs when this feature is released generally.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EAny feedback on sticky comments is appreciated! Feel free to leave it here if API related or on the beta thread if not.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Today we rolled out a new feature for mods for beta testing: the ability to sticky a comment at the top of a thread. [More on this change in general over at /r/beta](https://www.reddit.com/r/beta/comments/3vy7zl/new_beta_feature_for_mods_sticky_comments/).\\n\\nThis has a few updates to the API that should not be considered final until general release, but I wanted to give you a preview so that you're prepared:\\n\\n1. Comments will have a new field in their JSON representation, `stickied`, which indicates if they are stickied to the top of this comments thread. The stickied comment will still show up as the topmost comment in a post thread's JSON, so you won't need to worry about specifically reordering them and existing API clients should work just fine without changes. The stickied field will only show up for users in beta during the beta period, to avoid breaking brittle API clients.\\n\\n2. This provides a new optional boolean parameter, `sticky`, to pass to distinguish. If passed and set to `true`, the selected comment will be stickied for this post. If set to `false` or omitted, if will be unstickied if it is presently stickied. This will be documented in the API docs when this feature is released generally.\\n\\nAny feedback on sticky comments is appreciated! Feel free to leave it here if API related or on the beta thread if not.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3vy897\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"umbrae\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3vy897/api_support_for_sticky_comments_presently_in_beta/\", \"locked\": false, \"name\": \"t3_3vy897\", \"created\": 1449623202.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3vy897/api_support_for_sticky_comments_presently_in_beta/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API support for Sticky Comments (presently in beta)\", \"created_utc\": 1449594402.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am offering additional API endpoints to compliment the ones that reddit has already created.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cem\\u003EDisclosure: I am not affiliated with reddit\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EThis endpoint will allow you to search reddit comments!\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EExample API call:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search?q=Einstein\\u0026amp;limit=100\\\"\\u003Ehttps://api.pushshift.io/reddit/search?q=Einstein\\u0026amp;limit=100\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis will return the last 100 reddit comments that had the term Einstein in the comment body.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELimitations:  I am ingesting reddit comments in real-time, so the comment score will always be 1.  Eventually, I will have a complete reddit comment search for all publicly available reddit comments with accurate score information.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso, this search will only search the previous 90 days of reddit comments.  However, it currently goes back to around July 16 when I first began work on the API.  Going forward, it will hold the last 90 days worth of comments.  Eventually, it will hold all publicly available reddit comments (once I purchase a new server with enough RAM to handle it -- around half a terabyte).  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThere is a lot you can do with this API call, so let\\u0026#39;s dive in to the details of what you can do with this API endpoint!  There are a lot of parameters that make this an extremely powerful tool for reddit developers.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EParameters:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eq\\u003C/strong\\u003E:  This is the actual search term.  The query syntax allows for a lot of advanced functions.  Here are a few examples of how to use it.  (Make sure you properly encode all requests to the API!)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo search for an exact phrase, use double quotes.  If you wanted to search for all comments that contained the exact phrase \\u0026quot;this kills the\\u0026quot;, you would make the following API call:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search?q=%22This%20kills%20the%22\\\"\\u003Ehttps://api.pushshift.io/reddit/search?q=%22This%20kills%20the%22\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo search for comments that contain one word but do not contain another word, you would use the following format:  star!sun\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThat would return comments that contain the word star but not the word sun.  Here is an example for that API call:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search?q=star!sun\\\"\\u003Ehttps://api.pushshift.io/reddit/search?q=star!sun\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EProximity search\\u003C/strong\\u003E:  If you wanted to find comments that contain the word star and also contain the word quantum where quantum is near star within 5 words, you would use the following API call:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search?q=%22star%20quantum%22%7E5\\\"\\u003Ehttps://api.pushshift.io/reddit/search?q=%22star%20quantum%22~5\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EQuorum search\\u003C/strong\\u003E:  Let\\u0026#39;s say you wanted to find comments that contained at least X of Y words.  For instance, you want to find comments that contain at least 3 of the terms among star, quantum, sun, atom, fusion. You would use the following API call:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search?q=%22star%20quantum%20sun%20atom%20fusion%22/3\\\"\\u003Ehttps://api.pushshift.io/reddit/search?q=%22star%20quantum%20sun%20atom%20fusion%22/3\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThat means if someone made a comment like \\u0026quot;Our sun is a great star with many atoms\\u0026quot;, that comment would match because it contains at least 3 of the 5 terms.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EStrict Order search\\u003C/strong\\u003E:  If you want to find comments that contain terms but only in the order specified, you would use \\u0026quot;\\u0026lt;\\u0026lt;\\u0026quot; between terms.  For example, if you wanted to find comments where the word star occurred before sun, you would search for star \\u0026lt;\\u0026lt; sun.  Here is an example API call:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search?q=star%20%3C%3C%20sun\\\"\\u003Ehttps://api.pushshift.io/reddit/search?q=star%20%3C%3C%20sun\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EMore Extended Query Syntax Examples\\u003C/strong\\u003E:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo view an entire list of possible search methods, please review \\u003Ca href=\\\"http://sphinxsearch.com/docs/archives/1.10/extended-syntax.html\\\"\\u003Ethis Sphinxsearch page\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Elimit\\u003C/strong\\u003E:  The maximum number of comments to return.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Ebefore_id\\u003C/strong\\u003E:  If this parameter is set, the API will return comments before this id in descending order.  This is helpful if you wish to pull data going backwards in time.  Using the example call above, the last comment id that contains the word einstein is \\u0026quot;ctrlpei\\u0026quot; (it may be different when you try it).  So if you wanted to get the next 100 comments with the word einstein, you would make another call setting the before_id to \\u0026quot;ctrlpei\\u0026quot;.  Example:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search?q=Einstein\\u0026amp;limit=100\\u0026amp;before_id=ctrlpei\\\"\\u003Ehttps://api.pushshift.io/reddit/search?q=Einstein\\u0026amp;limit=100\\u0026amp;before_id=ctrlpei\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Esubreddit\\u003C/strong\\u003E:  This parameter will restrict the returned results to a particular subreddit.  For example, if you wanted to get 10 comments with the word einstein in them, but only from the subreddit askscience, you would use this call:  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search?q=Einstein\\u0026amp;limit=10\\u0026amp;subreddit=askscience\\\"\\u003Ehttps://api.pushshift.io/reddit/search?q=Einstein\\u0026amp;limit=10\\u0026amp;subreddit=askscience\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eauthor\\u003C/strong\\u003E:  This parameter will restrict the returned results to a particular author.  For example, if you wanted to search for the term \\u0026quot;removed\\u0026quot; by the author \\u0026quot;automoderator\\u0026quot;, you would use the following API call: \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search?q=removed\\u0026amp;author=automoderator\\\"\\u003Ehttps://api.pushshift.io/reddit/search?q=removed\\u0026amp;author=automoderator\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Efields\\u003C/strong\\u003E:  This parameter will restrict the returned results to specific fields.  For example, if you wanted to do a search for comments containing einstein, but only care about the comment body and the time it was posted, you would make the following call:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search?q=Einstein\\u0026amp;fields=body,created_utc\\\"\\u003Ehttps://api.pushshift.io/reddit/search?q=Einstein\\u0026amp;fields=body,created_utc\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe field names are the key names normally returned.  So if you wanted to search for comments containing \\u0026quot;victoria\\u0026quot; and only cared about the author and subreddit, you would make the following API call:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search?q=victora\\u0026amp;fields=author,subreddit\\\"\\u003Ehttps://api.pushshift.io/reddit/search?q=victora\\u0026amp;fields=author,subreddit\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Elink_id\\u003C/strong\\u003E:  This parameter is a bit special.  You don\\u0026#39;t use the q parameter with this parameter.  What this parameter does is return all comments for a submission.  Example call:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search?link_id=3fto0c\\\"\\u003Ehttps://api.pushshift.io/reddit/search?link_id=3fto0c\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThat API call will return all comments posted in \\u003Ca href=\\\"https://www.reddit.com/r/aww/comments/3fto0c/i_rescued_stanley_and_his_hippo_today_the_shelter/\\\"\\u003Ethis submission\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EFeature Requests\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs always, if you have a request for a new feature, I would be happy to hear from you!  If the request is easy to implement, you\\u0026#39;ll probably see the new feature added within 24 hours.  If the request is complicated, it may take longer.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso, I am looking for a kick-ass front-end developer.  If you love working with data and you are a front-end developer that knows how to make an awesome looking front-end, I\\u0026#39;d like to hear from you!\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EAdditional Notes\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe search API is real-time meaning that once someone makes a comment to reddit, it will show up via search usually within 5 seconds.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am offering additional API endpoints to compliment the ones that reddit has already created.  \\n\\n*Disclosure: I am not affiliated with reddit*\\n\\n**This endpoint will allow you to search reddit comments!**\\n\\nExample API call:\\n\\nhttps://api.pushshift.io/reddit/search?q=Einstein\\u0026limit=100\\n\\nThis will return the last 100 reddit comments that had the term Einstein in the comment body.  \\n\\nLimitations:  I am ingesting reddit comments in real-time, so the comment score will always be 1.  Eventually, I will have a complete reddit comment search for all publicly available reddit comments with accurate score information.  \\n\\nAlso, this search will only search the previous 90 days of reddit comments.  However, it currently goes back to around July 16 when I first began work on the API.  Going forward, it will hold the last 90 days worth of comments.  Eventually, it will hold all publicly available reddit comments (once I purchase a new server with enough RAM to handle it -- around half a terabyte).  \\n\\nThere is a lot you can do with this API call, so let's dive in to the details of what you can do with this API endpoint!  There are a lot of parameters that make this an extremely powerful tool for reddit developers.\\n\\n**Parameters:**\\n\\n--------------------------------------------------------\\n\\n**q**:  This is the actual search term.  The query syntax allows for a lot of advanced functions.  Here are a few examples of how to use it.  (Make sure you properly encode all requests to the API!)\\n\\nTo search for an exact phrase, use double quotes.  If you wanted to search for all comments that contained the exact phrase \\\"this kills the\\\", you would make the following API call:\\n\\nhttps://api.pushshift.io/reddit/search?q=%22This%20kills%20the%22\\n\\nTo search for comments that contain one word but do not contain another word, you would use the following format:  star!sun\\n\\nThat would return comments that contain the word star but not the word sun.  Here is an example for that API call:\\n\\nhttps://api.pushshift.io/reddit/search?q=star!sun\\n\\n**Proximity search**:  If you wanted to find comments that contain the word star and also contain the word quantum where quantum is near star within 5 words, you would use the following API call:\\n\\nhttps://api.pushshift.io/reddit/search?q=%22star%20quantum%22~5\\n\\n**Quorum search**:  Let's say you wanted to find comments that contained at least X of Y words.  For instance, you want to find comments that contain at least 3 of the terms among star, quantum, sun, atom, fusion. You would use the following API call:\\n\\nhttps://api.pushshift.io/reddit/search?q=%22star%20quantum%20sun%20atom%20fusion%22/3\\n\\nThat means if someone made a comment like \\\"Our sun is a great star with many atoms\\\", that comment would match because it contains at least 3 of the 5 terms.\\n\\n**Strict Order search**:  If you want to find comments that contain terms but only in the order specified, you would use \\\"\\u003C\\u003C\\\" between terms.  For example, if you wanted to find comments where the word star occurred before sun, you would search for star \\u003C\\u003C sun.  Here is an example API call:\\n\\nhttps://api.pushshift.io/reddit/search?q=star%20%3C%3C%20sun\\n\\n**More Extended Query Syntax Examples**:\\n\\nTo view an entire list of possible search methods, please review [this Sphinxsearch page](http://sphinxsearch.com/docs/archives/1.10/extended-syntax.html)\\n\\n--------------------------------------------------------\\n\\n**limit**:  The maximum number of comments to return.\\n\\n--------------------------------------------------------\\n\\n**before_id**:  If this parameter is set, the API will return comments before this id in descending order.  This is helpful if you wish to pull data going backwards in time.  Using the example call above, the last comment id that contains the word einstein is \\\"ctrlpei\\\" (it may be different when you try it).  So if you wanted to get the next 100 comments with the word einstein, you would make another call setting the before_id to \\\"ctrlpei\\\".  Example:\\n\\nhttps://api.pushshift.io/reddit/search?q=Einstein\\u0026limit=100\\u0026before_id=ctrlpei\\n\\n--------------------------------------------------------\\n\\n**subreddit**:  This parameter will restrict the returned results to a particular subreddit.  For example, if you wanted to get 10 comments with the word einstein in them, but only from the subreddit askscience, you would use this call:  \\n\\nhttps://api.pushshift.io/reddit/search?q=Einstein\\u0026limit=10\\u0026subreddit=askscience\\n\\n--------------------------------------------------------\\n\\n**author**:  This parameter will restrict the returned results to a particular author.  For example, if you wanted to search for the term \\\"removed\\\" by the author \\\"automoderator\\\", you would use the following API call: \\n\\nhttps://api.pushshift.io/reddit/search?q=removed\\u0026author=automoderator\\n\\n--------------------------------------------------------\\n\\n**fields**:  This parameter will restrict the returned results to specific fields.  For example, if you wanted to do a search for comments containing einstein, but only care about the comment body and the time it was posted, you would make the following call:\\n\\nhttps://api.pushshift.io/reddit/search?q=Einstein\\u0026fields=body,created_utc\\n\\nThe field names are the key names normally returned.  So if you wanted to search for comments containing \\\"victoria\\\" and only cared about the author and subreddit, you would make the following API call:\\n\\nhttps://api.pushshift.io/reddit/search?q=victora\\u0026fields=author,subreddit\\n\\n--------------------------------------------------------\\n\\n**link_id**:  This parameter is a bit special.  You don't use the q parameter with this parameter.  What this parameter does is return all comments for a submission.  Example call:\\n\\nhttps://api.pushshift.io/reddit/search?link_id=3fto0c\\n\\nThat API call will return all comments posted in [this submission](https://www.reddit.com/r/aww/comments/3fto0c/i_rescued_stanley_and_his_hippo_today_the_shelter/)\\n\\n\\n--------------------------------------------------------\\n\\n**Feature Requests**\\n\\nAs always, if you have a request for a new feature, I would be happy to hear from you!  If the request is easy to implement, you'll probably see the new feature added within 24 hours.  If the request is complicated, it may take longer.  \\n\\nAlso, I am looking for a kick-ass front-end developer.  If you love working with data and you are a front-end developer that knows how to make an awesome looking front-end, I'd like to hear from you!\\n\\n--------------------------------------------------------\\n\\n**Additional Notes**\\n\\nThe search API is real-time meaning that once someone makes a comment to reddit, it will show up via search usually within 5 seconds.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3fv8vv\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1438783617.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3fv8vv/new_api_endpoint_now_you_can_search_comments/\", \"locked\": false, \"name\": \"t3_3fv8vv\", \"created\": 1438806150.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3fv8vv/new_api_endpoint_now_you_can_search_comments/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New API endpoint -- Now you can search comments!\", \"created_utc\": 1438777350.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi, I\\u0026#39;m currently having a look through the code-base and I thought I could make minor commits as I go about.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAn example would be changing this:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Edef get_channel(self, reconnect = False):\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eto this:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Edef get_channel(self, reconnect=False):\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThe change makes the code adhere more closely to PEP8. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAre changes like this accepted or are they just a nuisance that waste the admins\\u0026#39; time to look at?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI was thinking of taking some time to make minor fixes like this but I\\u0026#39;d like a bit of input before I do.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi, I'm currently having a look through the code-base and I thought I could make minor commits as I go about.\\n\\nAn example would be changing this:\\n\\n    def get_channel(self, reconnect = False):\\n    \\nto this:\\n\\n    def get_channel(self, reconnect=False):\\n\\nThe change makes the code adhere more closely to PEP8. \\n\\nAre changes like this accepted or are they just a nuisance that waste the admins' time to look at?\\n\\nI was thinking of taking some time to make minor fixes like this but I'd like a bit of input before I do.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3fnc3w\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Matthew94\", \"media\": null, \"score\": 17, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3fnc3w/are_extremely_minor_commits_allowed/\", \"locked\": false, \"name\": \"t3_3fnc3w\", \"created\": 1438654932.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3fnc3w/are_extremely_minor_commits_allowed/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Are extremely minor commits allowed?\", \"created_utc\": 1438626132.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 17}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis project started out as a widget a little over 2 years ago and since then I\\u0026#39;ve been adding new features every once in a while. Over the last few weeks I\\u0026#39;ve put a lot of hours into it and it\\u0026#39;s now almost hard to believe how far it\\u0026#39;s come! From a simple widget to a feature filled app!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFeel free to use the code in your own applications, particularly the OAuth API wrapper, but please make sure you abide by the GPL license.\\nAlso feel free to give any feedback on the app as I\\u0026#39;m always looking for things to improve it.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOverview of the new version:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EReddinator 2.5 bring a fresh new look to the app, with many new features and customisation settings. It brings multireddit support \\u0026amp; can now completely change the appearance of each widget by creating custom themes.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EChanges in this release:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003ESubmit posts \\u0026amp; share URLs with Reddinator from other apps.\\u003C/li\\u003E\\n\\u003Cli\\u003ETheme editing: Create custom themes and apply them to individual widgets.\\u003C/li\\u003E\\n\\u003Cli\\u003EFresh new tabs \\u0026amp; layouts.\\u003C/li\\u003E\\n\\u003Cli\\u003EFull Multi-subredit support. Create, edit \\u0026amp; view multis on your Reddit account.\\u003C/li\\u003E\\n\\u003Cli\\u003EMailbox, clear unread flag when opening inbox (feature fix).\\u003C/li\\u003E\\n\\u003Cli\\u003EMany bug fixes, API \\u0026amp; error handling improvements.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://play.google.com/store/apps/details?id=au.com.wallaceit.reddinator\\u0026amp;hl=en\\\"\\u003Ehttps://play.google.com/store/apps/details?id=au.com.wallaceit.reddinator\\u0026amp;hl=en\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This project started out as a widget a little over 2 years ago and since then I've been adding new features every once in a while. Over the last few weeks I've put a lot of hours into it and it's now almost hard to believe how far it's come! From a simple widget to a feature filled app!\\n\\nFeel free to use the code in your own applications, particularly the OAuth API wrapper, but please make sure you abide by the GPL license.\\nAlso feel free to give any feedback on the app as I'm always looking for things to improve it.\\n\\nOverview of the new version:\\n\\nReddinator 2.5 bring a fresh new look to the app, with many new features and customisation settings. It brings multireddit support \\u0026 can now completely change the appearance of each widget by creating custom themes.\\n\\nChanges in this release:\\n\\n- Submit posts \\u0026 share URLs with Reddinator from other apps.\\n- Theme editing: Create custom themes and apply them to individual widgets.\\n- Fresh new tabs \\u0026 layouts.\\n- Full Multi-subredit support. Create, edit \\u0026 view multis on your Reddit account.\\n- Mailbox, clear unread flag when opening inbox (feature fix).\\n- Many bug fixes, API \\u0026 error handling improvements.\\n\\nhttps://play.google.com/store/apps/details?id=au.com.wallaceit.reddinator\\u0026hl=en\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"36594y\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"micwallace\", \"media\": null, \"score\": 17, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/36594y/hi_redditdevs_i_wanted_to_take_the_opportunity_to/\", \"locked\": false, \"name\": \"t3_36594y\", \"created\": 1431788761.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/36594y/hi_redditdevs_i_wanted_to_take_the_opportunity_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Hi RedditDevs, I wanted to take the opportunity to share my creation: Reddinator, an open source Reddit App \\u0026 Widget for Android.\", \"created_utc\": 1431759961.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 17}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ll spare you the overview by \\u003Ca href=\\\"https://www.reddit.com/r/changelog/comments/342y8k/reddit_change_new_comment_sorting_options/\\\"\\u003Elinking you to the change log describing the features themselves!\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EA few API relevant bits:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EIn the API, Q\\u0026amp;A will show up as a new sort type, \\u0026quot;qa\\u0026quot;. You can also sort by this by adding ?sort=qa to comments threads.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EThere\\u0026#39;s a new setting for users, \\u003Ccode\\u003Eignore_suggested_sorts\\u003C/code\\u003E. It\\u0026#39;s a boolean that operates as you\\u0026#39;d expect, and is accessible at the \\u003Ca href=\\\"https://www.reddit.com/dev/api#GET_api_v1_me_prefs\\\"\\u003Eme endpoint\\u003C/a\\u003E.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EMods with the appropriate permission may set suggested sorts for a thread. This is \\u003Ca href=\\\"https://www.reddit.com/dev/api#POST_api_set_suggested_sort\\\"\\u003Eset_suggested_sort, and here are the docs\\u003C/a\\u003E.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EMods with settings permission may also set suggested sorts for an entire subreddit. This is the \\u003Ca href=\\\"https://www.reddit.com/dev/api#POST_api_site_admin\\\"\\u003E\\u003Ccode\\u003Esuggested_comment_sort\\u003C/code\\u003E field on \\u003Ccode\\u003Esite_admin\\u003C/code\\u003E\\u003C/a\\u003E. One important note here is that site_admin operates a little wonkily, in that if you POST to it without the original fields set as well, you will clear them out. So make sure you\\u0026#39;re GETting all of those fields and then POSTing back all of them (including your edits) each time (including suggested_comment_sort).\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EI think that covers it, hopefully this is a useful feature for you all.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'll spare you the overview by [linking you to the change log describing the features themselves!](https://www.reddit.com/r/changelog/comments/342y8k/reddit_change_new_comment_sorting_options/)\\n\\nA few API relevant bits:\\n\\n1. In the API, Q\\u0026A will show up as a new sort type, \\\"qa\\\". You can also sort by this by adding ?sort=qa to comments threads.\\n\\n2. There's a new setting for users, `ignore_suggested_sorts`. It's a boolean that operates as you'd expect, and is accessible at the [me endpoint](https://www.reddit.com/dev/api#GET_api_v1_me_prefs).\\n\\n3. Mods with the appropriate permission may set suggested sorts for a thread. This is [set_suggested_sort, and here are the docs](https://www.reddit.com/dev/api#POST_api_set_suggested_sort).\\n\\n4. Mods with settings permission may also set suggested sorts for an entire subreddit. This is the [`suggested_comment_sort` field on `site_admin`](https://www.reddit.com/dev/api#POST_api_site_admin). One important note here is that site_admin operates a little wonkily, in that if you POST to it without the original fields set as well, you will clear them out. So make sure you're GETting all of those fields and then POSTing back all of them (including your edits) each time (including suggested_comment_sort).\\n\\nI think that covers it, hopefully this is a useful feature for you all.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3434ko\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"umbrae\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3434ko/devs_two_new_features_you_may_like_to_know_about/\", \"locked\": false, \"name\": \"t3_3434ko\", \"created\": 1430203904.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3434ko/devs_two_new_features_you_may_like_to_know_about/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Devs: Two new features you may like to know about (Q\\u0026A sort, Suggested Sort)\", \"created_utc\": 1430175104.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2eatps\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2eatps/oauth2_manual_token_revocation/\", \"locked\": false, \"name\": \"t3_2eatps\", \"created\": 1408761450.0, \"url\": \"https://github.com/reddit/reddit/wiki/OAuth2#manually-revoking-a-token\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OAuth2: Manual token revocation\", \"created_utc\": 1408732650.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey all,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPardon the self-promotion, but I\\u0026#39;d like to announce \\u003Ca href=\\\"https://github.com/kz26/GoReddit\\\"\\u003EGoReddit\\u003C/a\\u003E, a Golang Reddit client/library that I\\u0026#39;m working on. Obviously it\\u0026#39;s in a very early stage of development, but right now it\\u0026#39;s capable of getting a listing of posts from a subreddit, getting a listing of comments, and voting. All API calls go through a custom HTTP client that respects the guideline of waiting 2 seconds between requests.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ll be using GoReddit for a bot that I\\u0026#39;m writing, but I\\u0026#39;d also like the library to be useful to others interested in writing a bot in Go or something. With that said, what features do you find most important for a Reddit API client?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey all,\\n\\nPardon the self-promotion, but I'd like to announce [GoReddit](https://github.com/kz26/GoReddit), a Golang Reddit client/library that I'm working on. Obviously it's in a very early stage of development, but right now it's capable of getting a listing of posts from a subreddit, getting a listing of comments, and voting. All API calls go through a custom HTTP client that respects the guideline of waiting 2 seconds between requests.\\n\\nI'll be using GoReddit for a bot that I'm writing, but I'd also like the library to be useful to others interested in writing a bot in Go or something. With that said, what features do you find most important for a Reddit API client?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1jnthi\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"whitehat2k9\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1375585879.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1jnthi/goreddit_a_reddit_clientlibrary_for_go/\", \"locked\": false, \"name\": \"t3_1jnthi\", \"created\": 1375614247.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1jnthi/goreddit_a_reddit_clientlibrary_for_go/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"GoReddit, a Reddit client/library for Go\", \"created_utc\": 1375585447.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis is continuation of \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/19saz0/learning_reddits_code_journal_1/\\\"\\u003Ehttp://www.reddit.com/r/redditdev/comments/19saz0/learning_reddits_code_journal_1/\\u003C/a\\u003E. I\\u0026#39;m posting as I learn--to help others and to get corrections when I \\u0026quot;learn\\u0026quot; the wrong things.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E(Oh, and I switched from \\u003Ca href=\\\"/u/foolblog\\\"\\u003E/u/foolblog\\u003C/a\\u003E to \\u003Ca href=\\\"/u/fool_blog\\\"\\u003E/u/fool_blog\\u003C/a\\u003E because \\u0026quot;fool_blog\\u0026quot; is easier to read and \\u003Cem\\u003E\\u003Csup\\u003Ei\\u003C/sup\\u003E \\u003Csup\\u003Ekinda\\u003C/sup\\u003E \\u003Csup\\u003Eforgot\\u003C/sup\\u003E \\u003Csup\\u003Ethe\\u003C/sup\\u003E \\u003Csup\\u003Epassword,\\u003C/sup\\u003E \\u003Csup\\u003Eoopsie\\u003C/sup\\u003E\\u003C/em\\u003E .)\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EIn Journal #1, I got a local reddit started and running on a Ubuntu 12.04 system. \\u003Cem\\u003EEverything here assumes you\\u0026#39;re running Ubuntu 12.04.\\u003C/em\\u003E \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENow, the quest for admin powers begins.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EMake Me Admin!\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBy default, reddit has one admin \\u003Cdel\\u003Eaccount\\u003C/del\\u003E not-actually-an-account-yet. The first person to create an account named \\u0026quot;reddit\\u0026quot; is the admin.  Yikes! \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELet\\u0026#39;s change that.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELogin to your local reddit.  Create two accounts.  I\\u0026#39;m using \\u0026quot;fool\\u0026quot; and \\u0026quot;fool2\\u0026quot;.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOpen /home/reddit/reddit/r2/development.update in a text editor and add one line:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eadmins = fool, fool2\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ESave the file. Open a command line terminal and:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ecd /home/reddit/reddit/r2\\nsudo make\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EBoom!  Your two accounts are admin accounts.  \\u0026quot;reddit\\u0026quot; is no longer an admin.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHowever, there are still major problems--admin account have very limited powers until you verify email accounts, and I can\\u0026#39;t figure out how to get the local reddit to send out confirmation e-mails. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThat\\u0026#39;s okay. I\\u0026#39;m gonna cheat on this one.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EI\\u0026#39;m gonna hack my database.\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EDisclaimer:\\u003C/strong\\u003E \\u003Cem\\u003EThere\\u0026#39;s a high chance that doing things this way is extremely naughty.  With any luck, somebody will post corrections.\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EReddit saves important stuff in a PostreSQL database.  \\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EStep 1: Set the postgres password. \\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EI used the instructions at \\u003Ca href=\\\"http://library.linode.com/databases/postgresql/ubuntu-12.04-precise-pangolin#sph_set-the-postgres-user-s-password\\\"\\u003Ehttp://library.linode.com/databases/postgresql/ubuntu-12.04-precise-pangolin#sph_set-the-postgres-user-s-password\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EStep 2: Run the pgAdmin III GUI.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EIf you don\\u0026#39;t have it, get it:\\n    sudo apt-get install pgadmin3\\nOnce you do have it, run it.\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EStep 3: Add the local server.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003EClick on File -\\u0026gt; Add Server...\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EFill in the blanks on the Properties tab:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003EName: reddit [or something else, as long as it is not blank]\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHost: localhost\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E(Port should be 5432 by default.)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E(Maintence DB and Username should be \\u0026quot;postgres\\u0026quot; by default.)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPassword: [Whatever password you used in Step 1.]\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003ENow click OK to open the reddit database.\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EStep 4: Commence database hacking:\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EIn the object browser, open the tree: \\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003Ereddit (localhost: 5432) \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDatabases(2)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ereddit\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESchemas\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Epublic\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETables\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ereddit_data_account\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003ESelect reddit_data_account and press Ctrl-D to open the data in a new window.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENow find your admin accounts.  You\\u0026#39;re looking for a row like:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E3  |  name | fool | str\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E3 is the id number for the account with the name of \\u0026quot;fool\\u0026quot;.  (This number may vary.  The important thing is that the 2nd column is \\u0026quot;name\\u0026quot; and the third column has the name of the admin account.)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m going to hack fool into having a verified email account.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI add the following rows to reddit_data_account.  I use \\u0026quot;3\\u0026quot; in column one because that\\u0026#39;s the account ID number for \\u003Ca href=\\\"/u/fool\\\"\\u003E/u/fool\\u003C/a\\u003E .\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E3 | email          | fake@fakefake.com | str\\n3 | email_verified | t                 | bool\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EFool2 needs email verfied too.  The row for fool2\\u0026#39;s name has \\u0026quot;4\\u0026quot; in the thing_id column, so I\\u0026#39;ll add two rows for him, too.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E4 | email          | fakest@fakefake.com | str\\n4 | email_verified | t                   | bool\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENow, since doing things this way is \\u003Cstrong\\u003Eprobably very naughty\\u003C/strong\\u003E, I seem to have messed up reddit\\u0026#39;s cache system.  Alas, since I don\\u0026#39;t know how to fix that the easy way.  However, rebooting the computer forces the caches to update.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOne reboot sequence later and the hack is complete. The caches have been forced to refresh, and the local reddit is convinced that the email addresses were verified. I can use the admin accounts to do admin type stuff.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cem\\u003EAnd, yeah, this is the cue for people to tell me how clueless I am and how to do this sort of thing without a reboot.\\u003C/em\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This is continuation of http://www.reddit.com/r/redditdev/comments/19saz0/learning_reddits_code_journal_1/. I'm posting as I learn--to help others and to get corrections when I \\\"learn\\\" the wrong things.\\n\\n(Oh, and I switched from /u/foolblog to /u/fool_blog because \\\"fool_blog\\\" is easier to read and *^i ^kinda ^forgot ^the ^password, ^oopsie* .)\\n\\n----\\n\\nIn Journal #1, I got a local reddit started and running on a Ubuntu 12.04 system. *Everything here assumes you're running Ubuntu 12.04.* \\n\\nNow, the quest for admin powers begins.\\n\\n----\\n\\n**Make Me Admin!**\\n\\nBy default, reddit has one admin ~~account~~ not-actually-an-account-yet. The first person to create an account named \\\"reddit\\\" is the admin.  Yikes! \\n\\nLet's change that.\\n\\nLogin to your local reddit.  Create two accounts.  I'm using \\\"fool\\\" and \\\"fool2\\\".\\n\\nOpen /home/reddit/reddit/r2/development.update in a text editor and add one line:\\n\\n    admins = fool, fool2\\n\\nSave the file. Open a command line terminal and:\\n\\n    cd /home/reddit/reddit/r2\\n    sudo make\\n\\nBoom!  Your two accounts are admin accounts.  \\\"reddit\\\" is no longer an admin.\\n\\nHowever, there are still major problems--admin account have very limited powers until you verify email accounts, and I can't figure out how to get the local reddit to send out confirmation e-mails. \\n\\nThat's okay. I'm gonna cheat on this one.\\n\\n-----\\n\\n**I'm gonna hack my database.**\\n\\n**Disclaimer:** *There's a high chance that doing things this way is extremely naughty.  With any luck, somebody will post corrections.*\\n\\nReddit saves important stuff in a PostreSQL database.  \\n\\n* Step 1: Set the postgres password. \\n\\nI used the instructions at http://library.linode.com/databases/postgresql/ubuntu-12.04-precise-pangolin#sph_set-the-postgres-user-s-password\\n\\n* Step 2: Run the pgAdmin III GUI.\\n\\nIf you don't have it, get it:\\n    sudo apt-get install pgadmin3\\nOnce you do have it, run it.\\n\\n* Step 3: Add the local server.\\n\\n1. Click on File -\\u003E Add Server...\\n\\nFill in the blanks on the Properties tab:\\n\\n\\u003E Name: reddit [or something else, as long as it is not blank]\\n\\n\\u003E Host: localhost\\n\\n\\u003E (Port should be 5432 by default.)\\n\\n\\u003E (Maintence DB and Username should be \\\"postgres\\\" by default.)\\n\\n\\u003E Password: [Whatever password you used in Step 1.]\\n\\nNow click OK to open the reddit database.\\n\\n* Step 4: Commence database hacking:\\n\\nIn the object browser, open the tree: \\n\\n\\u003E  reddit (localhost: 5432) \\n\\n\\u003E  Databases(2)\\n\\n\\u003E  reddit\\n\\n\\u003E  Schemas\\n\\n\\u003E  public\\n\\n\\u003E  Tables\\n\\n\\u003E  reddit_data_account\\n\\nSelect reddit_data_account and press Ctrl-D to open the data in a new window.\\n\\nNow find your admin accounts.  You're looking for a row like:\\n\\n    3  |  name | fool | str\\n\\n3 is the id number for the account with the name of \\\"fool\\\".  (This number may vary.  The important thing is that the 2nd column is \\\"name\\\" and the third column has the name of the admin account.)\\n\\nI'm going to hack fool into having a verified email account.\\n\\nI add the following rows to reddit_data_account.  I use \\\"3\\\" in column one because that's the account ID number for /u/fool .\\n\\n    3 | email          | fake@fakefake.com | str\\n    3 | email_verified | t                 | bool\\n\\nFool2 needs email verfied too.  The row for fool2's name has \\\"4\\\" in the thing_id column, so I'll add two rows for him, too.\\n\\n    4 | email          | fakest@fakefake.com | str\\n    4 | email_verified | t                   | bool\\n\\nNow, since doing things this way is **probably very naughty**, I seem to have messed up reddit's cache system.  Alas, since I don't know how to fix that the easy way.  However, rebooting the computer forces the caches to update.  \\n\\nOne reboot sequence later and the hack is complete. The caches have been forced to refresh, and the local reddit is convinced that the email addresses were verified. I can use the admin accounts to do admin type stuff.\\n\\n*And, yeah, this is the cue for people to tell me how clueless I am and how to do this sort of thing without a reboot.*\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"19t48f\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"fool_blog\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1362629037.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/19t48f/learning_reddits_code_journal_2_admin_status_one/\", \"locked\": false, \"name\": \"t3_19t48f\", \"created\": 1362642040.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/19t48f/learning_reddits_code_journal_2_admin_status_one/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Learning reddit's code, Journal #2: Admin status (one way or the other)\", \"created_utc\": 1362613240.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"158jbq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"leavebot3000\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/158jbq/ill_just_leave_this_here_thanks_for_your_help_guys/\", \"locked\": false, \"name\": \"t3_158jbq\", \"created\": 1356140774.0, \"url\": \"https://github.com/chris3000/reddit_leavebot\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I'll just leave this here... (Thanks for your help, guys!)\", \"created_utc\": 1356111974.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWe now require a \\u0026quot;modhash\\u0026quot; \\u003Ccode\\u003Euh=\\u003C/code\\u003E parameter in \\u003Ccode\\u003E/api/submit\\u003C/code\\u003E, similar to other POSTs in the reddit API. This is necessary to prevent \\u003Ca href=\\\"http://en.wikipedia.org/wiki/Cross-site_request_forgery\\\"\\u003ECSRF\\u003C/a\\u003E attacks.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAccording to logs, the number of API clients making submit calls without a modhash is very low. However, if your bot/client is affected, you can easily request a modhash by GETing \\u003Ca href=\\\"http://www.reddit.com/api/me.json\\\"\\u003Ehttp://www.reddit.com/api/me.json\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"We now require a \\\"modhash\\\" `uh=` parameter in `/api/submit`, similar to other POSTs in the reddit API. This is necessary to prevent [CSRF](http://en.wikipedia.org/wiki/Cross-site_request_forgery) attacks.\\n\\nAccording to logs, the number of API clients making submit calls without a modhash is very low. However, if your bot/client is affected, you can easily request a modhash by GETing http://www.reddit.com/api/me.json.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"101kru\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"chromakode\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/101kru/api_notice_make_sure_youre_sending_a_modhash_with/\", \"locked\": false, \"name\": \"t3_101kru\", \"created\": 1347946293.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/101kru/api_notice_make_sure_youre_sending_a_modhash_with/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[api notice] Make sure you're sending a modhash with /api/submit.\", \"created_utc\": 1347917493.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI submitted the version of iReddit that\\u0026#39;s up on github to Apple, and they replied:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003EDear CondeNet, Thank you for submitting iReddit to the App Store. iReddit cannot be posted to the App Store because it is using private or undocumented APIs: \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EPrivate Symbol References \\nOBJC_IVAR_$_UITouch._locationInWindow\\nOBJC_IVAR_$_UITouch._phase\\nOBJC_IVAR_$_UITouch._previousLocationInWindow\\nOBJC_IVAR_$_UITouch._tapCount\\nOBJC_IVAR_$_UITouch._timestamp\\nOBJC_IVAR_$_UITouch._touchFlags\\nOBJC_IVAR_$_UITouch._view\\nOBJC_IVAR_$_UITouch._window\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAs you know, as outlined in the iPhone Developer Program License Agreement section 3.3.1, the use of non-public APIs is not permitted. Before your application can be reviewed by the App Review Team, please resolve this issue and upload a new binary to iTunes Connect. Sincerely, iPhone Developer Program       \\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EAny ideas why this would be true? Does Three20 use these APIs, perhaps?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I submitted the version of iReddit that's up on github to Apple, and they replied:\\n\\n\\u003E Dear CondeNet, Thank you for submitting iReddit to the App Store. iReddit cannot be posted to the App Store because it is using private or undocumented APIs: \\n\\n\\u003E     Private Symbol References \\n\\u003E     OBJC_IVAR_$_UITouch._locationInWindow\\n\\u003E     OBJC_IVAR_$_UITouch._phase\\n\\u003E     OBJC_IVAR_$_UITouch._previousLocationInWindow\\n\\u003E     OBJC_IVAR_$_UITouch._tapCount\\n\\u003E     OBJC_IVAR_$_UITouch._timestamp\\n\\u003E     OBJC_IVAR_$_UITouch._touchFlags\\n\\u003E     OBJC_IVAR_$_UITouch._view\\n\\u003E     OBJC_IVAR_$_UITouch._window\\n\\n\\u003E As you know, as outlined in the iPhone Developer Program License Agreement section 3.3.1, the use of non-public APIs is not permitted. Before your application can be reviewed by the App Review Team, please resolve this issue and upload a new binary to iTunes Connect. Sincerely, iPhone Developer Program       \\n\\nAny ideas why this would be true? Does Three20 use these APIs, perhaps?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"dwqcx\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ketralnis\", \"media\": null, \"score\": 17, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/dwqcx/iredditrelated_question_for_any_iphone_programmers/\", \"locked\": false, \"name\": \"t3_dwqcx\", \"created\": 1288144482.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/dwqcx/iredditrelated_question_for_any_iphone_programmers/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"iReddit-related question for any iPhone programmers\", \"created_utc\": 1288115682.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 17}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"cv1or\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"coob\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/cv1or/hey_rredditdev_ive_made_some_bug_fixes_to_ireddit/\", \"locked\": false, \"name\": \"t3_cv1or\", \"created\": 1280435024.0, \"url\": \"http://github.com/alastairstuart/iReddit\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Hey /r/redditdev, I've made some bug fixes to iReddit and added Retina Display compatitbility. [github]\", \"created_utc\": 1280406224.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"6p7bb\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"mrpeenut24\", \"media\": null, \"score\": 18, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/6p7bb/user_pages_dont_work_on_certain_people/\", \"locked\": false, \"name\": \"t3_6p7bb\", \"created\": 1214577281.0, \"url\": \"http://www.reddit.com/info/6p6al/comments/c04hwn1\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"User pages don't work on certain people\", \"created_utc\": 1214548481.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 18}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"praw.readthedocs.org\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4bw0z6\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"shaggorama\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4bw0z6/new_bot_tutorial_added_to_the_praw_docs_with_the/\", \"locked\": false, \"name\": \"t3_4bw0z6\", \"created\": 1458932829.0, \"url\": \"https://praw.readthedocs.org/en/stable/pages/call_and_response_bot.html\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New bot tutorial added to the PRAW docs with the last stable release, demonstrates a basic skeleton for most simple bots.\", \"created_utc\": 1458904029.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAbout 4 years ago I decided to delve into Android development. I had dabbled with Java before but for anyone who has done the same, they would understand that Android is an entirely different monster (although I have grown to like the platform, except themes.xml....yuk).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI decided to write a Reddit widget, partially because none of the current ones could satisfy my requirements, but also because I wanted one that I could tinker with and share with the Reddit community.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELooking back it\\u0026#39;s been a long journey. It all started as a small widget with no account support, to a full featured and highly customisable application. It\\u0026#39;s been an extremely gratifying experience over the years seeing it grow and all the hours I have put in have definitely paid off. The app was partially crafted by suggestions from the community but still I\\u0026#39;d love to get more people involved in the project in a development capacity, which is why I\\u0026#39;m posting here.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf anyone would like to get involved in development, join me on Github: \\u003Ca href=\\\"https://github.com/micwallace/reddinator/\\\"\\u003Ehttps://github.com/micwallace/reddinator/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf anyone wants to suggest a feature, start a discussion at (warning: ghost-town): \\u003Ca href=\\\"https://reddit.com/r/reddinator\\\"\\u003Ehttps://reddit.com/r/reddinator\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOr just give Reddinator a go: \\u003Ca href=\\\"https://play.google.com/store/apps/details?id=au.com.wallaceit.reddinator\\\"\\u003Ehttps://play.google.com/store/apps/details?id=au.com.wallaceit.reddinator\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"About 4 years ago I decided to delve into Android development. I had dabbled with Java before but for anyone who has done the same, they would understand that Android is an entirely different monster (although I have grown to like the platform, except themes.xml....yuk).\\n\\nI decided to write a Reddit widget, partially because none of the current ones could satisfy my requirements, but also because I wanted one that I could tinker with and share with the Reddit community.\\n\\nLooking back it's been a long journey. It all started as a small widget with no account support, to a full featured and highly customisable application. It's been an extremely gratifying experience over the years seeing it grow and all the hours I have put in have definitely paid off. The app was partially crafted by suggestions from the community but still I'd love to get more people involved in the project in a development capacity, which is why I'm posting here.\\n\\nIf anyone would like to get involved in development, join me on Github: https://github.com/micwallace/reddinator/\\n\\nIf anyone wants to suggest a feature, start a discussion at (warning: ghost-town): https://reddit.com/r/reddinator\\n\\nOr just give Reddinator a go: https://play.google.com/store/apps/details?id=au.com.wallaceit.reddinator\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"46enwn\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"micwallace\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/46enwn/reddinator_my_small_android_widget_for_reddit/\", \"locked\": false, \"name\": \"t3_46enwn\", \"created\": 1455829964.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/46enwn/reddinator_my_small_android_widget_for_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddinator: My small Android widget for Reddit, turned into a full featured app\", \"created_utc\": 1455801164.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWould it be possible to scrape all of reddit posts and comments, if so how long might it take from a single VPS and what is the estimated size of that data? And has anyone attempted something like this before?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Would it be possible to scrape all of reddit posts and comments, if so how long might it take from a single VPS and what is the estimated size of that data? And has anyone attempted something like this before?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3qe57l\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"avodaboi\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3qe57l/how_much_data_is_on_reddit/\", \"locked\": false, \"name\": \"t3_3qe57l\", \"created\": 1445960203.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3qe57l/how_much_data_is_on_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How much data is on reddit?\", \"created_utc\": 1445931403.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIs it the EC2 instance? The relational database?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Is it the EC2 instance? The relational database?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3k974p\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Swatieson\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3k974p/what_crashes_first_when_reddit_crashes/\", \"locked\": false, \"name\": \"t3_3k974p\", \"created\": 1441841927.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3k974p/what_crashes_first_when_reddit_crashes/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"What crashes first when reddit crashes?\", \"created_utc\": 1441813127.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAPI Endpoint:  \\u003Ca href=\\\"https://api.pushshift.io/reddit/topsubs?lookback=3600\\\"\\u003Ehttps://api.pushshift.io/reddit/topsubs?lookback=3600\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks to \\u003Ca href=\\\"/u/orionmelt\\\"\\u003E/u/orionmelt\\u003C/a\\u003E for the suggestion.  This is a very basic API call that will show every subreddit with activity over the past X seconds (where 0 \\u0026gt; X \\u0026gt; 7200).  Eventually I will have it go much further back (2007), but I need to rollup totals into hour, daily and yearly indexes.  \\u003C/p\\u003E\\n\\n\\u003Ch2\\u003E\\u003Cstrong\\u003EParameters:\\u003C/strong\\u003E\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Elookback\\u003C/strong\\u003E: The number of seconds to look back from the present.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Elimit\\u003C/strong\\u003E: The number of subreddits to return.  If you don\\u0026#39;t need all of them (could return thousands), please set a reasonable limit.  If you need all of them, great. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs always, I welcome your suggestions.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ll be releasing all Reddit submissions soon along with July comment data.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"API Endpoint:  https://api.pushshift.io/reddit/topsubs?lookback=3600\\n\\nThanks to /u/orionmelt for the suggestion.  This is a very basic API call that will show every subreddit with activity over the past X seconds (where 0 \\u003E X \\u003E 7200).  Eventually I will have it go much further back (2007), but I need to rollup totals into hour, daily and yearly indexes.  \\n\\n**Parameters:**\\n----------------------------------\\n**lookback**: The number of seconds to look back from the present.\\n\\n**limit**: The number of subreddits to return.  If you don't need all of them (could return thousands), please set a reasonable limit.  If you need all of them, great. \\n\\nAs always, I welcome your suggestions.  \\n\\nI'll be releasing all Reddit submissions soon along with July comment data.\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3g7295\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1438996018.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3g7295/new_api_endpoint_see_top_subreddits_where_at/\", \"locked\": false, \"name\": \"t3_3g7295\", \"created\": 1439024638.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3g7295/new_api_endpoint_see_top_subreddits_where_at/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New API Endpoint -- See top subreddits where at least one comment was posted over the past X seconds\", \"created_utc\": 1438995838.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe new API endpoint looks like this:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/topthreads?lookback=300\\u0026amp;limit=25\\\"\\u003Ehttps://api.pushshift.io/reddit/topthreads?lookback=300\\u0026amp;limit=25\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EMaximum value for lookback is 7200 (2 hours)\\u003C/strong\\u003E  \\u003Cem\\u003EIf you use a value larger than 7200, it will use 7200 for that parameter.\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis will show the top threads based on comment activity.  The lookback parameter is the number of seconds to look back for new comments.  The limit clause limits the number of threads returned.  For example, the API call referenced above will look back 5 minutes and count the number of comments made to all threads and return the top 25 threads based on comment activity.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe data returned is held in the data key, which is an array of hashes.  The output looks like this:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Edata: [\\n{\\n\\nsubreddit: \\u0026quot;AskReddit\\u0026quot;,\\n\\nurl: \\u0026quot;http://redd.it/3folst\\u0026quot;,\\n\\nlink_title: \\u0026quot;What\\u0026#39;s your \\u0026quot;I was the only one to get away\\u0026quot; story?\\u0026quot;,\\n\\ncount: 57,\\n\\nlink_id: \\u0026quot;3folst\\u0026quot;\\n\\n}, ...\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThe count is the number of new comments made within the lookback timeframe.  \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The new API endpoint looks like this:\\n\\nhttps://api.pushshift.io/reddit/topthreads?lookback=300\\u0026limit=25\\n\\n**Maximum value for lookback is 7200 (2 hours)**  *If you use a value larger than 7200, it will use 7200 for that parameter.*\\n\\nThis will show the top threads based on comment activity.  The lookback parameter is the number of seconds to look back for new comments.  The limit clause limits the number of threads returned.  For example, the API call referenced above will look back 5 minutes and count the number of comments made to all threads and return the top 25 threads based on comment activity.\\n\\nThe data returned is held in the data key, which is an array of hashes.  The output looks like this:\\n\\n    data: [\\n    {\\n\\n    subreddit: \\\"AskReddit\\\",\\n\\n    url: \\\"http://redd.it/3folst\\\",\\n\\n    link_title: \\\"What's your \\\"I was the only one to get away\\\" story?\\\",\\n\\n    count: 57,\\n\\n    link_id: \\\"3folst\\\"\\n\\n    }, ...\\n\\nThe count is the number of new comments made within the lookback timeframe.  \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3fqs0t\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 16, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1438776884.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3fqs0t/ive_created_a_new_api_endpoint_for_reddit/\", \"locked\": false, \"name\": \"t3_3fqs0t\", \"created\": 1438723022.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3fqs0t/ive_created_a_new_api_endpoint_for_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I've created a new API endpoint for Reddit developers -- find out which threads are most active.\", \"created_utc\": 1438694222.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ds8vy\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"peoplma\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ds8vy/i_made_a_script_to_archive_a_subreddits_json_data/\", \"locked\": false, \"name\": \"t3_3ds8vy\", \"created\": 1437290986.0, \"url\": \"https://github.com/peoplma/subredditarchive\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I made a script to archive a subreddit's .json data into a .txt file. Bypassed the 1000 entry limit by searching through a given timestamp timeframe. Writes data from time stamped search result to .txt file and can customize search interval or timeframe.\", \"created_utc\": 1437262186.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have a bot I wrote that I want to host on something besides my old laptop. I read that Linux VPS\\u0026#39;s can get the job done quite well. What are some good and cheap options? My program takes less than 30 mb at the moment, but I plan on expanding and maybe even hosting multiple things. Is 512 mb enough for most people?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have a bot I wrote that I want to host on something besides my old laptop. I read that Linux VPS's can get the job done quite well. What are some good and cheap options? My program takes less than 30 mb at the moment, but I plan on expanding and maybe even hosting multiple things. Is 512 mb enough for most people?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"37spq6\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"BKLCL\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 14, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/37spq6/what_linux_vpss_are_best_for_hosting_a_simple/\", \"locked\": false, \"name\": \"t3_37spq6\", \"created\": 1432980534.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/37spq6/what_linux_vpss_are_best_for_hosting_a_simple/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"What Linux VPS's are best for hosting a simple python-based reddit bot?\", \"created_utc\": 1432951734.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"benpryke.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2xduha\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Ninjakannon\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2xduha/i_used_the_reddit_api_to_make_subreddit_mashup/\", \"locked\": false, \"name\": \"t3_2xduha\", \"created\": 1425094546.0, \"url\": \"http://www.benpryke.com/projects/subreddit-mashup/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I used the Reddit API to make Subreddit Mashup, which turns out to be an entertaining way to find new subreddits\", \"created_utc\": 1425065746.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello devs,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI recently changed the \\u003Ca href=\\\"/prefs/apps\\\"\\u003Eapp management UI\\u003C/a\\u003E to NOT show secrets for \\u0026quot;installed\\u0026quot; apps. Installed apps, by definition, cannot keep secrets. For such apps, when using endpoints that require using HTTP basic auth to \\u0026quot;verify\\u0026quot; that the request came from the app owner, you should send an empty string.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDetails, as always, are on the docs: \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/OAuth2\\\"\\u003Ehttps://github.com/reddit/reddit/wiki/OAuth2\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you have questions or suggestions, as always, feel free to drop them here.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello devs,\\n\\nI recently changed the [app management UI](/prefs/apps) to NOT show secrets for \\\"installed\\\" apps. Installed apps, by definition, cannot keep secrets. For such apps, when using endpoints that require using HTTP basic auth to \\\"verify\\\" that the request came from the app owner, you should send an empty string.\\n\\nDetails, as always, are on the docs: https://github.com/reddit/reddit/wiki/OAuth2\\n\\nIf you have questions or suggestions, as always, feel free to drop them here.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2kmy03\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2kmy03/oauth2_no_secrets_for_installed_apps/\", \"locked\": false, \"name\": \"t3_2kmy03\", \"created\": 1414578593.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2kmy03/oauth2_no_secrets_for_installed_apps/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[OAuth2] No secrets for installed apps\", \"created_utc\": 1414549793.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESpecifically interested in anyone who periodically queries \\u003Ca href=\\\"http://reddit.com/r/all/comments.json\\\"\\u003Ehttp://reddit.com/r/all/comments.json\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnd does your host have anything that makes scheduling the scrapes easy? How often do you scrape?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Specifically interested in anyone who periodically queries http://reddit.com/r/all/comments.json\\n\\nAnd does your host have anything that makes scheduling the scrapes easy? How often do you scrape?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"27nr9u\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"grimtrigger\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 21, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1402320505.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/27nr9u/bot_devs_where_do_you_host_your_bots/\", \"locked\": false, \"name\": \"t3_27nr9u\", \"created\": 1402304120.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/27nr9u/bot_devs_where_do_you_host_your_bots/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Bot devs: where do you host your bots?\", \"created_utc\": 1402275320.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ENobody can post on Reddit without filling out a CAPTCHA for a certain number of posts (or until they have a certain amount of Karma? I\\u0026#39;m not sure what the criteria are).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo how do bots get past this? Do the people who make the bots post manually on its behalf for a a while, then let the bot take over once the CAPTCHA period is past?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Nobody can post on Reddit without filling out a CAPTCHA for a certain number of posts (or until they have a certain amount of Karma? I'm not sure what the criteria are).\\n\\nSo how do bots get past this? Do the people who make the bots post manually on its behalf for a a while, then let the bot take over once the CAPTCHA period is past?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1w6117\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1w6117/how_do_people_making_bots_get_past_the_captcha/\", \"locked\": false, \"name\": \"t3_1w6117\", \"created\": 1390735570.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1w6117/how_do_people_making_bots_get_past_the_captcha/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How do people making bots get past the CAPTCHA period?\", \"created_utc\": 1390706770.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI was wondering, reddit code is perfect, especially for forums, since you can answer to every comment without having to quote them. Why don\\u0026#39;t we see more of it?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I was wondering, reddit code is perfect, especially for forums, since you can answer to every comment without having to quote them. Why don't we see more of it?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1uyxre\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"how_u_doing\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 22, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1uyxre/why_dont_we_see_more_websites_using_reddit_code/\", \"locked\": false, \"name\": \"t3_1uyxre\", \"created\": 1389491195.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1uyxre/why_dont_we_see_more_websites_using_reddit_code/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why don't we see more websites using reddit code?\", \"created_utc\": 1389462395.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi, all.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETonight I have added Reddit login using the OAuth to \\u003Ca href=\\\"http://www.duckdns.org\\\"\\u003Ehttp://www.duckdns.org\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt was a rough ride, the Reddit API as usual is full of data, but no actual hints.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWithout the following 2 posts I would have never made it:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EThis one is fantastic - step by step what happens.\\u003C/strong\\u003E\\n\\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/197x36/using_oauth_to_send_valid_requests/\\\"\\u003Ehttp://www.reddit.com/r/redditdev/comments/197x36/using_oauth_to_send_valid_requests/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis one got me out of a 2 hour head scratcher - basically you get a 500 error from the /api/v1/me until you add a user-agent - I tried 1000 things that didn\\u0026#39;t work before I found that.\\n\\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/10njpx/help_with_oauth2_access_token/\\\"\\u003Ehttp://www.reddit.com/r/redditdev/comments/10njpx/help_with_oauth2_access_token/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnyway I have made a stand alone static Class that depends on the libs:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EGoogle      json-simple-1.1.1.jar\\nApache     httpclient-4.1.jar\\nApache     httpcore-4.1.jar\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYou will need to adjust the 2 statics strings for your site:\\n\\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/OAuth2\\\"\\u003Ehttps://github.com/reddit/reddit/wiki/OAuth2\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThen generate a URL for you site (mine is \\u003Ca href=\\\"https://ssl.reddit.com/api/v1/authorize?scope=identity\\u0026amp;response_type=code\\u0026amp;client_id=itWtg9oX-HlZkw\\u0026amp;redirect_uri=https://www.duckdns.org/login\\u0026amp;state=75453453435354\\u0026amp;duration=temporary\\\"\\u003Ehttps://ssl.reddit.com/api/v1/authorize?scope=identity\\u0026amp;response_type=code\\u0026amp;client_id=itWtg9oX-HlZkw\\u0026amp;redirect_uri=https://www.duckdns.org/login\\u0026amp;state=75453453435354\\u0026amp;duration=temporary\\u003C/a\\u003E)  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhen you get handed back you get a code, simply pass the code value to the method, getAccessToken (along with your redirectURL)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThen pass the result from this (if it\\u0026#39;s not null) to the getUserName\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEnjoy.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Epackage org.duckdns.reddit;\\n\\nimport java.io.BufferedReader;\\nimport java.io.IOException;\\nimport java.io.InputStreamReader;\\nimport java.util.ArrayList;\\nimport java.util.List;\\nimport java.util.Map;\\nimport org.apache.http.HttpEntity;\\nimport org.apache.http.HttpResponse;\\nimport org.apache.http.NameValuePair;\\nimport org.apache.http.auth.AuthScope;\\nimport org.apache.http.auth.UsernamePasswordCredentials;\\nimport org.apache.http.client.ClientProtocolException;\\nimport org.apache.http.client.entity.UrlEncodedFormEntity;\\nimport org.apache.http.client.methods.HttpGet;\\nimport org.apache.http.client.methods.HttpPost;\\nimport org.apache.http.impl.client.DefaultHttpClient;\\nimport org.apache.http.message.BasicNameValuePair;\\nimport org.apache.http.util.EntityUtils;\\nimport org.json.simple.JSONValue;\\n\\npublic class RedditOAuth {\\n\\n    private static final String REDDIT_APP_ID = \\u0026quot;XXXXXXXXXXX\\u0026quot;;\\n    private static final String REDDIT_APP_SECRET = \\u0026quot;YYYYYYYYYYYYYY\\u0026quot;;\\n\\n    public static String getUserName(String access_token) throws ClientProtocolException, IOException {\\n        DefaultHttpClient httpclient = new DefaultHttpClient();\\n        try {\\n            HttpGet httpget = new HttpGet(\\u0026quot;https://oauth.reddit.com/api/v1/me\\u0026quot;);\\n            httpget.setHeader(\\u0026quot;Authorization\\u0026quot;, \\u0026quot;bearer \\u0026quot;+access_token+\\u0026quot;\\u0026quot;);\\n\\n            //System.out.println(\\u0026quot;executing request\\u0026quot; + httpget.getRequestLine());\\n            HttpResponse response = httpclient.execute(httpget);\\n            HttpEntity entity = response.getEntity();\\n\\n            //System.out.println(response.getStatusLine());\\n            if (entity != null) {\\n                // System.out.println(\\u0026quot;Response content length: \\u0026quot; + entity.getContentLength());\\n                BufferedReader br = new BufferedReader(new InputStreamReader(response.getEntity().getContent()));\\n                StringBuilder content = new StringBuilder();\\n                String line;\\n                while (null != (line = br.readLine())) {\\n                    content.append(line);\\n                }\\n                Map json = (Map) JSONValue.parse(content.toString());\\n                if (json.containsKey(\\u0026quot;name\\u0026quot;)) {\\n                    return (String) (json.get(\\u0026quot;name\\u0026quot;));\\n                }\\n            }\\n            EntityUtils.consume(entity);\\n        } finally {\\n            // When HttpClient instance is no longer needed,\\n            // shut down the connection manager to ensure\\n            // immediate deallocation of all system resources\\n            httpclient.getConnectionManager().shutdown();\\n        }\\n\\n        return null;\\n    }\\n\\n    public static String getAccessToken(String code, String redirectUrl) throws ClientProtocolException, IOException {\\n        DefaultHttpClient httpclient = new DefaultHttpClient();\\n        try {\\n            httpclient.getCredentialsProvider().setCredentials(\\n                    new AuthScope(\\u0026quot;ssl.reddit.com\\u0026quot;, 443),\\n                    new UsernamePasswordCredentials(REDDIT_APP_ID, REDDIT_APP_SECRET));\\n\\n            HttpPost httppost = new HttpPost(\\u0026quot;https://ssl.reddit.com/api/v1/access_token\\u0026quot;);\\n\\n            List \\u0026lt;NameValuePair\\u0026gt; nvps = new ArrayList \\u0026lt;NameValuePair\\u0026gt;(3);\\n            nvps.add(new BasicNameValuePair(\\u0026quot;code\\u0026quot;, code));\\n            nvps.add(new BasicNameValuePair(\\u0026quot;grant_type\\u0026quot;, \\u0026quot;authorization_code\\u0026quot;));\\n            nvps.add(new BasicNameValuePair(\\u0026quot;redirect_uri\\u0026quot;, \\u0026quot;https://www.duckdns.org/login\\u0026quot;));\\n\\n            httppost.setEntity(new UrlEncodedFormEntity(nvps));\\n            httppost.addHeader(\\u0026quot;User-Agent\\u0026quot;, \\u0026quot;a unique user agent\\u0026quot;);\\n            httppost.setHeader(\\u0026quot;Accept\\u0026quot;,\\u0026quot;any;\\u0026quot;);\\n\\n            // System.out.println(\\u0026quot;executing request \\u0026quot; + httppost.getRequestLine());\\n\\n            HttpResponse response = httpclient.execute(httppost);\\n            HttpEntity entity = response.getEntity();\\n\\n            //System.out.println(response.getStatusLine());\\n            if (entity != null) {\\n                 BufferedReader br = new BufferedReader(new InputStreamReader(response.getEntity().getContent()));\\n                 StringBuilder content = new StringBuilder();\\n                 String line;\\n                 while (null != (line = br.readLine())) {\\n                     content.append(line);\\n                 }\\n                 System.out.println(content.toString());\\n                 Map json = (Map) JSONValue.parse(content.toString());\\n                 if (json.containsKey(\\u0026quot;access_token\\u0026quot;)) {\\n                    return (String) (json.get(\\u0026quot;access_token\\u0026quot;));\\n                 }\\n            }\\n            EntityUtils.consume(entity);\\n        } finally {\\n            // When HttpClient instance is no longer needed,\\n            // shut down the connection manager to ensure\\n            // immediate deallocation of all system resources\\n            httpclient.getConnectionManager().shutdown();\\n        }\\n        return null;\\n    }\\n\\n    public static void main(String[] args) throws Exception {\\n        String accessToken = RedditOAuth.getAccessToken(\\u0026quot;KIOOYr9O88LdjPfIyF9_0m2feAU\\u0026quot;,\\u0026quot;https://www.duckdns.org/login\\u0026quot;);\\n        System.out.println(\\u0026quot;Access Token is : \\u0026quot; + accessToken);\\n        System.out.println(\\u0026quot;Name is : \\u0026quot; + RedditOAuth.getUserName(accessToken));\\n    }\\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi, all.\\n\\nTonight I have added Reddit login using the OAuth to http://www.duckdns.org.\\n\\n\\nIt was a rough ride, the Reddit API as usual is full of data, but no actual hints.\\n\\n\\nWithout the following 2 posts I would have never made it:\\n\\n\\n**This one is fantastic - step by step what happens.**\\nhttp://www.reddit.com/r/redditdev/comments/197x36/using_oauth_to_send_valid_requests/\\n\\n\\nThis one got me out of a 2 hour head scratcher - basically you get a 500 error from the /api/v1/me until you add a user-agent - I tried 1000 things that didn't work before I found that.\\nhttp://www.reddit.com/r/redditdev/comments/10njpx/help_with_oauth2_access_token/\\n\\n\\nAnyway I have made a stand alone static Class that depends on the libs:\\n\\n\\nGoogle      json-simple-1.1.1.jar\\nApache     httpclient-4.1.jar\\nApache     httpcore-4.1.jar\\n\\n\\nYou will need to adjust the 2 statics strings for your site:\\nhttps://github.com/reddit/reddit/wiki/OAuth2\\n\\n\\nThen generate a URL for you site (mine is https://ssl.reddit.com/api/v1/authorize?scope=identity\\u0026response_type=code\\u0026client_id=itWtg9oX-HlZkw\\u0026redirect_uri=https://www.duckdns.org/login\\u0026state=75453453435354\\u0026duration=temporary)  \\n\\n\\nWhen you get handed back you get a code, simply pass the code value to the method, getAccessToken (along with your redirectURL)\\n\\n\\nThen pass the result from this (if it's not null) to the getUserName\\n\\n\\nEnjoy.\\n\\n    package org.duckdns.reddit;\\n    \\n    import java.io.BufferedReader;\\n    import java.io.IOException;\\n    import java.io.InputStreamReader;\\n    import java.util.ArrayList;\\n    import java.util.List;\\n    import java.util.Map;\\n    import org.apache.http.HttpEntity;\\n    import org.apache.http.HttpResponse;\\n    import org.apache.http.NameValuePair;\\n    import org.apache.http.auth.AuthScope;\\n    import org.apache.http.auth.UsernamePasswordCredentials;\\n    import org.apache.http.client.ClientProtocolException;\\n    import org.apache.http.client.entity.UrlEncodedFormEntity;\\n    import org.apache.http.client.methods.HttpGet;\\n    import org.apache.http.client.methods.HttpPost;\\n    import org.apache.http.impl.client.DefaultHttpClient;\\n    import org.apache.http.message.BasicNameValuePair;\\n    import org.apache.http.util.EntityUtils;\\n    import org.json.simple.JSONValue;\\n    \\n    public class RedditOAuth {\\n    \\t\\n    \\tprivate static final String REDDIT_APP_ID = \\\"XXXXXXXXXXX\\\";\\n    \\tprivate static final String REDDIT_APP_SECRET = \\\"YYYYYYYYYYYYYY\\\";\\n    \\t\\n    \\tpublic static String getUserName(String access_token) throws ClientProtocolException, IOException {\\n    \\t\\tDefaultHttpClient httpclient = new DefaultHttpClient();\\n            try {\\n                HttpGet httpget = new HttpGet(\\\"https://oauth.reddit.com/api/v1/me\\\");\\n                httpget.setHeader(\\\"Authorization\\\", \\\"bearer \\\"+access_token+\\\"\\\");\\n                \\n                //System.out.println(\\\"executing request\\\" + httpget.getRequestLine());\\n                HttpResponse response = httpclient.execute(httpget);\\n                HttpEntity entity = response.getEntity();\\n    \\n                //System.out.println(response.getStatusLine());\\n                if (entity != null) {\\n                \\t// System.out.println(\\\"Response content length: \\\" + entity.getContentLength());\\n                    BufferedReader br = new BufferedReader(new InputStreamReader(response.getEntity().getContent()));\\n                    StringBuilder content = new StringBuilder();\\n                    String line;\\n                    while (null != (line = br.readLine())) {\\n                        content.append(line);\\n                    }\\n                    Map json = (Map) JSONValue.parse(content.toString());\\n                    if (json.containsKey(\\\"name\\\")) {\\n                    \\treturn (String) (json.get(\\\"name\\\"));\\n                    }\\n                }\\n                EntityUtils.consume(entity);\\n            } finally {\\n                // When HttpClient instance is no longer needed,\\n                // shut down the connection manager to ensure\\n                // immediate deallocation of all system resources\\n                httpclient.getConnectionManager().shutdown();\\n            }\\n            \\n            return null;\\n    \\t}\\n    \\t\\n    \\tpublic static String getAccessToken(String code, String redirectUrl) throws ClientProtocolException, IOException {\\n    \\t\\tDefaultHttpClient httpclient = new DefaultHttpClient();\\n            try {\\n                httpclient.getCredentialsProvider().setCredentials(\\n                        new AuthScope(\\\"ssl.reddit.com\\\", 443),\\n                        new UsernamePasswordCredentials(REDDIT_APP_ID, REDDIT_APP_SECRET));\\n    \\n                HttpPost httppost = new HttpPost(\\\"https://ssl.reddit.com/api/v1/access_token\\\");\\n                \\n                List \\u003CNameValuePair\\u003E nvps = new ArrayList \\u003CNameValuePair\\u003E(3);\\n                nvps.add(new BasicNameValuePair(\\\"code\\\", code));\\n                nvps.add(new BasicNameValuePair(\\\"grant_type\\\", \\\"authorization_code\\\"));\\n                nvps.add(new BasicNameValuePair(\\\"redirect_uri\\\", \\\"https://www.duckdns.org/login\\\"));\\n    \\n                httppost.setEntity(new UrlEncodedFormEntity(nvps));\\n                httppost.addHeader(\\\"User-Agent\\\", \\\"a unique user agent\\\");\\n                httppost.setHeader(\\\"Accept\\\",\\\"any;\\\");\\n           \\n                // System.out.println(\\\"executing request \\\" + httppost.getRequestLine());\\n                \\n                HttpResponse response = httpclient.execute(httppost);\\n                HttpEntity entity = response.getEntity();\\n                \\n                //System.out.println(response.getStatusLine());\\n                if (entity != null) {\\n                \\t BufferedReader br = new BufferedReader(new InputStreamReader(response.getEntity().getContent()));\\n                     StringBuilder content = new StringBuilder();\\n                     String line;\\n                     while (null != (line = br.readLine())) {\\n                         content.append(line);\\n                     }\\n                     System.out.println(content.toString());\\n                     Map json = (Map) JSONValue.parse(content.toString());\\n                     if (json.containsKey(\\\"access_token\\\")) {\\n                     \\treturn (String) (json.get(\\\"access_token\\\"));\\n                     }\\n                }\\n                EntityUtils.consume(entity);\\n            } finally {\\n                // When HttpClient instance is no longer needed,\\n                // shut down the connection manager to ensure\\n                // immediate deallocation of all system resources\\n                httpclient.getConnectionManager().shutdown();\\n            }\\n            return null;\\n    \\t}\\n    \\t\\n    \\tpublic static void main(String[] args) throws Exception {\\n    \\t\\tString accessToken = RedditOAuth.getAccessToken(\\\"KIOOYr9O88LdjPfIyF9_0m2feAU\\\",\\\"https://www.duckdns.org/login\\\");\\n    \\t\\tSystem.out.println(\\\"Access Token is : \\\" + accessToken);\\n    \\t\\tSystem.out.println(\\\"Name is : \\\" + RedditOAuth.getUserName(accessToken));\\n    \\t}\\n    }\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ncl6v\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"stevethepirateuk\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1380421700.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ncl6v/working_reddit_oauth_example_for_java/\", \"locked\": false, \"name\": \"t3_1ncl6v\", \"created\": 1380449077.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1ncl6v/working_reddit_oauth_example_for_java/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Working Reddit OAuth example for Java\", \"created_utc\": 1380420277.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EMy latest bot ran once a minute, and when it malfunctioned I needed to know why. So i left a debug string in each comment reply. The debug string could not be seen by any normal viewer unless they actually clicked Source.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThere are methods to hide text in posts unless clicking \\u0026quot;Source\\u0026quot;. The methods are:\\u003C/p\\u003E\\n\\n\\u003Ch3\\u003E16 SUP\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EUsing 16 \\u003Cstrong\\u003E^\\u003C/strong\\u003E the text following disappears and won\\u0026#39;t even produce a new line if its on a new line. Viewing source now will show you what I mean.\\u003C/p\\u003E\\n\\n\\n\\u003Cp\\u003EAs you can see, even though I have 2 new lines in the source, you only visually see one.\\u003C/p\\u003E\\n\\n\\u003Ch3\\u003E16 Quotes\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003ESame method as above but using \\u003Cstrong\\u003E\\u0026gt;\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003E\\u003Csup\\u003E-\\u003C/sup\\u003E \\u003Csup\\u003E\\u003Ca href=\\\"/u/TimePath\\\"\\u003E/u/TimePath\\u003C/a\\u003E\\u003C/sup\\u003E\\u003C/p\\u003E\\n\\n\\u003Ch3\\u003ENew Line, Quote, Text\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EPutting a blank new line, then entering and putting |, then entering and putting text will make the text disappear. View source to see.\\u003C/p\\u003E\\n\\n\\u003Ctable\\u003E\\u003Cthead\\u003E\\n\\u003Ctr\\u003E\\n\\u003C/tr\\u003E\\n\\u003C/thead\\u003E\\u003Ctbody\\u003E\\n\\u003C/tbody\\u003E\\u003C/table\\u003E\\n\\n\\u003Cp\\u003EAfter another new line the text shows up again.\\u003C/p\\u003E\\n\\n\\u003Ch3\\u003EEmpty Link\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EUsing an empty link you can then stylesheet the text so  its visible on your end and no need to click view source\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"/debug\\\" title=\\\"Like this\\\"\\u003E\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E[](/debug \\u0026quot;This is some debug text\\u0026quot;)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003EWith a stylesheet like this:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ea[href$=\\u0026quot;/debug\\u0026quot;]:before {\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Econtent: attr(title);\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ecolor: red;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ecursor: pointer;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Etext-decoration: none;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E}\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003E\\u003Csup\\u003E-\\u003C/sup\\u003E \\u003Csup\\u003E\\u003Ca href=\\\"/u/AsterJ\\\"\\u003E/u/AsterJ\\u003C/a\\u003E\\u003C/sup\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"My latest bot ran once a minute, and when it malfunctioned I needed to know why. So i left a debug string in each comment reply. The debug string could not be seen by any normal viewer unless they actually clicked Source.\\n\\nThere are methods to hide text in posts unless clicking \\\"Source\\\". The methods are:\\n\\n###16 SUP\\nUsing 16 **^** the text following disappears and won't even produce a new line if its on a new line. Viewing source now will show you what I mean.\\n\\n^^^^^^^^^^^^^^^^Text-Here\\n\\nAs you can see, even though I have 2 new lines in the source, you only visually see one.\\n\\n###16 Quotes\\nSame method as above but using **\\u003E**\\n\\u003E\\u003E\\u003E\\u003E\\u003E\\u003E\\u003E\\u003E\\u003E\\u003E\\u003E\\u003E\\u003E\\u003E\\u003E\\u003EHello There\\n\\n^- ^/u/TimePath\\n\\n###New Line, Quote, Text\\nPutting a blank new line, then entering and putting |, then entering and putting text will make the text disappear. View source to see.\\n\\n|\\nThis Text can have spaces, unlike the other one.\\n\\nAfter another new line the text shows up again.\\n\\n###Empty Link\\nUsing an empty link you can then stylesheet the text so  its visible on your end and no need to click view source\\n\\n[](/debug \\\"Like this\\\")\\n\\n    [](/debug \\\"This is some debug text\\\")\\n\\u003E\\n\\u003EWith a stylesheet like this:\\n\\u003E\\n\\u003Ea[href$=\\\"/debug\\\"]:before {\\n\\n\\u003Econtent: attr(title);\\n\\n\\u003Ecolor: red;\\n\\n\\u003Ecursor: pointer;\\n\\n\\u003Etext-decoration: none;\\n\\n\\u003E}\\n\\n^- ^/u/AsterJ\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1gkkzo\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Ugleh\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1371597355.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1gkkzo/cool_tip_for_debugging_without_database/\", \"locked\": false, \"name\": \"t3_1gkkzo\", \"created\": 1371567248.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1gkkzo/cool_tip_for_debugging_without_database/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Cool Tip for debugging without database\", \"created_utc\": 1371538448.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAfter \\u003Ca href=\\\"http://www.reddit.com/r/ideasfortheadmins/comments/1blpok/an_option_to_restrict_posting_based_on_subreddit/\\\"\\u003Ethis discussion\\u003C/a\\u003E in \\u003Ca href=\\\"/r/ideasfortheadmins\\\"\\u003E/r/ideasfortheadmins\\u003C/a\\u003E, I\\u0026#39;ve decided to start work on adding an API call that will return a user\\u0026#39;s karma for a particular subreddit.  This would be the same value that appears under \\u0026#39;show karma breakdown by subreddit\\u0026#39; for the currently-logged-in user, but only for a specific subreddit so as not to be too expensive.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy questions at this point are:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003Ewhether to base it off of \\u003Ca href=\\\"http://www.reddit.com/dev/api#GET_user_%7Busername%7D_about.json\\\"\\u003E/user/username/about.json\\u003C/a\\u003E, \\u003Ca href=\\\"http://www.reddit.com/dev/api#GET_r_%7Bsubreddit%7D_about.json\\\"\\u003E/r/subreddit/about.json\\u003C/a\\u003E or somewhere else\\u003C/li\\u003E\\n\\u003Cli\\u003Ewhether there is some technical reason I\\u0026#39;m not yet aware of that makes this infeasible\\u003C/li\\u003E\\n\\u003Cli\\u003Ewhether there is some political reason that makes it unlikely a pull request with this feature would be accepted.\\u003C/li\\u003E\\n\\u003Cli\\u003Eif there\\u0026#39;s any other resources I should know about for help with what I\\u0026#39;m planning,\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"After [this discussion](http://www.reddit.com/r/ideasfortheadmins/comments/1blpok/an_option_to_restrict_posting_based_on_subreddit/) in /r/ideasfortheadmins, I've decided to start work on adding an API call that will return a user's karma for a particular subreddit.  This would be the same value that appears under 'show karma breakdown by subreddit' for the currently-logged-in user, but only for a specific subreddit so as not to be too expensive.\\n\\nMy questions at this point are:\\n\\n* whether to base it off of [/user/username/about.json](http://www.reddit.com/dev/api#GET_user_{username}_about.json), [/r/subreddit/about.json](http://www.reddit.com/dev/api#GET_r_{subreddit}_about.json) or somewhere else\\n* whether there is some technical reason I'm not yet aware of that makes this infeasible\\n* whether there is some political reason that makes it unlikely a pull request with this feature would be accepted.\\n* if there's any other resources I should know about for help with what I'm planning,\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1bney1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"WastedTruth\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 18, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1bney1/i_want_to_add_an_api_call_to_read_a_users/\", \"locked\": false, \"name\": \"t3_1bney1\", \"created\": 1365093513.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1bney1/i_want_to_add_an_api_call_to_read_a_users/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I want to add an API call to read a user's subreddit karma\", \"created_utc\": 1365064713.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have an idea for a reddit iPhone app.  What are the rules for using the reddit name and logo?  Part of the app will involve the user logging into their reddit account.\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003ECan I use reddit in the name of the app?\\u003C/li\\u003E\\n\\u003Cli\\u003ECan I make references to reddit in the app (e.g., \\u0026quot;Please enter your reddit username and password\\u0026quot;\\u003C/li\\u003E\\n\\u003Cli\\u003ECan I use the reddit logo?\\u003C/li\\u003E\\n\\u003Cli\\u003EDoes it matter if the app is free or paid?\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve seen the licensing page (\\u003Ca href=\\\"http://www.reddit.com/help/licensing\\\"\\u003Ehttp://www.reddit.com/help/licensing\\u003C/a\\u003E), but it seems to be related specifically to selling products.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have an idea for a reddit iPhone app.  What are the rules for using the reddit name and logo?  Part of the app will involve the user logging into their reddit account.\\n\\n* Can I use reddit in the name of the app?\\n* Can I make references to reddit in the app (e.g., \\\"Please enter your reddit username and password\\\"\\n* Can I use the reddit logo?\\n* Does it matter if the app is free or paid?\\n\\nI've seen the licensing page (http://www.reddit.com/help/licensing), but it seems to be related specifically to selling products.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"tcypp\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"secretcode\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/tcypp/can_i_use_the_reddit_name_and_logo_in_an_iphone/\", \"locked\": false, \"name\": \"t3_tcypp\", \"created\": 1336516324.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/tcypp/can_i_use_the_reddit_name_and_logo_in_an_iphone/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can I use the reddit name and logo in an iPhone app?\", \"created_utc\": 1336487524.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"frasm\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ketralnis\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/frasm/february_2011_merge/\", \"locked\": false, \"name\": \"t3_frasm\", \"created\": 1298523444.0, \"url\": \"https://github.com/reddit/reddit/commit/7fff900bbeba362b607821159f6419d7762c9957\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"February 2011 Merge\", \"created_utc\": 1298494644.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESay I go to reddit.com/search and use the query \\u003Ca href=\\\"https://www.reddit.com/search?q=interesting+subreddit%3Aaskreddit\\u0026amp;restrict_sr=\\u0026amp;sort=relevance2\\u0026amp;t=all\\\"\\u003E\\u0026quot;interesting subreddit:askreddit\\u0026quot;\\u003C/a\\u003E. For whatever reason the results of this search, though similar, are somewhat different than the results when going to \\u003Ca href=\\\"/r/AskReddit\\\"\\u003E/r/AskReddit\\u003C/a\\u003E and \\u003Ca href=\\\"https://www.reddit.com/r/AskReddit/search?q=interesting\\u0026amp;sort=relevance2\\u0026amp;restrict_sr=on\\u0026amp;t=all\\\"\\u003Esearching for \\u0026quot;interesting\\u0026quot;\\u003C/a\\u003E. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhy is this? Conceptually at least, shouldn\\u0026#39;t they be identical? \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Say I go to reddit.com/search and use the query [\\\"interesting subreddit:askreddit\\\"](https://www.reddit.com/search?q=interesting+subreddit%3Aaskreddit\\u0026restrict_sr=\\u0026sort=relevance2\\u0026t=all). For whatever reason the results of this search, though similar, are somewhat different than the results when going to /r/AskReddit and [searching for \\\"interesting\\\"](https://www.reddit.com/r/AskReddit/search?q=interesting\\u0026sort=relevance2\\u0026restrict_sr=on\\u0026t=all). \\n\\nWhy is this? Conceptually at least, shouldn't they be identical? \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4ifj4i\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"iamthatis\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4ifj4i/what_is_the_difference_between_searching/\", \"locked\": false, \"name\": \"t3_4ifj4i\", \"created\": 1462758619.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4ifj4i/what_is_the_difference_between_searching/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"What is the difference between \\\"searching AskReddit\\\" and \\\"searching restricted to AskReddit\\\"?\", \"created_utc\": 1462729819.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWe\\u0026#39;ve just made a couple changes to the OAuth authorization flow.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe \\u003Ca href=\\\"https://github.com/reddit/reddit/commit/a3929831c7004107a6c976f3de425302cf7b2b40\\\"\\u003Efirst one\\u003C/a\\u003E will give some more helpful error messages when something goes wrong in the authorize request. For instance, previously if you gave a bad redirect URI you would have been told you had an invalid client. We\\u0026#39;ll now say that the bad redirect URI was in fact the problem.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe \\u003Ca href=\\\"https://github.com/reddit/reddit/commit/29cc2149f477861b05973faf043ede3107ba90b3\\\"\\u003Esecond one\\u003C/a\\u003E will make it so a page will be displayed on reddit.com with any errors instead of redirecting to the OAuth client\\u0026#39;s \\u003Ccode\\u003Eredirect_uri\\u003C/code\\u003E with the error in the URL. This was an open redirect vulnerability (thanks \\u003Ca href=\\\"/u/avlidienbrunn\\\"\\u003E/u/avlidienbrunn\\u003C/a\\u003E for reporting!), and brings us more in line with how most other OAuth providers handle these sort of errors.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease let me know if you see anything weird because of these changes. It should generally only affect clients that were already sending bad requests, and should more target those using the web based authorization flow.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"We've just made a couple changes to the OAuth authorization flow.\\n\\nThe [first one](https://github.com/reddit/reddit/commit/a3929831c7004107a6c976f3de425302cf7b2b40) will give some more helpful error messages when something goes wrong in the authorize request. For instance, previously if you gave a bad redirect URI you would have been told you had an invalid client. We'll now say that the bad redirect URI was in fact the problem.\\n\\nThe [second one](https://github.com/reddit/reddit/commit/29cc2149f477861b05973faf043ede3107ba90b3) will make it so a page will be displayed on reddit.com with any errors instead of redirecting to the OAuth client's `redirect_uri` with the error in the URL. This was an open redirect vulnerability (thanks /u/avlidienbrunn for reporting!), and brings us more in line with how most other OAuth providers handle these sort of errors.\\n\\nPlease let me know if you see anything weird because of these changes. It should generally only affect clients that were already sending bad requests, and should more target those using the web based authorization flow.\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"49vpod\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"gooeyblob\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 17, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/49vpod/oauth2_authorize_page_changes/\", \"locked\": false, \"name\": \"t3_49vpod\", \"created\": 1457675760.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/49vpod/oauth2_authorize_page_changes/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OAuth2 authorize page changes\", \"created_utc\": 1457646960.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EJust a small update, we\\u0026#39;ve added two states for the current logged in user to their user resource: \\u003Ccode\\u003Ein_beta\\u003C/code\\u003E and \\u003Ccode\\u003Eis_employee\\u003C/code\\u003E. You can see these in the \\u003Ccode\\u003Eme.json\\u003C/code\\u003E response for example.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThey\\u0026#39;re additive and probably shouldn\\u0026#39;t cause much trouble, but wanted to let you all know.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/commit/7945ab154bb9851abf1e7ac76329f46e606b076a\\\"\\u003ESee the code behind this change on GitHub.\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Just a small update, we've added two states for the current logged in user to their user resource: `in_beta` and `is_employee`. You can see these in the `me.json` response for example.\\n\\nThey're additive and probably shouldn't cause much trouble, but wanted to let you all know.\\n\\n[See the code behind this change on GitHub.](https://github.com/reddit/reddit/commit/7945ab154bb9851abf1e7ac76329f46e606b076a)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"48hojo\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"umbrae\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/48hojo/api_change_two_user_states_now_available_for/\", \"locked\": false, \"name\": \"t3_48hojo\", \"created\": 1456883742.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/48hojo/api_change_two_user_states_now_available_for/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API Change: Two user states now available for logged in user (in_beta, is_employee)\", \"created_utc\": 1456854942.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have my own install of reddit source, but no posts are showing unless a user is logged in.  How do you make all posts public or show up on the main reddit page?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have my own install of reddit source, but no posts are showing unless a user is logged in.  How do you make all posts public or show up on the main reddit page?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"40tt49\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"TheUltimateSalesman\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/40tt49/i_have_my_own_install_of_reddit_source_but_no/\", \"locked\": false, \"name\": \"t3_40tt49\", \"created\": 1452743817.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/40tt49/i_have_my_own_install_of_reddit_source_but_no/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I have my own install of reddit source, but no posts are showing unless a user is logged in.\", \"created_utc\": 1452715017.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI made a post just over a couple of weeks ago (\\u003Ca href=\\\"https://www.reddit.com/r/redditdev/comments/3ic0r6/we_applied_for_an_android_license_a_few_days_ago/\\\"\\u003Ehttps://www.reddit.com/r/redditdev/comments/3ic0r6/we_applied_for_an_android_license_a_few_days_ago/\\u003C/a\\u003E) about applying for a Reddit license but not receiving any response. Since then I managed to get in touch with an admin (who doesn\\u0026#39;t deal with this) who confirmed that it had been received. However, it\\u0026#39;s now coming up to three weeks and there still hasn\\u0026#39;t been any response. The new licensing approach Reddit has taken on is fine, as long as they actually give them out. Perhaps they have stopped giving them out because they are making their own apps and so no longer want third party devs to come out with Android apps? Before you think we are being impatient, 3 weeks is a very long time - particularly with no correspondence. Is the process for them giving an application very difficult?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I made a post just over a couple of weeks ago (https://www.reddit.com/r/redditdev/comments/3ic0r6/we_applied_for_an_android_license_a_few_days_ago/) about applying for a Reddit license but not receiving any response. Since then I managed to get in touch with an admin (who doesn't deal with this) who confirmed that it had been received. However, it's now coming up to three weeks and there still hasn't been any response. The new licensing approach Reddit has taken on is fine, as long as they actually give them out. Perhaps they have stopped giving them out because they are making their own apps and so no longer want third party devs to come out with Android apps? Before you think we are being impatient, 3 weeks is a very long time - particularly with no correspondence. Is the process for them giving an application very difficult?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3kidb7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"VylarLtd\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 22, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1441956830.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3kidb7/3_weeks_since_reddit_license_application_and_no/\", \"locked\": false, \"name\": \"t3_3kidb7\", \"created\": 1441980660.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3kidb7/3_weeks_since_reddit_license_application_and_no/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"~3 weeks since Reddit license application and no response. Have they stopped giving them out?\", \"created_utc\": 1441951860.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}], \"after\": \"t3_3kidb7\", \"before\": null}}"
        },
        "headers": {
          "CF-RAY": "2b59b99da17b20c6-LAX",
          "Connection": "keep-alive",
          "Content-Type": "application/json; charset=UTF-8",
          "Date": "Sun, 19 Jun 2016 20:28:02 GMT",
          "Server": "cloudflare-nginx",
          "Strict-Transport-Security": "max-age=15552000; includeSubDomains; preload",
          "Transfer-Encoding": "chunked",
          "Vary": "accept-encoding",
          "X-Moose": "majestic",
          "cache-control": "private, s-maxage=0, max-age=0, must-revalidate",
          "expires": "-1",
          "x-content-type-options": "nosniff",
          "x-frame-options": "SAMEORIGIN",
          "x-ratelimit-remaining": "585.0",
          "x-ratelimit-reset": "119",
          "x-ratelimit-used": "15",
          "x-reddit-tracking": "https://pixel.redditmedia.com/pixel/of_destiny.png?v=2MqufZAzDStZ3m%2BypGzjHQQRInUncGvFJKyox%2BkT%2FW%2FtmbteMOwtyRXDC5QsCk446XLfDZNTClshBlL7H8H0%2BZzkHQvE4pCB",
          "x-ua-compatible": "IE=edge",
          "x-xss-protection": "1; mode=block"
        },
        "status": {
          "code": 200,
          "message": "OK"
        },
        "url": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_16bh8j&raw_json=1"
      }
    },
    {
      "recorded_at": "2016-06-19T20:28:04",
      "request": {
        "body": {
          "encoding": "utf-8",
          "string": ""
        },
        "headers": {
          "Accept": "*/*",
          "Accept-Encoding": "identity",
          "Authorization": "bearer YE4ESXZ9lTjIP0ukcopwXM6361g",
          "Connection": "keep-alive",
          "Cookie": "loid=SdAZIdDQoUDzjNqfRk; loidcreated=2016-06-19T20%3A27%3A59.507Z; __cfduid=da3dbedd6bfa050f4f12fea14395b830f1466368079",
          "User-Agent": "prawtools/0.19 PRAW/4.0.0b4 prawcore/0.0.8"
        },
        "method": "GET",
        "uri": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_3kidb7&raw_json=1"
      },
      "response": {
        "body": {
          "encoding": "UTF-8",
          "string": "{\"kind\": \"Listing\", \"data\": {\"modhash\": null, \"children\": [{\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been working overtime to bring you the latest and greatest.  The new SSE stream can now deliver all publicly available submissions and comments straight to your doorstep with a plethora of useful options.  Let\\u0026#39;s dive into the complete feature set and sprinkle some examples as we go along.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFirst, the SSE stream is almost out of beta, so you can start depending on it by early next week.  I\\u0026#39;ve had a lot of testers helping out and I\\u0026#39;ve gotten a ton of great feedback -- so I appreciate everyone who is using it.  \\u003C/p\\u003E\\n\\n\\u003Ch2\\u003E\\u003Cstrong\\u003EThe endpoint:\\u003C/strong\\u003E  \\u003Ca href=\\\"http://stream.pushshift.io\\\"\\u003Ehttp://stream.pushshift.io\\u003C/a\\u003E\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EConnecting directly to that will feed all publicly available comments and submissions to you.  For those who are unfamiliar with what an SSE stream is, \\u003Ca href=\\\"http://www.html5rocks.com/en/tutorials/eventsource/basics/\\\"\\u003Eplease take a look at this great introduction\\u003C/a\\u003E.  Each \\u0026quot;event\\u0026quot; contains three fields: id, event and data.  The id field for my SSE stream is an internal value I use to keep track of all new submissions and comments as they come in.  The event field can be one of two values -- \\u0026quot;t1\\u0026quot; for comments or \\u0026quot;t3\\u0026quot; for submissions.  When connecting to the stream without using any parameters, you will receive all events (both t1 and t3 events).  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EBut I don\\u0026#39;t care about submissions (t3 events), I just want a comment stream\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EVery well!  I have a solution for you.  Use the \\u0026quot;event\\u0026quot; parameter when making your request to specify what type of event you want delivered.  In this case, you would make the following API call: \\u003Cstrong\\u003E\\u003Ca href=\\\"http://stream.pushshift.io?event=t1\\\"\\u003Ehttp://stream.pushshift.io?event=t1\\u003C/a\\u003E\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EThat\\u0026#39;s better, but I don\\u0026#39;t want \\u003Cem\\u003Eall\\u003C/em\\u003E reddit comments, I just want comments from askreddit\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFair enough, I\\u0026#39;ve got you covered.  To restrict your new comment stream to just askreddit comments, use the following call: \\u003Ca href=\\\"http://stream.pushshift.io?event=t1\\u0026amp;subreddit=askreddit\\\"\\u003Ehttp://stream.pushshift.io?event=t1\\u0026amp;subreddit=askreddit\\u003C/a\\u003E  Now you\\u0026#39;ll just get comments from that subreddit.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EThat\\u0026#39;s all well and good, but you should give people the ability to limit to more than just one subreddit\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYou\\u0026#39;ve got it!  If you want to get comments only from specific subreddits, you can add more by using a comma to separate them.  For instance, if you want to get comments for the subreddits science, askscience, pics and funny, you would make the following API call:  \\u003Ca href=\\\"http://stream.pushshift.io?event=t1\\u0026amp;subreddit=askscience,science,pics,funny\\\"\\u003Ehttp://stream.pushshift.io?event=t1\\u0026amp;subreddit=askscience,science,pics,funny\\u003C/a\\u003E (make sure you use subreddit as the parameter and not subreddits!)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EWow, so you\\u0026#39;ve got a lot of functionality here.  I know what will stump you -- what if I want to get all comments but EXCLUDE the previous subreddits?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDemanding, aren\\u0026#39;t you!  I like that -- no sweat, you would make a call very similar to the previous call but instead of subreddit=askscience,science,pics,funny -- you would make this call:  \\u003Ca href=\\\"http://stream.pushshift.io?event=t1\\u0026amp;subreddit!=askscience,science,pics,funny\\\"\\u003Ehttp://stream.pushshift.io?event=t1\\u0026amp;subreddit!=askscience,science,pics,funny\\u003C/a\\u003E (!= instead of =)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EWhat other parameters can I use?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eauthor\\u003C/strong\\u003E will allow you to limit to a particular author.  You can follow multiple authors as well by using the same format as the subreddit call -- author=john,tony,ryan  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDo you want to get rid of comments from automoderator?  Just use this:  \\u003Ca href=\\\"http://stream.pushshift.io?event=t1\\u0026amp;author!=automoderator\\\"\\u003Ehttp://stream.pushshift.io?event=t1\\u0026amp;author!=automoderator\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Ematch\\u003C/strong\\u003E is a particularly powerful parameter.  Let\\u0026#39;s say you want to only receive comments with the words reddit, voat and jupiter --- you would use this: \\u003Ca href=\\\"http://stream.pushshift.io?event=t3\\u0026amp;match=reddit,voat,jupiter\\\"\\u003Ehttp://stream.pushshift.io?event=t3\\u0026amp;match=reddit,voat,jupiter\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eprevious\\u003C/strong\\u003E is a great parameter if you want to get a lot of comments and/or submissions that were made before you connect to the stream.  For instance, if you want to get 100,000 items when you connect and then have it go into real-time mode, just set previous=100000.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Estart_id\\u003C/strong\\u003E allows you to start the stream at a specific location.  This is really handy in your application logic if you happen to disconnect from the stream and want to reconnect and pick up where you left off.  Also, this SSE stream is fully compliant with the last-event-id header that browsers automatically send in case you are developing a web app for your users.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003ECan I mix and match all of these?  Let\\u0026#39;s say I wanted to get all comments and submissions made to nfl, but also follow certain users -- is that possible?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAbsolutely!  You would make a call like this \\u003Ca href=\\\"http://stream.pushshift.io?subreddit=nfl\\u0026amp;author=tony,billy,alexis,pao\\u0026amp;match=cowboys,football,draft,pick\\u0026amp;mode=or\\\"\\u003Ehttp://stream.pushshift.io?subreddit=nfl\\u0026amp;author=tony,billy,alexis,pao\\u0026amp;match=cowboys,football,draft,pick\\u0026amp;mode=or\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThat call would deliver all submissions and comments made to the subreddit nfl, any comments or submissions authored by tony, billy, alexis or pao and include any comments with the words cowboys,football,draft or pick.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EWait -- what if I only want those keywords within the nfl subreddit?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDid you see that new parameter at the end?  The mode parameter will allow you to make boolean choices.  Set the mode to \\u0026quot;and\\u0026quot; instead of \\u0026quot;or\\u0026quot; to limit it only to the subreddit nfl and only comments within that subreddit with those keyworks.  To demonstrate, you would make this call:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://stream.pushshift.io?event=t1\\u0026amp;subreddit=nfl\\u0026amp;match=cowboys,football,draft,pick\\u0026amp;mode=and\\\"\\u003Ehttp://stream.pushshift.io?event=t1\\u0026amp;subreddit=nfl\\u0026amp;match=cowboys,football,draft,pick\\u0026amp;mode=and\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThat would only pull matching comments with those keywords exclusively from within the nfl subreddit.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EWhat if I want to get only safe for work submissions?  How would I do that?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThat brings us to the \\u0026quot;over_18\\u0026quot; parameter.  If you wanted to keep nsfw submissions out of your stream, you would use over_18=false (or over_18=0)  If, after a long day at work, you wanted to kick back and deliver only nsfw content, you would make the following call:  \\u003Ca href=\\\"http://stream.pushshift.io?event=t3\\u0026amp;over_18=true\\\"\\u003Ehttp://stream.pushshift.io?event=t3\\u0026amp;over_18=true\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003E\\u003Cstrong\\u003EQuestions, Comments, Criticisms, etc.\\u003C/strong\\u003E\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EAs always, if you have any questions or comments, please let me know.  I\\u0026#39;ve spend a lot of time developing this, so if you find it useful for your application, I would appreciate a shout-out or a small donation.  If you notice any bugs with this stream, please feel free to post them in this thread.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAll the best and happy developing!\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003E\\u003Cstrong\\u003ECheat Sheet\\u003C/strong\\u003E\\u003C/h2\\u003E\\n\\n\\u003Ch2\\u003E\\u003Cstrong\\u003EParameter List:\\u003C/strong\\u003E\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003E\\u003Cem\\u003EBold value is the default value\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Ctable\\u003E\\u003Cthead\\u003E\\n\\u003Ctr\\u003E\\n\\u003Cth align=\\\"left\\\"\\u003EParameter\\u003C/th\\u003E\\n\\u003Cth align=\\\"right\\\"\\u003EValues\\u003C/th\\u003E\\n\\u003Cth align=\\\"left\\\"\\u003EDescription\\u003C/th\\u003E\\n\\u003C/tr\\u003E\\n\\u003C/thead\\u003E\\u003Ctbody\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003Eprevious\\u003C/td\\u003E\\n\\u003Ctd align=\\\"right\\\"\\u003E0 \\u0026lt; X \\u0026lt; 10000\\u003C/td\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003EScan X comments and apply filters when connecting\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003Estart_id\\u003C/td\\u003E\\n\\u003Ctd align=\\\"right\\\"\\u003Einteger\\u003C/td\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003ESet start point of stream when connecting\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003Esubreddit\\u003C/td\\u003E\\n\\u003Ctd align=\\\"right\\\"\\u003Estring\\u003C/td\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003ERestrict by subreddit (comma delimited for multiple)\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003Eauthor\\u003C/td\\u003E\\n\\u003Ctd align=\\\"right\\\"\\u003Estring\\u003C/td\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003ERestrict by author (comma delimited for multiple)\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003Elink_id\\u003C/td\\u003E\\n\\u003Ctd align=\\\"right\\\"\\u003Estring\\u003C/td\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003ERestrict to specific link_ids\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003Ematch\\u003C/td\\u003E\\n\\u003Ctd align=\\\"right\\\"\\u003Estring\\u003C/td\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003EMatch against comment body, submission title and self_text\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003Eevent\\u003C/td\\u003E\\n\\u003Ctd align=\\\"right\\\"\\u003E\\u0026quot;t1\\u0026quot;,\\u0026quot;t3\\u0026quot;\\u003C/td\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003ERestrict stream to comments or submissions only\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003Eover_18\\u003C/td\\u003E\\n\\u003Ctd align=\\\"right\\\"\\u003Etrue,false (1 or 0)\\u003C/td\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003ERestrict stream to SFW or NSFW content only\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003Ctr\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003Emode\\u003C/td\\u003E\\n\\u003Ctd align=\\\"right\\\"\\u003E\\u0026quot;and\\u0026quot;,\\u003Cstrong\\u003E\\u0026quot;or\\u0026quot;\\u003C/strong\\u003E\\u003C/td\\u003E\\n\\u003Ctd align=\\\"left\\\"\\u003EBoolean control when using author,match and subreddit fields\\u003C/td\\u003E\\n\\u003C/tr\\u003E\\n\\u003C/tbody\\u003E\\u003C/table\\u003E\\n\\n\\u003Ch2\\u003E\\u003Cstrong\\u003EAdvanced Control\\u003C/strong\\u003E\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EWhen using the parameters subreddit, author and match, you can \\u0026quot;invert\\u0026quot; them by using != instead of =\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor instance, to get comments for every subreddit EXCEPT askreddit, use subreddit!=askreddit\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003E\\u003Cstrong\\u003EArchitecture Overview\\u003C/strong\\u003E\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EThe comment stream is currently running on a dual core VPS box with 2 GB of RAM.  The entire platform uses Redis exclusively.  The VPS box used to host the stream runs Redis in slave mode.  It is fed data from the Redis master node running on a Xeon server with 32 GB of RAM.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPresently, the previous 7,200 seconds (2 hours) of submissions and comments are cached at all times.  I may expand this to an entire day eventually (I\\u0026#39;d need to bump up the VPS box to at least 4 GB.)  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETotal monthly bandwidth limit is \\u003Cstrong\\u003E3 TB\\u003C/strong\\u003E  When connecting, the server will respect a request header for gzip content.  Please use that to conserve bandwidth if at all possible.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you are running an application that needs redundancy and a more dependable connection, please contact me and we can set something up.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EThis stream is considered to be in a BETA state until further notice.  I anticipate having testing completed by next week.  Please keep that in mind if you plan to use this for a production application.  However, I do appreciate developers testing the stream and using it in a production environment so long as you have a backup ingest source until this passes QA.\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have tested the ingest methodology and it retrieves more comments and submissions than PRAW and other ingest wrappers that I\\u0026#39;ve benchmarked.  Generally, submissions and comments will appear in the order they were made to Reddit.  Delay is generally ~ 1 second from time of posting to Reddit until the stream disseminates it.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EBot Creators and Maintainers\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you would like to use this stream for your bots, I have additional tools that may help you out tremendously.  Please PM me with the details of your project and I\\u0026#39;ll give you developers access (with an API key) that will allow you to search the entire reddit comment and submission database (up to 9 years of data).  I am working to get funding for a dual Xeon server with 256 GB of RAM to make this project possible.  Donations are welcome here:  \\u003Ca href=\\\"https://pushshift.io/donations/\\\"\\u003Ehttps://pushshift.io/donations/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m currently spending $300 a month to make all of these projects possible to the academic and developer community.  I consider it a labor of love for all those who helped me in the past.  The cost for the new server will run around $9,000 and I hope to have the funds by the end of this year.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003E\\u003Cstrong\\u003ECheck out my Reddit Comment Search API!\\u003C/strong\\u003E\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search?q=explosion\\\"\\u003Ehttps://api.pushshift.io/reddit/search?q=explosion\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003E\\u003Cstrong\\u003EReal-time Visual Ingest Metrics that feed this SSE Stream\\u003C/strong\\u003E\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://pushshift.io/enhancing-reddit-api-and-search/\\\"\\u003Ehttps://pushshift.io/enhancing-reddit-api-and-search/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003E\\u003Cstrong\\u003EExample of using this SSE stream in JSFiddle to show picture links posted to Reddit in real-time\\u003C/strong\\u003E\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003E\\u003Cem\\u003EPossible NSFW content may show up\\u003C/em\\u003E \\u003Ca href=\\\"http://jsfiddle.net/wyktd9r6/\\\"\\u003Ehttp://jsfiddle.net/wyktd9r6/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003E\\u003Cstrong\\u003EA Big Heart-felt Thank You to the Big Data Community!\\u003C/strong\\u003E\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EI would like to thank the Reddit admins for their help (Especially Keith in the past).  Also, whoever fixed the /api/info endpoint to not bomb out when requesting non-existent ID\\u0026#39;s -- I owe you a beer.  Please out yourself!  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI also want to thank the open data team at Amazon for assisting me with the other large open data initiatives.  A lot of universities are using the data that I\\u0026#39;ve made available (\\u003Ca href=\\\"http://files.pushshift.io\\\"\\u003Ehttp://files.pushshift.io\\u003C/a\\u003E) -- including the entire publicly available Reddit comment database. Much of what I\\u0026#39;ve been doing wouldn\\u0026#39;t have been possible without your assistance and speeding the process for grant approval.  You guys are the best!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been working overtime to bring you the latest and greatest.  The new SSE stream can now deliver all publicly available submissions and comments straight to your doorstep with a plethora of useful options.  Let's dive into the complete feature set and sprinkle some examples as we go along.\\n\\nFirst, the SSE stream is almost out of beta, so you can start depending on it by early next week.  I've had a lot of testers helping out and I've gotten a ton of great feedback -- so I appreciate everyone who is using it.  \\n\\n**The endpoint:**  http://stream.pushshift.io\\n-------------------------------------------------------------------\\n\\nConnecting directly to that will feed all publicly available comments and submissions to you.  For those who are unfamiliar with what an SSE stream is, [please take a look at this great introduction](http://www.html5rocks.com/en/tutorials/eventsource/basics/).  Each \\\"event\\\" contains three fields: id, event and data.  The id field for my SSE stream is an internal value I use to keep track of all new submissions and comments as they come in.  The event field can be one of two values -- \\\"t1\\\" for comments or \\\"t3\\\" for submissions.  When connecting to the stream without using any parameters, you will receive all events (both t1 and t3 events).  \\n\\n**But I don't care about submissions (t3 events), I just want a comment stream**\\n\\nVery well!  I have a solution for you.  Use the \\\"event\\\" parameter when making your request to specify what type of event you want delivered.  In this case, you would make the following API call: **http://stream.pushshift.io?event=t1**\\n\\n**That's better, but I don't want *all* reddit comments, I just want comments from askreddit**\\n\\nFair enough, I've got you covered.  To restrict your new comment stream to just askreddit comments, use the following call: http://stream.pushshift.io?event=t1\\u0026subreddit=askreddit  Now you'll just get comments from that subreddit.\\n\\n**That's all well and good, but you should give people the ability to limit to more than just one subreddit**\\n\\nYou've got it!  If you want to get comments only from specific subreddits, you can add more by using a comma to separate them.  For instance, if you want to get comments for the subreddits science, askscience, pics and funny, you would make the following API call:  http://stream.pushshift.io?event=t1\\u0026subreddit=askscience,science,pics,funny (make sure you use subreddit as the parameter and not subreddits!)\\n\\n**Wow, so you've got a lot of functionality here.  I know what will stump you -- what if I want to get all comments but EXCLUDE the previous subreddits?**\\n\\nDemanding, aren't you!  I like that -- no sweat, you would make a call very similar to the previous call but instead of subreddit=askscience,science,pics,funny -- you would make this call:  http://stream.pushshift.io?event=t1\\u0026subreddit!=askscience,science,pics,funny (!= instead of =)\\n\\n**What other parameters can I use?**\\n\\n**author** will allow you to limit to a particular author.  You can follow multiple authors as well by using the same format as the subreddit call -- author=john,tony,ryan  \\n\\nDo you want to get rid of comments from automoderator?  Just use this:  http://stream.pushshift.io?event=t1\\u0026author!=automoderator\\n\\n**match** is a particularly powerful parameter.  Let's say you want to only receive comments with the words reddit, voat and jupiter --- you would use this: http://stream.pushshift.io?event=t3\\u0026match=reddit,voat,jupiter\\n\\n**previous** is a great parameter if you want to get a lot of comments and/or submissions that were made before you connect to the stream.  For instance, if you want to get 100,000 items when you connect and then have it go into real-time mode, just set previous=100000.\\n\\n**start_id** allows you to start the stream at a specific location.  This is really handy in your application logic if you happen to disconnect from the stream and want to reconnect and pick up where you left off.  Also, this SSE stream is fully compliant with the last-event-id header that browsers automatically send in case you are developing a web app for your users.  \\n\\n**Can I mix and match all of these?  Let's say I wanted to get all comments and submissions made to nfl, but also follow certain users -- is that possible?**\\n\\nAbsolutely!  You would make a call like this http://stream.pushshift.io?subreddit=nfl\\u0026author=tony,billy,alexis,pao\\u0026match=cowboys,football,draft,pick\\u0026mode=or\\n\\nThat call would deliver all submissions and comments made to the subreddit nfl, any comments or submissions authored by tony, billy, alexis or pao and include any comments with the words cowboys,football,draft or pick.\\n\\n**Wait -- what if I only want those keywords within the nfl subreddit?**\\n\\nDid you see that new parameter at the end?  The mode parameter will allow you to make boolean choices.  Set the mode to \\\"and\\\" instead of \\\"or\\\" to limit it only to the subreddit nfl and only comments within that subreddit with those keyworks.  To demonstrate, you would make this call:\\n\\nhttp://stream.pushshift.io?event=t1\\u0026subreddit=nfl\\u0026match=cowboys,football,draft,pick\\u0026mode=and\\n\\nThat would only pull matching comments with those keywords exclusively from within the nfl subreddit.\\n\\n**What if I want to get only safe for work submissions?  How would I do that?**\\n\\nThat brings us to the \\\"over_18\\\" parameter.  If you wanted to keep nsfw submissions out of your stream, you would use over_18=false (or over_18=0)  If, after a long day at work, you wanted to kick back and deliver only nsfw content, you would make the following call:  http://stream.pushshift.io?event=t3\\u0026over_18=true\\n\\n**Questions, Comments, Criticisms, etc.**\\n---------------------------------------------------------------\\n\\nAs always, if you have any questions or comments, please let me know.  I've spend a lot of time developing this, so if you find it useful for your application, I would appreciate a shout-out or a small donation.  If you notice any bugs with this stream, please feel free to post them in this thread.  \\n\\nAll the best and happy developing!\\n\\n\\n**Cheat Sheet**\\n---------------------------------------------------------------\\n\\n**Parameter List:**\\n----------------------------------------\\n\\n*Bold value is the default value*\\n\\n| Parameter   | Values    | Description     |\\n|:-----------|------------:|:-------------|\\n| previous | 0 \\u003C X \\u003C 10000|     Scan X comments and apply filters when connecting  \\n| start_id |  integer  |   Set start point of stream when connecting\\n|  subreddit    |   string    |     Restrict by subreddit (comma delimited for multiple)\\n| author | string | Restrict by author (comma delimited for multiple)\\n| link_id | string | Restrict to specific link_ids\\n| match | string | Match against comment body, submission title and self_text\\n| event | \\\"t1\\\",\\\"t3\\\" | Restrict stream to comments or submissions only\\n| over_18 | true,false (1 or 0) | Restrict stream to SFW or NSFW content only\\n| mode | \\\"and\\\",**\\\"or\\\"** | Boolean control when using author,match and subreddit fields\\n\\n**Advanced Control**\\n-----------------------------------------\\n\\nWhen using the parameters subreddit, author and match, you can \\\"invert\\\" them by using != instead of =\\n\\nFor instance, to get comments for every subreddit EXCEPT askreddit, use subreddit!=askreddit\\n\\n**Architecture Overview**\\n------------------------------------------\\n\\nThe comment stream is currently running on a dual core VPS box with 2 GB of RAM.  The entire platform uses Redis exclusively.  The VPS box used to host the stream runs Redis in slave mode.  It is fed data from the Redis master node running on a Xeon server with 32 GB of RAM.  \\n\\nPresently, the previous 7,200 seconds (2 hours) of submissions and comments are cached at all times.  I may expand this to an entire day eventually (I'd need to bump up the VPS box to at least 4 GB.)  \\n\\nTotal monthly bandwidth limit is **3 TB**  When connecting, the server will respect a request header for gzip content.  Please use that to conserve bandwidth if at all possible.  \\n\\nIf you are running an application that needs redundancy and a more dependable connection, please contact me and we can set something up.  \\n\\n**This stream is considered to be in a BETA state until further notice.  I anticipate having testing completed by next week.  Please keep that in mind if you plan to use this for a production application.  However, I do appreciate developers testing the stream and using it in a production environment so long as you have a backup ingest source until this passes QA.**\\n\\nI have tested the ingest methodology and it retrieves more comments and submissions than PRAW and other ingest wrappers that I've benchmarked.  Generally, submissions and comments will appear in the order they were made to Reddit.  Delay is generally ~ 1 second from time of posting to Reddit until the stream disseminates it.  \\n\\n**Bot Creators and Maintainers**\\n\\nIf you would like to use this stream for your bots, I have additional tools that may help you out tremendously.  Please PM me with the details of your project and I'll give you developers access (with an API key) that will allow you to search the entire reddit comment and submission database (up to 9 years of data).  I am working to get funding for a dual Xeon server with 256 GB of RAM to make this project possible.  Donations are welcome here:  https://pushshift.io/donations/\\n\\nI'm currently spending $300 a month to make all of these projects possible to the academic and developer community.  I consider it a labor of love for all those who helped me in the past.  The cost for the new server will run around $9,000 and I hope to have the funds by the end of this year.\\n\\n**Check out my Reddit Comment Search API!**\\n------------------------------------------------------------------------\\n\\nhttps://api.pushshift.io/reddit/search?q=explosion\\n\\n**Real-time Visual Ingest Metrics that feed this SSE Stream**\\n----------------------------------------------------------------\\n\\nhttps://pushshift.io/enhancing-reddit-api-and-search/\\n\\n**Example of using this SSE stream in JSFiddle to show picture links posted to Reddit in real-time**\\n-------------------------------------------------------------------------------\\n\\n*Possible NSFW content may show up* http://jsfiddle.net/wyktd9r6/\\n\\n**A Big Heart-felt Thank You to the Big Data Community!**\\n--------------------------------\\nI would like to thank the Reddit admins for their help (Especially Keith in the past).  Also, whoever fixed the /api/info endpoint to not bomb out when requesting non-existent ID's -- I owe you a beer.  Please out yourself!  \\n\\nI also want to thank the open data team at Amazon for assisting me with the other large open data initiatives.  A lot of universities are using the data that I've made available (http://files.pushshift.io) -- including the entire publicly available Reddit comment database. Much of what I've been doing wouldn't have been possible without your assistance and speeding the process for grant approval.  You guys are the best!\\n\\n\\n\\n\\n\\n\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3gy4zd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1439755953.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/\", \"locked\": false, \"name\": \"t3_3gy4zd\", \"created\": 1439562948.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Supercharging the SSE stream -- now supports a ton of new options. Save your precious API calls for other things!\", \"created_utc\": 1439534148.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Csup\\u003ESorry for probably being the 50th person to post this but I couldn\\u0026#39;t find any useful answers.\\u003C/sup\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m asking because 1) there are a metric ton of comments per second and I have no idea how to keep up and 2) I haven\\u0026#39;t found an API function that gives me \\u003Ca href=\\\"/r/all/comments\\\"\\u003E/r/all/comments\\u003C/a\\u003E. I know PRAW can do it but I\\u0026#39;d prefer C# over Python because everything else I need is already written in it.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"^(Sorry for probably being the 50th person to post this but I couldn't find any useful answers.)\\n\\nI'm asking because 1) there are a metric ton of comments per second and I have no idea how to keep up and 2) I haven't found an API function that gives me /r/all/comments. I know PRAW can do it but I'd prefer C# over Python because everything else I need is already written in it.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ct9m8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"asdfusernameasdf\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ct9m8/how_do_reddit_bots_read_every_single_comment/\", \"locked\": false, \"name\": \"t3_3ct9m8\", \"created\": 1436574145.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ct9m8/how_do_reddit_bots_read_every_single_comment/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How do Reddit bots read every single comment?\", \"created_utc\": 1436545345.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m looking at creating a reddit clone for my school to communicate and use as a private reddit, but I\\u0026#39;m looking at a Linode server for hosting. Would 1Gb of ram be enough to handle 300-500 people per day? \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm looking at creating a reddit clone for my school to communicate and use as a private reddit, but I'm looking at a Linode server for hosting. Would 1Gb of ram be enough to handle 300-500 people per day? \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3c9rnr\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MrTeale\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3c9rnr/setting_up_a_reddit_clone/\", \"locked\": false, \"name\": \"t3_3c9rnr\", \"created\": 1436188766.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3c9rnr/setting_up_a_reddit_clone/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Setting up a reddit clone.\", \"created_utc\": 1436159966.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EOkay, this is the strangest thing I\\u0026#39;ve ever seen, and I need advice on how to deal with it.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELook at \\u003Ca href=\\\"http://redd.it/39fadc/\\\"\\u003Ethis post made to /r/relationship_advice\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe relevant bit:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E\\u003Cem\\u003EThis was posted by a bot and it will not answer any PMs or the question it has posted. This bot is run by \\u003Ca href=\\\"/u/EliteMasterEric\\\"\\u003E/u/EliteMasterEric\\u003C/a\\u003E. Please contact him on \\u003Ca href=\\\"/r/stevenuniverse\\\"\\u003E/r/stevenuniverse\\u003C/a\\u003E if you have questions about the bot\\u003C/em\\u003E\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EHere\\u0026#39;s the problem: \\u003Cstrong\\u003EI didn\\u0026#39;t make this bot\\u003C/strong\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am not associated with \\u003Ca href=\\\"/u/RedditQBot_1\\\"\\u003E/u/RedditQBot_1\\u003C/a\\u003E, and I have never visited the \\u003Ca href=\\\"/r/relationship_advice\\\"\\u003E/r/relationship_advice\\u003C/a\\u003E subreddit before I received a username mention.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis is so far the only post it has made.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWeirdly enough, it refers to my position as moderator on \\u003Ca href=\\\"/r/StevenUniverse\\\"\\u003E/r/StevenUniverse\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have made a reddit bot in the past, but it only changes the header and updates flair statistics for \\u003Ca href=\\\"/r/StevenUniverse\\\"\\u003E/r/StevenUniverse\\u003C/a\\u003E, and does not create or modify posts.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis is probably the wrong place to post this, but I didn\\u0026#39;t know where else to go. I want to know who\\u0026#39;s doing this and why, or have the bot shut down, or both.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Okay, this is the strangest thing I've ever seen, and I need advice on how to deal with it.\\n\\nLook at [this post made to /r/relationship_advice](http://redd.it/39fadc/).\\n\\nThe relevant bit:\\n\\n\\u003E *This was posted by a bot and it will not answer any PMs or the question it has posted. This bot is run by /u/EliteMasterEric. Please contact him on /r/stevenuniverse if you have questions about the bot*\\n\\nHere's the problem: **I didn't make this bot**.\\n\\nI am not associated with /u/RedditQBot_1, and I have never visited the /r/relationship_advice subreddit before I received a username mention.\\n\\nThis is so far the only post it has made.\\n\\nWeirdly enough, it refers to my position as moderator on /r/StevenUniverse.\\n\\nI have made a reddit bot in the past, but it only changes the header and updates flair statistics for /r/StevenUniverse, and does not create or modify posts.\\n\\nThis is probably the wrong place to post this, but I didn't know where else to go. I want to know who's doing this and why, or have the bot shut down, or both.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"39g1lh\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"EliteMasterEric\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/39g1lh/is_this_bot_impersonating_me/\", \"locked\": false, \"name\": \"t3_39g1lh\", \"created\": 1434060118.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/39g1lh/is_this_bot_impersonating_me/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is this bot impersonating me?\", \"created_utc\": 1434031318.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESomething I\\u0026#39;ve noticed is that receiving a list of submissions gives you most of the JSON with the exception of one which you have to get manually by getting to the permalink: \\u003Ccode\\u003Eupvote_ratio\\u003C/code\\u003E. You can find that in the source \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/lib/jsontemplates.py#L634\\\"\\u003Ehere\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI looked into this some more and it seems to be a deliberate design decision \\u003Ca href=\\\"https://www.reddit.com/r/redditdev/comments/29i58s/reddit_change_api_availability_controversiality/\\\"\\u003E11 months ago\\u003C/a\\u003E but I just can\\u0026#39;t understand why.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs it possible to reconsider this decision? I have to make a lot more API calls and that slows down my program down to a crawl. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Something I've noticed is that receiving a list of submissions gives you most of the JSON with the exception of one which you have to get manually by getting to the permalink: `upvote_ratio`. You can find that in the source [here](https://github.com/reddit/reddit/blob/master/r2/r2/lib/jsontemplates.py#L634).\\n\\nI looked into this some more and it seems to be a deliberate design decision [11 months ago](https://www.reddit.com/r/redditdev/comments/29i58s/reddit_change_api_availability_controversiality/) but I just can't understand why.\\n\\nIs it possible to reconsider this decision? I have to make a lot more API calls and that slows down my program down to a crawl. \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"37k0ej\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Rapptz\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/37k0ej/why_do_i_have_to_go_through_a_permalink_to_get/\", \"locked\": false, \"name\": \"t3_37k0ej\", \"created\": 1432816672.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/37k0ej/why_do_i_have_to_go_through_a_permalink_to_get/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why do I have to go through a permalink to get upvote_ratio?\", \"created_utc\": 1432787872.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIn an effort to be more transparent, we have made a significant change to \\u003Ca href=\\\"https://www.reddit.com/r/announcements/comments/35uyil/transparency_is_important_to_us_and_today_we_take/\\\"\\u003Ehow we display and report on content we remove for legal reasons\\u003C/a\\u003E. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo ensure absolute clarity that we, as reddit.com, removed the content, I have updated the API to have:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EAPI responses for \\u003Ccode\\u003EComment\\u003C/code\\u003Es and \\u003Ccode\\u003ELinks\\u003C/code\\u003E have an additional field called \\u003Ccode\\u003Eremoval_reason\\u003C/code\\u003E. The reason will either be \\u003Ccode\\u003Enull\\u003C/code\\u003E (default) or \\u003Ccode\\u003Elegal\\u003C/code\\u003E if the content was removed by reddit for legal reasons.\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EExample removed link: \\u003Ca href=\\\"https://www.reddit.com/r/ChillingEffects/comments/35urvq/test_post_please_ignore/\\\"\\u003Ehttps://www.reddit.com/r/ChillingEffects/comments/35urvq/test_post_please_ignore/\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003EExample removed comment: \\u003Ca href=\\\"https://www.reddit.com/r/ChillingEffects/comments/35urvq/test_post_please_ignore/cr7z8fp\\\"\\u003Ehttps://www.reddit.com/r/ChillingEffects/comments/35urvq/test_post_please_ignore/cr7z8fp\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003EOnce a link or comment has been removed, it will no longer be able to be edited. Attempts to edit a removed comment or post will result in a \\u003Ccode\\u003E403\\u003C/code\\u003E response with a message of \\u003Ccode\\u003Ethis content is locked and cannot be edited\\u003C/code\\u003E. \\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EThere is a more detailed post to \\u003Ca href=\\\"/r/changelog\\\"\\u003E/r/changelog\\u003C/a\\u003E, which you can view \\u003Ca href=\\\"https://www.reddit.com/r/changelog/comments/35v313/reddit_change_public_display_of_content_removed/\\\"\\u003Ehere\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"In an effort to be more transparent, we have made a significant change to [how we display and report on content we remove for legal reasons](https://www.reddit.com/r/announcements/comments/35uyil/transparency_is_important_to_us_and_today_we_take/). \\n\\nTo ensure absolute clarity that we, as reddit.com, removed the content, I have updated the API to have:\\n\\n- API responses for `Comment`s and `Links` have an additional field called `removal_reason`. The reason will either be `null` (default) or `legal` if the content was removed by reddit for legal reasons.\\n   - Example removed link: https://www.reddit.com/r/ChillingEffects/comments/35urvq/test_post_please_ignore/\\n   - Example removed comment: https://www.reddit.com/r/ChillingEffects/comments/35urvq/test_post_please_ignore/cr7z8fp\\n- Once a link or comment has been removed, it will no longer be able to be edited. Attempts to edit a removed comment or post will result in a `403` response with a message of `this content is locked and cannot be edited`. \\n\\nThere is a more detailed post to /r/changelog, which you can view [here](https://www.reddit.com/r/changelog/comments/35v313/reddit_change_public_display_of_content_removed/).\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"35v3ax\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"weffey\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/35v3ax/reddit_change_api_changes_for_the_public_display/\", \"locked\": false, \"name\": \"t3_35v3ax\", \"created\": 1431574310.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/35v3ax/reddit_change_api_changes_for_the_public_display/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[reddit change] API changes for the public display of content removed for legal reasons\", \"created_utc\": 1431545510.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Cstrong\\u003EThe API Call\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehttp://api.redditanalytics.com/trending?subreddit=askhistorians\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditanalytics.com/trending?subreddit=askhistorians\\\"\\u003ETry the call here\\u003C/a\\u003E (returns JSON)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis call looks at the most recent 10,000 comments for any subreddit and spits out a list of trending topics.  I\\u0026#39;m open to entertaining enhancements for this.  I can put in by date range or even allow someone to pass in their own regex command to run against the list of comments.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe cache for this is set at 60 seconds.  (It doesn\\u0026#39;t change that much minute by minute).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you would like the data returned in another format besides JSON, I\\u0026#39;m open to requests.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"**The API Call**\\n\\n    http://api.redditanalytics.com/trending?subreddit=askhistorians\\n\\n[Try the call here](http://api.redditanalytics.com/trending?subreddit=askhistorians) (returns JSON)\\n\\n\\nThis call looks at the most recent 10,000 comments for any subreddit and spits out a list of trending topics.  I'm open to entertaining enhancements for this.  I can put in by date range or even allow someone to pass in their own regex command to run against the list of comments.\\n\\nThe cache for this is set at 60 seconds.  (It doesn't change that much minute by minute).\\n\\nIf you would like the data returned in another format besides JSON, I'm open to requests.\\n\\nThanks.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1t1y60\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1t1y60/new_api_call_whats_trending_on_reddit_globally_or/\", \"locked\": false, \"name\": \"t3_1t1y60\", \"created\": 1387272889.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1t1y60/new_api_call_whats_trending_on_reddit_globally_or/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New API call: What's trending on Reddit globally or for each subreddit\", \"created_utc\": 1387244089.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI know in general upvote/downvote \\u0026quot;fraud\\u0026quot; is an offence, but what when the bot upvotes/downvotes relevant content?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor example, consider a bot that users have to call to do a certain task. After task is completed successfully, bot upvotes the user comment for visibility.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI don\\u0026#39;t know whether there are any explicit rules for or against that.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I know in general upvote/downvote \\\"fraud\\\" is an offence, but what when the bot upvotes/downvotes relevant content?\\n\\nFor example, consider a bot that users have to call to do a certain task. After task is completed successfully, bot upvotes the user comment for visibility.\\n\\nI don't know whether there are any explicit rules for or against that.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1mpxwa\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"im14\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 16, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1mpxwa/what_is_the_policy_on_bots_upvoting_posts/\", \"locked\": false, \"name\": \"t3_1mpxwa\", \"created\": 1379638250.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1mpxwa/what_is_the_policy_on_bots_upvoting_posts/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"What is the policy on bots upvoting posts?\", \"created_utc\": 1379609450.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey everyone,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am the dev of \\u003Ca href=\\\"/u/VerseBot\\\"\\u003E/u/VerseBot\\u003C/a\\u003E (which runs on \\u003Ca href=\\\"/r/Christianity\\\"\\u003E/r/Christianity\\u003C/a\\u003E and \\u003Ca href=\\\"/r/TrueChristian\\\"\\u003E/r/TrueChristian\\u003C/a\\u003E), and I have come across a very strange issue lately.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe bot was working perfectly fine for over two weeks, and then all of a sudden it started spamming, replying to the same comment multiple times.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere\\u0026#39;s what seems to happen when the problem arises:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EBot finds a comment that it needs to reply to\\u003C/li\\u003E\\n\\u003Cli\\u003EBot replies to comment with appropriate response\\u003C/li\\u003E\\n\\u003Cli\\u003EPRAW throws a HTTPError exception (usually a 502 Bad Gateway error) from the PRAW reply calls within commenter.py\\u003C/li\\u003E\\n\\u003Cli\\u003EComment id of comment bot replies to does not get stored in the PostgreSQL database, OR a Python set\\u003C/li\\u003E\\n\\u003Cli\\u003EBot starts spamming on the comment\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EI am really clueless on why this is happening as it still sometimes works fine, but most of the time just spams like crazy. Has something changed in the reddit API that requires a PRAW update? Does anybody else running a bot with PRAW and/or Heroku have this problem? I have tried several variations of try/except blocks on the PRAW reply calls, but nothing seems to work.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/matthieugrieger/versebot\\\"\\u003EHere is my code on GitHub.\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHopefully somebody knows a solution to this annoying problem that has cropped up recently.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey everyone,\\n\\nI am the dev of /u/VerseBot (which runs on /r/Christianity and /r/TrueChristian), and I have come across a very strange issue lately.\\n\\nThe bot was working perfectly fine for over two weeks, and then all of a sudden it started spamming, replying to the same comment multiple times.\\n\\nHere's what seems to happen when the problem arises:\\n\\n- Bot finds a comment that it needs to reply to\\n- Bot replies to comment with appropriate response\\n- PRAW throws a HTTPError exception (usually a 502 Bad Gateway error) from the PRAW reply calls within commenter.py\\n- Comment id of comment bot replies to does not get stored in the PostgreSQL database, OR a Python set\\n- Bot starts spamming on the comment\\n\\nI am really clueless on why this is happening as it still sometimes works fine, but most of the time just spams like crazy. Has something changed in the reddit API that requires a PRAW update? Does anybody else running a bot with PRAW and/or Heroku have this problem? I have tried several variations of try/except blocks on the PRAW reply calls, but nothing seems to work.\\n\\n[Here is my code on GitHub.](https://github.com/matthieugrieger/versebot)\\n\\nHopefully somebody knows a solution to this annoying problem that has cropped up recently.\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1m85d0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"mgrieger\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 34, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1378958602.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/\", \"locked\": false, \"name\": \"t3_1m85d0\", \"created\": 1378987167.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Bot randomly started spamming with no changes to the code\", \"created_utc\": 1378958367.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1l0ym3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Snorrrlax\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1l0ym3/rchangemyviews_deltabot_is_now_open_source_feel/\", \"locked\": false, \"name\": \"t3_1l0ym3\", \"created\": 1377413275.0, \"url\": \"https://github.com/alexames/DeltaBot\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"/r/changemyview's DeltaBot is now open source - feel free to contribute!\", \"created_utc\": 1377384475.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m just wondering because it would be interesting to see it used somewhere else.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm just wondering because it would be interesting to see it used somewhere else.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1k506f\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Booty_Bumping\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1k506f/is_there_any_popular_websites_based_on_the_reddit/\", \"locked\": false, \"name\": \"t3_1k506f\", \"created\": 1376246890.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1k506f/is_there_any_popular_websites_based_on_the_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there any popular websites based on the reddit source code?\", \"created_utc\": 1376218090.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Ch3\\u003ENote:\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EFirst things first. I am a noob at this but Deimos asked me to enquire here. Hope this helps! :) I think this post can go here, right?\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Ch3\\u003EWhat is ChromaMarket?\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003ESo along with a few others, I am a mod at \\u003Cstrong\\u003E\\u003Ca href=\\\"/r/ChromaMarket\\\"\\u003E/r/ChromaMarket\\u003C/a\\u003E\\u003C/strong\\u003E, which is a place where we do some \\u003Cstrong\\u003Evirtual currency stuff\\u003C/strong\\u003E. We buy/sell virtual goods and everybody starts with a set amount of money. However, recently, we have gotten around \\u003Cstrong\\u003E30 active users\\u003C/strong\\u003E that frequently use the system and this is set to go up. Thus, we decided to find someone who could \\u003Cstrong\\u003Ehelp us with a bot\\u003C/strong\\u003E.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Ch3\\u003EAbout What We Need:\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EWe need a bot that can \\u003Cstrong\\u003Eread modmails\\u003C/strong\\u003E, and complete transactions that take place via modmail. These transactions only include \\u003Cstrong\\u003Ebuying and selling\\u003C/strong\\u003E. As such, the bot would need to \\u003Cstrong\\u003Ecalculate the money\\u003C/strong\\u003E after the purchase/sale and enter that info into our \\u003Cstrong\\u003Esubreddit wiki page\\u003C/strong\\u003E if possible. For buying, the bot would need to \\u003Cstrong\\u003Eadd\\u003C/strong\\u003E items to the list in the wiki page and for selling, the bot would need to \\u003Cstrong\\u003Eremove\\u003C/strong\\u003E items from the list.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Ch3\\u003EMore Info:\\u003C/h3\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EWe would like this to be done \\u003Cstrong\\u003Easap\\u003C/strong\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003EAnyone who \\u003Cstrong\\u003Ecan help\\u003C/strong\\u003E, please comment below\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003ETHANKS!\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"###Note:\\nFirst things first. I am a noob at this but Deimos asked me to enquire here. Hope this helps! :) I think this post can go here, right?\\n\\n---\\n\\n###What is ChromaMarket?\\nSo along with a few others, I am a mod at **/r/ChromaMarket**, which is a place where we do some **virtual currency stuff**. We buy/sell virtual goods and everybody starts with a set amount of money. However, recently, we have gotten around **30 active users** that frequently use the system and this is set to go up. Thus, we decided to find someone who could **help us with a bot**.\\n\\n---\\n\\n###About What We Need:\\nWe need a bot that can **read modmails**, and complete transactions that take place via modmail. These transactions only include **buying and selling**. As such, the bot would need to **calculate the money** after the purchase/sale and enter that info into our **subreddit wiki page** if possible. For buying, the bot would need to **add** items to the list in the wiki page and for selling, the bot would need to **remove** items from the list.\\n\\n---\\n\\n###More Info:\\n\\n- We would like this to be done **asap**\\n- Anyone who **can help**, please comment below\\n\\n---\\n\\n**THANKS!**\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ey9l1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Skafos_\", \"media\": null, \"score\": 16, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 23, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1369388358.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ey9l1/bot_help_virtual_currency_chromamarket/\", \"locked\": false, \"name\": \"t3_1ey9l1\", \"created\": 1369396790.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1ey9l1/bot_help_virtual_currency_chromamarket/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Bot Help] - Virtual Currency (ChromaMarket)\", \"created_utc\": 1369367990.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 16}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EOkay, so I run \\u003Ca href=\\\"/r/Nerdcubed\\\"\\u003E/r/Nerdcubed\\u003C/a\\u003E and I want a bot that will post a link to a video from two particular users when they\\u0026#39;re uploaded to Youtube (With the write title and whatnot). Now, I know some very basic coding but for all intensive purposes I\\u0026#39;m a newbie. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo, where do I start? How do I go about making this? Any help would be much appreciated. :p\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Okay, so I run /r/Nerdcubed and I want a bot that will post a link to a video from two particular users when they're uploaded to Youtube (With the write title and whatnot). Now, I know some very basic coding but for all intensive purposes I'm a newbie. \\n\\nSo, where do I start? How do I go about making this? Any help would be much appreciated. :p\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"18ucm2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Mattophobia\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/18ucm2/i_need_a_bot_where_do_i_start/\", \"locked\": false, \"name\": \"t3_18ucm2\", \"created\": 1361338836.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/18ucm2/i_need_a_bot_where_do_i_start/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I need a bot, where do I start?\", \"created_utc\": 1361310036.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m sorry if the title may seemed offensive, but I really need your attention. Please hear me out before getting mad.\\u003Cbr/\\u003E\\nI\\u0026#39;ve been urging myself to use the OAuth API and I love how it works. It\\u0026#39;s a lot more easy to just have a user press a button instead of making them enter their credentials on a third-party site.\\u003Cbr/\\u003E\\nWith that having said it has come to my attention that the OAuth API is really limited, It\\u0026#39;s in no way as good as the normal API.\\u003Cbr/\\u003E\\nFor example the front page (.json / hot.json) only gets the public frontpage.\\u003Cbr/\\u003E\\nI can update a reddit\\u0026#39;s stylesheet but not get prevstylesheet (trough /about/stylesheet.json) which makes it impossible to update it at all.\\u003Cbr/\\u003E\\nThere\\u0026#39;s just tons and tons of other stuff like this which makes using it quite impossible.\\u003Cbr/\\u003E\\nComing back to the title, In order to try to get the users frontpage I now have to get all of their subreddits (2 requests, and then I\\u0026#39;m still not sure if I got all of his subreddits) and then try something like \\u003Ca href=\\\"/r/subreddit1+subreddit2+etc\\\"\\u003E/r/subreddit1+subreddit2+etc\\u003C/a\\u003E.json which costs a lot of - unnessary - server load for us both.\\u003Cbr/\\u003E\\nThere also have been a few project I just completely had to cancel because of these limitations.\\u003Cbr/\\u003E\\nThere\\u0026#39;s just so many things missing compared to the normal API and as much as I love the work you guys have done on it so far, I\\u0026#39;m really urging you to continue working on it because it\\u0026#39;s unusable for me at the moment because of it\\u0026#39;s limitations.\\u003Cbr/\\u003E\\n/rant\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm sorry if the title may seemed offensive, but I really need your attention. Please hear me out before getting mad.  \\nI've been urging myself to use the OAuth API and I love how it works. It's a lot more easy to just have a user press a button instead of making them enter their credentials on a third-party site.  \\nWith that having said it has come to my attention that the OAuth API is really limited, It's in no way as good as the normal API.  \\nFor example the front page (.json / hot.json) only gets the public frontpage.  \\nI can update a reddit's stylesheet but not get prevstylesheet (trough /about/stylesheet.json) which makes it impossible to update it at all.  \\nThere's just tons and tons of other stuff like this which makes using it quite impossible.  \\nComing back to the title, In order to try to get the users frontpage I now have to get all of their subreddits (2 requests, and then I'm still not sure if I got all of his subreddits) and then try something like /r/subreddit1+subreddit2+etc.json which costs a lot of - unnessary - server load for us both.  \\nThere also have been a few project I just completely had to cancel because of these limitations.  \\nThere's just so many things missing compared to the normal API and as much as I love the work you guys have done on it so far, I'm really urging you to continue working on it because it's unusable for me at the moment because of it's limitations.  \\n/rant\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"16wiwe\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MoederPoeder\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1358641862.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/16wiwe/why_does_the_oauth_api_suck_i_cant_even_get_a/\", \"locked\": false, \"name\": \"t3_16wiwe\", \"created\": 1358670255.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/16wiwe/why_does_the_oauth_api_suck_i_cant_even_get_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why does the OAuth API suck? I can't even get a users frontpage.\", \"created_utc\": 1358641455.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHonestly this new fangled API structure is a bit foreign to me but I am learning some. However, I have hit a roadblock that is certain to ruin all my programmer cred.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI program in python and have gotten OAuth working. I am getting a access_token. Now I feel stupid. I have not the foggiest how to make an API call through PRAW or anything else that uses that access_token. I am simply trying to find the username (eventually some other things but not now) of the person who got the access_token. It really must be something so easy that I am missing that there isn\\u0026#39;t even a guide.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eimport requests\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eurl=\\u0026quot;\\u003Ca href=\\\"https://ssl.reddit.com/api/me.json?\\\"\\u003Ehttps://ssl.reddit.com/api/me.json?\\u003C/a\\u003E\\u0026quot;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Edata= {\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026#39;access_token\\u0026#39;: \\u0026#39;access_token\\u0026#39;,\\n\\n\\u0026#39;scope\\u0026#39; : \\u0026#39;http://www.my.url\\u0026#39;\\n\\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eprint requests.get(url, data=data).text\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis gets me an empty JSON response {} but, HTTP return of 200. . .I dunno\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Honestly this new fangled API structure is a bit foreign to me but I am learning some. However, I have hit a roadblock that is certain to ruin all my programmer cred.\\n\\nI program in python and have gotten OAuth working. I am getting a access_token. Now I feel stupid. I have not the foggiest how to make an API call through PRAW or anything else that uses that access_token. I am simply trying to find the username (eventually some other things but not now) of the person who got the access_token. It really must be something so easy that I am missing that there isn't even a guide.\\n\\nimport requests\\n\\nurl=\\\"https://ssl.reddit.com/api/me.json?\\\"\\n\\ndata= {\\n\\n    'access_token': 'access_token',\\n\\n    'scope' : 'http://www.my.url'\\n\\n    }\\n\\nprint requests.get(url, data=data).text\\n\\n\\nThis gets me an empty JSON response {} but, HTTP return of 200. . .I dunno\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"15kc2j\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"downbound\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1356684982.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/15kc2j/embarrased_i_need_the_hello_world_of_api/\", \"locked\": false, \"name\": \"t3_15kc2j\", \"created\": 1356710057.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/15kc2j/embarrased_i_need_the_hello_world_of_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Embarrased I need the \\\"Hello World\\\" of API\", \"created_utc\": 1356681257.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"venus.xelio.info\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"u73kt\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"aphexcoil\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/u73kt/gave_the_reddit_search_engine_i_created_a_trial/\", \"locked\": false, \"name\": \"t3_u73kt\", \"created\": 1338137251.0, \"url\": \"http://venus.xelio.info/redditsearch/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Gave the Reddit search engine I created a trial run with /r/askreddit.  Tested for 1 million hits per day off a laptop.  \", \"created_utc\": 1338108451.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIf you are just looking to interact  with the API easily, check out  \\u003Ca href=\\\"https://github.com/mellort/reddit_api\\\"\\u003Ebboe and mellort\\u0026#39;s excellent Python reddit API wrapper\\u003C/a\\u003E. I\\u0026#39;m just teaching how to do very basic things. That wrapper is a very complete tool.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EA few people have asked for my help, when I posted in that thread a few days ago, so I figured there might be a demand for personalized help.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo, if you\\u0026#39;re friendly, willing to listen, and do, what I tell you (exercises) send me your email in a PM, after you post a comment in this thread, and I\\u0026#39;ll walk you through the basics of logging in, submitting a story, getting links from a subreddit, and deleting your posts, among other things.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERequirements:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EPython 2.x or 3.x\\u003C/li\\u003E\\n\\u003Cli\\u003EModules to use: \\u003Ccode\\u003Erequests\\u003C/code\\u003E, \\u003Ccode\\u003Ejson\\u003C/code\\u003E, \\u003Ccode\\u003Epprint\\u003C/code\\u003E, \\u003Ccode\\u003Etime\\u003C/code\\u003E or \\u003Ccode\\u003Edatetime\\u003C/code\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003EFriendliness (I\\u0026#39;m not going to waste time with dicks, looking to spam reddit or something)\\u003C/li\\u003E\\n\\u003Cli\\u003EBasic Google-fu, ability to search Stack Overflow\\u003C/li\\u003E\\n\\u003Cli\\u003EWillingness to listen to critique, should the need arise.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EBonus:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EGoogle Talk/Chat if there\\u0026#39;s ever an issue you can\\u0026#39;t solve on your own\\u003C/li\\u003E\\n\\u003Cli\\u003ECreepy pics of forests. I just like looking at them.\\u003C/li\\u003E\\n\\u003Cli\\u003EA subreddit you can \\u0026#39;spam\\u0026#39; with test posts. I use my \\u003Ca href=\\\"/r/tankorsmash\\\"\\u003E/r/tankorsmash\\u003C/a\\u003E for this. It only takes a minute to create a subreddit.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EMy motivation is that I\\u0026#39;m going to write a generic tutorial for working with Python and the reddit API, and I\\u0026#39;d like to get a feel for what people have trouble with. I only expect a few people to respond to this, I can\\u0026#39;t handle more than that! ;)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eedit: For fun, here\\u0026#39;s my twitter, \\u003Ca href=\\\"https://twitter.com/#!/TankorSmash\\\"\\u003E@TankorSmash\\u003C/a\\u003E, and \\u003Ca href=\\\"http://www.tankorsmash.com\\\"\\u003Eblog\\u003C/a\\u003E if you need to contact me outside of reddit for more questions\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"If you are just looking to interact  with the API easily, check out  [bboe and mellort's excellent Python reddit API wrapper](https://github.com/mellort/reddit_api). I'm just teaching how to do very basic things. That wrapper is a very complete tool.\\n\\n----\\n\\nA few people have asked for my help, when I posted in that thread a few days ago, so I figured there might be a demand for personalized help.\\n\\nSo, if you're friendly, willing to listen, and do, what I tell you (exercises) send me your email in a PM, after you post a comment in this thread, and I'll walk you through the basics of logging in, submitting a story, getting links from a subreddit, and deleting your posts, among other things.\\n\\nRequirements:\\n\\n* Python 2.x or 3.x\\n* Modules to use: `requests`, `json`, `pprint`, `time` or `datetime`\\n* Friendliness (I'm not going to waste time with dicks, looking to spam reddit or something)\\n* Basic Google-fu, ability to search Stack Overflow\\n* Willingness to listen to critique, should the need arise.\\n\\nBonus:\\n\\n* Google Talk/Chat if there's ever an issue you can't solve on your own\\n* Creepy pics of forests. I just like looking at them.\\n* A subreddit you can 'spam' with test posts. I use my [/r/tankorsmash](/r/tankorsmash) for this. It only takes a minute to create a subreddit.\\n\\n\\nMy motivation is that I'm going to write a generic tutorial for working with Python and the reddit API, and I'd like to get a feel for what people have trouble with. I only expect a few people to respond to this, I can't handle more than that! ;)\\n\\nedit: For fun, here's my twitter, [@TankorSmash](https://twitter.com/#!/TankorSmash), and [blog](http://www.tankorsmash.com) if you need to contact me outside of reddit for more questions\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"srzof\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"TankorSmash\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/srzof/hey_rredditdev_im_looking_to_help_some_folks_out/\", \"locked\": false, \"name\": \"t3_srzof\", \"created\": 1335401634.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/srzof/hey_rredditdev_im_looking_to_help_some_folks_out/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Hey /r/redditdev, I'm looking to help some folks out with the reddit API, working in Python. If you're interested in learning the basics of it, read on.\", \"created_utc\": 1335372834.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ENot sure if this is intentional, but I\\u0026#39;ve noticed that the moderator log only registers moderators that I\\u0026#39;ve added by hand, and not ones that I\\u0026#39;ve added through a python script using the API.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Not sure if this is intentional, but I've noticed that the moderator log only registers moderators that I've added by hand, and not ones that I've added through a python script using the API.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"noe4o\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Bornhuetter\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/noe4o/the_moderator_log_does_not_register_moderator/\", \"locked\": false, \"name\": \"t3_noe4o\", \"created\": 1324706089.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/noe4o/the_moderator_log_does_not_register_moderator/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"The moderator log does not register moderator actions performed through a script.\", \"created_utc\": 1324677289.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI don\\u0026#39;t know that much about programming, but I was under the impression when programming for the internet/web you need to use scripting languages and such like php, asp, xhtml, html, xml, ajax, javascript, and what not. How exactly does python fit in to all of this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I don't know that much about programming, but I was under the impression when programming for the internet/web you need to use scripting languages and such like php, asp, xhtml, html, xml, ajax, javascript, and what not. How exactly does python fit in to all of this?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"hq4g9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Zaask786\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 31, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/hq4g9/when_you_say_developed_in_python_what_does/\", \"locked\": false, \"name\": \"t3_hq4g9\", \"created\": 1307069676.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/hq4g9/when_you_say_developed_in_python_what_does/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"When you say developed in Python, what does exactly mean?\", \"created_utc\": 1307040876.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ERight now, when someone wants to sign up for reddit gold, we send them off to \\u003Ca href=\\\"/help/gold\\\"\\u003E/help/gold\\u003C/a\\u003E, where we have a static HTML form that sends them off to a PayPal \\u0026quot;subscribe\\u0026quot; or \\u0026quot;buy now\\u0026quot; session (depending on whether they use the auto-renewing option or not).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThen, they finish paying, PayPal sends us an IPN with their email address, and we mail them a claim code that they have to enter in at \\u003Ca href=\\\"/thanks\\\"\\u003E/thanks\\u003C/a\\u003E while logged in to reddit.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s an annoying step that confuses people and leads to the vast majority of our customer support requests.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat I\\u0026#39;d prefer to do is this: when a logged-in user goes to pay for gold, send them off to PayPal with their username stored in a hidden (or overt) parameter. Then, when they pay, the IPN notification has their username and we can simply credit their reddit account, rather than having to send them an email with a claim code.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EUnfortunately, PayPal\\u0026#39;s documentation is complicated and confusing to me, and I\\u0026#39;m not getting very far. Does anyone here happen to have experience with this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Right now, when someone wants to sign up for reddit gold, we send them off to [/help/gold](/help/gold), where we have a static HTML form that sends them off to a PayPal \\\"subscribe\\\" or \\\"buy now\\\" session (depending on whether they use the auto-renewing option or not).\\n\\nThen, they finish paying, PayPal sends us an IPN with their email address, and we mail them a claim code that they have to enter in at [/thanks](/thanks) while logged in to reddit.\\n\\nIt's an annoying step that confuses people and leads to the vast majority of our customer support requests.\\n\\nWhat I'd prefer to do is this: when a logged-in user goes to pay for gold, send them off to PayPal with their username stored in a hidden (or overt) parameter. Then, when they pay, the IPN notification has their username and we can simply credit their reddit account, rather than having to send them an email with a claim code.\\n\\nUnfortunately, PayPal's documentation is complicated and confusing to me, and I'm not getting very far. Does anyone here happen to have experience with this?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"d5hyr\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"raldi\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 20, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/d5hyr/does_anyone_here_have_experience_working_with/\", \"locked\": false, \"name\": \"t3_d5hyr\", \"created\": 1282811724.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/d5hyr/does_anyone_here_have_experience_working_with/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Does anyone here have experience working with PayPal and its IPN system?\", \"created_utc\": 1282782924.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E(If you\\u0026#39;re out of the loop, see \\u003Ca href=\\\"/r/joinrobin\\\"\\u003Ehere\\u003C/a\\u003E)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe endpoint to vote to grow/stay/abandon a room is:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003EPOST /api/robin/\\u0026lt;room ID\\u0026gt;/vote\\u003C/code\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt accepts the form parameters \\u003Ccode\\u003Eroom_id\\u003C/code\\u003E, \\u003Ccode\\u003Eroom_name\\u003C/code\\u003E, \\u003Ccode\\u003Ewinning_vote\\u003C/code\\u003E, and \\u003Ccode\\u003Evote\\u003C/code\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Eroom_id\\u003C/code\\u003E is the UUID of the room\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Eroom_name\\u003C/code\\u003E is the name of the room that appears in the page header\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Ewinning_vote\\u003C/code\\u003E is always the string \\u003Ccode\\u003ENOVOTE\\u003C/code\\u003E, whatever that means\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Evote\\u003C/code\\u003E is the option that the user votes for -- one of \\u003Ccode\\u003EABANDON\\u003C/code\\u003E, \\u003Ccode\\u003EINCREASE\\u003C/code\\u003E, or \\u003Ccode\\u003ECONTINUE\\u003C/code\\u003E.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EAs far as I can tell, these endpoints do not work with OAuth, so modhashes are necessary.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI made a \\u003Ca href=\\\"https://gist.github.com/not-an-aardvark/7306c04362927f6a1539118ad8da418c\\\"\\u003Eproof-of-concept\\u003C/a\\u003E script that automatically gets the user\\u0026#39;s current room and votes to \\u0026quot;grow\\u0026quot;.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEnjoy your crontabs!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"(If you're out of the loop, see [here](/r/joinrobin))\\n\\nThe endpoint to vote to grow/stay/abandon a room is:\\n\\n`POST /api/robin/\\u003Croom ID\\u003E/vote`\\n\\nIt accepts the form parameters `room_id`, `room_name`, `winning_vote`, and `vote`.\\n\\n* `room_id` is the UUID of the room\\n* `room_name` is the name of the room that appears in the page header\\n* `winning_vote` is always the string `NOVOTE`, whatever that means\\n* `vote` is the option that the user votes for -- one of `ABANDON`, `INCREASE`, or `CONTINUE`.\\n\\nAs far as I can tell, these endpoints do not work with OAuth, so modhashes are necessary.\\n\\nI made a [proof-of-concept](https://gist.github.com/not-an-aardvark/7306c04362927f6a1539118ad8da418c) script that automatically gets the user's current room and votes to \\\"grow\\\".\\n\\nEnjoy your crontabs!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4cxe9a\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"not_an_aardvark\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4cxe9a/reddit_robin/\", \"locked\": false, \"name\": \"t3_4cxe9a\", \"created\": 1459566134.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4cxe9a/reddit_robin/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Robin\", \"created_utc\": 1459537334.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWould be epic to have something akin to the Facebook comments plugin but for Reddit (embed either subreddit or thread/post): \\u003Ca href=\\\"https://developers.facebook.com/docs/plugins/comments\\\"\\u003Ehttps://developers.facebook.com/docs/plugins/comments\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Would be epic to have something akin to the Facebook comments plugin but for Reddit (embed either subreddit or thread/post): https://developers.facebook.com/docs/plugins/comments\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3schc4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"treelovinhippie\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1447215131.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3schc4/reddit_is_probably_the_best_place_for_indepth/\", \"locked\": false, \"name\": \"t3_3schc4\", \"created\": 1447230752.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3schc4/reddit_is_probably_the_best_place_for_indepth/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit is probably the best place for in-depth discussions, why the hell can't I embed a post/thread on a website?\", \"created_utc\": 1447201952.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EToday I committed a change \\u003Ca href=\\\"https://github.com/reddit/reddit/commit/d990533d0b57a499cefcec70f4c51d8c5593c497\\\"\\u003Ehttps://github.com/reddit/reddit/commit/d990533d0b57a499cefcec70f4c51d8c5593c497\\u003C/a\\u003E that upgrades reddit from using pylons 0.9.7 to pylons 1.0.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe upgraded pylons as part of an effort to upgrade from ubuntu precise to ubuntu trusty. There are some backwards incompatibilities between the pylons versions so changes in the reddit code were required. You can find the details in the commit message and at \\u003Ca href=\\\"http://pylons-webframework.readthedocs.org/en/latest/upgrading.html\\\"\\u003Ehttp://pylons-webframework.readthedocs.org/en/latest/upgrading.html\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhen you get this commit on your local install you\\u0026#39;ll need to do a few things to get everything working again (all these steps assume that your reddit repo is at ~/src/reddit):\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003EGet the new commit\\u003C/li\\u003E\\n\\u003Cli\\u003EGet most recent commits from all plugins you\\u0026#39;re running\\u003C/li\\u003E\\n\\u003Cli\\u003Erun: \\u003Ccode\\u003E$ sudo apt-get install python-routes=1.12.3-1ubuntu1 python-pylons=1.0-2\\u003C/code\\u003E to upgrade your installed versions of routes and pylons to the default versions for ubuntu precise\\u003C/li\\u003E\\n\\u003Cli\\u003Ein ~/src/reddit/r2 run \\u003Ccode\\u003E$ sudo python setup.py develop --no-deps\\u003C/code\\u003E to update your r2 package so it will use the upgraded routes and pylons\\u003C/li\\u003E\\n\\u003Cli\\u003Ein ~/src/reddit/r2 run \\u003Ccode\\u003E$ rm -r data/templates\\u003C/code\\u003E to clear out your compiled mako templates that contain references to the old pylons objects\\u003C/li\\u003E\\n\\u003Cli\\u003Ein ~/src/reddit run \\u003Ccode\\u003E$ sudo cp upstart/* /etc/init/\\u003C/code\\u003E to update your upstart configs\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EPlease let me know if you have questions or run into any issues.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Today I committed a change https://github.com/reddit/reddit/commit/d990533d0b57a499cefcec70f4c51d8c5593c497 that upgrades reddit from using pylons 0.9.7 to pylons 1.0.\\n\\nWe upgraded pylons as part of an effort to upgrade from ubuntu precise to ubuntu trusty. There are some backwards incompatibilities between the pylons versions so changes in the reddit code were required. You can find the details in the commit message and at http://pylons-webframework.readthedocs.org/en/latest/upgrading.html\\n\\nWhen you get this commit on your local install you'll need to do a few things to get everything working again (all these steps assume that your reddit repo is at ~/src/reddit):\\n\\n1. Get the new commit\\n2. Get most recent commits from all plugins you're running\\n3. run: `$ sudo apt-get install python-routes=1.12.3-1ubuntu1 python-pylons=1.0-2` to upgrade your installed versions of routes and pylons to the default versions for ubuntu precise\\n4. in ~/src/reddit/r2 run `$ sudo python setup.py develop --no-deps` to update your r2 package so it will use the upgraded routes and pylons\\n5. in ~/src/reddit/r2 run `$ rm -r data/templates` to clear out your compiled mako templates that contain references to the old pylons objects\\n6. in ~/src/reddit run `$ sudo cp upstart/* /etc/init/` to update your upstart configs\\n\\n\\nPlease let me know if you have questions or run into any issues.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3l421z\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bsimpson\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3l421z/local_installs_pylons_upgrade/\", \"locked\": false, \"name\": \"t3_3l421z\", \"created\": 1442393064.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3l421z/local_installs_pylons_upgrade/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[local installs] pylons upgrade\", \"created_utc\": 1442364264.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIf there is a better subreddit to help me with development, let me know.  I\\u0026#39;m thinking about hosting it on \\u003Ca href=\\\"http://pythonanywhere.com/\\\"\\u003Ehttp://pythonanywhere.com/\\u003C/a\\u003E and using the free version.  Anyone have any experience with that?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"If there is a better subreddit to help me with development, let me know.  I'm thinking about hosting it on http://pythonanywhere.com/ and using the free version.  Anyone have any experience with that?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"34ts50\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"pizzaazzip\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/34ts50/im_looking_to_create_a_bot_that_invites_people_to/\", \"locked\": false, \"name\": \"t3_34ts50\", \"created\": 1430778566.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/34ts50/im_looking_to_create_a_bot_that_invites_people_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I'm looking to create a bot that invites people to a subreddit if their username contains a certain word. Anyone have any ideas?\", \"created_utc\": 1430749766.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI made a change to consolidate the 3 ad preferences (pref_show_adbox, pref_show_sponsors, pref_show_sponsorships) into one (pref_hide_adbox). This will affect the Account attributes (for gold users) returned by the API. You can see the code change \\u003Ca href=\\\"https://github.com/reddit/reddit/commit/a30cc40f77d2dded097068d58cea6995a0520de9\\\"\\u003Ehere\\u003C/a\\u003E. Please let me know if you run into issues with this!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I made a change to consolidate the 3 ad preferences (pref_show_adbox, pref_show_sponsors, pref_show_sponsorships) into one (pref_hide_adbox). This will affect the Account attributes (for gold users) returned by the API. You can see the code change [here](https://github.com/reddit/reddit/commit/a30cc40f77d2dded097068d58cea6995a0520de9). Please let me know if you run into issues with this!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"33miz3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MiamiZ\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/33miz3/update_to_prefs_page_attributes/\", \"locked\": false, \"name\": \"t3_33miz3\", \"created\": 1429844378.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/33miz3/update_to_prefs_page_attributes/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Update to prefs page attributes\", \"created_utc\": 1429815578.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ETwo fairly small API updates today:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003ECreating a subreddit may now require a captcha (using the same conditions as all other captchas on the site). Please see \\u003Ca href=\\\"https://www.reddit.com/r/changelog/comments/2wcnt0/reddit_change_creating_a_subreddit_may_now/\\\"\\u003Ethe post in /r/changelog\\u003C/a\\u003E for information about why this change was made. As always, you can check if the user will need to complete a captcha with \\u003Ca href=\\\"https://www.reddit.com/dev/api#GET_api_needs_captcha.json\\\"\\u003Ethe \\u003Ccode\\u003Eneeds_captcha\\u003C/code\\u003E endpoint\\u003C/a\\u003E.\\u003C/li\\u003E\\n\\u003Cli\\u003EAn \\u003Ccode\\u003Einbox_count\\u003C/code\\u003E attribute has been added to the logged-in user\\u0026#39;s data, so that API clients can easily duplicate the same functionality as we have with the new-ish \\u003Ca href=\\\"https://www.reddit.com/r/changelog/comments/2tb6gd/reddit_change_unread_counts_on_your_inbox/\\\"\\u003Eunread count on your inbox feature\\u003C/a\\u003E.\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Two fairly small API updates today:\\n\\n1. Creating a subreddit may now require a captcha (using the same conditions as all other captchas on the site). Please see [the post in /r/changelog](https://www.reddit.com/r/changelog/comments/2wcnt0/reddit_change_creating_a_subreddit_may_now/) for information about why this change was made. As always, you can check if the user will need to complete a captcha with [the `needs_captcha` endpoint](https://www.reddit.com/dev/api#GET_api_needs_captcha.json).\\n2. An `inbox_count` attribute has been added to the logged-in user's data, so that API clients can easily duplicate the same functionality as we have with the new-ish [unread count on your inbox feature](https://www.reddit.com/r/changelog/comments/2tb6gd/reddit_change_unread_counts_on_your_inbox/).\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2wconj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Deimorz\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1424292858.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2wconj/api_updates_creating_a_subreddit_may_now_require/\", \"locked\": false, \"name\": \"t3_2wconj\", \"created\": 1424321342.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2wconj/api_updates_creating_a_subreddit_may_now_require/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API updates: Creating a subreddit may now require a captcha, and inbox_count attribute added for logged-in user\", \"created_utc\": 1424292542.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe problem:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe frontpage is a sub-optimal way to discover great new content on reddit. The algorithms aren\\u0026#39;t perfect but even deeper than that, the entire focus being which new submissions have gotten a lot of upvotes in the subreddits you subscribe to, leaves out SO.MUCH.GREAT.STUFF.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe solution:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECompletely re-imagine the frontpage:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E-No longer just a list of links to submissions: it\\u0026#39;s a feed, like Quora or Facebook with multiple types of content (embedded videos/images, popular comments etc)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E-Allow the \\u0026#39;Following\\u0026#39; of users, so their submissions and comments will show up in your feed\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E-Once you\\u0026#39;ve viewed something, it disappears. You can scroll forever and when you come back you always have fresh content (the current homepage is somewhat static over short and even medium timeframes).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E-It\\u0026#39;s not just content from today. You\\u0026#39;ll see old stuff too. (Quora does this and it\\u0026#39;s fantastic. Some subs would need to be excluded, like \\u003Ca href=\\\"/r/news\\\"\\u003E/r/news\\u003C/a\\u003E, but \\u003Ca href=\\\"/r/funny\\\"\\u003E/r/funny\\u003C/a\\u003E for example is not time sensitive content. Why does everything on reddit have to die after 24 hours?).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve been thinking about how to fix the problem for awhile but just came up with the solution today. Those are my ideas for one day. They are all fluid and I\\u0026#39;ll likely have more. But if it sounds interesting to you, we should chat.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy background: entrepreneur, taught myself front-end web development. i could build the front-end MVP for this for a web app. i couldn\\u0026#39;t do any of the backend (api etc).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMobile first would probably be the better path, though. Sadly, I could offer very little technical assistance in that regard. Though I could plan every single detail of the app (if needed), including making some badass wireframes and mockups.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m just looking at this as a fun project, by the way. If all of reddit starts using it, then we can take it from there. But my goal would just be to build the reddit experience I desperately want - and hope some other people dig it too.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESound like fun?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EP.S. I\\u0026#39;m not a noob, this is a throwaway :)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The problem:\\n\\nThe frontpage is a sub-optimal way to discover great new content on reddit. The algorithms aren't perfect but even deeper than that, the entire focus being which new submissions have gotten a lot of upvotes in the subreddits you subscribe to, leaves out SO.MUCH.GREAT.STUFF.\\n\\nThe solution:\\n\\nCompletely re-imagine the frontpage:\\n\\n-No longer just a list of links to submissions: it's a feed, like Quora or Facebook with multiple types of content (embedded videos/images, popular comments etc)\\n\\n-Allow the 'Following' of users, so their submissions and comments will show up in your feed\\n\\n-Once you've viewed something, it disappears. You can scroll forever and when you come back you always have fresh content (the current homepage is somewhat static over short and even medium timeframes).\\n\\n-It's not just content from today. You'll see old stuff too. (Quora does this and it's fantastic. Some subs would need to be excluded, like /r/news, but /r/funny for example is not time sensitive content. Why does everything on reddit have to die after 24 hours?).\\n\\nI've been thinking about how to fix the problem for awhile but just came up with the solution today. Those are my ideas for one day. They are all fluid and I'll likely have more. But if it sounds interesting to you, we should chat.\\n\\nMy background: entrepreneur, taught myself front-end web development. i could build the front-end MVP for this for a web app. i couldn't do any of the backend (api etc).\\n\\nMobile first would probably be the better path, though. Sadly, I could offer very little technical assistance in that regard. Though I could plan every single detail of the app (if needed), including making some badass wireframes and mockups.\\n\\nI'm just looking at this as a fun project, by the way. If all of reddit starts using it, then we can take it from there. But my goal would just be to build the reddit experience I desperately want - and hope some other people dig it too.\\n\\nSound like fun?\\n\\nP.S. I'm not a noob, this is a throwaway :)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2r7aju\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"newapp\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1420292662.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2r7aju/i_think_i_have_a_great_idea_for_a_new_reddit_app/\", \"locked\": false, \"name\": \"t3_2r7aju\", \"created\": 1420320889.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2r7aju/i_think_i_have_a_great_idea_for_a_new_reddit_app/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I think I have a great idea for a new reddit app. Anybody interested in helping to build it?\", \"created_utc\": 1420292089.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"i.imgur.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"29u12c\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"wu2ad\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/29u12c/whoa_whats_happening_here_with_the_comment_scores/\", \"locked\": false, \"name\": \"t3_29u12c\", \"created\": 1404518142.0, \"url\": \"http://i.imgur.com/6Ppk2YS.png\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Whoa what's happening here with the comment scores?\", \"created_utc\": 1404489342.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey guys, I recently researched how to get my bot, \\u003Ca href=\\\"/u/HCE_Replacement_Bot\\\"\\u003E/u/HCE_Replacement_Bot\\u003C/a\\u003E, to post to Twitter. It turns out it\\u0026#39;s stupid easy, but I wanted to pass on the knowledge in case you wanted to do something similar. I created \\u003Ca href=\\\"http://twitter.com/hcebot\\\"\\u003E@hcebot\\u003C/a\\u003E, which posts highlights from \\u003Ca href=\\\"/r/guns\\\"\\u003E/r/guns\\u003C/a\\u003E onto Twitter. So here\\u0026#39;s how we do it!\\u003C/p\\u003E\\n\\n\\u003Ch3\\u003EAccount set up\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003ETwitter requires OAuth login, which is annoying but easy to handle. You just go to \\u003Ca href=\\\"http://dev.twitter.com\\\"\\u003Ehttp://dev.twitter.com\\u003C/a\\u003E and register your Twitter handle as an application. You need to then create an OAuth key (literally just click the button) and then take 4 pieces of information for your bot: The consumer key, the consumer secret, the API key, and the API secret. If you want to tweet with your bot, you will need to change permissions to enable \\u0026quot;read/write\\u0026quot; access in the menu.\\u003C/p\\u003E\\n\\n\\u003Ch3\\u003ETwitter API setup\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003ENext, you need to install the Twitter API library. I use Python for my bots and there are LOTS of python Twitter options. The one I settled on is python-twitter because it\\u0026#39;s well-documented and easy to use. You can just use:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Epip install python-twitter\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eand in your app, \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport twitter \\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch3\\u003ELogging in\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s not advisable to keep your OAuth params in your main app, but let\\u0026#39;s be honest, lots of devs are lazy. It\\u0026#39;s best to put them in a separate file and open the file in your app. For the example, I\\u0026#39;ll just keep it all in one file for simplicity. Just create 4 strings and copy/paste your API details from Twitter. It isn\\u0026#39;t necessary to specify your handle as it\\u0026#39;s linked to your OAuth. \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Econsumer_key = \\u0026#39;key\\u0026#39;\\nconsumer_secret = \\u0026#39;secret\\u0026#39;\\noauth_token = \\u0026#39;token\\u0026#39;\\noauth_secret = \\u0026#39;secret\\u0026#39;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThen just use the API login and store it in a variable much like we do for reddit:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Et = twitter.Api(consumer_key,consumer_secret,oauth_token,oauth_secret)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENow you\\u0026#39;re in! If you get an error from this, make sure you have your keys and permissions correct. You can use this function to verify your login:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eprint  t.VerifyCredentials()\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch3\\u003EImportant Functions\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EObviously, you want to tweet. Here\\u0026#39;s how:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Estatus = t.PostUpdate(string)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EBam. Super easy. If you want to be concise, create a method for tweeting and call it whenever you need:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Edef tweet(text): #posts a tweet\\n    t.PostUpdate(text)\\n\\ndef main(): #main method\\n    tweet(\\u0026quot;Yeah bitch I can tweet!\\u0026quot;)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ETo get a list of someone\\u0026#39;s tweets, you can use this function (with optional flags that I use):\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Estatuses = t.GetUserTimeline(screen_name=name,count=200,include_rts=False,exclude_replies = True)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EInclude_rts includes things the user retweeted, exclude_replies is self explanatory, and the count is the limit to grab. The max is 200 per request, which is plenty for my purposes. \\u003C/p\\u003E\\n\\n\\u003Ch3\\u003EFull sample program\\u003C/h3\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E#twitterbot.py\\nimport twitter\\n\\n\\u0026quot;\\u0026quot;\\u0026quot;login to twitter with oauth\\u0026quot;\\u0026quot;\\u0026quot;\\ndef login(): \\n    consumer_key = \\u0026#39;\\u0026#39;\\n    consumer_secret = \\u0026#39;\\u0026#39;\\n    oauth_token = \\u0026#39;\\u0026#39;\\n    oauth_secret = \\u0026#39;\\u0026#39;\\n    try:\\n        t = twitter.Api(consumer_key,consumer_secret,oauth_token,oauth_secret)\\n        print \\u0026quot;Logged in to twitter.\\u0026quot;\\n        return t\\n    except Exception as e:\\n        print e + \\u0026quot;Twitter login failed.\\u0026quot;\\n\\n\\u0026quot;\\u0026quot;\\u0026quot;new tweet\\u0026quot;\\u0026quot;\\u0026quot;\\ndef tweet(t,text):\\n    try:\\n        status = t.PostUpdate(text)\\n        print \\u0026quot;Tweet succeeded.\\u0026quot;\\n    except Exception as e:\\n        print str(e) + \\u0026quot;Tweet failed.\\u0026quot;\\n\\n\\u0026quot;\\u0026quot;\\u0026quot;already tweeted?\\u0026quot;\\u0026quot;\\u0026quot;\\ndef tweeted(tweet_text):\\n    statuses = t.GetUserTimeline(count=200,include_rts=False,exclude_replies = True)\\n    for status in statuses:\\n        if tweet_text == status.text:\\n            return True\\n    return False\\n\\n\\u0026quot;\\u0026quot;\\u0026quot;oauth session\\u0026quot;\\u0026quot;\\u0026quot;\\ndef authenticated():\\n    creds = t.VerifyCredentials()\\n    print creds\\n    return creds\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch3\\u003EIntegrate it with your code!\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EImport this sample program to your bot and you\\u0026#39;re done! Your bot is free as a little tweety bird! Now go annoy the internet with pointless shit from reddit! :D\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHappy to field any questions. Happy coding!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey guys, I recently researched how to get my bot, /u/HCE_Replacement_Bot, to post to Twitter. It turns out it's stupid easy, but I wanted to pass on the knowledge in case you wanted to do something similar. I created [@hcebot](http://twitter.com/hcebot), which posts highlights from /r/guns onto Twitter. So here's how we do it!\\n\\n###Account set up\\n\\nTwitter requires OAuth login, which is annoying but easy to handle. You just go to http://dev.twitter.com and register your Twitter handle as an application. You need to then create an OAuth key (literally just click the button) and then take 4 pieces of information for your bot: The consumer key, the consumer secret, the API key, and the API secret. If you want to tweet with your bot, you will need to change permissions to enable \\\"read/write\\\" access in the menu.\\n\\n###Twitter API setup\\n\\nNext, you need to install the Twitter API library. I use Python for my bots and there are LOTS of python Twitter options. The one I settled on is python-twitter because it's well-documented and easy to use. You can just use:\\n\\n    pip install python-twitter\\n\\nand in your app, \\n\\n    import twitter \\n\\n###Logging in\\n\\nIt's not advisable to keep your OAuth params in your main app, but let's be honest, lots of devs are lazy. It's best to put them in a separate file and open the file in your app. For the example, I'll just keep it all in one file for simplicity. Just create 4 strings and copy/paste your API details from Twitter. It isn't necessary to specify your handle as it's linked to your OAuth. \\n\\n    consumer_key = 'key'\\n    consumer_secret = 'secret'\\n    oauth_token = 'token'\\n    oauth_secret = 'secret'\\n\\nThen just use the API login and store it in a variable much like we do for reddit:\\n\\n    t = twitter.Api(consumer_key,consumer_secret,oauth_token,oauth_secret)\\n\\nNow you're in! If you get an error from this, make sure you have your keys and permissions correct. You can use this function to verify your login:\\n\\n    print  t.VerifyCredentials()\\n\\n\\n###Important Functions\\n\\nObviously, you want to tweet. Here's how:\\n\\n    status = t.PostUpdate(string)\\n\\nBam. Super easy. If you want to be concise, create a method for tweeting and call it whenever you need:\\n\\n    def tweet(text): #posts a tweet\\n        t.PostUpdate(text)\\n    \\n    def main(): #main method\\n        tweet(\\\"Yeah bitch I can tweet!\\\")\\n\\nTo get a list of someone's tweets, you can use this function (with optional flags that I use):\\n\\n    statuses = t.GetUserTimeline(screen_name=name,count=200,include_rts=False,exclude_replies = True)\\n\\nInclude_rts includes things the user retweeted, exclude_replies is self explanatory, and the count is the limit to grab. The max is 200 per request, which is plenty for my purposes. \\n\\n\\n###Full sample program\\n    #twitterbot.py\\n    import twitter\\n\\n    \\\"\\\"\\\"login to twitter with oauth\\\"\\\"\\\"\\n    def login(): \\n        consumer_key = ''\\n        consumer_secret = ''\\n        oauth_token = ''\\n        oauth_secret = ''\\n        try:\\n            t = twitter.Api(consumer_key,consumer_secret,oauth_token,oauth_secret)\\n            print \\\"Logged in to twitter.\\\"\\n            return t\\n        except Exception as e:\\n            print e + \\\"Twitter login failed.\\\"\\n\\n    \\\"\\\"\\\"new tweet\\\"\\\"\\\"\\n    def tweet(t,text):\\n        try:\\n            status = t.PostUpdate(text)\\n            print \\\"Tweet succeeded.\\\"\\n        except Exception as e:\\n            print str(e) + \\\"Tweet failed.\\\"\\n\\n    \\\"\\\"\\\"already tweeted?\\\"\\\"\\\"\\n    def tweeted(tweet_text):\\n        statuses = t.GetUserTimeline(count=200,include_rts=False,exclude_replies = True)\\n        for status in statuses:\\n            if tweet_text == status.text:\\n                return True\\n        return False\\n\\n    \\\"\\\"\\\"oauth session\\\"\\\"\\\"\\n    def authenticated():\\n        creds = t.VerifyCredentials()\\n        print creds\\n        return creds\\n\\n\\n###Integrate it with your code!\\n\\nImport this sample program to your bot and you're done! Your bot is free as a little tweety bird! Now go annoy the internet with pointless shit from reddit! :D\\n\\nHappy to field any questions. Happy coding!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"23faah\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Phteven_j\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1397897204.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/23faah/tutorial_how_to_get_your_bot_to_post_to_twitter/\", \"locked\": false, \"name\": \"t3_23faah\", \"created\": 1397924749.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/23faah/tutorial_how_to_get_your_bot_to_post_to_twitter/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Tutorial] How to get your bot to post to Twitter\", \"created_utc\": 1397895949.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"231wx9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"titanspybot\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/231wx9/can_somebody_explain_how_a_bot_sees_something/\", \"locked\": false, \"name\": \"t3_231wx9\", \"created\": 1397551471.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/231wx9/can_somebody_explain_how_a_bot_sees_something/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can somebody explain how a bot sees something like \\\"+/u/somebot OK\\\". Is it reading \\\"OK\\\"? Whats the point of the plus sign?\", \"created_utc\": 1397522671.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi everyone.  I was just tracking down why links to phoronix.com in \\u003Ca href=\\\"/r/linux\\\"\\u003E/r/linux\\u003C/a\\u003E never work right when using reddit sync.  Turns out it is related to \\u003Ca href=\\\"https://github.com/reddit/reddit/issues/283\\\"\\u003Ea two year old issue\\u003C/a\\u003E that is not going to be fixed.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs far as I can tell, Reddit uses \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/lib/filters.py?source=cc#L56\\\"\\u003EThis function\\u003C/a\\u003E for encoding json, which escapes \\u0026amp;,\\u0026lt;, and \\u0026gt;.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/JSON\\\"\\u003Ehttps://github.com/reddit/reddit/wiki/JSON\\u003C/a\\u003E Mentions that a few fields are escaped, but it would appear that all data is escaped.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFrom what I tested, clients don\\u0026#39;t url decode the url field that reddit returns.\\nfor example:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EIn [1]: import praw\\nIn [2]: r=praw.Reddit(user_agent=\\u0026#39;test\\u0026#39;)\\nIn [3]: thing=r.get_subreddit(\\u0026#39;linux\\u0026#39;).get_hot(limit=5).next()\\nIn [4]: thing.url\\nOut[4]: u\\u0026#39;http://www.phoronix.com/scan.php?page=news_item\\u0026amp;amp;px=MTU1NzA\\u0026#39;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENot sure what to do about this... minimally the documentation should be updated..\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi everyone.  I was just tracking down why links to phoronix.com in /r/linux never work right when using reddit sync.  Turns out it is related to [a two year old issue](https://github.com/reddit/reddit/issues/283) that is not going to be fixed.\\n\\nAs far as I can tell, Reddit uses [This function](https://github.com/reddit/reddit/blob/master/r2/r2/lib/filters.py?source=cc#L56) for encoding json, which escapes \\u0026,\\u003C, and \\u003E.\\n\\nhttps://github.com/reddit/reddit/wiki/JSON Mentions that a few fields are escaped, but it would appear that all data is escaped.\\n\\nFrom what I tested, clients don't url decode the url field that reddit returns.\\nfor example:\\n\\n    In [1]: import praw\\n    In [2]: r=praw.Reddit(user_agent='test')\\n    In [3]: thing=r.get_subreddit('linux').get_hot(limit=5).next()\\n    In [4]: thing.url\\n    Out[4]: u'http://www.phoronix.com/scan.php?page=news_item\\u0026amp;px=MTU1NzA'\\n\\nNot sure what to do about this... minimally the documentation should be updated..\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1u7553\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Justinsaccount\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1u7553/client_developers_i_think_most_all_of_your/\", \"locked\": false, \"name\": \"t3_1u7553\", \"created\": 1388656904.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1u7553/client_developers_i_think_most_all_of_your/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Client developers: I think most (all?) of your clients are subtly broken.\", \"created_utc\": 1388628104.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am looking for some open source reddit bots, to know more about how they are built.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am looking for some open source reddit bots, to know more about how they are built.\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ojsbq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"tomarina\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ojsbq/any_open_source_reddit_bots/\", \"locked\": false, \"name\": \"t3_1ojsbq\", \"created\": 1381921520.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1ojsbq/any_open_source_reddit_bots/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Any open source reddit bots ?\", \"created_utc\": 1381892720.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EDo you guys run your PRAW bots on a VPS or heroku or...?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Do you guys run your PRAW bots on a VPS or heroku or...?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ixqu0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"isolani\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 24, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/\", \"locked\": false, \"name\": \"t3_1ixqu0\", \"created\": 1374670160.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[PRAW] Where do you all host your python-based bots?\", \"created_utc\": 1374641360.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey guys!  I am going to put together a series of lessons for developers.  I will progress through the process of getting JSON data from Reddit\\u0026#39;s API, building a MySQL database and importing the Reddit data into a database.  With these lessons, you will be able to create a fully functional database using Reddit\\u0026#39;s API for your projects.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve just started coding in Python (I was previously a PERL programmer).  If you see any errors in the code, please let me know.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI hope these tutorials will help aspiring developers create awesome applications for other Redditors.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EToday\\u0026#39;s lesson is a simple one.  We will create a very basic Python script to retrieve Reddit submission data using the JSON format.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELet\\u0026#39;s dive in!  For these lessons, I will assume you are using a linux distribution (Ubuntu, Fedora, etc.)  If you are running windows, I would recommend that you download Virtualbox (Free Oracle product) and install Ubuntu under Virtualbox.  Virtualbox is a nice virtual machine which allows you to run a complete OS under your existing OS.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYou can learn more about Virtualbox and download it from \\u003Ca href=\\\"https://www.virtualbox.org/\\\"\\u003Ehere\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYou can download the install files (\\u003Cem\\u003Eboth 32 and 64 bit versions\\u003C/em\\u003E) for Ubuntu \\u003Ca href=\\\"http://www.ubuntu.com/download/desktop\\\"\\u003Ehere\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EOnce you have Ubuntu installed, you can install MySQL (if it is not already installed) by using these commands:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esudo apt-get update\\nsudo apt-get dist-upgrade\\nsudo apt-get install mysql-server mysql-client\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EYou can install phpmyadmin for Ubuntu and MySQL using this command:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esudo apt-get install phpmyadmin\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThe following code is very simplistic.  It authenticates with Reddit\\u0026#39;s server and makes requests for submission data.  Each request will return up to 100 submissions.  It is important that you change the setting in your Reddit profile to receive 100 listings per request.  The other alternative is to pass a URL variable with your GET request using the limit variable.  In this script, I have hard-coded the limit to the maximum of 100. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis is a very basic script with virtually no error control, so you may want to add additional error handling routines in your project.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EHere is the code for the Python script:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E#!/usr/bin/python\\n#\\n# Script Name: getRedditJSONSubmissionData.py\\n# Usage: ./getRedditJSONSubmissionData.py \\u0026gt; redditData.json\\n# ----------------------------------------------------------------------------------------------------------------\\n# This script will average one request every two seconds.  If the servers return data faster, you might\\n# need to change the sleep time to avoid going over the API limits.  \\n# Also, make sure you change the settings in your Reddit account to get 100 objects at a time.  You can\\n# also use the URL variable \\u0026quot;limit=100\\u0026quot; (it might be count=100?)\\n#\\n# Also, the code to handle errors if a non-status 200 response is received should be improved to \\n# eventually stop requesting after X amount of failures -- this might happen if Reddit\\u0026#39;s servers go down\\n# for an extended time period.\\n# ----------------------------------------------------------------------------------------------------------------\\n\\nimport requests\\nimport json\\nimport time\\nimport sys\\n\\nuser_pass_dict = {\\u0026#39;user\\u0026#39;: \\u0026#39;your_user_name\\u0026#39;,\\n              \\u0026#39;passwd\\u0026#39;: \\u0026#39;your_password\\u0026#39;,\\n              \\u0026#39;api_type\\u0026#39;: \\u0026#39;json\\u0026#39;}\\ns = requests.Session()\\ns.headers.update({\\u0026#39;User-Agent\\u0026#39; : \\u0026#39;short_description_of_yourself user:your_user_name\\u0026#39;})\\nr = s.post(r\\u0026#39;http://www.reddit.com/api/login\\u0026#39;, data=user_pass_dict)\\nj = json.loads(r.content)\\nafter = \\u0026quot;\\u0026quot; # Set this to a T3 object to start at a specific point or leave blank to start with the most recent submissions\\n\\nwhile True:\\n    time.sleep(1) # Sleep for one second to avoid going over API limits\\n    url = \\u0026quot;http://www.reddit.com/r/all/new/.json?limit=100\\u0026amp;after=\\u0026quot; + after\\n    html = s.get(url) # Make request to Reddit API\\n    if html.status_code != 200:  # This error handing is extremely basic.  Please improve.\\n        # Error handing block\\n        sys.stderr.write(str(html.status_code)) # Print HTTP error status code to STDOUT\\n        sys.stderr.write(url)\\n        continue\\n        # End Error handling block\\n    print html.content # Print the JSON object \\n    after = decode[\\u0026#39;data\\u0026#39;][\\u0026#39;after\\u0026#39;]  # Update after variable to receive the next batch of submissions in this loop\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey guys!  I am going to put together a series of lessons for developers.  I will progress through the process of getting JSON data from Reddit's API, building a MySQL database and importing the Reddit data into a database.  With these lessons, you will be able to create a fully functional database using Reddit's API for your projects.\\n\\nI've just started coding in Python (I was previously a PERL programmer).  If you see any errors in the code, please let me know.  \\n\\nI hope these tutorials will help aspiring developers create awesome applications for other Redditors.  \\n\\nToday's lesson is a simple one.  We will create a very basic Python script to retrieve Reddit submission data using the JSON format.  \\n\\nLet's dive in!  For these lessons, I will assume you are using a linux distribution (Ubuntu, Fedora, etc.)  If you are running windows, I would recommend that you download Virtualbox (Free Oracle product) and install Ubuntu under Virtualbox.  Virtualbox is a nice virtual machine which allows you to run a complete OS under your existing OS.  \\n\\nYou can learn more about Virtualbox and download it from [here](https://www.virtualbox.org/).\\n\\nYou can download the install files (*both 32 and 64 bit versions*) for Ubuntu [here](http://www.ubuntu.com/download/desktop).\\n\\n**Once you have Ubuntu installed, you can install MySQL (if it is not already installed) by using these commands:**\\n\\n    sudo apt-get update\\n    sudo apt-get dist-upgrade\\n    sudo apt-get install mysql-server mysql-client\\n\\n**You can install phpmyadmin for Ubuntu and MySQL using this command:**\\n\\n    sudo apt-get install phpmyadmin\\n\\nThe following code is very simplistic.  It authenticates with Reddit's server and makes requests for submission data.  Each request will return up to 100 submissions.  It is important that you change the setting in your Reddit profile to receive 100 listings per request.  The other alternative is to pass a URL variable with your GET request using the limit variable.  In this script, I have hard-coded the limit to the maximum of 100. \\n\\nThis is a very basic script with virtually no error control, so you may want to add additional error handling routines in your project.\\n\\n**Here is the code for the Python script:**\\n\\n    #!/usr/bin/python\\n    #\\n    # Script Name: getRedditJSONSubmissionData.py\\n    # Usage: ./getRedditJSONSubmissionData.py \\u003E redditData.json\\n    # ----------------------------------------------------------------------------------------------------------------\\n    # This script will average one request every two seconds.  If the servers return data faster, you might\\n    # need to change the sleep time to avoid going over the API limits.  \\n    # Also, make sure you change the settings in your Reddit account to get 100 objects at a time.  You can\\n    # also use the URL variable \\\"limit=100\\\" (it might be count=100?)\\n    #\\n    # Also, the code to handle errors if a non-status 200 response is received should be improved to \\n    # eventually stop requesting after X amount of failures -- this might happen if Reddit's servers go down\\n    # for an extended time period.\\n    # ----------------------------------------------------------------------------------------------------------------\\n\\n    import requests\\n    import json\\n    import time\\n    import sys\\n\\n    user_pass_dict = {'user': 'your_user_name',\\n                  'passwd': 'your_password',\\n                  'api_type': 'json'}\\n    s = requests.Session()\\n    s.headers.update({'User-Agent' : 'short_description_of_yourself user:your_user_name'})\\n    r = s.post(r'http://www.reddit.com/api/login', data=user_pass_dict)\\n    j = json.loads(r.content)\\n    after = \\\"\\\" # Set this to a T3 object to start at a specific point or leave blank to start with the most recent submissions\\n\\n    while True:\\n        time.sleep(1) # Sleep for one second to avoid going over API limits\\n        url = \\\"http://www.reddit.com/r/all/new/.json?limit=100\\u0026after=\\\" + after\\n        html = s.get(url) # Make request to Reddit API\\n        if html.status_code != 200:  # This error handing is extremely basic.  Please improve.\\n            # Error handing block\\n            sys.stderr.write(str(html.status_code)) # Print HTTP error status code to STDOUT\\n            sys.stderr.write(url)\\n            continue\\n            # End Error handling block\\n        print html.content # Print the JSON object \\n        after = decode['data']['after']  # Update after variable to receive the next batch of submissions in this loop\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1c75dh\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"aphexcoil\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1365777666.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1c75dh/lesson_1_how_to_get_submission_data_from_reddits/\", \"locked\": false, \"name\": \"t3_1c75dh\", \"created\": 1365790908.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1c75dh/lesson_1_how_to_get_submission_data_from_reddits/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Lesson 1: How to get submission data from Reddit's API\", \"created_utc\": 1365762108.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ENow, this is a really REALLY rough sketch, so don\\u0026#39;t worry, I\\u0026#39;m not an idiot who expects the modmail to exactly look like this. It\\u0026#39;s just an example of how it could be more convenient, possibly. I\\u0026#39;m still gonna work on making the borders look better and such, but for now I just wanted to make the idea a bit clear.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://redditstory.zxq.net/index.html\\\"\\u003Ehttp://redditstory.zxq.net/index.html\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOnly the link to /r/braveryjerk shows.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EI personally think having an UI like this, where you can more easily pick the modmail thread you want to read would be a great improvement. It might also save the modmail a lot of lag, when threads get really big. (No other threads would be loaded.)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Now, this is a really REALLY rough sketch, so don't worry, I'm not an idiot who expects the modmail to exactly look like this. It's just an example of how it could be more convenient, possibly. I'm still gonna work on making the borders look better and such, but for now I just wanted to make the idea a bit clear.\\n\\nhttp://redditstory.zxq.net/index.html\\n\\nOnly the link to \\\\/r/braveryjerk shows.\\n\\n______\\n\\nI personally think having an UI like this, where you can more easily pick the modmail thread you want to read would be a great improvement. It might also save the modmail a lot of lag, when threads get really big. (No other threads would be loaded.)\\n\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"18yfg7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"AerateMark\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/18yfg7/im_working_on_a_prototype_wo_functionality_yet_of/\", \"locked\": false, \"name\": \"t3_18yfg7\", \"created\": 1361487671.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/18yfg7/im_working_on_a_prototype_wo_functionality_yet_of/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I'm working on a prototype (w/o functionality yet) of a revamp of the modmail UI.\", \"created_utc\": 1361458871.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"16qmq2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bboe\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/16qmq2/praw_20_with_reddit_oauth2_support_has_been/\", \"locked\": false, \"name\": \"t3_16qmq2\", \"created\": 1358433239.0, \"url\": \"https://github.com/praw-dev/praw/wiki/Changelog\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"PRAW 2.0 with reddit OAuth2 support has been released!\", \"created_utc\": 1358404439.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Erecently, any time i try to use ssl-enabled reddit ive been getting \\u0026quot;Service Unavailable  The server is temporarily unable to service your request. Please try again later.\\u0026quot; with a reference code.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ei havent changed anything on my end, and its happening in multiple browsers.  going to ssl.reddit.come gets me a message about being a bad robot and a link to the api.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ehas reddit made some ssl-related changes that anyones aware of?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"recently, any time i try to use ssl-enabled reddit ive been getting \\\"Service Unavailable  The server is temporarily unable to service your request. Please try again later.\\\" with a reference code.\\n\\ni havent changed anything on my end, and its happening in multiple browsers.  going to ssl.reddit.come gets me a message about being a bad robot and a link to the api.\\n\\nhas reddit made some ssl-related changes that anyones aware of?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"155hs3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bonoboho\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 28, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/155hs3/recent_problem_with_sslenabled_reddit/\", \"locked\": false, \"name\": \"t3_155hs3\", \"created\": 1356008967.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/155hs3/recent_problem_with_sslenabled_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"recent problem with ssl-enabled reddit\", \"created_utc\": 1355980167.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"14ili4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ClockStalker-creator\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/14ili4/code_for_the_nowdefunct_clockstalker_bot_now/\", \"locked\": false, \"name\": \"t3_14ili4\", \"created\": 1355032677.0, \"url\": \"https://github.com/ClockStalker/clockstalker\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Code for the now-defunct ClockStalker bot now available on GitHub\", \"created_utc\": 1355003877.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve looked at the source code, and it seems from \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/models/vote.py#L275\\\"\\u003Ethis method\\u003C/a\\u003E (not sure if this the only way that votes are queried) that a database \\u003Ccode\\u003E_fast_query\\u003C/code\\u003E is used to get all of a user\\u0026#39;s votes for a set of \\u0026quot;things.\\u0026quot; Was there a design decision made to offload these queries to the database every time a user loads a page, instead of caching all votes a user has made in a single key-value pair per user, and selecting from this list in the frontend servers? My intuition would be that if the database is the bottleneck, the latter choice might make page loads quicker. Or is this just premature optimization?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've looked at the source code, and it seems from [this method](https://github.com/reddit/reddit/blob/master/r2/r2/models/vote.py#L275) (not sure if this the only way that votes are queried) that a database `_fast_query` is used to get all of a user's votes for a set of \\\"things.\\\" Was there a design decision made to offload these queries to the database every time a user loads a page, instead of caching all votes a user has made in a single key-value pair per user, and selecting from this list in the frontend servers? My intuition would be that if the database is the bottleneck, the latter choice might make page loads quicker. Or is this just premature optimization?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"x666h\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"btown_brony\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/x666h/why_doesnt_reddit_denormalize_all_votes_a_user/\", \"locked\": false, \"name\": \"t3_x666h\", \"created\": 1343303023.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/x666h/why_doesnt_reddit_denormalize_all_votes_a_user/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why doesn't Reddit denormalize all votes a user has made?\", \"created_utc\": 1343274223.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI was thinking that since reddit is becoming such a bigger and bigger deal, it would be nice to have for messages or what not.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I was thinking that since reddit is becoming such a bigger and bigger deal, it would be nice to have for messages or what not.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"uo4qy\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"quinbd\", \"media\": null, \"score\": 15, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/uo4qy/any_hope_for_a_push_api_soon/\", \"locked\": false, \"name\": \"t3_uo4qy\", \"created\": 1339028358.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/uo4qy/any_hope_for_a_push_api_soon/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Any hope for a push API soon?\", \"created_utc\": 1338999558.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 15}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe bot (\\u003Ca href=\\\"http://www.reddit.com/user/narwal_bot\\\"\\u003Enarwal_bot\\u003C/a\\u003E) aggregates an IAMA host\\u0026#39;s comments, formats them into an easy-to-read list, and posts the compilation into the IAMA as a chain of comments.  It does this for every \\u0026quot;hot\\u0026quot; IAMA that has (or will likely have) over 200 comments.  I have it set to run every hour.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo I want to open source it for the usual reasons, but I\\u0026#39;m hesitant that it\\u0026#39;ll be a cause of spam, since having more than one person run the script would most definitely be annoying.  Please let me know your thoughts and opinions.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E(Aside: I wrote the bot primarily to build up karma for my narwal_bot account, which I intend to use to test POST functions of \\u003Ca href=\\\"https://github.com/larryng/narwal\\\"\\u003Enarwal\\u003C/a\\u003E, my open source python reddit API wrapper.  I debuted narwal \\u003Ca href=\\\"http://www.reddit.com/r/programming/comments/sczy5/narwal_a_simple_and_concise_python_wrapper_for/\\\"\\u003Etwo weeks ago in r/programming\\u003C/a\\u003E, not knowing this subreddit exists.  I hope you guys check it out and find it useful in your own projects.  Note that I still consider it in alpha stages since I haven\\u0026#39;t created a test suite to test POSTs yet.)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: bad link\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The bot ([narwal_bot](http://www.reddit.com/user/narwal_bot)) aggregates an IAMA host's comments, formats them into an easy-to-read list, and posts the compilation into the IAMA as a chain of comments.  It does this for every \\\"hot\\\" IAMA that has (or will likely have) over 200 comments.  I have it set to run every hour.\\n\\nSo I want to open source it for the usual reasons, but I'm hesitant that it'll be a cause of spam, since having more than one person run the script would most definitely be annoying.  Please let me know your thoughts and opinions.\\n\\n(Aside: I wrote the bot primarily to build up karma for my narwal_bot account, which I intend to use to test POST functions of [narwal](https://github.com/larryng/narwal), my open source python reddit API wrapper.  I debuted narwal [two weeks ago in r/programming](http://www.reddit.com/r/programming/comments/sczy5/narwal_a_simple_and_concise_python_wrapper_for/), not knowing this subreddit exists.  I hope you guys check it out and find it useful in your own projects.  Note that I still consider it in alpha stages since I haven't created a test suite to test POSTs yet.)\\n\\nEdit: bad link\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"t067f\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"larryng\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/t067f/would_it_be_irresponsible_of_me_to_open_source_my/\", \"locked\": false, \"name\": \"t3_t067f\", \"created\": 1335841778.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/t067f/would_it_be_irresponsible_of_me_to_open_source_my/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Would it be irresponsible of me to open source my IAMA bot?\", \"created_utc\": 1335812978.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m looking at the reddit API and I can\\u0026#39;t seem to figure out how to get more comments from a post after the initial 50. The text below is from the reddit API:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cem\\u003EFetching more\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cem\\u003EIf you\\u0026#39;re fetching comments from a thread with more comments than the API will return in a single response, the last comment will look like this:\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cem\\u003E{\\u0026#39;data\\u0026#39;: {\\u0026#39;id\\u0026#39;: \\u0026#39;abc1010\\u0026#39;, \\u0026#39;name\\u0026#39;: \\u0026#39;t1_abc1010\\u0026#39;}, \\u0026#39;kind\\u0026#39;: \\u0026#39;more\\u0026#39;}\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cem\\u003ETo get these comments, you can fetch the url \\u003Ca href=\\\"http://reddit.com/comments/FULLNAME/abc1010.json\\\"\\u003Ehttp://reddit.com/comments/FULLNAME/abc1010.json\\u003C/a\\u003E, where FULLNAME is the FULLNAME of the story.\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo use as an example for my problem, when I get the json from:\\n\\u003Ca href=\\\"http://www.reddit.com/r/funny/comments/o0pk5/monopoly_is_an_old_game/.json\\\"\\u003Ehttp://www.reddit.com/r/funny/comments/o0pk5/monopoly_is_an_old_game/.json\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am able to see the first 50 parent comments. At the end of the JSON Array for these comments, there is another item labeled \\u0026quot;more\\u0026quot; which has an id of \\u0026quot;c3dibkp\\u0026quot;. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo according to the API, I should be able to fetch more comments by accessing:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://reddit.com/comments/FULLNAME/c3dibkp.json\\\"\\u003Ehttp://reddit.com/comments/FULLNAME/c3dibkp.json\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m guessing that FULLNAME should be equal to the id of the original story, which is \\u0026quot;o0pk5\\u0026quot;.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHowever I cannot seem to get the correct results using:\\n\\u003Ca href=\\\"http://reddit.com/comments/o0pk5/c3dibkp.json\\\"\\u003Ehttp://reddit.com/comments/o0pk5/c3dibkp.json\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat am I doing wrong?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm looking at the reddit API and I can't seem to figure out how to get more comments from a post after the initial 50. The text below is from the reddit API:\\n\\n\\n*Fetching more*\\n\\n*If you're fetching comments from a thread with more comments than the API will return in a single response, the last comment will look like this:*\\n\\n*{'data': {'id': 'abc1010', 'name': 't1_abc1010'}, 'kind': 'more'}*\\n\\n*To get these comments, you can fetch the url http://reddit.com/comments/FULLNAME/abc1010.json, where FULLNAME is the FULLNAME of the story.*\\n\\n\\nTo use as an example for my problem, when I get the json from:\\nhttp://www.reddit.com/r/funny/comments/o0pk5/monopoly_is_an_old_game/.json\\n\\nI am able to see the first 50 parent comments. At the end of the JSON Array for these comments, there is another item labeled \\\"more\\\" which has an id of \\\"c3dibkp\\\". \\n\\nSo according to the API, I should be able to fetch more comments by accessing:\\n\\nhttp://reddit.com/comments/FULLNAME/c3dibkp.json\\n\\nI'm guessing that FULLNAME should be equal to the id of the original story, which is \\\"o0pk5\\\".\\n\\nHowever I cannot seem to get the correct results using:\\nhttp://reddit.com/comments/o0pk5/c3dibkp.json\\n\\nWhat am I doing wrong?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"o1w7b\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"hyperpowerr\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/o1w7b/how_do_i_get_more_comments_in_json_format/\", \"locked\": false, \"name\": \"t3_o1w7b\", \"created\": 1325671169.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/o1w7b/how_do_i_get_more_comments_in_json_format/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How do I get more comments in JSON format?\", \"created_utc\": 1325642369.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI was watching this video: \\u003Ca href=\\\"http://thinkvitamin.com/code/steve-huffman-on-lessons-learned-at-reddit/\\\"\\u003Ehttp://thinkvitamin.com/code/steve-huffman-on-lessons-learned-at-reddit/\\u003C/a\\u003E and at 9:40 (about Lesson 3) Steve talks about how data is stored in a non-relational way.  This ideology really intrigued me as it seems like the perfect way to eliminate schema changes.  But I do have some questions about it hopefully someone more experienced than I can answer:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EHe says there are no more joins using this design. Does he mean that completely? Wouldn\\u0026#39;t it make sense to join the Things table to the Data table on ThingID?\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EI related this to objects in my mind where the Things table could be renamed the Objects table and the Data table holds all the object\\u0026#39;s fields/members/attributes.  Also, to make searches faster, you would need to index the Type column in Things, but indexing varchars is inefficient so you could instead index TypeID (integer) to improve performance.  This would require a 3rd table to map TypeID to it\\u0026#39;s text name.  This table could be called Classes, so our structure would go Class-\\u0026gt;Object-\\u0026gt;Data.  My question is, is that a more efficient way of doing this, and is it OK or a bad idea to join these three tables in each query, where you say need to select all objects of class User (for example)?\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EObviously since this structure uses only 2 (or 3 if I make my Classes table) tables these tables (and their indices) are going to get monstrous.  What performance issues are there with tables so big? I\\u0026#39;m pretty sure there is a max table size (even if it\\u0026#39;s the OS\\u0026#39;s maximum file size) so how do you split these up? When they do get split up, how do you determine what goes where (obviously it\\u0026#39;s up to the developer but what is the most optimal solution)?  And when things are split up across different boxes, how do you know where to look later?\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EIs MySQL a suitable platform for this type of database? If so, what database engine would be most efficient?\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m sorry for such wordy questions, I\\u0026#39;m still turning the concept over in my mind.  It seems like such a simple solution to a complex problem but I worry about its limitations.  Million thanks to whoever helps shed some light on this.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I was watching this video: [http://thinkvitamin.com/code/steve-huffman-on-lessons-learned-at-reddit/](http://thinkvitamin.com/code/steve-huffman-on-lessons-learned-at-reddit/) and at 9:40 (about Lesson 3) Steve talks about how data is stored in a non-relational way.  This ideology really intrigued me as it seems like the perfect way to eliminate schema changes.  But I do have some questions about it hopefully someone more experienced than I can answer:\\n\\n1.  He says there are no more joins using this design. Does he mean that completely? Wouldn't it make sense to join the Things table to the Data table on ThingID?\\n\\n2.  I related this to objects in my mind where the Things table could be renamed the Objects table and the Data table holds all the object's fields/members/attributes.  Also, to make searches faster, you would need to index the Type column in Things, but indexing varchars is inefficient so you could instead index TypeID (integer) to improve performance.  This would require a 3rd table to map TypeID to it's text name.  This table could be called Classes, so our structure would go Class-\\u003EObject-\\u003EData.  My question is, is that a more efficient way of doing this, and is it OK or a bad idea to join these three tables in each query, where you say need to select all objects of class User (for example)?\\n\\n3.  Obviously since this structure uses only 2 (or 3 if I make my Classes table) tables these tables (and their indices) are going to get monstrous.  What performance issues are there with tables so big? I'm pretty sure there is a max table size (even if it's the OS's maximum file size) so how do you split these up? When they do get split up, how do you determine what goes where (obviously it's up to the developer but what is the most optimal solution)?  And when things are split up across different boxes, how do you know where to look later?\\n\\n4.  Is MySQL a suitable platform for this type of database? If so, what database engine would be most efficient?\\n\\nI'm sorry for such wordy questions, I'm still turning the concept over in my mind.  It seems like such a simple solution to a complex problem but I worry about its limitations.  Million thanks to whoever helps shed some light on this.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"n4i9b\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"streetc0de\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/n4i9b/question_about_reddits_thingdata_database_design/\", \"locked\": false, \"name\": \"t3_n4i9b\", \"created\": 1323352987.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/n4i9b/question_about_reddits_thingdata_database_design/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Question about Reddit's Thing-\\u003EData database design\", \"created_utc\": 1323324187.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI believe normally, the max is 500 comments to load at a time and for Gold members it is 1000 comments to load at a time. Then you could hit load more comments the bottom of the page. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENavigating comment threads that are this large can be difficult from a user-interface stand point as well as far as server load. With a 1000 comments, it can take several minutes to load a page. For a self-post, like for IAMA posts, it can be extremely difficult to manage, especially just going through the inbox. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo I wanted to hear your thoughts if there are steps we could take, either from an interface standpoint or loading time, that we could try to alleviate the concern. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI believe as Reddit gets more traffic, these popular posts could easily hit the 10,000 mark. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMaybe we could make an outside application, like a control panel, to aid the management of self.posts for the OP? Maybe we just need some CSS or a greasemonkey script from the user-side? Or maybe we need to edit some code? What about a new sort-method for comments (like with Best, New, Controversial, Top)? I don\\u0026#39;t really have any bright ideas at the moment. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: We might need to make some visual examples with some perf-testing to show how some of our ideas could work. Don\\u0026#39;t want to re-invent the wheel. \\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E\\u0026lt;@ketralnis\\u0026gt; Just a Listing(CommentsBuilder).render() in a loop\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EAlso, the code in question:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Ca href=\\\"http://code.reddit.com/browser/r2/r2/models/_builder.pyx\\\"\\u003Ehttp://code.reddit.com/browser/r2/r2/models/_builder.pyx\\u003C/a\\u003E (CommentsBuilder)\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ca href=\\\"http://code.reddit.com/browser/r2/r2/controllers/api.py#L1554\\\"\\u003Ehttp://code.reddit.com/browser/r2/r2/controllers/api.py#L1554\\u003C/a\\u003E (POST_morechildren)\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ca href=\\\"http://code.reddit.com/browser/r2/r2/lib/comment_tree.py\\\"\\u003Ehttp://code.reddit.com/browser/r2/r2/lib/comment_tree.py\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EEdit2: This thread may be a good playground to come up with ideas and test (11,555 comments): \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/r/blog/comments/d14xg/everyone_on_team_reddit_would_like_to_raise_a/\\\"\\u003Ehttp://www.reddit.com/r/blog/comments/d14xg/everyone_on_team_reddit_would_like_to_raise_a/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I believe normally, the max is 500 comments to load at a time and for Gold members it is 1000 comments to load at a time. Then you could hit load more comments the bottom of the page. \\n\\nNavigating comment threads that are this large can be difficult from a user-interface stand point as well as far as server load. With a 1000 comments, it can take several minutes to load a page. For a self-post, like for IAMA posts, it can be extremely difficult to manage, especially just going through the inbox. \\n\\nSo I wanted to hear your thoughts if there are steps we could take, either from an interface standpoint or loading time, that we could try to alleviate the concern. \\n\\nI believe as Reddit gets more traffic, these popular posts could easily hit the 10,000 mark. \\n\\nMaybe we could make an outside application, like a control panel, to aid the management of self.posts for the OP? Maybe we just need some CSS or a greasemonkey script from the user-side? Or maybe we need to edit some code? What about a new sort-method for comments (like with Best, New, Controversial, Top)? I don't really have any bright ideas at the moment. \\n\\n\\nEdit: We might need to make some visual examples with some perf-testing to show how some of our ideas could work. Don't want to re-invent the wheel. \\n\\n\\u003E \\u003C@ketralnis\\u003E Just a Listing(CommentsBuilder).render() in a loop\\n\\nAlso, the code in question:\\n\\n\\u003E * http://code.reddit.com/browser/r2/r2/models/_builder.pyx (CommentsBuilder)\\n\\u003E * http://code.reddit.com/browser/r2/r2/controllers/api.py#L1554 (POST_morechildren)\\n\\u003E * http://code.reddit.com/browser/r2/r2/lib/comment_tree.py\\n\\nEdit2: This thread may be a good playground to come up with ideas and test (11,555 comments): \\n\\nhttp://www.reddit.com/r/blog/comments/d14xg/everyone_on_team_reddit_would_like_to_raise_a/\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"dl9e2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 41, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/dl9e2/thoughts_how_can_we_best_approach_handling/\", \"locked\": false, \"name\": \"t3_dl9e2\", \"created\": 1285920893.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/dl9e2/thoughts_how_can_we_best_approach_handling/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Thoughts: How can we best approach handling comments in the 1000s? \", \"created_utc\": 1285892093.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhen I work with web apps, one page can have a couple of queries with a couple of joins.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBut with reddit, it seems like there is a mountain of information retrieved per user.  How is it done?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs it done at the query level or mostly done by the application?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor example, just looking at the most visible stuff:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003EPer user, get the subreddits?\\u003C/li\\u003E\\n\\u003Cli\\u003EPer user, get the top links per the user selected subreddits\\u003C/li\\u003E\\n\\u003Cli\\u003EHave the ticker bar populate with the top links per the subreddits\\u003C/li\\u003E\\n\\u003Cli\\u003EHave the user selected last view links\\u003C/li\\u003E\\n\\u003Cli\\u003EHighlight the friends\\u003C/li\\u003E\\n\\u003Cli\\u003EShow the user\\u0026#39;s karma\\u003C/li\\u003E\\n\\u003Cli\\u003EShow the user\\u0026#39;s messages\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EDoes some have a quick overview how reddit can do this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"When I work with web apps, one page can have a couple of queries with a couple of joins.\\n\\nBut with reddit, it seems like there is a mountain of information retrieved per user.  How is it done?\\n\\nIs it done at the query level or mostly done by the application?\\n\\nFor example, just looking at the most visible stuff:\\n\\n1. Per user, get the subreddits?\\n2. Per user, get the top links per the user selected subreddits\\n3. Have the ticker bar populate with the top links per the subreddits\\n4. Have the user selected last view links\\n5. Highlight the friends\\n6. Show the user's karma\\n7. Show the user's messages\\n\\nDoes some have a quick overview how reddit can do this?\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"9fpco\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"berlinbrown\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 16, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/9fpco/noob_question_how_do_you_join_on_so_much_data/\", \"locked\": false, \"name\": \"t3_9fpco\", \"created\": 1251715408.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/9fpco/noob_question_how_do_you_join_on_so_much_data/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Noob question: how do you join on so much data? \", \"created_utc\": 1251686608.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo, you have a functioning reddit instance, but you\\u0026#39;d like to mess with it where it won\\u0026#39;t piss people off or generally make a mess.  I just worked out the following procedure for cloning a live reddit instance onto a staging machine.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EWarning:\\u003C/strong\\u003E I am not intimately familiar with reddit\\u0026#39;s code and can offer no guarantees; and while I\\u0026#39;m reasonably conversant with MySQL, I\\u0026#39;m pretty new to PostgreSQL and Cassandra, so there may well be better ways to do things than I show here.  I can only say that, after a \\u003Cem\\u003Every\\u003C/em\\u003E cursory glance, my clone appears to work.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe following assumes that:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003Eyou have a functioning instance on the target machine,\\u003C/li\\u003E\\n\\u003Cli\\u003Eyour target is a \\u003Cstrong\\u003Eseparate\\u003C/strong\\u003E machine (or VM) and \\u003Cstrong\\u003Eyou don\\u0026#39;t care about any existing data on it\\u003C/strong\\u003E (I don\\u0026#39;t think it\\u0026#39;s feasible to run two independent instances of reddit on the one machine, not without a goodly amount of customisation of quite a lot of reddit\\u0026#39;s service dependencies, anyway; and if you\\u0026#39;re capable of that, you don\\u0026#39;t need what\\u0026#39;s in this post).\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EIf you\\u0026#39;ve yet to set up reddit on the target, go ahead and do so in the usual way.  After it\\u0026#39;s installed, copy your production \\u003Ccode\\u003Er2/development.update\\u003C/code\\u003E to the target, edit the domain keywords and run \\u003Ccode\\u003Emake ini\\u003C/code\\u003E (because you never edit INI files directly, right?)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENB: Though you can rename an instance just by changing the domain keywords, there will be some database content that still refers to the old domain, such as media.  You can probably do a search-and-replace for such if it\\u0026#39;s really important, but then you\\u0026#39;ll also have to copy across said media to the target machine.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EJust be aware that the clone is not \\u003Cem\\u003Eentirely\\u003C/em\\u003E independent of the original and some things will break if the original disappeared.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003EBack up your live instance\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003ETo do it properly, there are three things that need to be copied: PostgreSQL, Cassandra and RabbitMQ.  If your instance is quiet, chances are that your message queues are fully consumed anyway and, if not, all that\\u0026#39;ll happen is that some actions will not make it through to the clone.  It seemed too much bother, so I didn\\u0026#39;t.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThere are several ways to dump databases from a running pgsql server, but I wanted to be sure that the target was an exact clone of the source.  I tried \\u003Ccode\\u003Epg_dumpall -c\\u003C/code\\u003E but restoring such a dump is not so straight forward (because of pre-existing relationships and keys, despite there being a DROP DATABASE command in the resultant dump).  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI therefore used \\u003Ccode\\u003Epg_basebackup\\u003C/code\\u003E, which requires a bit of set up first.\\u003C/p\\u003E\\n\\n\\u003Ch3\\u003EPreliminaries\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EIf you haven\\u0026#39;t already done this:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E Edit \\u003Ccode\\u003E/etc/postgresql/9.3/main/postgresql.conf\\u003C/code\\u003E; uncomment the line \\u003Ccode\\u003Emax_wal_senders\\u003C/code\\u003E and set it to 2; uncomment \\u003Ccode\\u003Ewal_level\\u003C/code\\u003E and set it to \\u003Ccode\\u003Ehot_standby\\u003C/code\\u003E;\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EEdit \\u003Ccode\\u003E/etc/postgresql/9.3/main/pg_hba.conf\\u003C/code\\u003E and uncomment the line near the bottom that reads:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003Elocal   replication     postgres                                peer\\u003C/code\\u003E\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003ERestart PostgreSQL and (because that\\u0026#39;ll break reddit\\u0026#39;s connection) reddit also (NB: this will temporarily interrupt service to your production instance):\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003Esudo service postgresql restart; sudo reddit-restart\\u003C/code\\u003E\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Ch3\\u003EMake backups\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003ENB: \\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Ccode\\u003Eprod-clone\\u003C/code\\u003E is an arbitrary name which you can replace with anything you like (and will need to if you don\\u0026#39;t clean up old Cassandra snapshots).\\u003C/li\\u003E\\n\\u003Cli\\u003EWhen I did this, I also cloned the \\u003Ccode\\u003Esystem\\u003C/code\\u003E Cassandra keyspace, just to be sure.  Since you probably don\\u0026#39;t have to do this and doing so can be dangerous (for example, should the two Cassandra nodes end up in the same cluster), I\\u0026#39;ve removed it from the example below.  (To add it back, add \\u003Ccode\\u003Esystem\\u003C/code\\u003E to the invocation of \\u003Ccode\\u003Enodetool\\u003C/code\\u003E).  If you have trouble with authentication, though, you may need to clone \\u003Ccode\\u003Esystem\\u003C/code\\u003E or parts of it, or else mess with Cassandra on the target machine.  Clone \\u003Ccode\\u003Esystem\\u003C/code\\u003E at your own risk (even though you wouldn\\u0026#39;t use a machine that mattered for staging anyway, right?)\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EExecute:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esudo nodetool snapshot -t prod-clone reddit \\u0026amp;\\u0026amp; sudo -upostgres pg_basebackup -D - -Ft -x \\u0026gt; ~/pg-prod-clone.tar\\ncd /var/lib/cassandra\\nfind -iname prod-clone | tar cvf ~/cas-prod-clone.tar -T-\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ECopy backups \\u003Ccode\\u003E~/pg-prod-clone.tar\\u003C/code\\u003E and \\u003Ccode\\u003E~/cas-prod-clone.tar\\u003C/code\\u003E to the target machine.  Compress them first if they\\u0026#39;re large.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003ERestore your backups\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EAssuming that you\\u0026#39;ve copied your backups to your home directory, execute (as root):\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E# Restore Cassandra\\nservice cassandra stop\\ncd /var/lib/cassandra/\\nrm commitlog/*\\ntar xvf ~/cas-prod-clone.tar\\nfor each in $(find -iname prod-clone); do (cd $each; pwd; rm ../../* 2\\u0026gt;/dev/null; cp -a * ../../); done\\nservice cassandra start\\n\\n# Restore PostgreSQL\\nservice postgresql stop\\ncd /var/lib/postgresql/9.3\\nmv main main.old # Can rm if you prefer\\nmkdir main\\nchown postgres.postgres main\\nchmod 700 main\\ncd main\\ntar xvf ~/pg-prod-clone.tar\\nservice postgresql start\\n\\n# memcached now has stale data\\nservice memcached restart\\n# and reddit\\u0026#39;s service connections will be broken having restarted services\\nreddit-restart\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIf all went well, your production database (note: not media) should now show up on your staging machine.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EGood luck!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So, you have a functioning reddit instance, but you'd like to mess with it where it won't piss people off or generally make a mess.  I just worked out the following procedure for cloning a live reddit instance onto a staging machine.\\n\\n**Warning:** I am not intimately familiar with reddit's code and can offer no guarantees; and while I'm reasonably conversant with MySQL, I'm pretty new to PostgreSQL and Cassandra, so there may well be better ways to do things than I show here.  I can only say that, after a *very* cursory glance, my clone appears to work.\\n\\nThe following assumes that:\\n\\n* you have a functioning instance on the target machine,\\n* your target is a **separate** machine (or VM) and **you don't care about any existing data on it** (I don't think it's feasible to run two independent instances of reddit on the one machine, not without a goodly amount of customisation of quite a lot of reddit's service dependencies, anyway; and if you're capable of that, you don't need what's in this post).\\n\\nIf you've yet to set up reddit on the target, go ahead and do so in the usual way.  After it's installed, copy your production `r2/development.update` to the target, edit the domain keywords and run `make ini` (because you never edit INI files directly, right?)\\n\\nNB: Though you can rename an instance just by changing the domain keywords, there will be some database content that still refers to the old domain, such as media.  You can probably do a search-and-replace for such if it's really important, but then you'll also have to copy across said media to the target machine.  \\n\\nJust be aware that the clone is not *entirely* independent of the original and some things will break if the original disappeared.\\n\\n## Back up your live instance\\n\\nTo do it properly, there are three things that need to be copied: PostgreSQL, Cassandra and RabbitMQ.  If your instance is quiet, chances are that your message queues are fully consumed anyway and, if not, all that'll happen is that some actions will not make it through to the clone.  It seemed too much bother, so I didn't.\\n\\nThere are several ways to dump databases from a running pgsql server, but I wanted to be sure that the target was an exact clone of the source.  I tried `pg_dumpall -c` but restoring such a dump is not so straight forward (because of pre-existing relationships and keys, despite there being a DROP DATABASE command in the resultant dump).  \\n\\nI therefore used `pg_basebackup`, which requires a bit of set up first.\\n\\n### Preliminaries\\n\\nIf you haven't already done this:\\n\\n1.  Edit `/etc/postgresql/9.3/main/postgresql.conf`; uncomment the line `max_wal_senders` and set it to 2; uncomment `wal_level` and set it to `hot_standby`;\\n2.  Edit `/etc/postgresql/9.3/main/pg_hba.conf` and uncomment the line near the bottom that reads:\\n\\n    `local   replication     postgres                                peer`\\n\\n3. Restart PostgreSQL and (because that'll break reddit's connection) reddit also (NB: this will temporarily interrupt service to your production instance):\\n\\n    `sudo service postgresql restart; sudo reddit-restart`\\n\\n### Make backups\\n\\nNB: \\n\\n* `prod-clone` is an arbitrary name which you can replace with anything you like (and will need to if you don't clean up old Cassandra snapshots).\\n* When I did this, I also cloned the `system` Cassandra keyspace, just to be sure.  Since you probably don't have to do this and doing so can be dangerous (for example, should the two Cassandra nodes end up in the same cluster), I've removed it from the example below.  (To add it back, add `system` to the invocation of `nodetool`).  If you have trouble with authentication, though, you may need to clone `system` or parts of it, or else mess with Cassandra on the target machine.  Clone `system` at your own risk (even though you wouldn't use a machine that mattered for staging anyway, right?)\\n\\nExecute:\\n\\n    sudo nodetool snapshot -t prod-clone reddit \\u0026\\u0026 sudo -upostgres pg_basebackup -D - -Ft -x \\u003E ~/pg-prod-clone.tar\\n    cd /var/lib/cassandra\\n    find -iname prod-clone | tar cvf ~/cas-prod-clone.tar -T-\\n\\nCopy backups `~/pg-prod-clone.tar` and `~/cas-prod-clone.tar` to the target machine.  Compress them first if they're large.\\n\\n## Restore your backups\\n\\nAssuming that you've copied your backups to your home directory, execute (as root):\\n\\n    # Restore Cassandra\\n    service cassandra stop\\n    cd /var/lib/cassandra/\\n    rm commitlog/*\\n    tar xvf ~/cas-prod-clone.tar\\n    for each in $(find -iname prod-clone); do (cd $each; pwd; rm ../../* 2\\u003E/dev/null; cp -a * ../../); done\\n    service cassandra start\\n    \\n    # Restore PostgreSQL\\n    service postgresql stop\\n    cd /var/lib/postgresql/9.3\\n    mv main main.old # Can rm if you prefer\\n    mkdir main\\n    chown postgres.postgres main\\n    chmod 700 main\\n    cd main\\n    tar xvf ~/pg-prod-clone.tar\\n    service postgresql start\\n    \\n    # memcached now has stale data\\n    service memcached restart\\n    # and reddit's service connections will be broken having restarted services\\n    reddit-restart\\n\\nIf all went well, your production database (note: not media) should now show up on your staging machine.\\n\\nGood luck!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4j7t6h\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"StrixTechnica\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1463189958.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4j7t6h/howto_clone_a_reddit_instance_experimental/\", \"locked\": false, \"name\": \"t3_4j7t6h\", \"created\": 1463194443.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4j7t6h/howto_clone_a_reddit_instance_experimental/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"HOWTO: Clone a reddit instance (experimental)\", \"created_utc\": 1463165643.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIs there a field to designate whether the current user has already viewed this comment? On the website, reddit gold users can have comments highlighted blue if they are new since their last visit.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Is there a field to designate whether the current user has already viewed this comment? On the website, reddit gold users can have comments highlighted blue if they are new since their last visit.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4d4but\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"det0ur\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4d4but/is_new_comment_highlighting_in_the_api/\", \"locked\": false, \"name\": \"t3_4d4but\", \"created\": 1459676110.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4d4but/is_new_comment_highlighting_in_the_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is new comment highlighting in the API\", \"created_utc\": 1459647310.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am trying to find out the code coverage of the tests reddit has. I found \\u003Ca href=\\\"http://coverage.readthedocs.org/en/latest/\\\"\\u003ECoverage.py\\u003C/a\\u003E and i was trying to use it to find out the coverage. However, i don\\u0026#39;t know how to run it with reddit, as there isn\\u0026#39;t only one file (to my knowledge) that runs reddit. I may sound a bit newbie, and that\\u0026#39;s because i am, having no previous experience with python. Can someone help me out with this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am trying to find out the code coverage of the tests reddit has. I found [Coverage.py](http://coverage.readthedocs.org/en/latest/) and i was trying to use it to find out the coverage. However, i don't know how to run it with reddit, as there isn't only one file (to my knowledge) that runs reddit. I may sound a bit newbie, and that's because i am, having no previous experience with python. Can someone help me out with this?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3tqvfk\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"tomislaaaav\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 14, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3tqvfk/software_testing_on_reddit_code_coverage/\", \"locked\": false, \"name\": \"t3_3tqvfk\", \"created\": 1448174518.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3tqvfk/software_testing_on_reddit_code_coverage/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Software Testing on Reddit (Code Coverage)\", \"created_utc\": 1448145718.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://i.imgur.com/Jb6PR6g.png\\\"\\u003EThis is worng\\u003C/a\\u003E. The text on the \\u0026#39;subscribe\\u0026#39; button should be \\u0026#39;abona\\u021bi-v\\u0103\\u0026#39;, not \\u0026#39;abona\\u021biv\\u0103\\u0026#39;.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"[This is worng](http://i.imgur.com/Jb6PR6g.png). The text on the 'subscribe' button should be 'abona\\u021bi-v\\u0103', not 'abona\\u021biv\\u0103'.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3h2wne\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3h2wne/reddit_romanian_translation/\", \"locked\": false, \"name\": \"t3_3h2wne\", \"created\": 1439658378.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3h2wne/reddit_romanian_translation/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Romanian translation\", \"created_utc\": 1439629578.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI accidentally discovered that you can access a subreddit\\u0026#39;s stylesheet and media in JSON format via /about/stylesheet.json, like this: \\u003Ca href=\\\"https://www.reddit.com/r/askreddit/about/stylesheet.json\\\"\\u003Ehttps://www.reddit.com/r/askreddit/about/stylesheet.json\\u003C/a\\u003E. Quite useful for cloning CSS and media.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I accidentally discovered that you can access a subreddit's stylesheet and media in JSON format via /about/stylesheet.json, like this: https://www.reddit.com/r/askreddit/about/stylesheet.json. Quite useful for cloning CSS and media.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3fkpyv\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"okofish\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3fkpyv/useful_endpoint_json_stylesheet/\", \"locked\": false, \"name\": \"t3_3fkpyv\", \"created\": 1438598571.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3fkpyv/useful_endpoint_json_stylesheet/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Useful endpoint: JSON stylesheet\", \"created_utc\": 1438569771.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EBecause now the stylesheet editor has dissapeared as well.. Thanks! :-P\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Because now the stylesheet editor has dissapeared as well.. Thanks! :-P\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"39q8bz\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Auxilium11\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/39q8bz/i_accidently_used_content_displayhidden_in_the/\", \"locked\": false, \"name\": \"t3_39q8bz\", \"created\": 1434248183.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/39q8bz/i_accidently_used_content_displayhidden_in_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I accidently used \\\".content{ display:hidden}\\\" in the css stylesheet. How do I undo this?\", \"created_utc\": 1434219383.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EFollowing \\u003Ca href=\\\"http://www.redditblog.com/2005/12/on-lisp.html\\\"\\u003Ethis old blog post\\u003C/a\\u003E by \\u003Ca href=\\\"/u/spez\\\"\\u003E/u/spez\\u003C/a\\u003E I searched for the original lisp code, unsuccessful thus far. \\u003Ca href=\\\"/r/redditdev\\\"\\u003E/r/redditdev\\u003C/a\\u003E appears suitable for asking. Forgive me if it is not the correct place to ask; please guide me to the correct place.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Following [this old blog post](http://www.redditblog.com/2005/12/on-lisp.html) by /u/spez I searched for the original lisp code, unsuccessful thus far. /r/redditdev appears suitable for asking. Forgive me if it is not the correct place to ask; please guide me to the correct place.  \\n\\nThanks.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"34erkc\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"mukt\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/34erkc/request_reddit_original_lisp_source_code_where/\", \"locked\": false, \"name\": \"t3_34erkc\", \"created\": 1430436470.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/34erkc/request_reddit_original_lisp_source_code_where/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Request] Reddit original Lisp source code. Where can one get it?\", \"created_utc\": 1430407670.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi guys, here\\u0026#39;s something that I started a long ago. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAfter initial implementation by me and a friend of mine, I started implementing a Reddit client on top of it. I kinda abandoned the project because I got busy with other stuff.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFrom the start I wanted to open source it. So here it is. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/fizzl/RedditEngine/\\\"\\u003Ehttps://github.com/fizzl/RedditEngine/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI hope it is \\u0026quot;almost good enough\\u0026quot; for your projects. I heartily welcome patches and pull requests. It is licensed under LGPL 3.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt should be possible to port this to be general purpose Java library quite easily. To try that, find references to RedditApi.getContext(). \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI recall also SimpleHttpClient uses some Android specific Apache HTTP things by Google, so that might need some prodding.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: Oh yeah, I made a video waffling about the architecture of this some time ago:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://www.youtube.com/watch?v=PFmB7dTdQyk\\\"\\u003Ehttps://www.youtube.com/watch?v=PFmB7dTdQyk\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi guys, here's something that I started a long ago. \\n\\nAfter initial implementation by me and a friend of mine, I started implementing a Reddit client on top of it. I kinda abandoned the project because I got busy with other stuff.\\n\\nFrom the start I wanted to open source it. So here it is. \\n\\nhttps://github.com/fizzl/RedditEngine/\\n\\nI hope it is \\\"almost good enough\\\" for your projects. I heartily welcome patches and pull requests. It is licensed under LGPL 3.\\n\\nIt should be possible to port this to be general purpose Java library quite easily. To try that, find references to RedditApi.getContext(). \\n\\nI recall also SimpleHttpClient uses some Android specific Apache HTTP things by Google, so that might need some prodding.\\n\\n\\nEdit: Oh yeah, I made a video waffling about the architecture of this some time ago:\\n\\nhttps://www.youtube.com/watch?v=PFmB7dTdQyk\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2y88ql\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"fizzl\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1425749895.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2y88ql/new_open_source_android_java_library_for/\", \"locked\": false, \"name\": \"t3_2y88ql\", \"created\": 1425750356.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2y88ql/new_open_source_android_java_library_for/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New Open Source Android Java library for interfacing with the Reddit API\", \"created_utc\": 1425721556.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey everyone :-)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo, I\\u0026#39;m quite new to bot development. One obnoxious problem I\\u0026#39;ve run into is getting my bot account the required karma to send private messages, without getting the whole captcha response in my python application.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat is the general way that bot developers use to gain enough karma? Also, is it link or comment karma that is required?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have tried to gain some link karma for my bot account by posting on some test subreddits, but to no avail :( Is this really the way to do it? It seems like quite an annoying thing to have to deal with when creating bots.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThank you all for your help :-)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey everyone :-)\\n\\nSo, I'm quite new to bot development. One obnoxious problem I've run into is getting my bot account the required karma to send private messages, without getting the whole captcha response in my python application.\\n\\nWhat is the general way that bot developers use to gain enough karma? Also, is it link or comment karma that is required?\\n\\nI have tried to gain some link karma for my bot account by posting on some test subreddits, but to no avail :( Is this really the way to do it? It seems like quite an annoying thing to have to deal with when creating bots.\\n\\nThank you all for your help :-)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2u7b3w\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"PastryGood\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2u7b3w/how_much_karma_is_required_for_a_bot_account_to/\", \"locked\": false, \"name\": \"t3_2u7b3w\", \"created\": 1422659598.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2u7b3w/how_much_karma_is_required_for_a_bot_account_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How much karma is required for a bot account to send PMs (without captcha)?\", \"created_utc\": 1422630798.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHas there been any improvements in the API to get all comments in a very large thread instead of using 1 API call to get child comments for every branch?  I know the maximum amount of comments per request with Reddit Gold is 1,500 comments.  Ideally, one could do something like this:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EGet 1,500 oldest comments, set after parameter to get next 1,500 comments, etc.  Instead of treating it like a forest, treat it as flattened where the comment object\\u0026#39;s parent_id could be used to restore the forest for analysis later on.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHave the devs looked at this?  Is Reddit still using Cassandra to store comments in such a way that doing this is not feasible on their end?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf a thread has 15,000 comments, I can see it costing 10 API calls to get the entire thread -- but following every child branch becomes extremely expensive in terms of API calls.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Has there been any improvements in the API to get all comments in a very large thread instead of using 1 API call to get child comments for every branch?  I know the maximum amount of comments per request with Reddit Gold is 1,500 comments.  Ideally, one could do something like this:\\n\\nGet 1,500 oldest comments, set after parameter to get next 1,500 comments, etc.  Instead of treating it like a forest, treat it as flattened where the comment object's parent_id could be used to restore the forest for analysis later on.\\n\\nHave the devs looked at this?  Is Reddit still using Cassandra to store comments in such a way that doing this is not feasible on their end?\\n\\nIf a thread has 15,000 comments, I can see it costing 10 API calls to get the entire thread -- but following every child branch becomes extremely expensive in terms of API calls.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2l7t0o\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2l7t0o/least_expensive_way_to_get_all_comments_from/\", \"locked\": false, \"name\": \"t3_2l7t0o\", \"created\": 1415092911.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2l7t0o/least_expensive_way_to_get_all_comments_from/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Least expensive way to get all comments from threads with 10,000+ comments?\", \"created_utc\": 1415064111.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve tried to read through the CPAL licence, but I\\u0026#39;m still not sure if this is OK.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI ask this because I am currently working on a site (not a reddit clone) and I plan to build it on top of Reddit\\u0026#39;s code. At one point, I may want to monetize it.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've tried to read through the CPAL licence, but I'm still not sure if this is OK.\\n\\nI ask this because I am currently working on a site (not a reddit clone) and I plan to build it on top of Reddit's code. At one point, I may want to monetize it.\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2keu9l\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"nusgam\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2keu9l/can_i_monetize_a_website_that_is_built_on_top_of/\", \"locked\": false, \"name\": \"t3_2keu9l\", \"created\": 1414394019.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2keu9l/can_i_monetize_a_website_that_is_built_on_top_of/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can I monetize a website that is built on top of Reddit's source code?\", \"created_utc\": 1414365219.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EMine\\u0026#39;s costing me about a hundred a month (running it on NearlyFreeSpeech)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Mine's costing me about a hundred a month (running it on NearlyFreeSpeech)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2ciqe2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"aakilfernandes\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 39, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2ciqe2/how_much_does_your_reddit_bot_cost/\", \"locked\": false, \"name\": \"t3_2ciqe2\", \"created\": 1407120125.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2ciqe2/how_much_does_your_reddit_bot_cost/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How much does your Reddit bot cost?\", \"created_utc\": 1407091325.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E/r/{subreddit}/about/spam, /about/modqueue, /about/reports and /about/unmoderated have been added to \\u003Ca href=\\\"http://www.reddit.com/dev/api/oauth#GET_about_%7Blocation%7D\\\"\\u003EAPI documentation\\u003C/a\\u003E and given the OAuth \\u0026quot;read\\u0026quot; scope.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEnjoy!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"/r/{subreddit}/about/spam, /about/modqueue, /about/reports and /about/unmoderated have been added to [API documentation](http://www.reddit.com/dev/api/oauth#GET_about_{location}) and given the OAuth \\\"read\\\" scope.\\n\\nEnjoy!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2a5sk8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2a5sk8/oauth2_endpoints_added_aboutspam_and_friends/\", \"locked\": false, \"name\": \"t3_2a5sk8\", \"created\": 1404869821.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2a5sk8/oauth2_endpoints_added_aboutspam_and_friends/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[OAuth2] Endpoints added - /about/spam and friends\", \"created_utc\": 1404841021.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been searching for a way to make my website more social in that I would really like to expand discussion but I am not really happy the the current state of forum software. So I am considering using reddit open source software. I was wondering if anyone had any experience using it and if so what kind of difficulties have you come across?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been searching for a way to make my website more social in that I would really like to expand discussion but I am not really happy the the current state of forum software. So I am considering using reddit open source software. I was wondering if anyone had any experience using it and if so what kind of difficulties have you come across?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1z6zfx\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1z6zfx/has_anyone_successfully_used_the_reddit_open/\", \"locked\": false, \"name\": \"t3_1z6zfx\", \"created\": 1393637604.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1z6zfx/has_anyone_successfully_used_the_reddit_open/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Has anyone successfully used the Reddit open source code? If so, how difficult is it to install/use/admin, what kind of difficulties did you face using it?\", \"created_utc\": 1393608804.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello. I am a Computer Science student currently on my placement year. I have a good amount of experience in JavaScript / jQuery, hybrid mobile app development, Sass, Ruby, Java EE and small amount of experience with Python specifically for use in AI. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI was wondering are there any \\u0026quot;low hanging fruits\\u0026quot; or recommendation of a place to start to help better understand the beast that is known as reddit. \\u003Cstrong\\u003EIf anyone has some grunt work they\\u0026#39;ve been putting off feel free to PM me.\\u003C/strong\\u003E I\\u0026#39;m thinking of looking into the API as I imagine the rabbit hole gets deeper.... \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnyway just wanting to say \\u003Cem\\u003EHi\\u003C/em\\u003E and seek some advice from Reddit devs. Hopefully a pull request coming soon ;) \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello. I am a Computer Science student currently on my placement year. I have a good amount of experience in JavaScript / jQuery, hybrid mobile app development, Sass, Ruby, Java EE and small amount of experience with Python specifically for use in AI. \\n\\nI was wondering are there any \\\"low hanging fruits\\\" or recommendation of a place to start to help better understand the beast that is known as reddit. **If anyone has some grunt work they've been putting off feel free to PM me.** I'm thinking of looking into the API as I imagine the rabbit hole gets deeper.... \\n\\nAnyway just wanting to say *Hi* and seek some advice from Reddit devs. Hopefully a pull request coming soon ;) \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1y6dl3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"lordqwerty\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1y6dl3/low_hanging_fruits/\", \"locked\": false, \"name\": \"t3_1y6dl3\", \"created\": 1392699780.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1y6dl3/low_hanging_fruits/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Low hanging fruits\", \"created_utc\": 1392670980.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWe\\u0026#39;d really like it if more devs used OAuth when connecting to reddit and away from cookies when managing requests on behalf of users. To help convince more of you to make the switch, I\\u0026#39;m happy to announce two new features of reddit\\u0026#39;s OAuth implementation to encourage you to make the switch for your app: custom redirect schemes and easier token requests for simple scripts. Both of these features are active now, so feel free to start using them immediately, and please reply with any feedback, questions, or issues!\\u003C/p\\u003E\\n\\n\\u003Ch1\\u003ECustom Schemes\\u003C/h1\\u003E\\n\\n\\u003Cp\\u003EYou may specify a custom redirect scheme for certain categories of OAuth apps.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u0026quot;Categories of apps?\\u0026quot; you ask. \\u0026quot;Why, whatever do you mean?\\u0026quot; Glad you asked! App creators will now have one of three options when creating an app:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EWeb app: An app that you run on your own server, with users able to pull up a web page and perform actions by providing you with OAuth\\u0026#39;ed access. Since you run the app on your own hardware, we trust that you can keep the client secret, well, secret.\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cstrong\\u003EInstalled app:\\u003C/strong\\u003E An app that you install on a device, such as a mobile phone. We won\\u0026#39;t pretend that you can keep the client secret a secret, since you have to give away binaries with the \\u0026quot;secret\\u0026quot; embedded. (You may want to look at how \\u003Ca href=\\\"https://developers.google.com/accounts/docs/OAuth2InstalledApp\\\"\\u003Egoogle handles\\u003C/a\\u003E that case).\\u003C/li\\u003E\\n\\u003Cli\\u003EScript: A script you run on your own server. Able to keep a secret. See below for goodies!\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EInstalled apps will be allowed to use custom redirect schemes. Web apps will still be required to redirect to an http or https schemed URI.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EApp types cannot be changed after creation\\u003C/strong\\u003E. All existing apps have been marked as \\u0026quot;web apps.\\u0026quot;\\u003C/p\\u003E\\n\\n\\u003Ch1\\u003EScripts\\u003C/h1\\u003E\\n\\n\\u003Cp\\u003ENow you might ask why we would bother differentiating a \\u0026quot;script\\u0026quot; from a \\u0026quot;web app.\\u0026quot; The answer is this: the OAuth2 protocol can be somewhat complicated, particularly for a one off script or bot that really just needs to access one account. The complicated nature might cause such developers to just go to cookie authentication. The \\u0026quot;script\\u0026quot; app type attempts to bridge that gap - you\\u0026#39;ll be able to use the \\u003Ca href=\\\"http://tools.ietf.org/html/rfc6749#section-4.3\\\"\\u003E\\u0026quot;password\\u0026quot; grant type\\u003C/a\\u003E to get access tokens for that script or bot. To put it into code, here\\u0026#39;s the curl commands you\\u0026#39;d need to do to \\u0026quot;login\\u0026quot; and hit /api/v1/me.json with a script app:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E# Token request\\nkemitche@kemitche-laptop$ curl --user \\u0026quot;$CLIENT_ID:$CLIENT_SECRET\\u0026quot; \\\\\\n\\u0026gt; -d \\u0026quot;grant_type=password\\u0026amp;username=$REDDIT_USER\\u0026amp;password=$REDDIT_PASSWORD\\u0026quot; \\\\\\n\\u0026gt; -X POST https://ssl.reddit.com/api/v1/access_token\\n# Token response\\n{\\u0026quot;access_token\\u0026quot;: \\u0026quot;SOME_TOKEN\\u0026quot;, \\u0026quot;token_type\\u0026quot;: \\u0026quot;bearer\\u0026quot;, \\u0026quot;expires_in\\u0026quot;: 3600, \\u0026quot;scope\\u0026quot;: \\u0026quot;*\\u0026quot;}\\n# /api/v1/me request with token\\nkemitche@kemitche-laptop$ curl --header \\u0026quot;Authorization: bearer $SOME_TOKEN\\u0026quot; \\\\\\n\\u0026gt; https://oauth.reddit.com/api/v1/me\\n# /api/v1/me response\\n{\\u0026quot;name\\u0026quot;: \\u0026quot;reddit\\u0026quot;, \\u0026quot;created\\u0026quot;: 1389649907.0, \\u0026quot;created_utc\\u0026quot;: 1389649907.0, \\u0026quot;link_karma\\u0026quot;: 1,\\n\\u0026quot;comment_karma\\u0026quot;: 0, \\u0026quot;over_18\\u0026quot;: false, \\u0026quot;is_gold\\u0026quot;: false, \\u0026quot;is_mod\\u0026quot;: true,\\n\\u0026quot;has_verified_email\\u0026quot;: null, \\u0026quot;id\\u0026quot;: \\u0026quot;1\\u0026quot;}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENote that for a script app using password grants, \\u003Ccode\\u003Escope\\u003C/code\\u003E is an optional parameter. If provided, the returned access token will have limited scopes. If not provided, the token will have access to all existing scopes (but no access to endpoints not otherwise available over OAuth).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThere\\u0026#39;s just one caveat: to discourage devs from using this \\u0026quot;short circuit\\u0026quot; method widely, you can only authenticate this way using a script app, and \\u003Cem\\u003Eonly\\u003C/em\\u003E as a user that is considered a \\u0026quot;developer\\u0026quot; of that app.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"We'd really like it if more devs used OAuth when connecting to reddit and away from cookies when managing requests on behalf of users. To help convince more of you to make the switch, I'm happy to announce two new features of reddit's OAuth implementation to encourage you to make the switch for your app: custom redirect schemes and easier token requests for simple scripts. Both of these features are active now, so feel free to start using them immediately, and please reply with any feedback, questions, or issues!\\n\\nCustom Schemes\\n=============\\n\\nYou may specify a custom redirect scheme for certain categories of OAuth apps.\\n\\n\\\"Categories of apps?\\\" you ask. \\\"Why, whatever do you mean?\\\" Glad you asked! App creators will now have one of three options when creating an app:\\n\\n* Web app: An app that you run on your own server, with users able to pull up a web page and perform actions by providing you with OAuth'ed access. Since you run the app on your own hardware, we trust that you can keep the client secret, well, secret.\\n* **Installed app:** An app that you install on a device, such as a mobile phone. We won't pretend that you can keep the client secret a secret, since you have to give away binaries with the \\\"secret\\\" embedded. (You may want to look at how [google handles](https://developers.google.com/accounts/docs/OAuth2InstalledApp) that case).\\n* Script: A script you run on your own server. Able to keep a secret. See below for goodies!\\n\\nInstalled apps will be allowed to use custom redirect schemes. Web apps will still be required to redirect to an http or https schemed URI.\\n\\n**App types cannot be changed after creation**. All existing apps have been marked as \\\"web apps.\\\"\\n\\nScripts\\n=============\\n\\nNow you might ask why we would bother differentiating a \\\"script\\\" from a \\\"web app.\\\" The answer is this: the OAuth2 protocol can be somewhat complicated, particularly for a one off script or bot that really just needs to access one account. The complicated nature might cause such developers to just go to cookie authentication. The \\\"script\\\" app type attempts to bridge that gap - you'll be able to use the [\\\"password\\\" grant type](http://tools.ietf.org/html/rfc6749#section-4.3) to get access tokens for that script or bot. To put it into code, here's the curl commands you'd need to do to \\\"login\\\" and hit /api/v1/me.json with a script app:\\n\\n    # Token request\\n    kemitche@kemitche-laptop$ curl --user \\\"$CLIENT_ID:$CLIENT_SECRET\\\" \\\\\\n    \\u003E -d \\\"grant_type=password\\u0026username=$REDDIT_USER\\u0026password=$REDDIT_PASSWORD\\\" \\\\\\n    \\u003E -X POST https://ssl.reddit.com/api/v1/access_token\\n    # Token response\\n    {\\\"access_token\\\": \\\"SOME_TOKEN\\\", \\\"token_type\\\": \\\"bearer\\\", \\\"expires_in\\\": 3600, \\\"scope\\\": \\\"*\\\"}\\n    # /api/v1/me request with token\\n    kemitche@kemitche-laptop$ curl --header \\\"Authorization: bearer $SOME_TOKEN\\\" \\\\\\n    \\u003E https://oauth.reddit.com/api/v1/me\\n    # /api/v1/me response\\n    {\\\"name\\\": \\\"reddit\\\", \\\"created\\\": 1389649907.0, \\\"created_utc\\\": 1389649907.0, \\\"link_karma\\\": 1,\\n    \\\"comment_karma\\\": 0, \\\"over_18\\\": false, \\\"is_gold\\\": false, \\\"is_mod\\\": true,\\n    \\\"has_verified_email\\\": null, \\\"id\\\": \\\"1\\\"}\\n\\nNote that for a script app using password grants, `scope` is an optional parameter. If provided, the returned access token will have limited scopes. If not provided, the token will have access to all existing scopes (but no access to endpoints not otherwise available over OAuth).\\n\\nThere's just one caveat: to discourage devs from using this \\\"short circuit\\\" method widely, you can only authenticate this way using a script app, and *only* as a user that is considered a \\\"developer\\\" of that app.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1xk8wf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1xk8wf/oauth2_custom_schemes_and_other_goodies/\", \"locked\": false, \"name\": \"t3_1xk8wf\", \"created\": 1392103554.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1xk8wf/oauth2_custom_schemes_and_other_goodies/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[OAuth2] Custom schemes and other goodies\", \"created_utc\": 1392074754.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EGood news everyone! I\\u0026#39;ve added some new scopes to reddit\\u0026#39;s OAuth2 implementation. They are:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cstrong\\u003Emodwiki\\u003C/strong\\u003E: For moderator actions of wiki pages\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cstrong\\u003Ewikiread\\u003C/strong\\u003E: For reading wiki pages\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cstrong\\u003Ewikiedit\\u003C/strong\\u003E: For editing/modifying wiki pages\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EBonus Time! I also added scopes to the following endpoints:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E/random: \\u003Cstrong\\u003Eread\\u003C/strong\\u003E scope added. Bear in mind that /random will perform an HTTP redirect, so be sure to handle that appropriately in your client. Either read the \\u003Ccode\\u003ELocation\\u003C/code\\u003E header or silently follow the redirect to ... wherever!\\u003C/li\\u003E\\n\\u003Cli\\u003E/api/morechildren: \\u003Cstrong\\u003Eread\\u003C/strong\\u003E scope added. You may now request deeper comment trees!\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EFor details on the endpoints, as always, check out the \\u003Ca href=\\\"/dev/api/oauth\\\"\\u003EDeveloper API\\u003C/a\\u003E page.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EKeep an eye out for more scopes and endpoints in the coming weeks!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: For those interested, here are the \\u003Ca href=\\\"http://git.io/z-XRwQ\\\"\\u003Erelevant commits\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Good news everyone! I've added some new scopes to reddit's OAuth2 implementation. They are:\\n\\n* **modwiki**: For moderator actions of wiki pages\\n* **wikiread**: For reading wiki pages\\n* **wikiedit**: For editing/modifying wiki pages\\n\\nBonus Time! I also added scopes to the following endpoints:\\n\\n* /random: **read** scope added. Bear in mind that /random will perform an HTTP redirect, so be sure to handle that appropriately in your client. Either read the `Location` header or silently follow the redirect to ... wherever!\\n* /api/morechildren: **read** scope added. You may now request deeper comment trees!\\n\\nFor details on the endpoints, as always, check out the [Developer API](/dev/api/oauth) page.\\n\\nKeep an eye out for more scopes and endpoints in the coming weeks!\\n\\nEdit: For those interested, here are the [relevant commits](http://git.io/z-XRwQ)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1wxkbb\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1391476222.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1wxkbb/oauth2_wiki_scopes/\", \"locked\": false, \"name\": \"t3_1wxkbb\", \"created\": 1391495933.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1wxkbb/oauth2_wiki_scopes/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OAuth2: wiki scopes!\", \"created_utc\": 1391467133.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi there!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBefore I start development on a new bot, I want to ask if it is \\u0026quot;legal\\u0026quot; for a bot to post links to products that can be bought.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe bot would only post when summoned so it won\\u0026#39;t be spamming links. I just want to be a good reddit citizen.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: Any other comments? Please voice your opinion. Thanks in advance :)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi there!\\n\\nBefore I start development on a new bot, I want to ask if it is \\\"legal\\\" for a bot to post links to products that can be bought.\\n\\nThe bot would only post when summoned so it won't be spamming links. I just want to be a good reddit citizen.\\n\\nEDIT: Any other comments? Please voice your opinion. Thanks in advance :)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1tco66\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1387852287.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1tco66/reddit_bot_rules_links_to_products/\", \"locked\": false, \"name\": \"t3_1tco66\", \"created\": 1387603236.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1tco66/reddit_bot_rules_links_to_products/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Bot Rules - links to products\", \"created_utc\": 1387574436.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1r0u8n\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"chromakode\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1r0u8n/api_change_third_party_websites_can_now_make/\", \"locked\": false, \"name\": \"t3_1r0u8n\", \"created\": 1384938175.0, \"url\": \"http://www.reddit.com/r/changelog/comments/1r0u3v/reddit_change_third_party_websites_can_now_make/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[api change] Third party websites can now make logged-out requests to the reddit API using CORS.\", \"created_utc\": 1384909375.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI woke up with a strange idea in my head:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EA two player game like chess / battleship to be played over a network.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EA game is started by player 1 (p1) mentioning player 2 (p2) in a comment together with a move in some notation.\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003Ep1 challenges p2. e4 (chess notation for a pawn moving)\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003Ep2 makes his move\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003Ee5\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003Ethis continues until the game is won.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat this allows is:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EA stable back end without any over head or cost\\u003C/li\\u003E\\n\\u003Cli\\u003Ea method of saving an entire games history\\u003C/li\\u003E\\n\\u003Cli\\u003Ea way to undo moves by simply deleting comments \\u003Cstrong\\u003Eor\\u003C/strong\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003Ea way to pick up a game / branching the game from any point by simply adding a second comment to a move\\u003C/li\\u003E\\n\\u003Cli\\u003Ea rudimentary player scoring system over a number of games (ie, winning player gets an upvote from losing player)\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EA bot could be used to make sure all moves are legal, or the subreddit could be made private / read only and the games could be completely controlled by a bot/service running on a third party website.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe game service could even build an entire graphical interface and representation of the game board so the users have no interaction with reddit what soever. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnyway, Has anybody any thoughts on this? I don\\u0026#39;t plan on making it into a serious service, but I may try to make a proof of concept for fun.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I woke up with a strange idea in my head:\\n\\nA two player game like chess / battleship to be played over a network.\\n\\nA game is started by player 1 (p1) mentioning player 2 (p2) in a comment together with a move in some notation.\\n\\u003Ep1 challenges p2. e4 (chess notation for a pawn moving)\\n   \\np2 makes his move\\n\\u003E e5\\n\\nthis continues until the game is won.\\n\\nWhat this allows is:\\n\\n* A stable back end without any over head or cost\\n* a method of saving an entire games history\\n* a way to undo moves by simply deleting comments **or**\\n* a way to pick up a game / branching the game from any point by simply adding a second comment to a move\\n* a rudimentary player scoring system over a number of games (ie, winning player gets an upvote from losing player)\\n\\nA bot could be used to make sure all moves are legal, or the subreddit could be made private / read only and the games could be completely controlled by a bot/service running on a third party website.\\n\\nThe game service could even build an entire graphical interface and representation of the game board so the users have no interaction with reddit what soever. \\n\\n\\nAnyway, Has anybody any thoughts on this? I don't plan on making it into a serious service, but I may try to make a proof of concept for fun.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1of4u4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"AlmostARockstar\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 19, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1of4u4/proof_of_concept_has_anybody_ever_used_reddit_as/\", \"locked\": false, \"name\": \"t3_1of4u4\", \"created\": 1381781104.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1of4u4/proof_of_concept_has_anybody_ever_used_reddit_as/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[proof of concept] Has anybody ever used reddit as a back end for a networked game?\", \"created_utc\": 1381752304.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis one goes out to \\u003Ca href=\\\"/u/radd_it\\\"\\u003E/u/radd_it\\u003C/a\\u003E, who I think may have smashed multiple innocent things due to it in the past. Once again, brought to you by \\u003Ca href=\\\"/u/slyf\\\"\\u003E/u/slyf\\u003C/a\\u003E:\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E/by_id will now only 404 if all links were not found. I noticed some users pointing out our inconsistent and counter intuitive behavior of /by_id. /by_id is an endpoint to get links by ID. However, if given an invalid link ID, the entire result would be a 404, even if some of the IDs were valid...giving no indication of which IDs were invalid. HOWEVER, if given an invalid id with a invalid prefix (eg \\u0026quot;/by_id/t3_1iy9ku,tomatosoup\\u0026quot;) the endpoint would not 404 and would be very happy to just give you the valid items. This behavior was inconsistent and made it difficult to deal with having invalid ids. The new behavior is to only 404 if all of the ids were not found.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/compare/9b037e0...c4fa6d3\\\"\\u003EView the code for this change on github\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This one goes out to /u/radd_it, who I think may have smashed multiple innocent things due to it in the past. Once again, brought to you by /u/slyf:\\n\\n---\\n\\n/by_id will now only 404 if all links were not found. I noticed some users pointing out our inconsistent and counter intuitive behavior of /by_id. /by_id is an endpoint to get links by ID. However, if given an invalid link ID, the entire result would be a 404, even if some of the IDs were valid...giving no indication of which IDs were invalid. HOWEVER, if given an invalid id with a invalid prefix (eg \\\"/by_id/t3_1iy9ku,tomatosoup\\\") the endpoint would not 404 and would be very happy to just give you the valid items. This behavior was inconsistent and made it difficult to deal with having invalid ids. The new behavior is to only 404 if all of the ids were not found.\\n\\n---\\n\\n[View the code for this change on github](https://github.com/reddit/reddit/compare/9b037e0...c4fa6d3)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1iz2qe\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Deimorz\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1374693321.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1iz2qe/api_change_by_id_will_no_longer_404_if_a_single/\", \"locked\": false, \"name\": \"t3_1iz2qe\", \"created\": 1374721109.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1iz2qe/api_change_by_id_will_no_longer_404_if_a_single/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API change: /by_id/ will no longer 404 if a single link ID is invalid\", \"created_utc\": 1374692309.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ijq4u\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"karangoeluw\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ijq4u/help_complete_jreddit_the_java_wrapper_for_reddit/\", \"locked\": false, \"name\": \"t3_1ijq4u\", \"created\": 1374164832.0, \"url\": \"https://github.com/thekarangoel/jReddit/blob/karan/implemented_methods.md\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Help complete jReddit - The Java wrapper for reddit API\", \"created_utc\": 1374136032.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI understand that Reddit uses Amazon CloudSearch for its indexing needs, however that doesn\\u0026#39;t work for the instance of Reddit I\\u0026#39;ve stood up. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve a bit of experience with Elasticsearch, and I\\u0026#39;m curious how the core Reddit devs would suggest I go about implementing integration. I would love to be able to contribute Elasticsearch support as a plugin, but it is clear that there\\u0026#39;s a little bit of work that would need to be done before Reddit could support alternatives to CloudSearch. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWould it be acceptable for me to submit a pull-request that would make the search handling more modular and \\u0026quot;pluginified\\u0026quot;? That would allow me to package Elasticsearch support as plugin, as well as allow others to provide their own plugins for their preferred search index system. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve spent a bit of time looking at this problem, and believe that it shouldn\\u0026#39;t be too complicated to abstract Reddit from CloudSearch, allowing for any indexing system to be dropped in place. I guess I just want to make sure that, should I do this, it would stand a good chance of being found useful by the Reddit core devs. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThoughts?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I understand that Reddit uses Amazon CloudSearch for its indexing needs, however that doesn't work for the instance of Reddit I've stood up. \\n\\nI've a bit of experience with Elasticsearch, and I'm curious how the core Reddit devs would suggest I go about implementing integration. I would love to be able to contribute Elasticsearch support as a plugin, but it is clear that there's a little bit of work that would need to be done before Reddit could support alternatives to CloudSearch. \\n\\nWould it be acceptable for me to submit a pull-request that would make the search handling more modular and \\\"pluginified\\\"? That would allow me to package Elasticsearch support as plugin, as well as allow others to provide their own plugins for their preferred search index system. \\n\\nI've spent a bit of time looking at this problem, and believe that it shouldn't be too complicated to abstract Reddit from CloudSearch, allowing for any indexing system to be dropped in place. I guess I just want to make sure that, should I do this, it would stand a good chance of being found useful by the Reddit core devs. \\n\\nThoughts?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1gwq0l\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"falterx\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 14, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1gwq0l/alternatives_to_cloudsearch_via_modular_search/\", \"locked\": false, \"name\": \"t3_1gwq0l\", \"created\": 1372025266.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1gwq0l/alternatives_to_cloudsearch_via_modular_search/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Alternatives to CloudSearch via modular search handling? Core-devs, please advise!\", \"created_utc\": 1371996466.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWasn\\u0026#39;t sure where else to ask this.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIn the sidebar there is a form that\\u0026#39;s the parent of the markdown area, \\u003Ca href=\\\"http://i.imgur.com/r5CNuv9.png\\\"\\u003Ehere\\u003C/a\\u003E. I\\u0026#39;ve been using the id to achieve a pseudo-random tips bar at the top of \\u003Ca href=\\\"/r/RESissues\\\"\\u003E/r/RESissues\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAbout 2 years ago or so, when I first noticed it, it would change every page refresh and so would the tip text. More recently I\\u0026#39;ve noticed that it only changes every so often, but it can be anything from 5 minutes to 30 minutes. Doesn\\u0026#39;t seem to be any way to pin down why/when it changes.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECan anyone glean from the source as to what causes it to change and why it varies? Cheers.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Wasn't sure where else to ask this.\\n\\nIn the sidebar there is a form that's the parent of the markdown area, [here](http://i.imgur.com/r5CNuv9.png). I've been using the id to achieve a pseudo-random tips bar at the top of /r/RESissues.\\n\\nAbout 2 years ago or so, when I first noticed it, it would change every page refresh and so would the tip text. More recently I've noticed that it only changes every so often, but it can be anything from 5 minutes to 30 minutes. Doesn't seem to be any way to pin down why/when it changes.\\n\\nCan anyone glean from the source as to what causes it to change and why it varies? Cheers.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1gcxsf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"gavin19\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1gcxsf/query_about_the_t5_id_in_the_side_bar/\", \"locked\": false, \"name\": \"t3_1gcxsf\", \"created\": 1371268726.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1gcxsf/query_about_the_t5_id_in_the_side_bar/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Query about the t5 id in the side bar\", \"created_utc\": 1371239926.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E409 Conflict: Indicates that the request could not be processed because of conflict in the request, such as an edit conflict.\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EGetting a garbled response from the Reddit API whenever I try to login. Stuff like:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E\\u0081\\u00b27I\\u00daE\\u00cb\\u00b2\\u00bfjFy\\u203a\\u00fa.\\u00b9\\u00b6\\u00e7z3\\u009d\\u017d\\u0026quot;@H0\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003ENot sure what the issue could be?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"\\u003E409 Conflict: Indicates that the request could not be processed because of conflict in the request, such as an edit conflict.\\n\\nGetting a garbled response from the Reddit API whenever I try to login. Stuff like:\\n\\n\\u003E\\u0081\\u00b27I\\u00da\\u000bE\\u00cb\\u00b2\\u001d\\u00bfjFy\\u203a\\u00fa.\\u00b9\\u00b6\\u0019\\u00e7z3\\u009d\\u017d\\u0006\\\"@\\u0011\\u0016H0\\n\\n Not sure what the issue could be?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1bde0f\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"onefingerattack\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1bde0f/getting_409_error_from_reddit_login_api/\", \"locked\": false, \"name\": \"t3_1bde0f\", \"created\": 1364785304.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1bde0f/getting_409_error_from_reddit_login_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Getting 409 error from Reddit login API.\", \"created_utc\": 1364756504.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ccode\\u003Epip install -U praw\\u003C/code\\u003E will update you to the latest version that supports listing wiki pages via \\u003Ccode\\u003Er.get_wiki_pages(\\u0026#39;subreddit\\u0026#39;)\\u003C/code\\u003E, creating / editing wiki pages via \\u003Ccode\\u003Er.edit_wiki_page(\\u0026#39;subreddit\\u0026#39;, \\u0026#39;page_title\\u0026#39;, \\u0026#39;content\\u0026#39;, \\u0026#39;reason\\u0026#39;)\\u003C/code\\u003E. Read the \\u003Ca href=\\\"http://praw.readthedocs.org/en/latest/pages/changelog.html#praw-2-0-13\\\"\\u003E2.0.13 changelog\\u003C/a\\u003E for additional changes. If there is additional functionality that you would like to see added, please \\u003Ca href=\\\"https://github.com/praw-dev/praw/issues?state=open\\\"\\u003Ecreate an issue\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"`pip install -U praw` will update you to the latest version that supports listing wiki pages via `r.get_wiki_pages('subreddit')`, creating / editing wiki pages via `r.edit_wiki_page('subreddit', 'page_title', 'content', 'reason')`. Read the [2.0.13 changelog](http://praw.readthedocs.org/en/latest/pages/changelog.html#praw-2-0-13) for additional changes. If there is additional functionality that you would like to see added, please [create an issue](https://github.com/praw-dev/praw/issues?state=open).\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1aczcr\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bboe\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1aczcr/praw_2013_adds_basic_wiki_editing_support/\", \"locked\": false, \"name\": \"t3_1aczcr\", \"created\": 1363397266.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1aczcr/praw_2013_adds_basic_wiki_editing_support/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"PRAW 2.0.13 adds basic wiki editing support\", \"created_utc\": 1363368466.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EEven something completely understandable, as the decision to make subreddits undeletable can turn horrendous when developing. Does any of you know a practical way of removing them from the sr-bar?\\nApparently they appear and disappear at will but there seems to be no way of making the changes persistent.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Even something completely understandable, as the decision to make subreddits undeletable can turn horrendous when developing. Does any of you know a practical way of removing them from the sr-bar?\\nApparently they appear and disappear at will but there seems to be no way of making the changes persistent.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1a2x0a\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"csosa\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1a2x0a/deleting_annoying_testreddits/\", \"locked\": false, \"name\": \"t3_1a2x0a\", \"created\": 1363037895.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1a2x0a/deleting_annoying_testreddits/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Deleting annoying test-reddits\", \"created_utc\": 1363009095.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ECould have sworn I read somewhere that the reddit team is working on adding wikis for subreddits, but now my reddit and google fu are failing me and I can\\u0026#39;t find anything about it.  Did I just dream this, or was it cancelled, or still a wip?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Could have sworn I read somewhere that the reddit team is working on adding wikis for subreddits, but now my reddit and google fu are failing me and I can't find anything about it.  Did I just dream this, or was it cancelled, or still a wip?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1279n0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"SkyMarshal\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1279n0/is_reddit_adding_a_wiki_feature/\", \"locked\": false, \"name\": \"t3_1279n0\", \"created\": 1351419407.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1279n0/is_reddit_adding_a_wiki_feature/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is reddit adding a wiki feature?\", \"created_utc\": 1351390607.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI updated the minor version of PRAW to account for some changes in the behavior of subreddit listing functions. The previous sort/filter defauts didn\\u0026#39;t make sense for the Subreddit instance functions \\u003Ccode\\u003Eget_new\\u003C/code\\u003E, \\u003Ccode\\u003Eget_controversial\\u003C/code\\u003E and \\u003Ccode\\u003Eget_top\\u003C/code\\u003E, thus I\\u0026#39;ve made it such that these generic functions use either the reddit or account default for non-logged in and logged in users respectively. As part of the change I\\u0026#39;ve added explicit functions for each of the possible listings. Those are:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003Eget_controversial_from_all\\u003C/li\\u003E\\n\\u003Cli\\u003Eget_controversial_from_day\\u003C/li\\u003E\\n\\u003Cli\\u003Eget_controversial_from_hour\\u003C/li\\u003E\\n\\u003Cli\\u003Eget_controversial_from_month\\u003C/li\\u003E\\n\\u003Cli\\u003Eget_controversial_from_week\\u003C/li\\u003E\\n\\u003Cli\\u003Eget_controversial_from_year\\u003C/li\\u003E\\n\\u003Cli\\u003Eget_new_by_date (this already existed)\\u003C/li\\u003E\\n\\u003Cli\\u003Eget_new_by_rising\\u003C/li\\u003E\\n\\u003Cli\\u003Eget_top_from_all\\u003C/li\\u003E\\n\\u003Cli\\u003Eget_top_from_day\\u003C/li\\u003E\\n\\u003Cli\\u003Eget_top_from_hour\\u003C/li\\u003E\\n\\u003Cli\\u003Eget_top_from_month\\u003C/li\\u003E\\n\\u003Cli\\u003Eget_top_from_week\\u003C/li\\u003E\\n\\u003Cli\\u003Eget_top_from_year\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EI recommend you use one of the explicit listings in your code. If you want to dynamically handle a filter the best way is to do something like:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esubreddit.get_top(url_data={\\u0026#39;t\\u0026#39;: top})\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Ewhere \\u003Ccode\\u003Etop\\u003C/code\\u003E is one of \\u003Ccode\\u003Eall\\u003C/code\\u003E, \\u003Ccode\\u003Eday\\u003C/code\\u003E, \\u003Ccode\\u003Ehour\\u003C/code\\u003E, \\u003Ccode\\u003Emonth\\u003C/code\\u003E, \\u003Ccode\\u003Eweek\\u003C/code\\u003E, or \\u003Ccode\\u003Eyear\\u003C/code\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/mellort/reddit_api/commit/447116bd25ae4404b1b5085e6f1d0a82ac2f0082\\\"\\u003EHere\\u0026#39;s\\u003C/a\\u003E the relevant change for those interested.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I updated the minor version of PRAW to account for some changes in the behavior of subreddit listing functions. The previous sort/filter defauts didn't make sense for the Subreddit instance functions `get_new`, `get_controversial` and `get_top`, thus I've made it such that these generic functions use either the reddit or account default for non-logged in and logged in users respectively. As part of the change I've added explicit functions for each of the possible listings. Those are:\\n\\n* get_controversial_from_all\\n* get_controversial_from_day\\n* get_controversial_from_hour\\n* get_controversial_from_month\\n* get_controversial_from_week\\n* get_controversial_from_year\\n* get_new_by_date (this already existed)\\n* get_new_by_rising\\n* get_top_from_all\\n* get_top_from_day\\n* get_top_from_hour\\n* get_top_from_month\\n* get_top_from_week\\n* get_top_from_year\\n\\nI recommend you use one of the explicit listings in your code. If you want to dynamically handle a filter the best way is to do something like:\\n\\n    subreddit.get_top(url_data={'t': top})\\n\\nwhere `top` is one of `all`, `day`, `hour`, `month`, `week`, or `year`.\\n\\n[Here's](https://github.com/mellort/reddit_api/commit/447116bd25ae4404b1b5085e6f1d0a82ac2f0082) the relevant change for those interested.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"vjqb7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bboe\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/vjqb7/python_reddit_api_wrapper_praw_version_140_changes/\", \"locked\": false, \"name\": \"t3_vjqb7\", \"created\": 1340614130.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/vjqb7/python_reddit_api_wrapper_praw_version_140_changes/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Python Reddit API Wrapper (PRAW) version 1.4.0 changes\", \"created_utc\": 1340585330.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAs \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/txuic/search_syntax_update_returning_to_lucene_by/\\\"\\u003Epreviously mentioned\\u003C/a\\u003E, the search syntax now, by default, uses the same limited Lucene syntax from days of yore. API developers desiring to use CloudSearch syntax directly may continue to do so by adding \\u003Ccode\\u003E\\u0026amp;syntax=cloudsearch\\u003C/code\\u003E to their queries (I recommend explicitly adding \\u003Ccode\\u003E\\u0026amp;syntax=lucene\\u003C/code\\u003E for forwards compatibility, if you\\u0026#39;re utilizing the lucene syntax).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EErrors should be reported to \\u003Ca href=\\\"/r/bugs\\\"\\u003E/r/bugs\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/r/changelog/comments/uytnm/lucene_search_syntax_has_returned/\\\"\\u003E/r/changelog post\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"As [previously mentioned](http://www.reddit.com/r/redditdev/comments/txuic/search_syntax_update_returning_to_lucene_by/), the search syntax now, by default, uses the same limited Lucene syntax from days of yore. API developers desiring to use CloudSearch syntax directly may continue to do so by adding `\\u0026syntax=cloudsearch` to their queries (I recommend explicitly adding `\\u0026syntax=lucene` for forwards compatibility, if you're utilizing the lucene syntax).\\n\\nErrors should be reported to /r/bugs.\\n\\n[/r/changelog post](http://www.reddit.com/r/changelog/comments/uytnm/lucene_search_syntax_has_returned/)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"uytxd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/uytxd/search_syntax_update_lucene_is_the_default_search/\", \"locked\": false, \"name\": \"t3_uytxd\", \"created\": 1339572608.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/uytxd/search_syntax_update_lucene_is_the_default_search/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Search syntax update: Lucene is the default search syntax\", \"created_utc\": 1339543808.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi !\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s even more mysterious actually : sorting the month or day or hour comments by score works partially, sorting the week comments by score doesn\\u0026#39;t work at all, and sorting the all time comments by score actually sorts them by date.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi !\\n\\nIt's even more mysterious actually : sorting the month or day or hour comments by score works partially, sorting the week comments by score doesn't work at all, and sorting the all time comments by score actually sorts them by date.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"uusnj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/uusnj/why_is_it_impossible_to_sort_my_comments_by_score/\", \"locked\": false, \"name\": \"t3_uusnj\", \"created\": 1339378378.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/uusnj/why_is_it_impossible_to_sort_my_comments_by_score/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why is it impossible to sort my comments by score ?\", \"created_utc\": 1339349578.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have a small reddit installation for an intranet.  There are so few people that only giving each person 1 vote per topic/comment provides really poor resolution to the point where it is almost useless.  It would be more useful to be able to vote up good topics say 10 times and mediocre topics 3 times, etc.  It\\u0026#39;s private so there isn\\u0026#39;t going to be abuse, but even 10 votes per voting opportunity would be sufficient.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have a small reddit installation for an intranet.  There are so few people that only giving each person 1 vote per topic/comment provides really poor resolution to the point where it is almost useless.  It would be more useful to be able to vote up good topics say 10 times and mediocre topics 3 times, etc.  It's private so there isn't going to be abuse, but even 10 votes per voting opportunity would be sufficient.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ukvpb\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"rox0r\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ukvpb/possible_to_let_people_vote_more_than_once/\", \"locked\": false, \"name\": \"t3_ukvpb\", \"created\": 1338873741.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ukvpb/possible_to_let_people_vote_more_than_once/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Possible to let people vote more than once?  \", \"created_utc\": 1338844941.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI work on experimental search technologies that I believe have better result ranking for domain-specific types of documents than general purpose search engines.   It\\u0026#39;s well tested in law enforcement crime data, recognizing things like how matching terms in a victim\\u0026#39;s description are more relevant terms than a vehicle\\u0026#39;s description a sexual assault; while the opposite is true for a vehicle theft document.   I\\u0026#39;m interested to see how it does on other types of documents.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI believe reddit data would be an excellent data set for such an engine -- where things like upvotes, and the reputation of a commenter, and the amount of further discussion a comment inspires, etc. could all influence ranking.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo test this, I\\u0026#39;d love to crawl (or suck through an API) quite a bit of reddit articles and comments.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there a recommended friendly way to do this?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf it matters, the search engine\\u0026#39;s F/OSS based (lucene extensions), and we\\u0026#39;d have no problem releasing them - as well as any reddit-specific stuff on top of them - as open source.   We\\u0026#39;ve happily submitted previous patches back to lucene for earlier extensions that proved useful to us.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I work on experimental search technologies that I believe have better result ranking for domain-specific types of documents than general purpose search engines.   It's well tested in law enforcement crime data, recognizing things like how matching terms in a victim's description are more relevant terms than a vehicle's description a sexual assault; while the opposite is true for a vehicle theft document.   I'm interested to see how it does on other types of documents.\\n\\nI believe reddit data would be an excellent data set for such an engine -- where things like upvotes, and the reputation of a commenter, and the amount of further discussion a comment inspires, etc. could all influence ranking.\\n\\nTo test this, I'd love to crawl (or suck through an API) quite a bit of reddit articles and comments.\\n\\nIs there a recommended friendly way to do this?\\n\\n\\nIf it matters, the search engine's F/OSS based (lucene extensions), and we'd have no problem releasing them - as well as any reddit-specific stuff on top of them - as open source.   We've happily submitted previous patches back to lucene for earlier extensions that proved useful to us.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"q49ml\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"rmxz\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/q49ml/whats_the_friendliest_to_the_reddit_servers_way/\", \"locked\": false, \"name\": \"t3_q49ml\", \"created\": 1330132860.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/q49ml/whats_the_friendliest_to_the_reddit_servers_way/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"What's the friendliest (to the reddit servers) way of downloading a significant amount of articles and comments from Reddit for an experimental search engine?\", \"created_utc\": 1330104060.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m writing some mod bots for a couple default subreddits and one thing I may do is scan a users history before approving posts. I\\u0026#39;m worried that scanning a couple hundred users every couple hours (in additional to doing other stuff) will get me banned for abuse.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm writing some mod bots for a couple default subreddits and one thing I may do is scan a users history before approving posts. I'm worried that scanning a couple hundred users every couple hours (in additional to doing other stuff) will get me banned for abuse.\\n\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"owf76\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"roger_\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/owf76/how_many_requests_can_i_make_per_minute_before_im/\", \"locked\": false, \"name\": \"t3_owf76\", \"created\": 1327548372.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/owf76/how_many_requests_can_i_make_per_minute_before_im/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How many requests can I make per minute before I'm banned?\", \"created_utc\": 1327519572.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"olz33\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ugart\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/olz33/so_is_new_code_being_pushed_or_maintenance_being/\", \"locked\": false, \"name\": \"t3_olz33\", \"created\": 1326891204.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/olz33/so_is_new_code_being_pushed_or_maintenance_being/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"So is new code being pushed or maintenance being done while the site is down for SOPA?\", \"created_utc\": 1326862404.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ELet me prepend this by saying I don\\u0026#39;t want to sound self-righteous or that I even know anything about Reddit arch at all.  I want to try to help make self posts faster.  I hate it when users try to tell me technically how to fix something when they have no idea :)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECould someone please explain why self-links are so slow?  For me, it\\u0026#39;s taking a whole 6 seconds just to fetch a little bit of text.  Is the actual operation expensive or are there other issues that are making it slow?  I can\\u0026#39;t see how looking up a simple key from a database could be slow, no joins or other craziness should be involved.  Also, does it really need to be a POST?  If you made it a GET instead of a POST, your caches might work better with it (and it doesn\\u0026#39;t seem like the post data is used anywhere).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease excuse my ignorance, I just really think that self-posts should be near instant.  The reddit I love is being destroyed by the lack of decent self-post support, people just post it as an image to imgur.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Let me prepend this by saying I don't want to sound self-righteous or that I even know anything about Reddit arch at all.  I want to try to help make self posts faster.  I hate it when users try to tell me technically how to fix something when they have no idea :)\\n\\nCould someone please explain why self-links are so slow?  For me, it's taking a whole 6 seconds just to fetch a little bit of text.  Is the actual operation expensive or are there other issues that are making it slow?  I can't see how looking up a simple key from a database could be slow, no joins or other craziness should be involved.  Also, does it really need to be a POST?  If you made it a GET instead of a POST, your caches might work better with it (and it doesn't seem like the post data is used anywhere).\\n\\nPlease excuse my ignorance, I just really think that self-posts should be near instant.  The reddit I love is being destroyed by the lack of decent self-post support, people just post it as an image to imgur.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"lh0yv\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"BBHoss\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/lh0yv/slow_selfposts/\", \"locked\": false, \"name\": \"t3_lh0yv\", \"created\": 1319020219.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/lh0yv/slow_selfposts/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Slow self-posts\", \"created_utc\": 1318991419.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThere\\u0026#39;s a attribute of a story that appears in the JSON response that I\\u0026#39;m not sure how to manipulate via API or just using the site with my browser. Anyone know how to mark a Story as \\u0026#39;clicked\\u0026#39;?\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EHere\\u0026#39;s a snippet of the JSON listing for the story:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E{\\n  \\u0026quot;kind\\u0026quot;: \\u0026quot;t3\\u0026quot;,\\n  \\u0026quot;data\\u0026quot;: {\\n      \\u0026quot;domain\\u0026quot;: \\u0026quot;self.subreddit\\u0026quot;,\\n      \\u0026quot;media_embed\\u0026quot;: {},\\n      \\u0026quot;levenshtein\\u0026quot;: null,\\n      ...\\n      \\u0026quot;id\\u0026quot;: \\u0026quot;my1id\\u0026quot;,\\n-\\u0026gt;    \\u0026quot;clicked\\u0026quot;: false, \\u0026lt;-\\n      \\u0026quot;title\\u0026quot;: \\u0026quot;Test\\u0026quot;,\\n      ...\\n      \\u0026quot;created_utc\\u0026quot;: 1317846372.0,\\n      \\u0026quot;num_comments\\u0026quot;: 0,\\n      \\u0026quot;ups\\u0026quot;: 1\\n  }\\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"There's a attribute of a story that appears in the JSON response that I'm not sure how to manipulate via API or just using the site with my browser. Anyone know how to mark a Story as 'clicked'?\\n***\\nHere's a snippet of the JSON listing for the story:\\n\\n    {\\n      \\\"kind\\\": \\\"t3\\\",\\n      \\\"data\\\": {\\n          \\\"domain\\\": \\\"self.subreddit\\\",\\n          \\\"media_embed\\\": {},\\n          \\\"levenshtein\\\": null,\\n          ...\\n          \\\"id\\\": \\\"my1id\\\",\\n    -\\u003E    \\\"clicked\\\": false, \\u003C-\\n          \\\"title\\\": \\\"Test\\\",\\n          ...\\n          \\\"created_utc\\\": 1317846372.0,\\n          \\\"num_comments\\\": 0,\\n          \\\"ups\\\": 1\\n      }\\n    }\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"l2ymq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"cloudedice\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/l2ymq/api_to_click_a_story/\", \"locked\": false, \"name\": \"t3_l2ymq\", \"created\": 1317936127.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/l2ymq/api_to_click_a_story/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API to 'click' a story?\", \"created_utc\": 1317907327.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESome of you probably won\\u0026#39;t see this as a problem, and if you don\\u0026#39;t, I can respect that. I\\u0026#39;m not saying smaller subreddits deserve to have more subscribers, but I think there needs to be a new method built into Reddit to allow that to happen, like a recommendation system, since the current method isn\\u0026#39;t working as well as it should. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThere are a couple of ways for people to find out about new reddits currently:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003EPost in \\u003Ca href=\\\"/r/newreddits\\\"\\u003E/r/newreddits\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003EAllow the reddit to be shown in the default set\\u003C/li\\u003E\\n\\u003Cli\\u003Esearch via /Reddits\\u003C/li\\u003E\\n\\u003Cli\\u003ESame links that can be shown to have been placed in different reddits\\u003C/li\\u003E\\n\\u003Cli\\u003ELinks from generous mods in the subreddit description like so ----\\u0026gt;\\u003C/li\\u003E\\n\\u003Cli\\u003EPost topics in similar reddits and link off\\u003C/li\\u003E\\n\\u003Cli\\u003EPaid (sometimes free) Advertise on the site\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ca href=\\\"http://userscripts.org/scripts/show/75183\\\"\\u003ERandomit\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003ESome subreddits that are active, make good posts often, and can get an active following with good word of mouth, can expand greatly as far as subscribers. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBut for instance, KeyserSosa or Raldi said on \\u003Ca href=\\\"http://thedrilldown.com/2010/09/02/the-drill-down-151-surfing-with-the-aliens/\\\"\\u003EDrillDown\\u003C/a\\u003E that they just heard of \\u003Ca href=\\\"/r/zombies\\\"\\u003E/r/zombies\\u003C/a\\u003E this past week, a community of ~8,500 for 2 years. There are about ~100 subreddits with over 10,000 subscribers and thousands of subreddits with less than 100 subscribers. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMost have less than 100 for many reasons like not being active, no content, unusual topic, etc. This isn\\u0026#39;t about that, but about how to develop and spur activity more uniformly across smaller subreddits. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESites like \\u003Ca href=\\\"http://metareddit.com/\\\"\\u003Emetareddit\\u003C/a\\u003E or \\u003Ca href=\\\"http://subredditfinder.com/\\\"\\u003Esubredditfinder\\u003C/a\\u003E can help in their own way, but are not close to being efficient. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETags will not work and the Reddit Admins despise it with a passion. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI was thinking of add a recommend button to /reddits to show reddits that you may like based off of the reddits you currently subscribe to and posts you may like. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe issues I can see is not just creating an algorithm to analyze and recommend subreddits, but to reduce the server load to perform such an analysis as well. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat do you developers think about creating that kind of system for reddit and taking on this massive project?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOr is this something better left for the Reddit Admins to decide once they have the resources to do so? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECan you guys think of some other solutions?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks for your input! I certainly would like to see more than 0.56% of \\u003Ca href=\\\"/r/programming\\\"\\u003E/r/programming\\u003C/a\\u003E subscribed to \\u003Ca href=\\\"/r/redditdev\\\"\\u003E/r/redditdev\\u003C/a\\u003E. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT\\u003C/strong\\u003E: ketralnis made an announcement asking for help in this project and people to allow their data to be shared in preferences. \\u003Ca href=\\\"http://www.reddit.com/r/announcements/comments/ddz0s/reddit_wants_your_permission_to_use_your_data_for/\\\"\\u003Ehttp://www.reddit.com/r/announcements/comments/ddz0s/reddit_wants_your_permission_to_use_your_data_for/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EThank you ketralnis! :)\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Some of you probably won't see this as a problem, and if you don't, I can respect that. I'm not saying smaller subreddits deserve to have more subscribers, but I think there needs to be a new method built into Reddit to allow that to happen, like a recommendation system, since the current method isn't working as well as it should. \\n\\n\\nThere are a couple of ways for people to find out about new reddits currently:\\n\\n1. Post in /r/newreddits\\n2. Allow the reddit to be shown in the default set\\n3. search via /Reddits\\n4. Same links that can be shown to have been placed in different reddits\\n5. Links from generous mods in the subreddit description like so ----\\u003E\\n6. Post topics in similar reddits and link off\\n7. Paid (sometimes free) Advertise on the site\\n8. [Randomit](http://userscripts.org/scripts/show/75183)\\n\\nSome subreddits that are active, make good posts often, and can get an active following with good word of mouth, can expand greatly as far as subscribers. \\n\\nBut for instance, KeyserSosa or Raldi said on [DrillDown](http://thedrilldown.com/2010/09/02/the-drill-down-151-surfing-with-the-aliens/) that they just heard of /r/zombies this past week, a community of ~8,500 for 2 years. There are about ~100 subreddits with over 10,000 subscribers and thousands of subreddits with less than 100 subscribers. \\n\\nMost have less than 100 for many reasons like not being active, no content, unusual topic, etc. This isn't about that, but about how to develop and spur activity more uniformly across smaller subreddits. \\n\\nSites like [metareddit](http://metareddit.com/) or [subredditfinder](http://subredditfinder.com/) can help in their own way, but are not close to being efficient. \\n\\nTags will not work and the Reddit Admins despise it with a passion. \\n\\nI was thinking of add a recommend button to /reddits to show reddits that you may like based off of the reddits you currently subscribe to and posts you may like. \\n\\nThe issues I can see is not just creating an algorithm to analyze and recommend subreddits, but to reduce the server load to perform such an analysis as well. \\n\\nWhat do you developers think about creating that kind of system for reddit and taking on this massive project?\\n\\nOr is this something better left for the Reddit Admins to decide once they have the resources to do so? \\n\\nCan you guys think of some other solutions?\\n\\nThanks for your input! I certainly would like to see more than 0.56% of /r/programming subscribed to /r/redditdev. \\n\\n**EDIT**: ketralnis made an announcement asking for help in this project and people to allow their data to be shared in preferences. http://www.reddit.com/r/announcements/comments/ddz0s/reddit_wants_your_permission_to_use_your_data_for/\\n\\n**Thank you ketralnis! :)**\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"d95ad\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 14, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/d95ad/request_we_need_to_work_on_a_solution_to_the/\", \"locked\": false, \"name\": \"t3_d95ad\", \"created\": 1283549293.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/d95ad/request_we_need_to_work_on_a_solution_to_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Request: We need to work on a solution to the subreddit size-ratio problem. If we could fix search, then in Valve time, we can fix this! \", \"created_utc\": 1283520493.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 14}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;d really appreciate it if any of you with the reddit VM could spare a couple of minutes and give my patch a whirl.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s available here: \\u003Ca href=\\\"http://code.reddit.com/ticket/940\\\"\\u003Ehttp://code.reddit.com/ticket/940\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEssentially it saves any collapsed comments into localStorage so that if you return to that particular page it remembers and auto-hides it when you come back. Make sure to use the last patch in the comment thread.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'd really appreciate it if any of you with the reddit VM could spare a couple of minutes and give my patch a whirl.\\n\\nIt's available here: http://code.reddit.com/ticket/940\\n\\nEssentially it saves any collapsed comments into localStorage so that if you return to that particular page it remembers and auto-hides it when you come back. Make sure to use the last patch in the comment thread.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"cszqw\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"umbrae\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/cszqw/patch_review_save_hidden_comments_across_page/\", \"locked\": false, \"name\": \"t3_cszqw\", \"created\": 1279941298.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/cszqw/patch_review_save_hidden_comments_across_page/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Patch Review] Save Hidden Comments across Page Loads\", \"created_utc\": 1279912498.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m thinking along the lines of links clicked per hour/day, time spent on reddit each day/month/year, profile of an average redditor generated using the aggregated stats of each individual, male/female percentage, etc. Feel free to suggest other stats you would like to see.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm thinking along the lines of links clicked per hour/day, time spent on reddit each day/month/year, profile of an average redditor generated using the aggregated stats of each individual, male/female percentage, etc. Feel free to suggest other stats you would like to see.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"9rz47\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"JHKennedy4\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 14, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/9rz47/would_anyone_else_be_interested_in_in_depth_user/\", \"locked\": false, \"name\": \"t3_9rz47\", \"created\": 1255019287.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/9rz47/would_anyone_else_be_interested_in_in_depth_user/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Would anyone else be interested in in depth user statistics as well as statistics on reddit users as a whole?\", \"created_utc\": 1254990487.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Egeoredd\\u0026#39;s been spamming the same nonsense over and over, and regardless of whether you agree with his political beliefs you have to admit that his constant \\u0026quot;vote up or the secret PR cabal will bury this story\\u0026quot; is getting extremely annoying. Even on his stories that are successful (end up in the positive point range), a lot of the comments are complaints about his style, but he refuses to stop. Since it\\u0026#39;s not appropriate to ban him, I feel it is more appropriate to allow us to block him. Maybe this could be implemented as an auto-hide feature, in which all of a blocked users stories are hidden by default.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/user/georedd/submitted\\\"\\u003Ehttp://www.reddit.com/user/georedd/submitted\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"georedd's been spamming the same nonsense over and over, and regardless of whether you agree with his political beliefs you have to admit that his constant \\\"vote up or the secret PR cabal will bury this story\\\" is getting extremely annoying. Even on his stories that are successful (end up in the positive point range), a lot of the comments are complaints about his style, but he refuses to stop. Since it's not appropriate to ban him, I feel it is more appropriate to allow us to block him. Maybe this could be implemented as an auto-hide feature, in which all of a blocked users stories are hidden by default.\\n\\nhttp://www.reddit.com/user/georedd/submitted\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"98omj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"m1ss1ontomars2k4\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/98omj/this_has_been_asked_before_but_its_never_been_as/\", \"locked\": false, \"name\": \"t3_98omj\", \"created\": 1249742771.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/98omj/this_has_been_asked_before_but_its_never_been_as/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"This has been asked before, but it's never been as dire as it is now: Can we have the option to block certain users?\", \"created_utc\": 1249713971.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI heard that reddit will be bringing in an image service to supersede Imgur.  Will the image url\\u0026#39;s be accessible via the reddit REST API?  \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I heard that reddit will be bringing in an image service to supersede Imgur.  Will the image url's be accessible via the reddit REST API?  \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4liqma\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"kiwiheretic\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4liqma/the_new_reddit_image_service/\", \"locked\": false, \"name\": \"t3_4liqma\", \"created\": 1464518108.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4liqma/the_new_reddit_image_service/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"The new reddit image service\", \"created_utc\": 1464489308.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey guys! \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo what started as a simple script to log into my account became a large and fun app that flies orange envelopes across your screen whenever you get a notification. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI made \\u003Ca href=\\\"https://www.youtube.com/watch?v=dJA7WzcGM9I\\\"\\u003Ea demo video\\u003C/a\\u003E for you guys to checkout which explains more and shows the installations process.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis project took me a while because I wanted it to install and update seamlessly, and I also wanted it to be extendable. You configure it to check different things on reddit and you can instantly notify yourself when something happens. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECheckout that video and \\u003Ca href=\\\"https://github.com/FlyingOranger/FlyingOranger\\\"\\u003Ethe github\\u003C/a\\u003E if you want.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks guys! Let me know what you think.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey guys! \\n\\nSo what started as a simple script to log into my account became a large and fun app that flies orange envelopes across your screen whenever you get a notification. \\n\\nI made [a demo video](https://www.youtube.com/watch?v=dJA7WzcGM9I) for you guys to checkout which explains more and shows the installations process.\\n\\nThis project took me a while because I wanted it to install and update seamlessly, and I also wanted it to be extendable. You configure it to check different things on reddit and you can instantly notify yourself when something happens. \\n\\nCheckout that video and [the github](https://github.com/FlyingOranger/FlyingOranger) if you want.\\n\\nThanks guys! Let me know what you think.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4l62l5\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"JarofHearts\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4l62l5/i_just_finished_my_first_app_for_reddit_flying/\", \"locked\": false, \"name\": \"t3_4l62l5\", \"created\": 1464307875.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4l62l5/i_just_finished_my_first_app_for_reddit_flying/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I just finished my first app for reddit, Flying Oranger! I'd really appreciate any feedback.\", \"created_utc\": 1464279075.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESearch capabilities for Reddit are much to be desired, so I\\u0026#39;ve created some tools to help with that.  First, this is a back-end API that returns JSON and is primarily geared towards front-end developers -- but that doesn\\u0026#39;t mean you can\\u0026#39;t use it.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EI would recommend before continuing that you make sure you have a JSON prettifier installed.\\u003C/strong\\u003E  If you use Chrome, this is the one I use:  \\u003Ca href=\\\"https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc?hl=en\\\"\\u003Ehttps://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc?hl=en\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EGetting Started\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThese API calls will search the previous 2-3 months of Reddit data.  Eventually, if I can get enough server power, I\\u0026#39;ll increase this to the entirety of Reddit historical submissions and comments.  There are two primary API calls.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo search comments, use the endpoint: \\u003Ca href=\\\"https://api.pushshift.io/reddit/search/comment\\\"\\u003Ehttps://api.pushshift.io/reddit/search/comment\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo search submissions, use the endpoint: \\u003Ca href=\\\"https://api.pushshift.io/reddit/search/submission\\\"\\u003Ehttps://api.pushshift.io/reddit/search/submission\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EHow to search Reddit comments\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELet\\u0026#39;s pick a topic and some scenarios for our example.  Linode has been experiencing a lot of problems lately.  You are interested in viewing recent comments that mention linode.  I\\u0026#39;m going to walk through some API examples on how you could do searches.  Please keep in mind that searches are not case-sensitive.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EView the past 5 comments where someone mentions Linode\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search/comment?q=linode\\u0026amp;limit=5\\\"\\u003Ehttps://api.pushshift.io/reddit/search/comment?q=linode\\u0026amp;limit=5\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EView the next 5 comments before a specific comment id\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search/comment?q=linode\\u0026amp;limit=5\\u0026amp;before_id=cyowu24\\\"\\u003Ehttps://api.pushshift.io/reddit/search/comment?q=linode\\u0026amp;limit=5\\u0026amp;before_id=cyowu24\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EView comments older than a day but newer than 2 days ago\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search/comment?q=linode\\u0026amp;before=1d\\u0026amp;after=2d\\\"\\u003Ehttps://api.pushshift.io/reddit/search/comment?q=linode\\u0026amp;before=1d\\u0026amp;after=2d\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EView comments older than 2 hours ago but newer than 8 hours ago only in the subreddit sysadmin\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search/comment?q=linode\\u0026amp;before=2h\\u0026amp;after=8h\\u0026amp;subreddit=sysadmin\\\"\\u003Ehttps://api.pushshift.io/reddit/search/comment?q=linode\\u0026amp;before=2h\\u0026amp;after=8h\\u0026amp;subreddit=sysadmin\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EView comments from the author \\u0026quot;automoderator\\u0026quot; mentioning the term \\u0026quot;submissions\\u0026quot;\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search/comment?q=submission\\u0026amp;author=automoderator\\\"\\u003Ehttps://api.pushshift.io/reddit/search/comment?q=submission\\u0026amp;author=automoderator\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EHow to search Reddit submissions\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESearching Reddit submissions is almost exactly like searching Reddit comments.  However, the submission search works a little bit differently than Reddit\\u0026#39;s.  It doesn\\u0026#39;t look for the search parameter in the submission title or selftext itself.  Instead, it actually scans every reddit comment, sums up the number of hits by thread and shows the threads mentioning the search term most frequently.  This allows for a more exhaustive search.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EView submissions related to linode over the past 3 days in the subreddit sysadmin\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search/submission?q=linode\\u0026amp;subreddit=sysadmin\\u0026amp;after=3d\\\"\\u003Ehttps://api.pushshift.io/reddit/search/submission?q=linode\\u0026amp;subreddit=sysadmin\\u0026amp;after=3d\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EView submissions related to linode in any subreddit over the past week\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search/submission?q=linode\\u0026amp;after=1w\\\"\\u003Ehttps://api.pushshift.io/reddit/search/submission?q=linode\\u0026amp;after=1w\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EView submissions related to germany in the subreddit worldnews\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.pushshift.io/reddit/search/submission?q=germany\\u0026amp;subreddit=worldnews\\\"\\u003Ehttps://api.pushshift.io/reddit/search/submission?q=germany\\u0026amp;subreddit=worldnews\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Search capabilities for Reddit are much to be desired, so I've created some tools to help with that.  First, this is a back-end API that returns JSON and is primarily geared towards front-end developers -- but that doesn't mean you can't use it.  \\n\\n**I would recommend before continuing that you make sure you have a JSON prettifier installed.**  If you use Chrome, this is the one I use:  https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc?hl=en\\n\\n**Getting Started**\\n\\nThese API calls will search the previous 2-3 months of Reddit data.  Eventually, if I can get enough server power, I'll increase this to the entirety of Reddit historical submissions and comments.  There are two primary API calls.  \\n\\nTo search comments, use the endpoint: https://api.pushshift.io/reddit/search/comment\\n\\nTo search submissions, use the endpoint: https://api.pushshift.io/reddit/search/submission\\n\\n**How to search Reddit comments**\\n\\nLet's pick a topic and some scenarios for our example.  Linode has been experiencing a lot of problems lately.  You are interested in viewing recent comments that mention linode.  I'm going to walk through some API examples on how you could do searches.  Please keep in mind that searches are not case-sensitive.\\n\\n**View the past 5 comments where someone mentions Linode**\\n\\nhttps://api.pushshift.io/reddit/search/comment?q=linode\\u0026limit=5\\n\\n**View the next 5 comments before a specific comment id**\\n\\nhttps://api.pushshift.io/reddit/search/comment?q=linode\\u0026limit=5\\u0026before_id=cyowu24\\n\\n**View comments older than a day but newer than 2 days ago**\\n\\nhttps://api.pushshift.io/reddit/search/comment?q=linode\\u0026before=1d\\u0026after=2d\\n\\n**View comments older than 2 hours ago but newer than 8 hours ago only in the subreddit sysadmin**\\n\\nhttps://api.pushshift.io/reddit/search/comment?q=linode\\u0026before=2h\\u0026after=8h\\u0026subreddit=sysadmin\\n\\n**View comments from the author \\\"automoderator\\\" mentioning the term \\\"submissions\\\"**\\n\\nhttps://api.pushshift.io/reddit/search/comment?q=submission\\u0026author=automoderator\\n\\n\\n**How to search Reddit submissions**\\n\\nSearching Reddit submissions is almost exactly like searching Reddit comments.  However, the submission search works a little bit differently than Reddit's.  It doesn't look for the search parameter in the submission title or selftext itself.  Instead, it actually scans every reddit comment, sums up the number of hits by thread and shows the threads mentioning the search term most frequently.  This allows for a more exhaustive search.\\n\\n**View submissions related to linode over the past 3 days in the subreddit sysadmin**\\n\\nhttps://api.pushshift.io/reddit/search/submission?q=linode\\u0026subreddit=sysadmin\\u0026after=3d\\n\\n**View submissions related to linode in any subreddit over the past week**\\n\\nhttps://api.pushshift.io/reddit/search/submission?q=linode\\u0026after=1w\\n\\n**View submissions related to germany in the subreddit worldnews**\\n\\nhttps://api.pushshift.io/reddit/search/submission?q=germany\\u0026subreddit=worldnews\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3zug2y\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3zug2y/a_tool_for_searching_reddit_comments_and/\", \"locked\": false, \"name\": \"t3_3zug2y\", \"created\": 1452180209.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3zug2y/a_tool_for_searching_reddit_comments_and/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"A tool for searching Reddit Comments and Submissions (X-post from /r/sysadmin)\", \"created_utc\": 1452151409.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m seeing some super strange behavior here.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhen using an old Chrome User-Agent header, the Reddit API seems to respond two orders of magnitude slower than when I use a recent curl UA string. Anyone else seen this? Verify it for yourself.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u279c  ~  for i in `seq 1 10`; do time curl -o /dev/null -L -H\\u0026quot;User-Agent:curl/7.37.1\\u0026quot; -H\\u0026quot;Accept:application/json\\u0026quot; http://api.reddit.com/r/random.json 2\\u0026gt;/dev/null; done; for i in `seq 1 10`; do time curl -o /dev/null -L -H\\u0026quot;User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36\\u0026quot; -H\\u0026quot;Accept:application/json\\u0026quot; http://api.reddit.com/r/random.json 2\\u0026gt;/dev/null; done;\\ncurl -o /dev/null -L -H\\u0026quot;User-Agent:curl/7.37.1\\u0026quot; -H\\u0026quot;Accept:application/json\\u0026quot;    0.00s user 0.01s system 2% cpu 0.450 total\\ncurl -o /dev/null -L -H\\u0026quot;User-Agent:curl/7.37.1\\u0026quot; -H\\u0026quot;Accept:application/json\\u0026quot;    0.00s user 0.01s system 2% cpu 0.415 total\\ncurl -o /dev/null -L -H\\u0026quot;User-Agent:curl/7.37.1\\u0026quot; -H\\u0026quot;Accept:application/json\\u0026quot;    0.00s user 0.00s system 2% cpu 0.370 total\\ncurl -o /dev/null -L -H\\u0026quot;User-Agent:curl/7.37.1\\u0026quot; -H\\u0026quot;Accept:application/json\\u0026quot;    0.00s user 0.01s system 3% cpu 0.297 total\\ncurl -o /dev/null -L -H\\u0026quot;User-Agent:curl/7.37.1\\u0026quot; -H\\u0026quot;Accept:application/json\\u0026quot;    0.00s user 0.00s system 2% cpu 0.283 total\\ncurl -o /dev/null -L -H\\u0026quot;User-Agent:curl/7.37.1\\u0026quot; -H\\u0026quot;Accept:application/json\\u0026quot;    0.00s user 0.01s system 2% cpu 0.342 total\\ncurl -o /dev/null -L -H\\u0026quot;User-Agent:curl/7.37.1\\u0026quot; -H\\u0026quot;Accept:application/json\\u0026quot;    0.00s user 0.01s system 3% cpu 0.324 total\\ncurl -o /dev/null -L -H\\u0026quot;User-Agent:curl/7.37.1\\u0026quot; -H\\u0026quot;Accept:application/json\\u0026quot;    0.00s user 0.01s system 3% cpu 0.331 total\\ncurl -o /dev/null -L -H\\u0026quot;User-Agent:curl/7.37.1\\u0026quot; -H\\u0026quot;Accept:application/json\\u0026quot;    0.00s user 0.01s system 1% cpu 0.576 total\\ncurl -o /dev/null -L -H\\u0026quot;User-Agent:curl/7.37.1\\u0026quot; -H\\u0026quot;Accept:application/json\\u0026quot;    0.00s user 0.01s system 2% cpu 0.348 total\\ncurl -o /dev/null -L  -H\\u0026quot;Accept:application/json\\u0026quot;  2\\u0026gt; /dev/null  0.01s user 0.01s system 0% cpu 19.026 total\\ncurl -o /dev/null -L  -H\\u0026quot;Accept:application/json\\u0026quot;  2\\u0026gt; /dev/null  0.01s user 0.01s system 0% cpu 19.769 total\\ncurl -o /dev/null -L  -H\\u0026quot;Accept:application/json\\u0026quot;  2\\u0026gt; /dev/null  0.00s user 0.01s system 1% cpu 0.618 total\\ncurl -o /dev/null -L  -H\\u0026quot;Accept:application/json\\u0026quot;  2\\u0026gt; /dev/null  0.01s user 0.01s system 0% cpu 20.066 total\\ncurl -o /dev/null -L  -H\\u0026quot;Accept:application/json\\u0026quot;  2\\u0026gt; /dev/null  0.01s user 0.01s system 0% cpu 18.676 total\\ncurl -o /dev/null -L  -H\\u0026quot;Accept:application/json\\u0026quot;  2\\u0026gt; /dev/null  0.01s user 0.01s system 0% cpu 20.104 total\\ncurl -o /dev/null -L  -H\\u0026quot;Accept:application/json\\u0026quot;  2\\u0026gt; /dev/null  0.01s user 0.01s system 0% cpu 20.299 total\\ncurl -o /dev/null -L  -H\\u0026quot;Accept:application/json\\u0026quot;  2\\u0026gt; /dev/null  0.01s user 0.01s system 0% cpu 10.778 total\\ncurl -o /dev/null -L  -H\\u0026quot;Accept:application/json\\u0026quot;  2\\u0026gt; /dev/null  0.01s user 0.01s system 0% cpu 9.397 total\\ncurl -o /dev/null -L  -H\\u0026quot;Accept:application/json\\u0026quot;  2\\u0026gt; /dev/null  0.01s user 0.01s system 0% cpu 10.856 total\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm seeing some super strange behavior here.\\n\\nWhen using an old Chrome User-Agent header, the Reddit API seems to respond two orders of magnitude slower than when I use a recent curl UA string. Anyone else seen this? Verify it for yourself.\\n\\n    \\u279c  ~  for i in `seq 1 10`; do time curl -o /dev/null -L -H\\\"User-Agent:curl/7.37.1\\\" -H\\\"Accept:application/json\\\" http://api.reddit.com/r/random.json 2\\u003E/dev/null; done; for i in `seq 1 10`; do time curl -o /dev/null -L -H\\\"User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36\\\" -H\\\"Accept:application/json\\\" http://api.reddit.com/r/random.json 2\\u003E/dev/null; done;\\n    curl -o /dev/null -L -H\\\"User-Agent:curl/7.37.1\\\" -H\\\"Accept:application/json\\\"    0.00s user 0.01s system 2% cpu 0.450 total\\n    curl -o /dev/null -L -H\\\"User-Agent:curl/7.37.1\\\" -H\\\"Accept:application/json\\\"    0.00s user 0.01s system 2% cpu 0.415 total\\n    curl -o /dev/null -L -H\\\"User-Agent:curl/7.37.1\\\" -H\\\"Accept:application/json\\\"    0.00s user 0.00s system 2% cpu 0.370 total\\n    curl -o /dev/null -L -H\\\"User-Agent:curl/7.37.1\\\" -H\\\"Accept:application/json\\\"    0.00s user 0.01s system 3% cpu 0.297 total\\n    curl -o /dev/null -L -H\\\"User-Agent:curl/7.37.1\\\" -H\\\"Accept:application/json\\\"    0.00s user 0.00s system 2% cpu 0.283 total\\n    curl -o /dev/null -L -H\\\"User-Agent:curl/7.37.1\\\" -H\\\"Accept:application/json\\\"    0.00s user 0.01s system 2% cpu 0.342 total\\n    curl -o /dev/null -L -H\\\"User-Agent:curl/7.37.1\\\" -H\\\"Accept:application/json\\\"    0.00s user 0.01s system 3% cpu 0.324 total\\n    curl -o /dev/null -L -H\\\"User-Agent:curl/7.37.1\\\" -H\\\"Accept:application/json\\\"    0.00s user 0.01s system 3% cpu 0.331 total\\n    curl -o /dev/null -L -H\\\"User-Agent:curl/7.37.1\\\" -H\\\"Accept:application/json\\\"    0.00s user 0.01s system 1% cpu 0.576 total\\n    curl -o /dev/null -L -H\\\"User-Agent:curl/7.37.1\\\" -H\\\"Accept:application/json\\\"    0.00s user 0.01s system 2% cpu 0.348 total\\n    curl -o /dev/null -L  -H\\\"Accept:application/json\\\"  2\\u003E /dev/null  0.01s user 0.01s system 0% cpu 19.026 total\\n    curl -o /dev/null -L  -H\\\"Accept:application/json\\\"  2\\u003E /dev/null  0.01s user 0.01s system 0% cpu 19.769 total\\n    curl -o /dev/null -L  -H\\\"Accept:application/json\\\"  2\\u003E /dev/null  0.00s user 0.01s system 1% cpu 0.618 total\\n    curl -o /dev/null -L  -H\\\"Accept:application/json\\\"  2\\u003E /dev/null  0.01s user 0.01s system 0% cpu 20.066 total\\n    curl -o /dev/null -L  -H\\\"Accept:application/json\\\"  2\\u003E /dev/null  0.01s user 0.01s system 0% cpu 18.676 total\\n    curl -o /dev/null -L  -H\\\"Accept:application/json\\\"  2\\u003E /dev/null  0.01s user 0.01s system 0% cpu 20.104 total\\n    curl -o /dev/null -L  -H\\\"Accept:application/json\\\"  2\\u003E /dev/null  0.01s user 0.01s system 0% cpu 20.299 total\\n    curl -o /dev/null -L  -H\\\"Accept:application/json\\\"  2\\u003E /dev/null  0.01s user 0.01s system 0% cpu 10.778 total\\n    curl -o /dev/null -L  -H\\\"Accept:application/json\\\"  2\\u003E /dev/null  0.01s user 0.01s system 0% cpu 9.397 total\\n    curl -o /dev/null -L  -H\\\"Accept:application/json\\\"  2\\u003E /dev/null  0.01s user 0.01s system 0% cpu 10.856 total\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3xvl50\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"baconmania\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3xvl50/reddit_api_responses_100x_slower_depending_on/\", \"locked\": false, \"name\": \"t3_3xvl50\", \"created\": 1450845134.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3xvl50/reddit_api_responses_100x_slower_depending_on/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit API responses ~100x slower depending on User-Agent?\", \"created_utc\": 1450816334.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Eimage is \\u003Cem\\u003Emildly\\u003C/em\\u003E nsfw. \\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003EXMLHttpRequest cannot load \\u003Ca href=\\\"https://oauth.reddit.com/r/ImaginaryBoners/comments/3geyvm/faye_by_ilya_kuvshinov/\\\"\\u003Ehttps://oauth.reddit.com/r/ImaginaryBoners/comments/3geyvm/faye_by_ilya_kuvshinov/\\u003C/a\\u003E. The request was redirected to \\u0026#39;\\u003Ca href=\\\"https://www.reddit.com/over18?dest=https%3A%2F%2Foauth.reddit.com%2Fr%2FImaginaryBoners%2Fcomments%2F3geyvm%2Ffaye_by_ilya_kuvshinov%2F\\\"\\u003Ehttps://www.reddit.com/over18?dest=https%3A%2F%2Foauth.reddit.com%2Fr%2FImaginaryBoners%2Fcomments%2F3geyvm%2Ffaye_by_ilya_kuvshinov%2F\\u003C/a\\u003E\\u0026#39;, which is disallowed for cross-origin requests that require preflight.\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EUser is already cleared for NSFW content, why am I getting re-directed to the \\u0026quot;are you 18+\\u0026quot; page and how do I prevent this?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Erequest code:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E//getpage(\\u0026quot;https://oauth.reddit.com/r/ImaginaryBoners/comments/3geyvm/faye_by_ilya_kuvshinov/\\u0026quot;, callback);\\n\\nfunction getpage(page, callback){   \\n\\n$.ajax({\\n    url: page,\\n    method: \\u0026quot;GET\\u0026quot;,\\n    dataType: \\u0026quot;json\\u0026quot;,\\n    timeout: 6000,\\n    beforeSend: function (jqXHR) {\\n        jqXHR.setRequestHeader(\\u0026quot;Authorization\\u0026quot;, \\u0026quot;bearer \\u0026quot; + localStorage.access_token);\\n    },\\n    success: function (response) {\\n        console.log(\\u0026#39;getpage success!\\u0026#39;);\\n        callback(response, true);\\n    },\\n    error: function (request) {         \\n        console.log(\\u0026quot;log out message: \\u0026quot;+request.responseText);      \\n        callback(request, false);\\n    }\\n}); \\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eis there some parameter I\\u0026#39;m meant to send that explicitly says \\u0026quot;allow nsfw\\u0026quot;? Also, why on earth does this \\u003Cem\\u003Eonly\\u003C/em\\u003E prevent authenticated users?? A regular .json request or a \\u003Cstrong\\u003Ewww\\u003C/strong\\u003E.reddit ajax request doesn\\u0026#39;t get the \\u0026quot;allow 18\\u0026quot; redirect- I\\u0026#39;m very confused by this. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"image is *mildly* nsfw. \\n\\n\\u003EXMLHttpRequest cannot load https://oauth.reddit.com/r/ImaginaryBoners/comments/3geyvm/faye_by_ilya_kuvshinov/. The request was redirected to 'https://www.reddit.com/over18?dest=https%3A%2F%2Foauth.reddit.com%2Fr%2FImaginaryBoners%2Fcomments%2F3geyvm%2Ffaye_by_ilya_kuvshinov%2F', which is disallowed for cross-origin requests that require preflight.\\n\\n\\nUser is already cleared for NSFW content, why am I getting re-directed to the \\\"are you 18+\\\" page and how do I prevent this?\\n\\n\\nrequest code:\\n\\n    //getpage(\\\"https://oauth.reddit.com/r/ImaginaryBoners/comments/3geyvm/faye_by_ilya_kuvshinov/\\\", callback);\\n\\n    function getpage(page, callback){\\t\\n\\n\\t$.ajax({\\n\\t\\turl: page,\\n\\t\\tmethod: \\\"GET\\\",\\n\\t\\tdataType: \\\"json\\\",\\n\\t\\ttimeout: 6000,\\n\\t\\tbeforeSend: function (jqXHR) {\\n\\t\\t\\tjqXHR.setRequestHeader(\\\"Authorization\\\", \\\"bearer \\\" + localStorage.access_token);\\n\\t\\t},\\n\\t\\tsuccess: function (response) {\\n\\t\\t\\tconsole.log('getpage success!');\\n\\t\\t\\tcallback(response, true);\\n\\t\\t},\\n\\t\\terror: function (request) {\\t\\t\\t\\n\\t\\t\\tconsole.log(\\\"log out message: \\\"+request.responseText);\\t\\t\\n\\t\\t\\tcallback(request, false);\\n\\t\\t}\\n\\t});\\t\\n    }\\n\\nis there some parameter I'm meant to send that explicitly says \\\"allow nsfw\\\"? Also, why on earth does this *only* prevent authenticated users?? A regular .json request or a **www**.reddit ajax request doesn't get the \\\"allow 18\\\" redirect- I'm very confused by this. \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3gv2r7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"SamSlate\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3gv2r7/oauth_redirects_to_over_18_page_even_when_user_is/\", \"locked\": false, \"name\": \"t3_3gv2r7\", \"created\": 1439510543.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3gv2r7/oauth_redirects_to_over_18_page_even_when_user_is/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Oauth redirects to \\\"over 18?\\\" page, even when user is already set as \\\"over 18 and willing...\\\"\", \"created_utc\": 1439481743.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAPI Endpoint:  \\u003Ca href=\\\"http://stream.pushshift.io\\\"\\u003Ehttp://stream.pushshift.io\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis will return SSE styled events for all new comments and submissions (99.99% of the time in correct order -- still QA\\u0026#39;ing this part).  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease let me know if you notice any issues.  Eventually I will have parameters to only stream submissions or comments or specific subreddits.  The event type is either \\u0026quot;t1\\u0026quot; for comments, or \\u0026quot;t3\\u0026quot; for submissions.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEvent data is a JSON string.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003E\\u003Cstrong\\u003EParameters\\u003C/strong\\u003E:\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003E*If more than one parameter is specified, they are treated as OR operations.  Meaning that if you are filtering on the subreddit \\u0026quot;askreddit\\u0026quot; and also on the author \\u0026quot;automoderator\\u0026quot;, you will get both in your stream.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Esubreddit\\u003C/strong\\u003E: Include any submissions or comments with this subreddit in the stream.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eauthor\\u003C/strong\\u003E: Include any submissions or comments with this author.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eover_18\\u003C/strong\\u003E:  Restrict returned submissions (\\u0026quot;t3\\u0026quot; events) to either NSFW(over_18=1) or non-NSFW (over_18=0)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eevent\\u003C/strong\\u003E: Limit to only comments or subreddits.  Values are \\u0026quot;t1\\u0026quot; for comments and \\u0026quot;t3\\u0026quot; for submissions. (i.e. event=t3 to get only submissions)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Ematch\\u003C/strong\\u003E:  Does a regex on the body (comments) and title or self_text (submissions).  Limit in any way you want.  (i.e. match=subreddit\\u0026quot;:\\u0026quot;askreddit for only askreddit comments and submissions -- case insensitive).  If you search for \\u0026quot;star\\u0026quot;, it will match start.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Estart_id\\u003C/strong\\u003E:  If you lose your connection to the stream and want to reconnect at a specific id location, pass the start_id parameter and the stream will replay from that id until it goes real-time.  The stream buffers the last half hour of all reddit activity.  Use the last processed event id.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eprevious\\u003C/strong\\u003E:  Get the last X events starting from the current event.  In other words, if previous=1000 is passed, the stream will give you the last 1,000 events in order and then resume in real-time mode.  Max value is 100,000.\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003E\\u003Cstrong\\u003EExamples\\u003C/strong\\u003E:\\u003C/h2\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E wget -qO- \\u0026#39;http://stream.pushshift.io/?subreddit=askreddit\\u0026#39;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EFilter only comments or submissions from the subreddit askreddit\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ewget -qO- \\u0026#39;http://stream.pushshift.io/?author=\\u0026quot;automoderator\\u0026quot;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EFilter only comments or submissions from the author automoderator\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E wget -qO- \\u0026#39;http://stream.pushshift.io/?event=t3\\u0026amp;match=imgur\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EShow submissions that contain imgur anywhere in the JSON response\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ewget -qO- \\u0026quot;http://stream.pushshift.io/?event=t3\\u0026amp;over_18=1\\u0026quot;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EGet only NSFW submissions\\u003C/strong\\u003E \\u003Cem\\u003EThere is an underscore between over and 18.  It should be over_18\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ewget -qO- --header=\\u0026#39;Accept-Encoding: gzip,deflate\\u0026#39; \\u0026quot;http://stream.pushshift.io/?event=t1\\u0026amp;previous=10000\\u0026quot; | gzip -dc\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EStream the previous 10,000 events using compression (saves bandwidth) and then resume in real-time.\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Ch2\\u003E\\u003Cstrong\\u003ENotes\\u003C/strong\\u003E\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EThe timeout is set to 600 seconds.  If you filter by something very esoteric, the stream could disconnect after 600 seconds.  Please be aware of that.  The timeout resets for any activity.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso, there can only be one active stream per IP address.  If you need more streams, we can talk.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEdit\\u003C/strong\\u003E:  Working out a few kinks -- consider this to be in beta for the next week while I continue to QA this under load.  Thanks!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEdit 2\\u003C/strong\\u003E:  When using a start_id that is behind the most current id, the stream will deliver content 100 times faster until it catches up.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEdit 3\\u003C/strong\\u003E:  When using the previous attribute, you can now get up to the last \\u003Cstrong\\u003E100,000\\u003C/strong\\u003E events!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"API Endpoint:  http://stream.pushshift.io\\n\\nThis will return SSE styled events for all new comments and submissions (99.99% of the time in correct order -- still QA'ing this part).  \\n\\nPlease let me know if you notice any issues.  Eventually I will have parameters to only stream submissions or comments or specific subreddits.  The event type is either \\\"t1\\\" for comments, or \\\"t3\\\" for submissions.\\n\\nEvent data is a JSON string.\\n\\n**Parameters**:\\n-------------------------------------------------------\\n\\n*If more than one parameter is specified, they are treated as OR operations.  Meaning that if you are filtering on the subreddit \\\"askreddit\\\" and also on the author \\\"automoderator\\\", you will get both in your stream.\\n\\n**subreddit**: Include any submissions or comments with this subreddit in the stream.\\n\\n**author**: Include any submissions or comments with this author.\\n\\n**over_18**:  Restrict returned submissions (\\\"t3\\\" events) to either NSFW(over_18=1) or non-NSFW (over_18=0)\\n\\n**event**: Limit to only comments or subreddits.  Values are \\\"t1\\\" for comments and \\\"t3\\\" for submissions. (i.e. event=t3 to get only submissions)\\n\\n**match**:  Does a regex on the body (comments) and title or self_text (submissions).  Limit in any way you want.  (i.e. match=subreddit\\\":\\\"askreddit for only askreddit comments and submissions -- case insensitive).  If you search for \\\"star\\\", it will match start.\\n\\n**start_id**:  If you lose your connection to the stream and want to reconnect at a specific id location, pass the start_id parameter and the stream will replay from that id until it goes real-time.  The stream buffers the last half hour of all reddit activity.  Use the last processed event id.\\n\\n**previous**:  Get the last X events starting from the current event.  In other words, if previous=1000 is passed, the stream will give you the last 1,000 events in order and then resume in real-time mode.  Max value is 100,000.\\n\\n**Examples**:\\n---------------------------------------------------------\\n\\n     wget -qO- 'http://stream.pushshift.io/?subreddit=askreddit'\\n\\n**Filter only comments or submissions from the subreddit askreddit**\\n\\n    wget -qO- 'http://stream.pushshift.io/?author=\\\"automoderator\\\"\\n\\n**Filter only comments or submissions from the author automoderator**\\n\\n     wget -qO- 'http://stream.pushshift.io/?event=t3\\u0026match=imgur\\n\\n**Show submissions that contain imgur anywhere in the JSON response**\\n\\n    wget -qO- \\\"http://stream.pushshift.io/?event=t3\\u0026over_18=1\\\"\\n\\n**Get only NSFW submissions** *There is an underscore between over and 18.  It should be over_18*\\n\\n    wget -qO- --header='Accept-Encoding: gzip,deflate' \\\"http://stream.pushshift.io/?event=t1\\u0026previous=10000\\\" | gzip -dc\\n\\n**Stream the previous 10,000 events using compression (saves bandwidth) and then resume in real-time.**\\n\\n**Notes**\\n-----------------------------------------------------------\\n\\nThe timeout is set to 600 seconds.  If you filter by something very esoteric, the stream could disconnect after 600 seconds.  Please be aware of that.  The timeout resets for any activity.\\n\\nAlso, there can only be one active stream per IP address.  If you need more streams, we can talk.  \\n\\n**Edit**:  Working out a few kinks -- consider this to be in beta for the next week while I continue to QA this under load.  Thanks!\\n\\n**Edit 2**:  When using a start_id that is behind the most current id, the stream will deliver content 100 times faster until it catches up.\\n\\n**Edit 3**:  When using the previous attribute, you can now get up to the last **100,000** events!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3gbrac\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1439434340.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3gbrac/new_api_endpoint_sse_server_side_events_for_all/\", \"locked\": false, \"name\": \"t3_3gbrac\", \"created\": 1439130828.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3gbrac/new_api_endpoint_sse_server_side_events_for_all/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New API Endpoint: SSE (Server side Events) for all new comments and submissions in real-time\", \"created_utc\": 1439102028.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"pastebin.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3euimk\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3euimk/has_this_url_been_submitted_to_reddit_bookmarklet/\", \"locked\": false, \"name\": \"t3_3euimk\", \"created\": 1438077580.0, \"url\": \"http://pastebin.com/GCPkPNwq\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"\\\"Has this URL been submitted to Reddit?\\\" bookmarklet\", \"created_utc\": 1438048780.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI want to create a feature that I feel is sorely needed.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy first instinct is to fork and pull request, but it looks like reddit\\u0026#39;s github doesn\\u0026#39;t accept pull requests (there are 70 in the queue right now).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf that\\u0026#39;s the case, I\\u0026#39;ll need to develop an extension right?  It\\u0026#39;s not ideal because I don\\u0026#39;t have experience developing extensions.  Also, I feel like it would be annoying having to deal with compatibility with RES.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhich is the recommended route?  And can the sidebar be updated with this helpful information?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I want to create a feature that I feel is sorely needed.\\n\\nMy first instinct is to fork and pull request, but it looks like reddit's github doesn't accept pull requests (there are 70 in the queue right now).\\n\\nIf that's the case, I'll need to develop an extension right?  It's not ideal because I don't have experience developing extensions.  Also, I feel like it would be annoying having to deal with compatibility with RES.\\n\\nWhich is the recommended route?  And can the sidebar be updated with this helpful information?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3d17qk\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"robby_w_g\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3d17qk/question_i_want_to_implement_a_feature_for_reddit/\", \"locked\": false, \"name\": \"t3_3d17qk\", \"created\": 1436752670.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3d17qk/question_i_want_to_implement_a_feature_for_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Question] I want to implement a feature for Reddit. Should I issue a pull request or develop an extension?\", \"created_utc\": 1436723870.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ELet\\u0026#39;s say I restrict it to one subreddit, does it really need to request and parse every thread and every comment looking for keywords or is there a better way?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Let's say I restrict it to one subreddit, does it really need to request and parse every thread and every comment looking for keywords or is there a better way?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"33jrln\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"3to20characters\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 14, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/33jrln/how_does_a_reddit_bot_know_its_been_called/\", \"locked\": false, \"name\": \"t3_33jrln\", \"created\": 1429783669.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/33jrln/how_does_a_reddit_bot_know_its_been_called/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How does a reddit bot know its been called?\", \"created_utc\": 1429754869.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am working on a bot and have almost everything done but just wondering how they do that?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDo they get special permission or?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am working on a bot and have almost everything done but just wondering how they do that?\\n\\nDo they get special permission or?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"322sd6\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"jdf2\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/322sd6/how_do_bots_like_utrollabot_not_get_limited_with/\", \"locked\": false, \"name\": \"t3_322sd6\", \"created\": 1428663136.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/322sd6/how_do_bots_like_utrollabot_not_get_limited_with/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How do bots like /u/TrollaBot not get limited with the \\\"You are doing that too fast\\\" comment thing?\", \"created_utc\": 1428634336.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EPRAW client developers,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have made a PRAW branch to test using only HTTPS over the API. This change requires some testers to see if there any issues that did not come up from our set of unit tests. This is the first of a few improvements that will (hopefully soon) be released with PRAW version 3. If you want to start using HTTPS exclusively through PRAW please update via the following:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Epip install git+git://github.com/praw-dev/praw.git@praw3\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIf you experience any issues feel free to report them here, however filing a bug on github (\\u003Ca href=\\\"https://github.com/praw-dev/praw/issues\\\"\\u003Ehttps://github.com/praw-dev/praw/issues\\u003C/a\\u003E) would be ideal. Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"PRAW client developers,\\n\\nI have made a PRAW branch to test using only HTTPS over the API. This change requires some testers to see if there any issues that did not come up from our set of unit tests. This is the first of a few improvements that will (hopefully soon) be released with PRAW version 3. If you want to start using HTTPS exclusively through PRAW please update via the following:\\n\\n    pip install git+git://github.com/praw-dev/praw.git@praw3\\n\\nIf you experience any issues feel free to report them here, however filing a bug on github (https://github.com/praw-dev/praw/issues) would be ideal. Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2gmzqe\", \"from_kind\": null, \"gilded\": 1, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bboe\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2gmzqe/praw_https_enabled_praw_testing_needed/\", \"locked\": false, \"name\": \"t3_2gmzqe\", \"created\": 1410964471.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2gmzqe/praw_https_enabled_praw_testing_needed/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[PRAW] HTTPS enabled PRAW testing needed\", \"created_utc\": 1410935671.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m trying to work out if it\\u0026#39;s a good idea to use reddit open-source sotfware to start a similar site. However most of the things I\\u0026#39;ve read on it online suggest it\\u0026#39;s too complicated to use and generally not a good CMS ((1)[\\u003Ca href=\\\"http://www.deseret-tech.com/journal/reddit-the-open-source-software/\\\"\\u003Ehttp://www.deseret-tech.com/journal/reddit-the-open-source-software/\\u003C/a\\u003E] (2)[\\u003Ca href=\\\"http://stackoverflow.com/questions/7394037/any-reddit-alternatives%5D\\\"\\u003Ehttp://stackoverflow.com/questions/7394037/any-reddit-alternatives]\\u003C/a\\u003E). It\\u0026#39;s from 2011 though, maybe something has changed.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo is reddit software good for anything other than reddit.com? Does anybody know of other sites using it?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm trying to work out if it's a good idea to use reddit open-source sotfware to start a similar site. However most of the things I've read on it online suggest it's too complicated to use and generally not a good CMS ((1)[http://www.deseret-tech.com/journal/reddit-the-open-source-software/] (2)[http://stackoverflow.com/questions/7394037/any-reddit-alternatives]). It's from 2011 though, maybe something has changed.\\n\\nSo is reddit software good for anything other than reddit.com? Does anybody know of other sites using it?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2fx6e9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"arbooe\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2fx6e9/are_there_any_sites_successfully_using_reddit/\", \"locked\": false, \"name\": \"t3_2fx6e9\", \"created\": 1410310413.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2fx6e9/are_there_any_sites_successfully_using_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Are there any sites successfully using reddit source code?\", \"created_utc\": 1410281613.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello everyone! I\\u0026#39;m here to give some quick tips on what to put in your \\u003Ca href=\\\"/r/redditdev\\\"\\u003E/r/redditdev\\u003C/a\\u003E post to make it easier for us to help you. \\u0026quot;XYZ isn\\u0026#39;t working\\u0026quot;, on its own, is not enough information for us to help you! We need more specifics to be able to understand what exactly isn\\u0026#39;t working for you.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease try and include the following information in your posts. If you\\u0026#39;re not sure how to get some/all of it, google around or ask for additional assistance in your post.\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EThe exact API URL you were hitting\\u003C/li\\u003E\\n\\u003Cli\\u003EThe request headers (but obfuscate cookies and secrets!)\\u003C/li\\u003E\\n\\u003Cli\\u003EThe POST data you\\u0026#39;re sending, if any\\u003C/li\\u003E\\n\\u003Cli\\u003EAny response headers\\u003C/li\\u003E\\n\\u003Cli\\u003EThe response \\u003Cstrong\\u003Ebody\\u003C/strong\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EIn other words, include as much information about the raw HTTP request as you can.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you\\u0026#39;re using a library of some kind, be aware that not everyone is going to be familiar with whatever language, library, or SDK you\\u0026#39;re working with. That means that while including the code in your post can help, it\\u0026#39;s not \\u003Cem\\u003Enearly\\u003C/em\\u003E as helpful as the raw HTTP information. The main exception here is PRAW, as it has wide enough use that someone may be able to help without the raw HTTP info.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello everyone! I'm here to give some quick tips on what to put in your /r/redditdev post to make it easier for us to help you. \\\"XYZ isn't working\\\", on its own, is not enough information for us to help you! We need more specifics to be able to understand what exactly isn't working for you.\\n\\nPlease try and include the following information in your posts. If you're not sure how to get some/all of it, google around or ask for additional assistance in your post.\\n\\n* The exact API URL you were hitting\\n* The request headers (but obfuscate cookies and secrets!)\\n* The POST data you're sending, if any\\n* Any response headers\\n* The response **body**\\n\\nIn other words, include as much information about the raw HTTP request as you can.\\n\\nIf you're using a library of some kind, be aware that not everyone is going to be familiar with whatever language, library, or SDK you're working with. That means that while including the code in your post can help, it's not *nearly* as helpful as the raw HTTP information. The main exception here is PRAW, as it has wide enough use that someone may be able to help without the raw HTTP info.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2enl27\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2enl27/help_us_help_you_include_important_debugging_info/\", \"locked\": false, \"name\": \"t3_2enl27\", \"created\": 1409109885.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2enl27/help_us_help_you_include_important_debugging_info/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Help us help you - include important debugging info!\", \"created_utc\": 1409081085.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"test.lukekuzmish.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"270bnk\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"sack_lunch\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/270bnk/i_created_a_web_site_that_generates_a_bar_chart/\", \"locked\": false, \"name\": \"t3_270bnk\", \"created\": 1401625730.0, \"url\": \"http://test.lukekuzmish.com/redditstats\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I created a web site that generates a bar chart of a user's comments \\u0026 threads per subreddit\", \"created_utc\": 1401596930.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}], \"after\": \"t3_270bnk\", \"before\": null}}"
        },
        "headers": {
          "CF-RAY": "2b59b9a5c24a20c6-LAX",
          "Connection": "keep-alive",
          "Content-Type": "application/json; charset=UTF-8",
          "Date": "Sun, 19 Jun 2016 20:28:03 GMT",
          "Server": "cloudflare-nginx",
          "Strict-Transport-Security": "max-age=15552000; includeSubDomains; preload",
          "Transfer-Encoding": "chunked",
          "Vary": "accept-encoding",
          "X-Moose": "majestic",
          "cache-control": "private, s-maxage=0, max-age=0, must-revalidate",
          "expires": "-1",
          "x-content-type-options": "nosniff",
          "x-frame-options": "SAMEORIGIN",
          "x-ratelimit-remaining": "584.0",
          "x-ratelimit-reset": "118",
          "x-ratelimit-used": "16",
          "x-reddit-tracking": "https://pixel.redditmedia.com/pixel/of_destiny.png?v=wOZ2GiEqGwvqXM7ZI3wtnTgDZHuUIA5qjPKr1GK0DgDhUglffNyqHCu%2FRx8Vft3LyW%2F4ucazTVIoF9BUyjPbWfNbCcQm6PzG",
          "x-ua-compatible": "IE=edge",
          "x-xss-protection": "1; mode=block"
        },
        "status": {
          "code": 200,
          "message": "OK"
        },
        "url": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_3kidb7&raw_json=1"
      }
    },
    {
      "recorded_at": "2016-06-19T20:28:05",
      "request": {
        "body": {
          "encoding": "utf-8",
          "string": ""
        },
        "headers": {
          "Accept": "*/*",
          "Accept-Encoding": "identity",
          "Authorization": "bearer YE4ESXZ9lTjIP0ukcopwXM6361g",
          "Connection": "keep-alive",
          "Cookie": "loid=SdAZIdDQoUDzjNqfRk; loidcreated=2016-06-19T20%3A27%3A59.507Z; __cfduid=da3dbedd6bfa050f4f12fea14395b830f1466368079",
          "User-Agent": "prawtools/0.19 PRAW/4.0.0b4 prawcore/0.0.8"
        },
        "method": "GET",
        "uri": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_270bnk&raw_json=1"
      },
      "response": {
        "body": {
          "encoding": "UTF-8",
          "string": "{\"kind\": \"Listing\", \"data\": {\"modhash\": null, \"children\": [{\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Cstrong\\u003E\\u003Ca href=\\\"/r/BotTestingField\\\"\\u003E/r/BotTestingField\\u003C/a\\u003E\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s just a junkyard/warehouse sort of deal. You are allowed to go crazy with submissions and comments to test out your bots\\u0026#39; code. It\\u0026#39;s also a shared space, so it will be interesting to see what others are posting and what kinds of bots are tested there.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m not expecting it to be super popular. I just made it to test my own bot and thought it would be a cool idea to share it with my fellow devs.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"**/r/BotTestingField**\\n\\nIt's just a junkyard/warehouse sort of deal. You are allowed to go crazy with submissions and comments to test out your bots' code. It's also a shared space, so it will be interesting to see what others are posting and what kinds of bots are tested there.\\n\\nI'm not expecting it to be super popular. I just made it to test my own bot and thought it would be a cool idea to share it with my fellow devs.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"22n938\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Antrikshy\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/22n938/sorry_if_something_like_this_exists_but_i_just/\", \"locked\": false, \"name\": \"t3_22n938\", \"created\": 1397111829.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/22n938/sorry_if_something_like_this_exists_but_i_just/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Sorry if something like this exists, but I just created a subreddit for bot-makers to test their bots out\", \"created_utc\": 1397083029.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/user/edward_r_servo\\\"\\u003Ehttp://www.reddit.com/user/edward_r_servo\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHe scrapes Twitter feeds of news outlets around the country and organizes them by state.  The idea is to let Reddit be an army of wire editors and filter out the crap and find the good, weird stories that small town newspapers print.  He only posts to my own subreddits and I have him slowed down to a minimum of 2 seconds between posts but he posts so goddamned much... \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/user/edwardunknown/m/states\\\"\\u003EHere\\u0026#39;s a Multi of what I have so far\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWith less than half the states it posts over 100 posts every 10 minutes, when I\\u0026#39;m done it will dominate \\u003Ca href=\\\"/r/all/new\\\"\\u003E/r/all/new\\u003C/a\\u003E.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat do you think?  My intentions are good, I think this could be a cool thing and really only annoy the people hanging out in \\u003Ca href=\\\"/r/all/new\\\"\\u003E/r/all/new\\u003C/a\\u003E but it still feels like I\\u0026#39;m kind of abusing Reddit somehow.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"http://www.reddit.com/user/edward_r_servo\\n\\nHe scrapes Twitter feeds of news outlets around the country and organizes them by state.  The idea is to let Reddit be an army of wire editors and filter out the crap and find the good, weird stories that small town newspapers print.  He only posts to my own subreddits and I have him slowed down to a minimum of 2 seconds between posts but he posts so goddamned much... \\n\\n[Here's a Multi of what I have so far](http://www.reddit.com/user/edwardunknown/m/states)\\n\\nWith less than half the states it posts over 100 posts every 10 minutes, when I'm done it will dominate /r/all/new.  \\n\\nWhat do you think?  My intentions are good, I think this could be a cool thing and really only annoy the people hanging out in /r/all/new but it still feels like I'm kind of abusing Reddit somehow.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"20yrkh\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/20yrkh/does_this_bot_cross_the_line/\", \"locked\": false, \"name\": \"t3_20yrkh\", \"created\": 1395400134.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/20yrkh/does_this_bot_cross_the_line/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Does this bot cross the line?\", \"created_utc\": 1395371334.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI ingest data for academic research (as reflected by our ingest server here: \\u003Ca href=\\\"http://ingest.redditanalytics.com\\\"\\u003Ehttp://ingest.redditanalytics.com\\u003C/a\\u003E (front-end is a huge work in progress)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve noticed that during heavy loads, this API call is one of the first to get wonky.  Would it be possible to increase the buffer to a larger amount than the previous 1,000 comments?  If each comment is, on average, 1k bytes when represented as a JSON object, increasing the buffer to 10k should only take around 10 megabytes of space.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso, how are recent comments being cached?  Memcache / Redis / etc.?  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve noticed frequent 500 errors when ingesting, so I\\u0026#39;m not sure how the architecture is laid out.  I\\u0026#39;m assuming that when a comment comes in, it hits some type of cache first before going into the DB to be distributed among all the nodes?  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt is exceedingly difficult to get comments after they\\u0026#39;re out of this pool because requesting comments by id is a waste of an API call when multiple comments cannot be requested by using \\u0026quot;by_id\\u0026quot; (which works with submissions).  I\\u0026#39;m assuming this is an expensive operation due to the way reddit has Cassandra set up?  Is reddit still using that?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I ingest data for academic research (as reflected by our ingest server here: [http://ingest.redditanalytics.com](http://ingest.redditanalytics.com) (front-end is a huge work in progress)\\n\\nI've noticed that during heavy loads, this API call is one of the first to get wonky.  Would it be possible to increase the buffer to a larger amount than the previous 1,000 comments?  If each comment is, on average, 1k bytes when represented as a JSON object, increasing the buffer to 10k should only take around 10 megabytes of space.\\n\\nAlso, how are recent comments being cached?  Memcache / Redis / etc.?  \\n\\nI've noticed frequent 500 errors when ingesting, so I'm not sure how the architecture is laid out.  I'm assuming that when a comment comes in, it hits some type of cache first before going into the DB to be distributed among all the nodes?  \\n\\nIt is exceedingly difficult to get comments after they're out of this pool because requesting comments by id is a waste of an API call when multiple comments cannot be requested by using \\\"by_id\\\" (which works with submissions).  I'm assuming this is an expensive operation due to the way reddit has Cassandra set up?  Is reddit still using that?\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ui73s\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1388974815.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ui73s/suggestion_increase_the_main_comment_buffer_on/\", \"locked\": false, \"name\": \"t3_1ui73s\", \"created\": 1389002816.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1ui73s/suggestion_increase_the_main_comment_buffer_on/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Suggestion] Increase the main comment buffer on /r/all/comments from the previous 1k comments to 10k comments.\", \"created_utc\": 1388974016.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi, I\\u0026#39;m making a conversion bot that\\u0026#39;ll be specific to a few select subreddits. It\\u0026#39;ll search for comments that match criteria and reply to them. Obviously you won\\u0026#39;t want to a bot to reply to the same thing more than once in this situation. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy first thought to solving this was to go though the replies for the match to see if my bot has already reached it but that\\u0026#39;s slow and takes to many API calls. My second thought was to make a hash table containing the IDs of the comments that have been replied to, but I would have to stores them in an external file when the bot goes down and reload them. Anyone have any efficient tricks for doing this? I\\u0026#39;m using python/praw if that matters.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi, I'm making a conversion bot that'll be specific to a few select subreddits. It'll search for comments that match criteria and reply to them. Obviously you won't want to a bot to reply to the same thing more than once in this situation. \\n\\nMy first thought to solving this was to go though the replies for the match to see if my bot has already reached it but that's slow and takes to many API calls. My second thought was to make a hash table containing the IDs of the comments that have been replied to, but I would have to stores them in an external file when the bot goes down and reload them. Anyone have any efficient tricks for doing this? I'm using python/praw if that matters.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1rnznl\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"SeaCowVengeance\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1rnznl/best_way_for_a_bot_to_keep_track_of_the_replies/\", \"locked\": false, \"name\": \"t3_1rnznl\", \"created\": 1385698981.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1rnznl/best_way_for_a_bot_to_keep_track_of_the_replies/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Best way for a bot to keep track of the replies it's already made.\", \"created_utc\": 1385670181.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI wanted to provide an update on where we are with redditanalytics as a whole and the comment stream that many of you depend on.  I want to apologize for the stream being down the last few days.  The reason it went down is due to the fact that we have totally changed how we\\u2019re handling data, and some restructuring of the APIs; but more on that later in the post.  The official launch date for production is slated for November 3rd of this year.  We have moved the stream to far more reliable and powerful servers. We are also including additional functionality for the stream that will hopefully assist all of our users in their analyses.  Our goal is to provide 99.99% uptime by using our new high-availability servers.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBefore I go into the update, I want to express my immense appreciation for all the help that \\u003Ca href=\\\"http://reddit.com/u/AnkhMorporkian\\\"\\u003EAnkhMorporkian\\u003C/a\\u003E has provided to the project.  AnkhMorporkian joined the project approximately two months ago and his contributions have been invaluable.  His background as an expert Python developer and sysadmin has really added a lot to the project and has helped move the project along at a faster pace.  Whereas I am more of a perl junkie, he is a Python guru through and through :)  His assistance and contribution towards the code-base also allows us to fix bugs faster and to add new features.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo, where are we?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe\\u2019re now using ElasticSearch as our main backend for reddit submissions and comments.  Originally, I started with MySQL and Sphinxsearch, which was an adequate combination for basic search functions.  AnkhMorporkian did some research on elasticsearch towards the beginning of our collaboration.  At first, I was a bit skeptical to make a major backend change since what we had been using worked, but the features and benefits just blew everything we had been using out of the water.  I\\u2019ll let AnkhMorporkian elaborate on his views and opinions on ElasticSearch if he chooses to chime in.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERedditanalytics doesn\\u2019t have a narrow scope, but perhaps the most useful thing redditanalytics does for you \\u003Ca href=\\\"/r/redditdev\\\"\\u003E/r/redditdev\\u003C/a\\u003E\\u2019ers is make data posted to reddit searchable in any fashion that the user wishes.  We are also creating an API for other developers to use so that they can incorporate many of the features into their services.  Our intention is to make every submission made to reddit searchable.  Our main index in elasticsearch (es) has approximately 50 million submissions.  We haven\\u2019t included submissions that have no comments into the main index at this point.  We also want to make it possible to search the previous 30 days of all publicly available reddit comments.  We\\u2019ll need to have a discussion with the reddit admins about that, though.  There are issues with privacy where, if a user deletes their comment, we would not want to keep that in our own index.  (We\\u2019ll have more details at a later point).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe are also using \\u003Ca href=\\\"/r/redditanalytics\\\"\\u003E/r/redditanalytics\\u003C/a\\u003E as a hub for any questions you may have concerning what is and isn\\u2019t possible with RA (redditanalytics).  Please subscribe to that subreddit if you are a developer and are interested in using our data repository and API.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe comment stream is now functional with some added features.  The stream location is: \\u003Ca href=\\\"http://stream.redditanalytics.com\\\"\\u003Ehttp://stream.redditanalytics.com\\u003C/a\\u003E.  The stream keeps an open connection and pipes all publicly available comments made to reddit in real-time (within 5-10 seconds of their being posted to reddit).  The purpose of the stream is to save API calls for reddit and to save your API calls.  It will reduce load on reddit and give developers a twitteresque data stream (similar to their streaming API).  AnkhMorporkian was able to include some new features for the stream.  Those features are:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EAbility to filter a stream by multiple subreddits.  Example:  \\u003Ca href=\\\"http://stream.redditanalytics.com/?subreddit=askreddit+funny+pics\\\"\\u003Ehttp://stream.redditanalytics.com/?subreddit=askreddit+funny+pics\\u003C/a\\u003E would deliver only comments made to those three subreddits.  All other comments would not be delivered.  This helps conserve bandwidth if your application does not require comments from every subreddit.  \\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EAbility to filter by keyword.  Example:\\n\\u003Ca href=\\\"http://stream.redditanalytics.com/?keyword=science+technology+nasa+space\\\"\\u003Ehttp://stream.redditanalytics.com/?keyword=science+technology+nasa+space\\u003C/a\\u003E\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EAbility to filter by author.  Example:\\n\\u003Ca href=\\\"http://stream.redditanalytics.com/?author=stuck_in_the_matrix+unidan\\\"\\u003Ehttp://stream.redditanalytics.com/?author=stuck_in_the_matrix+unidan\\u003C/a\\u003E\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EYou may also mix and match the filters.  The filters act as an OR condition, so if any filter in your request is satisfied, you will receive that particular comment.  For instance, you could set up something like this:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://stream.redditanalytics.com/?author=unidan\\u0026amp;keywork=biology+insect+evolution+animal+DNA\\u0026amp;subreddit=askscience+askreddit\\\"\\u003Ehttp://stream.redditanalytics.com/?author=unidan\\u0026amp;keywork=biology+insect+evolution+animal+DNA\\u0026amp;subreddit=askscience+askreddit\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis stream would match:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnything posted by user \\u003Ca href=\\\"/u/unidan\\\"\\u003E/u/unidan\\u003C/a\\u003E, anything posted to the subreddits askscience or askreddit and any comment containing the keywords \\u0026quot;biology,insect, evolution,animal and DNA.\\u0026quot;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you have any questions, just ask!  We\\u0026#39;re working on some amazing things that we hope to have ready in three weeks.  Thanks!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT:\\u003C/strong\\u003E  I forgot to mention that we will be creating a submission stream as well (probably tonight).  So we may have to put an option in the stream request so that the end-user can specify if they want the comment stream, submission stream or both.  I\\u0026#39;ll keep you guys updated.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I wanted to provide an update on where we are with redditanalytics as a whole and the comment stream that many of you depend on.  I want to apologize for the stream being down the last few days.  The reason it went down is due to the fact that we have totally changed how we\\u2019re handling data, and some restructuring of the APIs; but more on that later in the post.  The official launch date for production is slated for November 3rd of this year.  We have moved the stream to far more reliable and powerful servers. We are also including additional functionality for the stream that will hopefully assist all of our users in their analyses.  Our goal is to provide 99.99% uptime by using our new high-availability servers.\\n\\nBefore I go into the update, I want to express my immense appreciation for all the help that [AnkhMorporkian](http://reddit.com/u/AnkhMorporkian) has provided to the project.  AnkhMorporkian joined the project approximately two months ago and his contributions have been invaluable.  His background as an expert Python developer and sysadmin has really added a lot to the project and has helped move the project along at a faster pace.  Whereas I am more of a perl junkie, he is a Python guru through and through :)  His assistance and contribution towards the code-base also allows us to fix bugs faster and to add new features.\\n\\nSo, where are we?\\n\\nWe\\u2019re now using ElasticSearch as our main backend for reddit submissions and comments.  Originally, I started with MySQL and Sphinxsearch, which was an adequate combination for basic search functions.  AnkhMorporkian did some research on elasticsearch towards the beginning of our collaboration.  At first, I was a bit skeptical to make a major backend change since what we had been using worked, but the features and benefits just blew everything we had been using out of the water.  I\\u2019ll let AnkhMorporkian elaborate on his views and opinions on ElasticSearch if he chooses to chime in.  \\n\\nRedditanalytics doesn\\u2019t have a narrow scope, but perhaps the most useful thing redditanalytics does for you /r/redditdev\\u2019ers is make data posted to reddit searchable in any fashion that the user wishes.  We are also creating an API for other developers to use so that they can incorporate many of the features into their services.  Our intention is to make every submission made to reddit searchable.  Our main index in elasticsearch (es) has approximately 50 million submissions.  We haven\\u2019t included submissions that have no comments into the main index at this point.  We also want to make it possible to search the previous 30 days of all publicly available reddit comments.  We\\u2019ll need to have a discussion with the reddit admins about that, though.  There are issues with privacy where, if a user deletes their comment, we would not want to keep that in our own index.  (We\\u2019ll have more details at a later point).\\n\\nWe are also using /r/redditanalytics as a hub for any questions you may have concerning what is and isn\\u2019t possible with RA (redditanalytics).  Please subscribe to that subreddit if you are a developer and are interested in using our data repository and API.  \\n\\nThe comment stream is now functional with some added features.  The stream location is: http://stream.redditanalytics.com.  The stream keeps an open connection and pipes all publicly available comments made to reddit in real-time (within 5-10 seconds of their being posted to reddit).  The purpose of the stream is to save API calls for reddit and to save your API calls.  It will reduce load on reddit and give developers a twitteresque data stream (similar to their streaming API).  AnkhMorporkian was able to include some new features for the stream.  Those features are:\\n\\n- Ability to filter a stream by multiple subreddits.  Example:  http://stream.redditanalytics.com/?subreddit=askreddit+funny+pics would deliver only comments made to those three subreddits.  All other comments would not be delivered.  This helps conserve bandwidth if your application does not require comments from every subreddit.  \\n\\n- Ability to filter by keyword.  Example:\\nhttp://stream.redditanalytics.com/?keyword=science+technology+nasa+space\\n\\n- Ability to filter by author.  Example:\\nhttp://stream.redditanalytics.com/?author=stuck_in_the_matrix+unidan\\n\\nYou may also mix and match the filters.  The filters act as an OR condition, so if any filter in your request is satisfied, you will receive that particular comment.  For instance, you could set up something like this:\\n\\nhttp://stream.redditanalytics.com/?author=unidan\\u0026keywork=biology+insect+evolution+animal+DNA\\u0026subreddit=askscience+askreddit\\n\\nThis stream would match:\\n\\nAnything posted by user /u/unidan, anything posted to the subreddits askscience or askreddit and any comment containing the keywords \\\"biology,insect, evolution,animal and DNA.\\\"\\n\\nIf you have any questions, just ask!  We're working on some amazing things that we hope to have ready in three weeks.  Thanks!\\n\\n**EDIT:**  I forgot to mention that we will be creating a submission stream as well (probably tonight).  So we may have to put an option in the stream request so that the end-user can specify if they want the comment stream, submission stream or both.  I'll keep you guys updated.\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1oc5dt\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1381645978.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1oc5dt/redditanalytics_progress_and_an_update_on_the/\", \"locked\": false, \"name\": \"t3_1oc5dt\", \"created\": 1381668529.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1oc5dt/redditanalytics_progress_and_an_update_on_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Redditanalytics progress and an update on the comment stream\", \"created_utc\": 1381639729.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have a Python bot that I\\u0026#39;d like to run, automatically, once a week.  What are the best practices?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHeroku has a scheduler add-on, from what I can tell of reading the documents it basically requires a dyno to be running full-time, which will eat up all of the free hours they give you.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI read a blog post saying to use Google App Engine essentially as my cron-job app.  So I\\u0026#39;d write a simple app in GAE that\\u0026#39;d wake up my Heroku bot to run once weekly.  Is this really the best way to handle this problem?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have a Python bot that I'd like to run, automatically, once a week.  What are the best practices?\\n\\nHeroku has a scheduler add-on, from what I can tell of reading the documents it basically requires a dyno to be running full-time, which will eat up all of the free hours they give you.\\n\\nI read a blog post saying to use Google App Engine essentially as my cron-job app.  So I'd write a simple app in GAE that'd wake up my Heroku bot to run once weekly.  Is this really the best way to handle this problem?\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1jzca9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"acoard\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1jzca9/how_to_best_run_a_bot_on_a_schedule_for_free/\", \"locked\": false, \"name\": \"t3_1jzca9\", \"created\": 1376024048.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1jzca9/how_to_best_run_a_bot_on_a_schedule_for_free/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to best run a bot on a schedule (for free)?\", \"created_utc\": 1375995248.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThree minor updates to the API recently:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003EMod permissions are now available for all of a subreddit\\u0026#39;s moderators in \\u003Ca href=\\\"/r/something/about/moderators/\\\"\\u003E/r/something/about/moderators/\\u003C/a\\u003E.json, in the same format as \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/1iuged/api_change_your_moderator_permissions_are_now/\\\"\\u003Ethe previous addition to /subreddits/mine/moderator/.json\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003ESubmissions now have a \\u003Ccode\\u003Estickied\\u003C/code\\u003E attribute that will be true if the link is \\u003Ca href=\\\"http://www.reddit.com/r/modnews/comments/1jr429/moderators_you_can_now_sticky_a_selfpost_to_the/\\\"\\u003Estickied in its subreddit\\u003C/a\\u003E.\\u003C/li\\u003E\\n\\u003Cli\\u003ESubreddit settings at \\u003Ca href=\\\"/r/something/about/edit/\\\"\\u003E/r/something/about/edit/\\u003C/a\\u003E.json now includes a \\u003Ccode\\u003Esticky_permalink\\u003C/code\\u003E attribute that links to the currently-sticked post in that subreddit.\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Three minor updates to the API recently:\\n\\n1. Mod permissions are now available for all of a subreddit's moderators in /r/something/about/moderators/.json, in the same format as [the previous addition to /subreddits/mine/moderator/.json](http://www.reddit.com/r/redditdev/comments/1iuged/api_change_your_moderator_permissions_are_now/)\\n2. Submissions now have a `stickied` attribute that will be true if the link is [stickied in its subreddit](http://www.reddit.com/r/modnews/comments/1jr429/moderators_you_can_now_sticky_a_selfpost_to_the/).\\n3. Subreddit settings at /r/something/about/edit/.json now includes a `sticky_permalink` attribute that links to the currently-sticked post in that subreddit.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1jrvzx\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Deimorz\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1jrvzx/api_changes_mod_permissions_in/\", \"locked\": false, \"name\": \"t3_1jrvzx\", \"created\": 1375773932.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1jrvzx/api_changes_mod_permissions_in/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API changes: mod permissions in /about/moderators.json, sticky-related updates\", \"created_utc\": 1375745132.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve never seen a working/live site using reddit.com\\u0026#39;s code. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAre there any such sites? Why no list of sites, or is there one? I\\u0026#39;m amazed there are so little \\u0026quot;reddit clones\\u0026quot; with it being so popular.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've never seen a working/live site using reddit.com's code. \\n\\nAre there any such sites? Why no list of sites, or is there one? I'm amazed there are so little \\\"reddit clones\\\" with it being so popular.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ij39q\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Piedraz\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ij39q/sites_using_redditcoms_code/\", \"locked\": false, \"name\": \"t3_1ij39q\", \"created\": 1374141364.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1ij39q/sites_using_redditcoms_code/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Sites using reddit.com's code?\", \"created_utc\": 1374112564.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"redditinsight.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1i97ix\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"erict15\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1i97ix/reddit_insight_an_interactive_analytics_suite_for/\", \"locked\": false, \"name\": \"t3_1i97ix\", \"created\": 1373800322.0, \"url\": \"http://www.redditinsight.com/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Insight: An interactive analytics suite for Reddit.com using its public API, combined with real-time data analysis and d3 visualizations.\", \"created_utc\": 1373771522.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1gn3za\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"im14\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1gn3za/announcement_altcointip_bot_is_now_opensource/\", \"locked\": false, \"name\": \"t3_1gn3za\", \"created\": 1371654280.0, \"url\": \"http://www.reddit.com/r/ALTcointip/comments/1gn2bo/announcement_altcointip_bot_is_now_opensource/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Announcement: ALTcointip bot is now open-source! [x-post from /r/ALTcointip]\", \"created_utc\": 1371625480.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am currently running \\u003Ca href=\\\"http://www.reddit.com/r/memeifier/\\\"\\u003Ehere\\u003C/a\\u003E, with an extremely long sleep time. Was built with PRAW and am using the PRAW defaults for timeouts.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am currently running [here](http://www.reddit.com/r/memeifier/), with an extremely long sleep time. Was built with PRAW and am using the PRAW defaults for timeouts.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1fyxv7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Memeifier\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1fyxv7/am_i_an_evil_bot_generates_memes_from_comments/\", \"locked\": false, \"name\": \"t3_1fyxv7\", \"created\": 1370787767.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1fyxv7/am_i_an_evil_bot_generates_memes_from_comments/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Am I an evil bot? (Generates memes from comments)\", \"created_utc\": 1370758967.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EDoes anyone have any interesting ideas for projects that could involve the use of SMS (via Twilio) with the Reddit API in some way? Basically, if you stick Reddit and SMS together, what cool stuff could you make?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Does anyone have any interesting ideas for projects that could involve the use of SMS (via Twilio) with the Reddit API in some way? Basically, if you stick Reddit and SMS together, what cool stuff could you make?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1en471\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"dylanmaryk\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 30, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1en471/ideas_for_sms_uses_of_reddit_api/\", \"locked\": false, \"name\": \"t3_1en471\", \"created\": 1369015728.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1en471/ideas_for_sms_uses_of_reddit_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Ideas for SMS uses of Reddit API?\", \"created_utc\": 1368986928.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi, I added a suggestion earlier on \\u003Ca href=\\\"http://www.reddit.com/r/ideasfortheadmins/comments/1aac47/use_upper_bound_of_wilson_score_confidence/\\\"\\u003Eideasfortheadmins\\u003C/a\\u003E, which I\\u0026#39;d really love to see online, and it was quite well-received.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo I wrote a \\u003Ca href=\\\"http://pastebin.com/K8YbqFTC\\\"\\u003Epatch\\u003C/a\\u003E! What now?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi, I added a suggestion earlier on [ideasfortheadmins](http://www.reddit.com/r/ideasfortheadmins/comments/1aac47/use_upper_bound_of_wilson_score_confidence/), which I'd really love to see online, and it was quite well-received.\\n\\nSo I wrote a [patch](http://pastebin.com/K8YbqFTC)! What now?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1e5u0t\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"autoencoder\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1e5u0t/patch_optimistic_sort/\", \"locked\": false, \"name\": \"t3_1e5u0t\", \"created\": 1368352717.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1e5u0t/patch_optimistic_sort/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Patch] Optimistic sort\", \"created_utc\": 1368323917.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey everyone,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI spent a little time over the last few days refactoring how PRAW actually handles requests including how PRAW performs both rate limiting and caching. The new approach is modular allowing you to more easily change PRAW\\u0026#39;s request handling behavior by providing your own handler class, or utilizing a non-default handler provided by PRAW.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOne such handler I wrote is a multiprocess handler that interfaces with a request-handler server that is now included in PRAW. Before I push this code out with version 2.1.0 it would be awesome if some of the regular PRAW users could test the multiprocess handler with your programs.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe primary benefit of the multiprocess handler is you no longer have to worry about rate limiting when running multiple versions of your PRAW programs.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo get started, fetch PRAW from github. While using git is recommended, here\\u0026#39;s a \\u003Ca href=\\\"https://github.com/praw-dev/praw/archive/master.zip\\\"\\u003Ezip\\u003C/a\\u003E of the latest source. Installing PRAW this way requires you to run \\u003Ccode\\u003Epython setup.py install\\u003C/code\\u003E from the root of the source tree (where setup.py lives).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOnce up and running give this script a try: \\u003Ca href=\\\"https://gist.github.com/bboe/5458377\\\"\\u003Ehttps://gist.github.com/bboe/5458377\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAt this point you\\u0026#39;ll probably notice warnings like:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003ECannot connect to multiprocess server. Is it running? Retrying in 2 seconds.\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EFrom a separate terminal, start PRAW\\u0026#39;s multiprocess request handling server by running \\u003Ccode\\u003Epraw-multiprocess\\u003C/code\\u003E. From here you should be able to run as many PRAW instances you want so long as you pass in an instance of the \\u003Ccode\\u003EMultiprocessHandler\\u003C/code\\u003E when creating the Reddit instance.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat I\\u0026#39;m looking for is any issues you encounter. I\\u0026#39;ve handled most of the obvious connection issues on my ubuntu OS, but perhaps the socket error messages are different on other OSes, or I completely neglected to test something. That\\u0026#39;s where you can help!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAt the moment the only unresolved issue that I am aware of is when the request times out, the server fails to pickle the resulting TimeOut object. This in turn will cause an EOFError on the client that it already handles. However, after 3 consecutive EOFErrors PRAW will raise a ClientException.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny feedback you have would be greatly appreciated. Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey everyone,\\n\\nI spent a little time over the last few days refactoring how PRAW actually handles requests including how PRAW performs both rate limiting and caching. The new approach is modular allowing you to more easily change PRAW's request handling behavior by providing your own handler class, or utilizing a non-default handler provided by PRAW.\\n\\nOne such handler I wrote is a multiprocess handler that interfaces with a request-handler server that is now included in PRAW. Before I push this code out with version 2.1.0 it would be awesome if some of the regular PRAW users could test the multiprocess handler with your programs.\\n\\nThe primary benefit of the multiprocess handler is you no longer have to worry about rate limiting when running multiple versions of your PRAW programs.\\n\\nTo get started, fetch PRAW from github. While using git is recommended, here's a [zip](https://github.com/praw-dev/praw/archive/master.zip) of the latest source. Installing PRAW this way requires you to run `python setup.py install` from the root of the source tree (where setup.py lives).\\n\\nOnce up and running give this script a try: https://gist.github.com/bboe/5458377\\n\\nAt this point you'll probably notice warnings like:\\n\\n    Cannot connect to multiprocess server. Is it running? Retrying in 2 seconds.\\n\\nFrom a separate terminal, start PRAW's multiprocess request handling server by running `praw-multiprocess`. From here you should be able to run as many PRAW instances you want so long as you pass in an instance of the `MultiprocessHandler` when creating the Reddit instance.\\n\\nWhat I'm looking for is any issues you encounter. I've handled most of the obvious connection issues on my ubuntu OS, but perhaps the socket error messages are different on other OSes, or I completely neglected to test something. That's where you can help!\\n\\nAt the moment the only unresolved issue that I am aware of is when the request times out, the server fails to pickle the resulting TimeOut object. This in turn will cause an EOFError on the client that it already handles. However, after 3 consecutive EOFErrors PRAW will raise a ClientException.\\n\\nAny feedback you have would be greatly appreciated. Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1d2nr4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bboe\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1d2nr4/multiprocess_praw_testing_needed/\", \"locked\": false, \"name\": \"t3_1d2nr4\", \"created\": 1366911040.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1d2nr4/multiprocess_praw_testing_needed/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Multiprocess PRAW -- testing needed\", \"created_utc\": 1366882240.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo for music-related subreddits, the majority of the posts are typically youtube videos of songs.  Is there any application available to sequentially play all of these videos?  I\\u0026#39;d like to make a playlist where I could listen to all the songs posted in a particular subreddit.  If it doesn\\u0026#39;t exist, would people be interested in using something like this? \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So for music-related subreddits, the majority of the posts are typically youtube videos of songs.  Is there any application available to sequentially play all of these videos?  I'd like to make a playlist where I could listen to all the songs posted in a particular subreddit.  If it doesn't exist, would people be interested in using something like this? \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1cnagg\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"TheLinguaFranca\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1cnagg/play_all_videos_in_subreddit_sequentially/\", \"locked\": false, \"name\": \"t3_1cnagg\", \"created\": 1366361264.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1cnagg/play_all_videos_in_subreddit_sequentially/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Play all videos in subreddit sequentially?\", \"created_utc\": 1366332464.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve downloaded and archived around 65 million Reddit submissions.  I still have a few hundred million more to go before I have a fairly complete archive of submissions.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EA few of you have mentioned that it would be great if I created an API that you can make calls to and download data in bulk.  I am working on creating such an API right now and will have something available in the next week or so.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere is my question -- I need a reliable dedicated server to handle the load.  I would like to get something with the following specifications (or something close):\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E8 gigs of memory (or 16 gigs if cheap enough)\\n128 gigabytes of SSD storage\\n1 terabyte of regular HD storage\\nQuad-core CPU (second or third generation Intel Core)\\n1,000 Mbps unmetered bandwidth (with option to go to 10Gpbs unmetered)\\nUbuntu 12.10 64 bit server edition\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe most important thing for this project is having a large enough pipe to handle the bandwidth requirements.  I want to at least guarantee 500 kilobytes a second downloads for the API.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EQ1)  What company would you recommend for getting a dedicated server with specifications similar to what I mentioned above?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere is what the API will initially offer:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EAbility to make requests and have data returned in JSON format.  (XML and Google Bit Protocol will eventually be supported)\\u003C/li\\u003E\\n\\u003Cli\\u003EAbility to request submissions by date range and subreddits.\\u003Cbr/\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003EAbility to return 1,000 submissions per API call.\\u003C/li\\u003E\\n\\u003Cli\\u003EAllow 5 API requests per second.\\u003C/li\\u003E\\n\\u003Cli\\u003E1,000 API call limit per day for unregistered developers.\\u003C/li\\u003E\\n\\u003Cli\\u003E10,000 API call limit per day for registered developers.\\u003C/li\\u003E\\n\\u003Cli\\u003E100,000 API calls per day for partner developers.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EAn example API call:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo retrieve 100 submissions for the subreddit \\u0026quot;askreddit\\u0026quot; between two dates:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditdata.com/submissions?subreddit=askreddit\\u0026amp;startdate=%5Bunix\\\"\\u003Ehttp://api.redditdata.com/submissions?subreddit=askreddit\\u0026amp;startdate=[unix\\u003C/a\\u003E timestamp]\\u0026amp;enddate=[unix timestamp]\\u0026amp;limit=100\\u0026amp;format=json\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo retrieve 100 top level comments for a particular thread in the subreddit \\u0026quot;askreddit\\u0026quot;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditdata.com/comments?subreddit=askreddit\\u0026amp;id=z8492\\u0026amp;limit=100\\u0026amp;level=1\\u0026amp;format=json\\\"\\u003Ehttp://api.redditdata.com/comments?subreddit=askreddit\\u0026amp;id=z8492\\u0026amp;limit=100\\u0026amp;level=1\\u0026amp;format=json\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo retrieve all Reddit submissions for April 18, 2013 for the subreddit \\u0026quot;askreddit\\u0026quot;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditdata.com/submissions?subreddit=askreddit\\u0026amp;date=20130418\\u0026amp;limit=0\\\"\\u003Ehttp://api.redditdata.com/submissions?subreddit=askreddit\\u0026amp;date=20130418\\u0026amp;limit=0\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo retrieve all Reddit submissions for the subreddit \\u0026quot;askreddit\\u0026quot; with a minimum score of at least 500 for the above date:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditdata.com/submissions?subreddit=askreddit\\u0026amp;date=20130418\\u0026amp;limit=0\\u0026amp;minscore=500\\\"\\u003Ehttp://api.redditdata.com/submissions?subreddit=askreddit\\u0026amp;date=20130418\\u0026amp;limit=0\\u0026amp;minscore=500\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EHere are the API fields that I am going to support:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Elimit\\u003C/strong\\u003E (specify the maximum number of returned objects)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Esubreddit\\u003C/strong\\u003E (specify a particular subreddit or an array of subreddits)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eminscore\\u003C/strong\\u003E (specify the minimum score of a particular submission)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Emincomments\\u003C/strong\\u003E (specify the minimum number of comments)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Estartdate\\u003C/strong\\u003E (the oldest date possible for objects)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eenddate\\u003C/strong\\u003E (the newest date possible for objects)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Edate\\u003C/strong\\u003E (return objects with this exact date)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eauthor\\u003C/strong\\u003E (return objects submitted / commented by this particular author)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eq\\u003C/strong\\u003E (return objects that have this keyword or array of keywords using boolean logic -- AND, OR , etc.)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Edomain\\u003C/strong\\u003E (return objects pertaining to a certain domain)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eover18\\u003C/strong\\u003E (return objects that are over18 only, both or none)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eid\\u003C/strong\\u003E (return objects with this id or an array of ids)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've downloaded and archived around 65 million Reddit submissions.  I still have a few hundred million more to go before I have a fairly complete archive of submissions.  \\n\\nA few of you have mentioned that it would be great if I created an API that you can make calls to and download data in bulk.  I am working on creating such an API right now and will have something available in the next week or so.\\n\\nHere is my question -- I need a reliable dedicated server to handle the load.  I would like to get something with the following specifications (or something close):\\n\\n8 gigs of memory (or 16 gigs if cheap enough)\\n128 gigabytes of SSD storage\\n1 terabyte of regular HD storage\\nQuad-core CPU (second or third generation Intel Core)\\n1,000 Mbps unmetered bandwidth (with option to go to 10Gpbs unmetered)\\nUbuntu 12.10 64 bit server edition\\n\\nThe most important thing for this project is having a large enough pipe to handle the bandwidth requirements.  I want to at least guarantee 500 kilobytes a second downloads for the API.  \\n\\nQ1)  What company would you recommend for getting a dedicated server with specifications similar to what I mentioned above?\\n\\nHere is what the API will initially offer:\\n\\n* Ability to make requests and have data returned in JSON format.  (XML and Google Bit Protocol will eventually be supported)\\n* Ability to request submissions by date range and subreddits.  \\n* Ability to return 1,000 submissions per API call.\\n* Allow 5 API requests per second.\\n* 1,000 API call limit per day for unregistered developers.\\n* 10,000 API call limit per day for registered developers.\\n* 100,000 API calls per day for partner developers.\\n\\n----------------------------------------------------------------------------------------\\n\\nAn example API call:\\n\\nTo retrieve 100 submissions for the subreddit \\\"askreddit\\\" between two dates:\\n\\nhttp://api.redditdata.com/submissions?subreddit=askreddit\\u0026startdate=[unix timestamp]\\u0026enddate=[unix timestamp]\\u0026limit=100\\u0026format=json\\n\\nTo retrieve 100 top level comments for a particular thread in the subreddit \\\"askreddit\\\"\\n\\nhttp://api.redditdata.com/comments?subreddit=askreddit\\u0026id=z8492\\u0026limit=100\\u0026level=1\\u0026format=json\\n\\nTo retrieve all Reddit submissions for April 18, 2013 for the subreddit \\\"askreddit\\\"\\n\\nhttp://api.redditdata.com/submissions?subreddit=askreddit\\u0026date=20130418\\u0026limit=0\\n\\nTo retrieve all Reddit submissions for the subreddit \\\"askreddit\\\" with a minimum score of at least 500 for the above date:\\n\\nhttp://api.redditdata.com/submissions?subreddit=askreddit\\u0026date=20130418\\u0026limit=0\\u0026minscore=500\\n\\n\\n\\n\\n**Here are the API fields that I am going to support:**\\n\\n**limit** (specify the maximum number of returned objects)\\n\\n**subreddit** (specify a particular subreddit or an array of subreddits)\\n\\n**minscore** (specify the minimum score of a particular submission)\\n\\n**mincomments** (specify the minimum number of comments)\\n\\n**startdate** (the oldest date possible for objects)\\n\\n**enddate** (the newest date possible for objects)\\n\\n**date** (return objects with this exact date)\\n\\n**author** (return objects submitted / commented by this particular author)\\n\\n**q** (return objects that have this keyword or array of keywords using boolean logic -- AND, OR , etc.)\\n\\n**domain** (return objects pertaining to a certain domain)\\n\\n**over18** (return objects that are over18 only, both or none)\\n\\n**id** (return objects with this id or an array of ids)\\n\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1cm5as\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"aphexcoil\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1366319639.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1cm5as/advanced_reddit_api_project_some_questions_for/\", \"locked\": false, \"name\": \"t3_1cm5as\", \"created\": 1366332375.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1cm5as/advanced_reddit_api_project_some_questions_for/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Advanced Reddit API Project -- Some questions for you guys.\", \"created_utc\": 1366303575.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey guys, \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI hope that you are able to help me with this.\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EDuring the creation of the new subreddit, is it possible to change the default value of thumbnails to the link, and make it ticked? So that way, when creating a new subreddit, it will have thumbnails shown by default for all users.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EIs it possible to set default users from the admin interface? I am aware of the settings in the ini file, and I am wondering is that the only place or an admin can change it somewhere on the site.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EIs it possible to delete the subreddit? Like, completely delete it and everything it contains? Or, can someone explain what the banning actually does, what happenes with the mods, users who are subscribed, top menu bar, search indices and so on, does the banned subreddit leave trails or its deteled-like, but not completely?\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003ESomehow, my clone is not detecting new search submissions, so the data has to be pushed manualy to CloudSearch to Amazon. (via paster run r2/lib/cloudsearch.py -c \\u0026quot;rebuild_subreddit_index()\\u0026quot; and rebuild_link_index(). I\\u0026#39;ve added a cron job for this, but this might pose issues later when (if) the sites become bigger, so can you suggest some potential solutions why the process_new_links() is not working (its not working when running directly via paster run, so its not the consumers issue)\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EI realize this is a lot of questions, so thanks in advance for any kind of help and assistance with either of those.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey guys, \\n\\nI hope that you are able to help me with this.\\n\\n1. During the creation of the new subreddit, is it possible to change the default value of thumbnails to the link, and make it ticked? So that way, when creating a new subreddit, it will have thumbnails shown by default for all users.\\n\\n2. Is it possible to set default users from the admin interface? I am aware of the settings in the ini file, and I am wondering is that the only place or an admin can change it somewhere on the site.\\n\\n3. Is it possible to delete the subreddit? Like, completely delete it and everything it contains? Or, can someone explain what the banning actually does, what happenes with the mods, users who are subscribed, top menu bar, search indices and so on, does the banned subreddit leave trails or its deteled-like, but not completely?\\n\\n4. Somehow, my clone is not detecting new search submissions, so the data has to be pushed manualy to CloudSearch to Amazon. (via paster run r2/lib/cloudsearch.py -c \\\"rebuild_subreddit_index()\\\" and rebuild_link_index(). I've added a cron job for this, but this might pose issues later when (if) the sites become bigger, so can you suggest some potential solutions why the process_new_links() is not working (its not working when running directly via paster run, so its not the consumers issue)\\n\\nI realize this is a lot of questions, so thanks in advance for any kind of help and assistance with either of those.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"181cu9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"elAhmo\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/181cu9/a_couple_of_questions/\", \"locked\": false, \"name\": \"t3_181cu9\", \"created\": 1360230167.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/181cu9/a_couple_of_questions/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"A couple of questions\", \"created_utc\": 1360201367.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAfter reading \\u003Ca href=\\\"/u/dakta\\\"\\u003E/u/dakta\\u003C/a\\u003E\\u0026#39;s comment \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/15v7l4/retrieving_a_users_comments_seems_to_be_limited/c7q7lcy\\\"\\u003Ehere\\u003C/a\\u003E, it sounded like a decent revenue model for Reddit by allowing further limits on API calls for a cost.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThinking about it from a service kind of level, what is currently being offered now is the free tier, while allowing for a \\u0026quot;pay for what you use as you use it\\u0026quot; type of model that AWS currently has.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPossible options could be:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003ERate limit bypass - allow the application to bypass current rate limits when the application has the need to. Price it at something like $0.001 (numbers completely made up) per rate limit bypass.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EComments returned limit bypass (allow 2000 comments to be retrieved at a time instead of 1000).\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EThere are a multitude of ways you could do this, perhaps even by \\u003Cem\\u003Ejust\\u003C/em\\u003E offering the rate limit bypass while keeping the other parameters the same (ex: you can\\u0026#39;t bypass the 1000 comments returned, but you can request 5000/sec and be charged rate limit charges).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EPlease Note:\\u003C/strong\\u003E This is just a brainstorm of possibilities, and I do know it would take quite a bit to implement something like this. But from the dedicated Reddit community (especially the programming community) I\\u0026#39;d like to think that some users \\u003Cem\\u003Ewould\\u003C/em\\u003E pay premium for something like this.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"After reading /u/dakta's comment [here](http://www.reddit.com/r/redditdev/comments/15v7l4/retrieving_a_users_comments_seems_to_be_limited/c7q7lcy), it sounded like a decent revenue model for Reddit by allowing further limits on API calls for a cost.\\n\\nThinking about it from a service kind of level, what is currently being offered now is the free tier, while allowing for a \\\"pay for what you use as you use it\\\" type of model that AWS currently has.\\n\\nPossible options could be:\\n\\n- Rate limit bypass - allow the application to bypass current rate limits when the application has the need to. Price it at something like $0.001 (numbers completely made up) per rate limit bypass.\\n\\n- Comments returned limit bypass (allow 2000 comments to be retrieved at a time instead of 1000).\\n\\nThere are a multitude of ways you could do this, perhaps even by *just* offering the rate limit bypass while keeping the other parameters the same (ex: you can't bypass the 1000 comments returned, but you can request 5000/sec and be charged rate limit charges).\\n\\n**Please Note:** This is just a brainstorm of possibilities, and I do know it would take quite a bit to implement something like this. But from the dedicated Reddit community (especially the programming community) I'd like to think that some users *would* pay premium for something like this.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"15wmo2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bobbonew\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1357254630.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/15wmo2/suggestionbrainstorm_premium_api_access_at_cost/\", \"locked\": false, \"name\": \"t3_15wmo2\", \"created\": 1357273404.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/15wmo2/suggestionbrainstorm_premium_api_access_at_cost/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Suggestion/Brainstorm: Premium API Access at Cost\", \"created_utc\": 1357244604.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi Redditors,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am writing a reddit metro app for Windows 8 and would like to know if there is any copyright on the Alien Logo/character, and any legalities (is that a word?) I should be aware of before using the feed or logo?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks Much\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi Redditors,\\n\\nI am writing a reddit metro app for Windows 8 and would like to know if there is any copyright on the Alien Logo/character, and any legalities (is that a word?) I should be aware of before using the feed or logo?\\n\\nThanks Much\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"wqg83\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"liquidnitrogen\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/wqg83/reddit_alien_logo_copyright/\", \"locked\": false, \"name\": \"t3_wqg83\", \"created\": 1342603317.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/wqg83/reddit_alien_logo_copyright/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Alien Logo Copyright\", \"created_utc\": 1342574517.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi everyone,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m reading the API to create a bot, while reading the source code for the API I found POST_juryvote on line 1083 at the time of writing, so i\\u0026#39;m wondering what is Jury Vote on reddit?, Because didn\\u0026#39;t see it before on the site\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi everyone,\\n\\nI'm reading the API to create a bot, while reading the source code for the API I found POST_juryvote on line 1083 at the time of writing, so i'm wondering what is Jury Vote on reddit?, Because didn't see it before on the site\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"uh028\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"StompingBrokenGlass\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/uh028/what_does_juryvote_do/\", \"locked\": false, \"name\": \"t3_uh028\", \"created\": 1338658131.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/uh028/what_does_juryvote_do/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"What does juryvote do ?\", \"created_utc\": 1338629331.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHow do I add in the not_spammer=true flag into an upvote request in the reddit api wrapper. Please give me an example.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"How do I add in the not_spammer=true flag into an upvote request in the reddit api wrapper. Please give me an example.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"suh3z\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"the_reddit_warlock\", \"media\": null, \"score\": 13, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/suh3z/reddit_api_wrapper_not_spammertrue_flag_answer/\", \"locked\": false, \"name\": \"t3_suh3z\", \"created\": 1335515167.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/suh3z/reddit_api_wrapper_not_spammertrue_flag_answer/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit api wrapper not_spammer=true flag. Answer would be greatly appreciated. \", \"created_utc\": 1335486367.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 13}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m building a Q\\u0026amp;A site, and I would like to use Reddit\\u0026#39;s intelligent spam filtering system.  How do I get it, or how would I build it.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm building a Q\\u0026A site, and I would like to use Reddit's intelligent spam filtering system.  How do I get it, or how would I build it.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ezjwu\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"youtubehead\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ezjwu/did_you_guys_build_your_spam_filtering_or_are_you/\", \"locked\": false, \"name\": \"t3_ezjwu\", \"created\": 1294701474.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ezjwu/did_you_guys_build_your_spam_filtering_or_are_you/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Did you guys build your spam filtering? Or are you using a an open source package?\", \"created_utc\": 1294672674.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"eyl5x\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"youtubehead\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/eyl5x/can_someone_direct_me_to_the_formulaalgorithm_for/\", \"locked\": false, \"name\": \"t3_eyl5x\", \"created\": 1294544141.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/eyl5x/can_someone_direct_me_to_the_formulaalgorithm_for/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can someone direct me to the formula/algorithm for reddit's hot/new/rising feature\", \"created_utc\": 1294515341.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhere it says \\u0026quot;Send message\\u0026quot;, it says \\u0026quot;Skicka medellande\\u0026quot; in Swedish, but it\\u0026#39;s supposed to be \\u0026quot;Skicka meddelande\\u0026quot;. :)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Where it says \\\"Send message\\\", it says \\\"Skicka medellande\\\" in Swedish, but it's supposed to be \\\"Skicka meddelande\\\". :)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"aeg0n\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"frankichiro\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/aeg0n/i_just_want_to_notify_someone_that_the_word/\", \"locked\": false, \"name\": \"t3_aeg0n\", \"created\": 1260815425.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/aeg0n/i_just_want_to_notify_someone_that_the_word/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I just want to notify someone that the word \\\"message\\\" in the Swedish translation is spelled wrong.\", \"created_utc\": 1260786625.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EiReddit is the \\u0026quot;official\\u0026quot; iPhone client for Reddit, right? Is the source available for submission/modification like the Reddit source itself?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve been using iReddit pretty regularly and I have both some improvements to make and some features I\\u0026#39;d like to add throughout the app. There are some portions that don\\u0026#39;t behave terribly iPhone-y that with some small changes could really benefit the experience. There are also some clear performance problems that I\\u0026#39;d like to look at.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am a professional iPhone developer (a reasonably successful one)  and can back up my technical and user experience know-how. Someone responsible can e-mail me for the more personal credentials.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks. Sweet app so far, would love to see big improvements or even take over part of the development as a side project!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"iReddit is the \\\"official\\\" iPhone client for Reddit, right? Is the source available for submission/modification like the Reddit source itself?\\n\\nI've been using iReddit pretty regularly and I have both some improvements to make and some features I'd like to add throughout the app. There are some portions that don't behave terribly iPhone-y that with some small changes could really benefit the experience. There are also some clear performance problems that I'd like to look at.\\n\\nI am a professional iPhone developer (a reasonably successful one)  and can back up my technical and user experience know-how. Someone responsible can e-mail me for the more personal credentials.\\n\\nThanks. Sweet app so far, would love to see big improvements or even take over part of the development as a side project!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"a9jin\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"firstmanonmars\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/a9jin/is_ireddit_open_for_community_development_how_can/\", \"locked\": false, \"name\": \"t3_a9jin\", \"created\": 1259630122.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/a9jin/is_ireddit_open_for_community_development_how_can/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is iReddit open for community development? How can I contribute to it?\", \"created_utc\": 1259601322.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"downloadsquad.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"6oh8x\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"7oby\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/6oh8x/digg_has_legions_of_followers_theyre_quite/\", \"locked\": false, \"name\": \"t3_6oh8x\", \"created\": 1214192952.0, \"url\": \"http://www.downloadsquad.com/2008/06/18/digg-this-kevin-rose-reddit-goes-completely-open-source/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"\\\"Digg has legions of followers. They're quite fanatical. The similar service Reddit doesn't have that type of following.\\\"\", \"created_utc\": 1214164152.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI know \\u003Ca href=\\\"https://www.reddit.com/r/redditdev/comments/1mpxwa/what_is_the_policy_on_bots_upvoting_posts/\\\"\\u003Ebots aren\\u0026#39;t allowed to vote on reddit\\u003C/a\\u003E but am I allowed to upvote the posts and comments made by my bot? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have a bot operating a time capsule scheme over at \\u003Ca href=\\\"/r/DearFuture\\\"\\u003Er/DearFuture\\u003C/a\\u003E and I occasionally upvote the bot\\u0026#39;s submissions with my normal account as a joke. Now I\\u0026#39;ve noticed that the votes sometimes don\\u0026#39;t count and I\\u0026#39;m worried reddit might flag me as a vote manipulator or a spammer. Is the fear justified?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I know [bots aren't allowed to vote on reddit](https://www.reddit.com/r/redditdev/comments/1mpxwa/what_is_the_policy_on_bots_upvoting_posts/) but am I allowed to upvote the posts and comments made by my bot? \\n\\nI have a bot operating a time capsule scheme over at r/DearFuture and I occasionally upvote the bot's submissions with my normal account as a joke. Now I've noticed that the votes sometimes don't count and I'm worried reddit might flag me as a vote manipulator or a spammer. Is the fear justified?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4m690q\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"Naurgul\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4m690q/can_i_upvote_my_own_bot/\", \"locked\": false, \"name\": \"t3_4m690q\", \"created\": 1464885834.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4m690q/can_i_upvote_my_own_bot/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can I upvote my own bot?\", \"created_utc\": 1464857034.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EMy department is interested in setting up an internal forum for general discussion (some work-related, some not). Since a lot of us use reddit, and since reddit is open-source, we\\u0026#39;d like to set up an instance of it on the intranet.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve spun up a Ubuntu virtual machine and installed reddit using \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu\\\"\\u003Ethe install script\\u003C/a\\u003E, and that seemed to work well enough. However, the instance appears to be in a debug/develop mode. There are also certain features that we\\u0026#39;d like to disable; for example, the reddit gold donation system isn\\u0026#39;t really necessary since this would be hosted on an internal company server.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHow can I switch this reddit server into normal (i.e., non-debug) mode, disable the features we don\\u0026#39;t need, and lock it down so that no business-related posts find their way out to the internet at large?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks in advance for the help, and apologies if this is the wrong sub to be posting this in. I couldn\\u0026#39;t find anywhere more appropriate with a quick search.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"My department is interested in setting up an internal forum for general discussion (some work-related, some not). Since a lot of us use reddit, and since reddit is open-source, we'd like to set up an instance of it on the intranet.\\n\\nI've spun up a Ubuntu virtual machine and installed reddit using [the install script](https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu), and that seemed to work well enough. However, the instance appears to be in a debug/develop mode. There are also certain features that we'd like to disable; for example, the reddit gold donation system isn't really necessary since this would be hosted on an internal company server.\\n\\nHow can I switch this reddit server into normal (i.e., non-debug) mode, disable the features we don't need, and lock it down so that no business-related posts find their way out to the internet at large?\\n\\nThanks in advance for the help, and apologies if this is the wrong sub to be posting this in. I couldn't find anywhere more appropriate with a quick search.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4ikcpe\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"DarkMorford\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4ikcpe/setting_up_an_internal_reddit_instance_for_the/\", \"locked\": false, \"name\": \"t3_4ikcpe\", \"created\": 1462840148.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4ikcpe/setting_up_an_internal_reddit_instance_for_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Setting up an internal reddit instance for the office?\", \"created_utc\": 1462811348.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/r/redditdev/comments/4c2zzh/made_a_reddit_broswsing_program_check_it_out/\\\"\\u003EPrevious post on reddit\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/BestLucarioFan/Reddit-Gatherer\\\"\\u003EGithub link to the code\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIts my first real project i\\u0026#39;ve been working on.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you have any suggestions, please tell me.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EYou need to replace the client id and client secret with your own.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYou also need to use Windows media player for the text to speech to work properly\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EFor the modules, you\\u0026#39;ll need:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EPraw 4.0.0b\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EgTTS\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;d love some feedback :)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"[Previous post on reddit](https://www.reddit.com/r/redditdev/comments/4c2zzh/made_a_reddit_broswsing_program_check_it_out/)\\n\\n[Github link to the code](https://github.com/BestLucarioFan/Reddit-Gatherer)\\n\\nIts my first real project i've been working on.\\n\\nIf you have any suggestions, please tell me.\\n\\n------------------------\\n\\nYou need to replace the client id and client secret with your own.\\n\\nYou also need to use Windows media player for the text to speech to work properly\\n\\n----------------------\\n\\nFor the modules, you'll need:\\n\\n**Praw 4.0.0b**\\n\\n**gTTS**\\n\\nI'd love some feedback :)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4c63a8\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"BestLucarioFan\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4c63a8/reddit_gatherer_02_reddit_rrs_feed_with/\", \"locked\": false, \"name\": \"t3_4c63a8\", \"created\": 1459124187.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4c63a8/reddit_gatherer_02_reddit_rrs_feed_with/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Reddit Gatherer 0.2] Reddit RRS feed with Text-to-Speech! \\u003C3 bboe\", \"created_utc\": 1459095387.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAll they\\u0026#39;ve done to promote this requirement is made \\u003Ca href=\\\"https://www.reddit.com/r/redditdev/comments/3xdf11/introducing_new_api_terms/\\\"\\u003Ethis post to r/redditdev\\u003C/a\\u003E and added \\u003Ca href=\\\"https://www.reddit.com/wiki/api\\\"\\u003Ethis page to the wiki\\u003C/a\\u003E. There are no mentions of this requirement when you create an app or when you are looking at the API documentation, two obvious places where you would want to alert users about the terms/registration.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI just find it annoying that every time I want to create a new app, I have to remember to register it and then hunt down the URL for the terms/registration page. Considering they are making it this difficult, I\\u0026#39;m thinking they really don\\u0026#39;t care that much about this new requirement. I\\u0026#39;d be surprised if 10% of new apps are being registered.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor the record, \\u003Ca href=\\\"https://docs.google.com/forms/d/1ao_gme8e_xfZ41q4QymFqg5HD29HggOD8I9-MFTG7So/viewform\\\"\\u003Ehere is the API terms/registration page\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEdit:\\u003C/strong\\u003E I\\u0026#39;m dumb and there is a line about this when you create an app. The ideal solution to all of this would be to simply require users to check a box that says they\\u0026#39;ve read the terms or even better, integrate the registration process into the app creation process.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"All they've done to promote this requirement is made [this post to r/redditdev](https://www.reddit.com/r/redditdev/comments/3xdf11/introducing_new_api_terms/) and added [this page to the wiki](https://www.reddit.com/wiki/api). There are no mentions of this requirement when you create an app or when you are looking at the API documentation, two obvious places where you would want to alert users about the terms/registration.\\n\\nI just find it annoying that every time I want to create a new app, I have to remember to register it and then hunt down the URL for the terms/registration page. Considering they are making it this difficult, I'm thinking they really don't care that much about this new requirement. I'd be surprised if 10% of new apps are being registered.\\n\\nFor the record, [here is the API terms/registration page](https://docs.google.com/forms/d/1ao_gme8e_xfZ41q4QymFqg5HD29HggOD8I9-MFTG7So/viewform).\\n\\n**Edit:** I'm dumb and there is a line about this when you create an app. The ideal solution to all of this would be to simply require users to check a box that says they've read the terms or even better, integrate the registration process into the app creation process.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4403we\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"letgoandflow\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1454515257.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4403we/do_the_admins_really_expect_devs_to_read_the_api/\", \"locked\": false, \"name\": \"t3_4403we\", \"created\": 1454539881.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4403we/do_the_admins_really_expect_devs_to_read_the_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Do the admins really expect devs to read the API terms and register their apps?\", \"created_utc\": 1454511081.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI set up a reddit installation using the install script, which seemed to work fine. However, when I switch the language from English to anything else, it takes over 6 seconds to load a page: \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEnglish: \\u003Ca href=\\\"http://i.imgur.com/sTMkZ38.png\\\"\\u003Ehttp://i.imgur.com/sTMkZ38.png\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EGerman: \\u003Ca href=\\\"http://i.imgur.com/hWasOhH.png\\\"\\u003Ehttp://i.imgur.com/hWasOhH.png\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFrench: \\u003Ca href=\\\"http://i.imgur.com/tdO43sc.png\\\"\\u003Ehttp://i.imgur.com/tdO43sc.png\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDuring this time, the machine seems to be pretty busy executing python code, so the behaviour is probably not caused by network timeouts: \\u003Ca href=\\\"http://i.imgur.com/hLLJRHK.png\\\"\\u003Ehttp://i.imgur.com/hLLJRHK.png\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMachine: Ubuntu 14.04 minimal install on a system with 16 cores, 48 GB RAM, SSD storage.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECan anyone help me with this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I set up a reddit installation using the install script, which seemed to work fine. However, when I switch the language from English to anything else, it takes over 6 seconds to load a page: \\n\\n\\nEnglish: http://i.imgur.com/sTMkZ38.png\\n\\nGerman: http://i.imgur.com/hWasOhH.png\\n\\nFrench: http://i.imgur.com/tdO43sc.png\\n\\n\\nDuring this time, the machine seems to be pretty busy executing python code, so the behaviour is probably not caused by network timeouts: http://i.imgur.com/hLLJRHK.png\\n\\n\\nMachine: Ubuntu 14.04 minimal install on a system with 16 cores, 48 GB RAM, SSD storage.\\n\\n\\nCan anyone help me with this?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3xtlkb\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"joheines\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3xtlkb/reddit_becomes_very_slow_as_soon_as_i_switch_the/\", \"locked\": false, \"name\": \"t3_3xtlkb\", \"created\": 1450810160.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3xtlkb/reddit_becomes_very_slow_as_soon_as_i_switch_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit becomes very slow as soon as I switch the language to anything other than English\", \"created_utc\": 1450781360.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey guys, \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m trying to use OAuth to authorize a reddit user. I can currently get to the authorization screen - but only when testing the output of my code in a console. If I try connecting through my app, I get a popup that has the reddit login screen and when I try logging in through that I just get a blank screen instead of the authorization screen I\\u0026#39;d expect. Does anyone know what to do when a user is not logged in? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere\\u0026#39;s an example API URL:  \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehttps://www.reddit.com/api/v1/authorize.compact?client_id=D8y1kaQN-4gnwQ\\u0026amp;response_type=json\\u0026amp;state=MaphhNhQPh\\u0026amp;redirect_uri=https://providit.outsystemscloud.com/Extension_Tutorial/Home.aspx\\u0026amp;duration=permanent\\u0026amp;scope=submit\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThe code for my auth request is \\u003Ca href=\\\"http://pastebin.com/hGTWU9gd\\\"\\u003Ehere\\u003C/a\\u003E. I haven\\u0026#39;t implemented full authorization (the token retrieval) yet as I was curious to see if my first code worked. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETL;DR: Can get proper response from reddit OAuth in console, application requires login first (which redirects to empty screen after logging in). What do?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EKindest regards,\\u003Cbr/\\u003E\\nBoogy\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey guys, \\n\\nI'm trying to use OAuth to authorize a reddit user. I can currently get to the authorization screen - but only when testing the output of my code in a console. If I try connecting through my app, I get a popup that has the reddit login screen and when I try logging in through that I just get a blank screen instead of the authorization screen I'd expect. Does anyone know what to do when a user is not logged in? \\n\\nHere's an example API URL:  \\n\\n\\n    https://www.reddit.com/api/v1/authorize.compact?client_id=D8y1kaQN-4gnwQ\\u0026response_type=json\\u0026state=MaphhNhQPh\\u0026redirect_uri=https://providit.outsystemscloud.com/Extension_Tutorial/Home.aspx\\u0026duration=permanent\\u0026scope=submit\\n\\nThe code for my auth request is [here](http://pastebin.com/hGTWU9gd). I haven't implemented full authorization (the token retrieval) yet as I was curious to see if my first code worked. \\n\\nTL;DR: Can get proper response from reddit OAuth in console, application requires login first (which redirects to empty screen after logging in). What do?\\n\\nKindest regards,  \\nBoogy\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3xpagd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Boogy\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3xpagd/oauth_what_to_do_when_user_is_not_logged_in/\", \"locked\": false, \"name\": \"t3_3xpagd\", \"created\": 1450734291.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3xpagd/oauth_what_to_do_when_user_is_not_logged_in/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OAuth - what to do when user is not logged in\", \"created_utc\": 1450705491.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3utfis\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"master_of_deception\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3utfis/does_reddit_checks_if_a_user_has_been_banned_from/\", \"locked\": false, \"name\": \"t3_3utfis\", \"created\": 1448891847.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3utfis/does_reddit_checks_if_a_user_has_been_banned_from/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Does Reddit checks if a user has been banned from a certain sub by requesting the database in each request or using a token?\", \"created_utc\": 1448863047.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve seen posts from users that have waited around a month for answer from \\u003Ca href=\\\"mailto:licensing@reddit.com\\\"\\u003Elicensing@reddit.com\\u003C/a\\u003E, but I\\u0026#39;ve started to doubt that I might not get an answer. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve sent very detailed mail to the licensing team, three weeks ago, about the application that I\\u0026#39;ve developed (am developing). My app is planned to be totally free (no pro version or in app purchases) and I use reddit content and I give reddit credit, but I don\\u0026#39;t have any reddit user interaction nor do I use snoo or \\u0026quot;for reddit\\u0026quot; in the app logo/name. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs it me or can this be the case why I\\u0026#39;m not getting answered? Is the granting of these kind of licenses siezed? \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've seen posts from users that have waited around a month for answer from licensing@reddit.com, but I've started to doubt that I might not get an answer. \\n\\nI've sent very detailed mail to the licensing team, three weeks ago, about the application that I've developed (am developing). My app is planned to be totally free (no pro version or in app purchases) and I use reddit content and I give reddit credit, but I don't have any reddit user interaction nor do I use snoo or \\\"for reddit\\\" in the app logo/name. \\n\\nIs it me or can this be the case why I'm not getting answered? Is the granting of these kind of licenses siezed? \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ofkc3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ViksaaSkool\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ofkc3/license_wait_time/\", \"locked\": false, \"name\": \"t3_3ofkc3\", \"created\": 1444669901.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ofkc3/license_wait_time/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"License wait time\", \"created_utc\": 1444641101.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI installed the Reddit server code in a VM because I was curious what Reddit looked like for admins. One thing I noticed was that there\\u0026#39;s a new \\u0026quot;admin\\u0026quot; tab on user pages. But when I click it, the \\u0026quot;menu\\u0026quot; that appears is empty. \\u003Ca href=\\\"http://i.imgur.com/zsWMzfR.png\\\"\\u003EThis is what it looks like.\\u003C/a\\u003E The browser console is telling me it\\u0026#39;s just an empty \\u003Ccode\\u003Ediv\\u003C/code\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat\\u0026#39;s going on?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I installed the Reddit server code in a VM because I was curious what Reddit looked like for admins. One thing I noticed was that there's a new \\\"admin\\\" tab on user pages. But when I click it, the \\\"menu\\\" that appears is empty. [This is what it looks like.](http://i.imgur.com/zsWMzfR.png) The browser console is telling me it's just an empty `div`.\\n\\nWhat's going on?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3m0csx\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"flarn2006\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3m0csx/why_is_the_admin_menu_on_user_pages_empty/\", \"locked\": false, \"name\": \"t3_3m0csx\", \"created\": 1442999271.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3m0csx/why_is_the_admin_menu_on_user_pages_empty/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why is the \\\"admin\\\" menu on user pages empty?\", \"created_utc\": 1442970471.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIf you have a script based on PRAW and get an error like this when logging in:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EERROR: Unexpected redirect from http://www.reddit.com/user/\\u0026lt;botusername\\u0026gt;/about/.json to https://www.reddit.com/user/\\u0026lt;botusername\\u0026gt;/about/.json\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Ethat is likely because you\\u0026#39;re running a very old version of PRAW that doesn\\u0026#39;t support https. The switch seems to have been made today.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"If you have a script based on PRAW and get an error like this when logging in:\\n\\n    ERROR: Unexpected redirect from http://www.reddit.com/user/\\u003Cbotusername\\u003E/about/.json to https://www.reddit.com/user/\\u003Cbotusername\\u003E/about/.json\\n\\nthat is likely because you're running a very old version of PRAW that doesn't support https. The switch seems to have been made today.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3jjqlq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"green_flash\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3jjqlq/reddit_login_is_https_only_now_it_seems_be_sure/\", \"locked\": false, \"name\": \"t3_3jjqlq\", \"created\": 1441349550.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3jjqlq/reddit_login_is_https_only_now_it_seems_be_sure/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit login is https only now it seems, be sure to use an up-to-date PRAW version\", \"created_utc\": 1441320750.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ELocation:  \\u003Ca href=\\\"http://pan.whatbox.ca:36975/reddit/\\\"\\u003Ehttp://pan.whatbox.ca:36975/reddit/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEvery hour at 2 minutes past, the previous hour of all publicly available Reddit comments is dumped to this location Location:  \\u003Ca href=\\\"http://pan.whatbox.ca:36975/reddit/comments/hourly/\\\"\\u003Ehttp://pan.whatbox.ca:36975/reddit/comments/hourly/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe last hour is always symlinked to this location:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://pan.whatbox.ca:36975/reddit/RCS_latest.gz\\\"\\u003Ehttp://pan.whatbox.ca:36975/reddit/RCS_latest.gz\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you want to make a script to always do something with the previous hours comments, just grabbed that file at 2 minutes past the hour (it will grab the previous hour)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Location:  http://pan.whatbox.ca:36975/reddit/\\n\\nEvery hour at 2 minutes past, the previous hour of all publicly available Reddit comments is dumped to this location Location:  http://pan.whatbox.ca:36975/reddit/comments/hourly/\\n\\nThe last hour is always symlinked to this location:\\n\\nhttp://pan.whatbox.ca:36975/reddit/RCS_latest.gz\\n\\nIf you want to make a script to always do something with the previous hours comments, just grabbed that file at 2 minutes past the hour (it will grab the previous hour)\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3g734q\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3g734q/hourly_reddit_comment_dumps_available/\", \"locked\": false, \"name\": \"t3_3g734q\", \"created\": 1439025147.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3g734q/hourly_reddit_comment_dumps_available/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Hourly Reddit comment dumps available\", \"created_utc\": 1438996347.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI know that very soon, PRAW will be transitioning to another authentication protocol for logging into reddit accounts. Does anybody know exactly when this change will occur? And I assume the simple fix is to switch to the OAuth protocol mentioned in the documents. Am I correct in doing so?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I know that very soon, PRAW will be transitioning to another authentication protocol for logging into reddit accounts. Does anybody know exactly when this change will occur? And I assume the simple fix is to switch to the OAuth protocol mentioned in the documents. Am I correct in doing so?\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3f5rzi\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"DontKillTheMedic\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 28, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/\", \"locked\": false, \"name\": \"t3_3f5rzi\", \"created\": 1438293079.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[PRAW] Reddit.login(), when is it leaving?\", \"created_utc\": 1438264279.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis is very noob-ish, but curiosity is killing me.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat was Reddit architecture at the launch time? Was it just one database server? Did we have both the webserver, and the database server running on just one physical server? Or was it more sophisticated like how it is now (multiple servers)?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have been reading up on how reddit ranks posts too. Now, if it uses SQL to get highly ranked posts to render the pages, does it go through \\u003Cem\\u003Eall\\u003C/em\\u003E the posts that were \\u003Cem\\u003Eever\\u003C/em\\u003E posted to reddit to score them, order them, and choose the top ones in each sub-reddit to generate the pages? (I know it caches all the posts in memory now, so it shouldn\\u0026#39;t be that IO intensive, but what was it like in the beginning?)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso: If it was just one database server, how did they split it up in multiple database servers \\u003Cem\\u003Ewhile\\u003C/em\\u003E things were running?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny resources, directions would be really, really helpful! Thanks! \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This is very noob-ish, but curiosity is killing me.\\n\\nWhat was Reddit architecture at the launch time? Was it just one database server? Did we have both the webserver, and the database server running on just one physical server? Or was it more sophisticated like how it is now (multiple servers)?\\n\\nI have been reading up on how reddit ranks posts too. Now, if it uses SQL to get highly ranked posts to render the pages, does it go through _all_ the posts that were _ever_ posted to reddit to score them, order them, and choose the top ones in each sub-reddit to generate the pages? (I know it caches all the posts in memory now, so it shouldn't be that IO intensive, but what was it like in the beginning?)\\n\\nAlso: If it was just one database server, how did they split it up in multiple database servers _while_ things were running?\\n\\nAny resources, directions would be really, really helpful! Thanks! \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ev9id\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"yeahzero\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 29, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1438067135.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ev9id/reddit_architecture_at_the_launch_time/\", \"locked\": false, \"name\": \"t3_3ev9id\", \"created\": 1438092598.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ev9id/reddit_architecture_at_the_launch_time/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit architecture at the launch time\", \"created_utc\": 1438063798.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EDoesn\\u0026#39;t make sense for me to work on this anymore, but there might be something useful for someone in there doing work in Swift.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/jkolb/midnightbacon\\\"\\u003Ehttps://github.com/jkolb/midnightbacon\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Doesn't make sense for me to work on this anymore, but there might be something useful for someone in there doing work in Swift.\\n\\nhttps://github.com/jkolb/midnightbacon\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"360sck\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"frantic_apparatus\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/360sck/open_sourcing_my_incomplete_ios_reddit_app/\", \"locked\": false, \"name\": \"t3_360sck\", \"created\": 1431688493.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/360sck/open_sourcing_my_incomplete_ios_reddit_app/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Open sourcing my incomplete iOS Reddit app\", \"created_utc\": 1431659693.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve added an API endpoint that can be used to get information about (and validate) a scope string:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/dev/api#GET_api_v1_scopes\\\"\\u003Ehttps://www.reddit.com/dev/api#GET_api_v1_scopes\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPrimary use case for 3rd parties would be to present a user with a list of scopes that your app might need, and let them opt in to which scopes prior to making the request.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe endpoint will return a 400 status with a list of invalid_scopes if you pass it scope strings that don\\u0026#39;t exist, and it will return information on ALL scopes if you don\\u0026#39;t pass it anything.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt does NOT require an access token to use, since its intended use is prior to acquiring the token.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EExamples:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/api/v1/scopes.json\\\"\\u003Ehttps://www.reddit.com/api/v1/scopes.json\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/api/v1/scopes.json?scopes=wikiedit%20read\\\"\\u003Ehttps://www.reddit.com/api/v1/scopes.json?scopes=wikiedit%20read\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've added an API endpoint that can be used to get information about (and validate) a scope string:\\n\\nhttps://www.reddit.com/dev/api#GET_api_v1_scopes\\n\\nPrimary use case for 3rd parties would be to present a user with a list of scopes that your app might need, and let them opt in to which scopes prior to making the request.\\n\\nThe endpoint will return a 400 status with a list of invalid_scopes if you pass it scope strings that don't exist, and it will return information on ALL scopes if you don't pass it anything.\\n\\nIt does NOT require an access token to use, since its intended use is prior to acquiring the token.\\n\\nExamples:\\n\\nhttps://www.reddit.com/api/v1/scopes.json\\n\\nhttps://www.reddit.com/api/v1/scopes.json?scopes=wikiedit%20read\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"35z5zd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/35z5zd/oauth2_endpoint_for_information_about_scopes/\", \"locked\": false, \"name\": \"t3_35z5zd\", \"created\": 1431659179.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/35z5zd/oauth2_endpoint_for_information_about_scopes/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OAuth2: Endpoint for information about scopes: /api/v1/scopes\", \"created_utc\": 1431630379.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe thing is I can setup it all but I want to restrict users from creating new subreddits , for my use case i want users to be able to participate on the discussion with no right to create any new subreddits. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The thing is I can setup it all but I want to restrict users from creating new subreddits , for my use case i want users to be able to participate on the discussion with no right to create any new subreddits. \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"312by8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kpskps\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/312by8/using_reddit_code_to_make_a_new_reddit_type_forum/\", \"locked\": false, \"name\": \"t3_312by8\", \"created\": 1427929769.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/312by8/using_reddit_code_to_make_a_new_reddit_type_forum/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Using reddit code to make a new reddit type forum\", \"created_utc\": 1427900969.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis is just a heads-up to app / library devs that \\u003Ca href=\\\"https://github.com/reddit/reddit/commit/cde43e88a39acbbe4534751fca06e1b750838520\\\"\\u003E/api/morechildren and /api/expando now accept GET requests\\u003C/a\\u003E. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPOSTs will continue to work on those endpoints for the foreseeable future, but switching to GETs will allow you to take advantage of our caching tier for a speed boost.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This is just a heads-up to app / library devs that [/api/morechildren and /api/expando now accept GET requests](https://github.com/reddit/reddit/commit/cde43e88a39acbbe4534751fca06e1b750838520). \\n\\nPOSTs will continue to work on those endpoints for the foreseeable future, but switching to GETs will allow you to take advantage of our caching tier for a speed boost.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2peth5\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"largenocream\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2peth5/now_in_get_flavor_apimorechildren_and_apiexpando/\", \"locked\": false, \"name\": \"t3_2peth5\", \"created\": 1418715596.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2peth5/now_in_get_flavor_apimorechildren_and_apiexpando/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Now in GET flavor: /api/morechildren and /api/expando\", \"created_utc\": 1418686796.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"imgur.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2msuzf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"shamelessguy\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2msuzf/compact_login_page_style_is_broken/\", \"locked\": false, \"name\": \"t3_2msuzf\", \"created\": 1416455717.0, \"url\": \"http://imgur.com/YdtxO0w\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Compact login page style is broken\", \"created_utc\": 1416426917.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ERight now it just displays the total score.  For my apps, I want to display the percentage if I can\\u0026#39;t get the actual up/down count.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Right now it just displays the total score.  For my apps, I want to display the percentage if I can't get the actual up/down count.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"28jm67\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"CobraCabana\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/28jm67/will_the_new_liked_it_value_be_added_to_the_api/\", \"locked\": false, \"name\": \"t3_28jm67\", \"created\": 1403207940.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/28jm67/will_the_new_liked_it_value_be_added_to_the_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Will the new (%## liked it) value be added to the api?\", \"created_utc\": 1403179140.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/user/CommentTreeBot\\\"\\u003ECommentTreeBot\\u003C/a\\u003E...\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003Euses praw to re-construct the comment tree for a posting;\\u003C/li\\u003E\\n\\u003Cli\\u003Ecreates a tree visualization of it using GraphViz;\\u003C/li\\u003E\\n\\u003Cli\\u003Eposts the resulting image to imgur; and\\u003C/li\\u003E\\n\\u003Cli\\u003Eposts a comment in that posting pointing to the visualization.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"[CommentTreeBot](http://www.reddit.com/user/CommentTreeBot)...\\n\\n* uses praw to re-construct the comment tree for a posting;\\n* creates a tree visualization of it using GraphViz;\\n* posts the resulting image to imgur; and\\n* posts a comment in that posting pointing to the visualization.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"26m45h\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"DrJosh\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 22, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/26m45h/i_created_commenttreebot/\", \"locked\": false, \"name\": \"t3_26m45h\", \"created\": 1401235055.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I created CommentTreeBot.\", \"created_utc\": 1401206255.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve exposed a handful more existing API endpoints over OAuth. Details:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEndpoints getting \\u0026quot;read\\u0026quot; scope:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E/api/search_reddit_names\\u003C/li\\u003E\\n\\u003Cli\\u003E/api/subreddits_by_topic\\u003C/li\\u003E\\n\\u003Cli\\u003E/api/recommend/sr/{srnames}\\u003C/li\\u003E\\n\\u003Cli\\u003E/subreddits\\u003C/li\\u003E\\n\\u003Cli\\u003E/subreddits/new\\u003C/li\\u003E\\n\\u003Cli\\u003E/subreddits/popular\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EEndpoint getting \\u0026quot;modconfig\\u0026quot; scope:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E/r/{subreddit}/about/edit\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003ESee \\u003Ca href=\\\"/dev/api\\\"\\u003E/dev/api\\u003C/a\\u003E for details on each of those endpoints.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've exposed a handful more existing API endpoints over OAuth. Details:\\n    \\nEndpoints getting \\\"read\\\" scope:\\n\\n* /api/search_reddit_names\\n* /api/subreddits_by_topic\\n* /api/recommend/sr/{srnames}\\n* /subreddits\\n* /subreddits/new\\n* /subreddits/popular\\n    \\nEndpoint getting \\\"modconfig\\\" scope:\\n\\n* /r/{subreddit}/about/edit\\n\\nSee [/dev/api](/dev/api) for details on each of those endpoints.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"23anv1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/23anv1/oauth2_more_endpoints_ready_for_use/\", \"locked\": false, \"name\": \"t3_23anv1\", \"created\": 1397792905.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/23anv1/oauth2_more_endpoints_ready_for_use/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[OAuth2] More endpoints ready for use\", \"created_utc\": 1397764105.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EUp until now, we\\u0026#39;ve only really had one API endpoint under \\u0026quot;v1\\u0026quot; of the OAuth API: /api/v1/me.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EToday, that endpoint gets a new friend: /api/v1/me/prefs, for viewing and updating a user\\u0026#39;s preferences. The endpoint supports two actions: GET and PATCH.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/dev/api/oauth#GET_api_v1_prefs\\\"\\u003EGET /api/v1/me/prefs\\u003C/a\\u003E* will return the user\\u0026#39;s current preference information. Apps are encouraged to make use of the preference information locally in cases where it makes sense. GET /api/v1/me/prefs is available to OAuth applications with the \\u0026quot;identity\\u0026quot; scope. You can pass in a comma-separated list of preference names as the \\u003Ccode\\u003Efields\\u003C/code\\u003E parameter to limit the response to just those preference fields.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E* There\\u0026#39;s a small bug in the docs causing it show as /api/v1/prefs - the correct endpoint is /api/v1/me/prefs.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/dev/api/oauth#PATCH_api_v1_prefs\\\"\\u003EPATCH /api/v1/me/prefs\\u003C/a\\u003E allows for updating the user\\u0026#39;s preference. PATCH expects JSON data in the same format as returned by GET /api/v1/me/prefs. All fields are optional; only preferences sent will be updated. This means that you can perform a \\u003Cem\\u003Epartial update\\u003C/em\\u003E of a user\\u0026#39;s preferences. (Unlike performing a POST to /post/options, the existing, non-API preference endpoint, which requires sending in all existing fields). PATCH /api/v1/me/prefs is available to applications with the new \\u0026quot;account\\u0026quot; scope.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ecurl example of GET:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ereddit@reddit:~$ curl --header \\u0026quot;Authorization: bearer $TOKEN\\u0026quot; \\\\\\n\\u0026gt; \\u0026#39;https://oauth.reddit.com/api/v1/me/prefs\\u0026#39;\\n\\n{\\n\\u0026quot;clickgadget\\u0026quot;: true, \\n\\u0026quot;collapse_left_bar\\u0026quot;: false, \\n\\u0026quot;collapse_read_messages\\u0026quot;: false, \\n\\u0026quot;compress\\u0026quot;: false, \\n\\u0026quot;content_langs\\u0026quot;: [\\n    \\u0026quot;en\\u0026quot;\\n], \\n\\u0026quot;domain_details\\u0026quot;: false, \\n\\u0026quot;frame\\u0026quot;: true, \\n\\u0026quot;frame_commentspanel\\u0026quot;: false, \\n\\u0026quot;hide_downs\\u0026quot;: false, \\n\\u0026quot;hide_from_robots\\u0026quot;: false, \\n\\u0026quot;hide_ups\\u0026quot;: false, \\n\\u0026quot;highlight_new_comments\\u0026quot;: true, \\n\\u0026quot;label_nsfw\\u0026quot;: true, \\n\\u0026quot;lang\\u0026quot;: \\u0026quot;en-gb\\u0026quot;, \\n\\u0026quot;local_js\\u0026quot;: false, \\n\\u0026quot;mark_messages_read\\u0026quot;: true, \\n\\u0026quot;media\\u0026quot;: \\u0026quot;subreddit\\u0026quot;, \\n\\u0026quot;min_comment_score\\u0026quot;: -4, \\n\\u0026quot;min_link_score\\u0026quot;: -4, \\n\\u0026quot;monitor_mentions\\u0026quot;: true, \\n\\u0026quot;newwindow\\u0026quot;: false, \\n\\u0026quot;no_profanity\\u0026quot;: true, \\n\\u0026quot;num_comments\\u0026quot;: 200, \\n\\u0026quot;numsites\\u0026quot;: 25, \\n\\u0026quot;organic\\u0026quot;: true, \\n\\u0026quot;over_18\\u0026quot;: true, \\n\\u0026quot;private_feeds\\u0026quot;: true, \\n\\u0026quot;public_server_seconds\\u0026quot;: false, \\n\\u0026quot;public_votes\\u0026quot;: false, \\n\\u0026quot;research\\u0026quot;: false, \\n\\u0026quot;show_adbox\\u0026quot;: true, \\n\\u0026quot;show_flair\\u0026quot;: true, \\n\\u0026quot;show_link_flair\\u0026quot;: true, \\n\\u0026quot;show_promote\\u0026quot;: true, \\n\\u0026quot;show_sponsors\\u0026quot;: true, \\n\\u0026quot;show_sponsorships\\u0026quot;: true, \\n\\u0026quot;show_stylesheets\\u0026quot;: true, \\n\\u0026quot;store_visits\\u0026quot;: false, \\n\\u0026quot;threaded_messages\\u0026quot;: true\\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ERequesting just the \\u003Ccode\\u003Eclickgadget\\u003C/code\\u003E, \\u003Ccode\\u003Ecompress\\u003C/code\\u003E, and \\u003Ccode\\u003Elang\\u003C/code\\u003E fields:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ereddit@reddit:~$ curl --header \\u0026quot;Authorization: bearer $TOKEN\\u0026quot; \\\\\\n\\u0026gt; \\u0026#39;https://oauth.reddit.com/api/v1/me/prefs?fields=clickgadget,compress,lang\\u0026#39;\\n{\\n\\u0026quot;clickgadget\\u0026quot;: true, \\n\\u0026quot;compress\\u0026quot;: false, \\n\\u0026quot;lang\\u0026quot;: \\u0026quot;en-gb\\u0026quot;\\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Ecurl example of PATCH:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ereddit@reddit:~$ curl --header \\u0026quot;Authorization: bearer $TOKEN\\u0026quot; \\\\\\n\\u0026gt; \\u0026#39;https://oauth.reddit.com/api/v1/me/prefs\\u0026#39; -X PATCH -d \\u0026#39;{\\u0026quot;lang\\u0026quot;: \\u0026quot;en-us\\u0026quot;}\\u0026#39;\\n{\\n\\u0026quot;clickgadget\\u0026quot;: true, \\n\\u0026quot;collapse_left_bar\\u0026quot;: false, \\n\\u0026quot;collapse_read_messages\\u0026quot;: false, \\n\\u0026quot;compress\\u0026quot;: false, \\n\\u0026quot;content_langs\\u0026quot;: [\\n    \\u0026quot;en\\u0026quot;\\n], \\n\\u0026quot;domain_details\\u0026quot;: false, \\n\\u0026quot;frame\\u0026quot;: true, \\n\\u0026quot;frame_commentspanel\\u0026quot;: false, \\n\\u0026quot;hide_downs\\u0026quot;: false, \\n\\u0026quot;hide_from_robots\\u0026quot;: false, \\n\\u0026quot;hide_ups\\u0026quot;: false, \\n\\u0026quot;highlight_new_comments\\u0026quot;: true, \\n\\u0026quot;label_nsfw\\u0026quot;: true, \\n\\u0026quot;lang\\u0026quot;: \\u0026quot;en-us\\u0026quot;, \\n\\u0026quot;local_js\\u0026quot;: false, \\n\\u0026quot;mark_messages_read\\u0026quot;: true, \\n\\u0026quot;media\\u0026quot;: \\u0026quot;subreddit\\u0026quot;, \\n\\u0026quot;min_comment_score\\u0026quot;: -4, \\n\\u0026quot;min_link_score\\u0026quot;: -4, \\n\\u0026quot;monitor_mentions\\u0026quot;: true, \\n\\u0026quot;newwindow\\u0026quot;: false, \\n\\u0026quot;no_profanity\\u0026quot;: true, \\n\\u0026quot;num_comments\\u0026quot;: 200, \\n\\u0026quot;numsites\\u0026quot;: 25, \\n\\u0026quot;organic\\u0026quot;: true, \\n\\u0026quot;over_18\\u0026quot;: true, \\n\\u0026quot;private_feeds\\u0026quot;: true, \\n\\u0026quot;public_server_seconds\\u0026quot;: false, \\n\\u0026quot;public_votes\\u0026quot;: false, \\n\\u0026quot;research\\u0026quot;: false, \\n\\u0026quot;show_adbox\\u0026quot;: true, \\n\\u0026quot;show_flair\\u0026quot;: true, \\n\\u0026quot;show_link_flair\\u0026quot;: true, \\n\\u0026quot;show_promote\\u0026quot;: true, \\n\\u0026quot;show_sponsors\\u0026quot;: true, \\n\\u0026quot;show_sponsorships\\u0026quot;: true, \\n\\u0026quot;show_stylesheets\\u0026quot;: true, \\n\\u0026quot;store_visits\\u0026quot;: false, \\n\\u0026quot;threaded_messages\\u0026quot;: true\\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Up until now, we've only really had one API endpoint under \\\"v1\\\" of the OAuth API: /api/v1/me.\\n\\nToday, that endpoint gets a new friend: /api/v1/me/prefs, for viewing and updating a user's preferences. The endpoint supports two actions: GET and PATCH.\\n\\n[GET /api/v1/me/prefs](http://www.reddit.com/dev/api/oauth#GET_api_v1_prefs)* will return the user's current preference information. Apps are encouraged to make use of the preference information locally in cases where it makes sense. GET /api/v1/me/prefs is available to OAuth applications with the \\\"identity\\\" scope. You can pass in a comma-separated list of preference names as the `fields` parameter to limit the response to just those preference fields.\\n\\n\\\\* There's a small bug in the docs causing it show as /api/v1/prefs - the correct endpoint is /api/v1/me/prefs.\\n\\n[PATCH /api/v1/me/prefs](http://www.reddit.com/dev/api/oauth#PATCH_api_v1_prefs) allows for updating the user's preference. PATCH expects JSON data in the same format as returned by GET /api/v1/me/prefs. All fields are optional; only preferences sent will be updated. This means that you can perform a *partial update* of a user's preferences. (Unlike performing a POST to /post/options, the existing, non-API preference endpoint, which requires sending in all existing fields). PATCH /api/v1/me/prefs is available to applications with the new \\\"account\\\" scope.\\n\\ncurl example of GET:\\n\\n    reddit@reddit:~$ curl --header \\\"Authorization: bearer $TOKEN\\\" \\\\\\n    \\u003E 'https://oauth.reddit.com/api/v1/me/prefs'\\n    \\n    {\\n    \\\"clickgadget\\\": true, \\n    \\\"collapse_left_bar\\\": false, \\n    \\\"collapse_read_messages\\\": false, \\n    \\\"compress\\\": false, \\n    \\\"content_langs\\\": [\\n        \\\"en\\\"\\n    ], \\n    \\\"domain_details\\\": false, \\n    \\\"frame\\\": true, \\n    \\\"frame_commentspanel\\\": false, \\n    \\\"hide_downs\\\": false, \\n    \\\"hide_from_robots\\\": false, \\n    \\\"hide_ups\\\": false, \\n    \\\"highlight_new_comments\\\": true, \\n    \\\"label_nsfw\\\": true, \\n    \\\"lang\\\": \\\"en-gb\\\", \\n    \\\"local_js\\\": false, \\n    \\\"mark_messages_read\\\": true, \\n    \\\"media\\\": \\\"subreddit\\\", \\n    \\\"min_comment_score\\\": -4, \\n    \\\"min_link_score\\\": -4, \\n    \\\"monitor_mentions\\\": true, \\n    \\\"newwindow\\\": false, \\n    \\\"no_profanity\\\": true, \\n    \\\"num_comments\\\": 200, \\n    \\\"numsites\\\": 25, \\n    \\\"organic\\\": true, \\n    \\\"over_18\\\": true, \\n    \\\"private_feeds\\\": true, \\n    \\\"public_server_seconds\\\": false, \\n    \\\"public_votes\\\": false, \\n    \\\"research\\\": false, \\n    \\\"show_adbox\\\": true, \\n    \\\"show_flair\\\": true, \\n    \\\"show_link_flair\\\": true, \\n    \\\"show_promote\\\": true, \\n    \\\"show_sponsors\\\": true, \\n    \\\"show_sponsorships\\\": true, \\n    \\\"show_stylesheets\\\": true, \\n    \\\"store_visits\\\": false, \\n    \\\"threaded_messages\\\": true\\n    }\\n\\nRequesting just the `clickgadget`, `compress`, and `lang` fields:\\n\\n    reddit@reddit:~$ curl --header \\\"Authorization: bearer $TOKEN\\\" \\\\\\n    \\u003E 'https://oauth.reddit.com/api/v1/me/prefs?fields=clickgadget,compress,lang'\\n    {\\n    \\\"clickgadget\\\": true, \\n    \\\"compress\\\": false, \\n    \\\"lang\\\": \\\"en-gb\\\"\\n    }\\n\\ncurl example of PATCH:\\n\\n    reddit@reddit:~$ curl --header \\\"Authorization: bearer $TOKEN\\\" \\\\\\n    \\u003E 'https://oauth.reddit.com/api/v1/me/prefs' -X PATCH -d '{\\\"lang\\\": \\\"en-us\\\"}'\\n    {\\n    \\\"clickgadget\\\": true, \\n    \\\"collapse_left_bar\\\": false, \\n    \\\"collapse_read_messages\\\": false, \\n    \\\"compress\\\": false, \\n    \\\"content_langs\\\": [\\n        \\\"en\\\"\\n    ], \\n    \\\"domain_details\\\": false, \\n    \\\"frame\\\": true, \\n    \\\"frame_commentspanel\\\": false, \\n    \\\"hide_downs\\\": false, \\n    \\\"hide_from_robots\\\": false, \\n    \\\"hide_ups\\\": false, \\n    \\\"highlight_new_comments\\\": true, \\n    \\\"label_nsfw\\\": true, \\n    \\\"lang\\\": \\\"en-us\\\", \\n    \\\"local_js\\\": false, \\n    \\\"mark_messages_read\\\": true, \\n    \\\"media\\\": \\\"subreddit\\\", \\n    \\\"min_comment_score\\\": -4, \\n    \\\"min_link_score\\\": -4, \\n    \\\"monitor_mentions\\\": true, \\n    \\\"newwindow\\\": false, \\n    \\\"no_profanity\\\": true, \\n    \\\"num_comments\\\": 200, \\n    \\\"numsites\\\": 25, \\n    \\\"organic\\\": true, \\n    \\\"over_18\\\": true, \\n    \\\"private_feeds\\\": true, \\n    \\\"public_server_seconds\\\": false, \\n    \\\"public_votes\\\": false, \\n    \\\"research\\\": false, \\n    \\\"show_adbox\\\": true, \\n    \\\"show_flair\\\": true, \\n    \\\"show_link_flair\\\": true, \\n    \\\"show_promote\\\": true, \\n    \\\"show_sponsors\\\": true, \\n    \\\"show_sponsorships\\\": true, \\n    \\\"show_stylesheets\\\": true, \\n    \\\"store_visits\\\": false, \\n    \\\"threaded_messages\\\": true\\n    }\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"21jh28\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1395956783.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/21jh28/oauth_2_new_preferences_endpoints_get_patch/\", \"locked\": false, \"name\": \"t3_21jh28\", \"created\": 1395985165.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/21jh28/oauth_2_new_preferences_endpoints_get_patch/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[OAuth 2] New Preferences Endpoints: GET, PATCH /api/v1/me/prefs\", \"created_utc\": 1395956365.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been visiting the site for some time, but haven\\u0026#39;t got time to post lately. So when I actually did, I noticed some new features, that I think are pretty cool.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo in short: Thank you reddit devs for your work! I really appreciate it!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been visiting the site for some time, but haven't got time to post lately. So when I actually did, I noticed some new features, that I think are pretty cool.\\n\\nSo in short: Thank you reddit devs for your work! I really appreciate it!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"20flmp\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"reality_bugger\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/20flmp/just_a_short_thank_you/\", \"locked\": false, \"name\": \"t3_20flmp\", \"created\": 1394860235.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/20flmp/just_a_short_thank_you/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Just a short thank you.\", \"created_utc\": 1394831435.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI don\\u0026#39;t know how to change the /r/ inside the URL for example \\u003Ca href=\\\"http://www.reddit.com/r/redditdev\\\"\\u003Ehttp://www.reddit.com/r/redditdev\\u003C/a\\u003E to something custom.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor example: \\u003Ca href=\\\"http://www.myreddit.com/x/redditdev\\\"\\u003Ewww.myreddit.com/x/redditdev\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI think I have to modify before the installation. But I don\\u0026#39;t know what\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I don't know how to change the /r/ inside the URL for example http://www.reddit.com/r/redditdev to something custom.\\n\\n\\nFor example: www.myreddit.com/x/redditdev\\n\\n\\nI think I have to modify before the installation. But I don't know what\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1yu85g\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"wajia\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1yu85g/how_to_change_r_inside_the_url_to_a_custom_letter/\", \"locked\": false, \"name\": \"t3_1yu85g\", \"created\": 1393313290.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1yu85g/how_to_change_r_inside_the_url_to_a_custom_letter/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to change /r/ inside the URL to a custom letter?\", \"created_utc\": 1393284490.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EUsed to be that a subreddit\\u0026#39;s flair list was accessible via the API w/ no auth required.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EToday it seems that there has been a change. Requests for flair listings return a 403 Forbidden error.  I\\u0026#39;m able to access the flair for subs I moderate, but not for other subs.  This is new.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDid I miss an announcement?\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EExample of code that now returns a 403 forbidden. (problem solved by adding an r.login(), but only for subs I mod.)\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E#!/usr/bin/env python\\n\\nimport praw\\n\\nr = praw.Reddit(user_agent=\\u0026#39;/u/offtherocks bot\\u0026#39;)\\n\\nfor item in  r.get_subreddit(\\u0026#39;stopdrinking\\u0026#39;).get_flair_list(limit=None):\\n    print str(item[\\u0026#39;flair_text\\u0026#39;]) + \\u0026quot;\\\\t\\u0026quot; + item[\\u0026#39;user\\u0026#39;]\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Used to be that a subreddit's flair list was accessible via the API w/ no auth required.  \\n\\nToday it seems that there has been a change. Requests for flair listings return a 403 Forbidden error.  I'm able to access the flair for subs I moderate, but not for other subs.  This is new.\\n\\nDid I miss an announcement?\\n\\n--- \\n\\nExample of code that now returns a 403 forbidden. (problem solved by adding an r.login(), but only for subs I mod.)\\n\\n    #!/usr/bin/env python\\n    \\n    import praw\\n    \\n    r = praw.Reddit(user_agent='/u/offtherocks bot')\\n    \\n    for item in  r.get_subreddit('stopdrinking').get_flair_list(limit=None):\\n        print str(item['flair_text']) + \\\"\\\\t\\\" + item['user']\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1xreor\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1xreor/has_there_been_a_change_to_the_permissions/\", \"locked\": false, \"name\": \"t3_1xreor\", \"created\": 1392283053.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1xreor/has_there_been_a_change_to_the_permissions/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Has there been a change to the permissions required for accessing flair?\", \"created_utc\": 1392254253.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi, we are running a private instance of reddit. Everything works fine, but during the first days we created a lot of test subreddits and stuff. There\\u00b4s any way to delete unwanted subreddits?\\nJ.M.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi, we are running a private instance of reddit. Everything works fine, but during the first days we created a lot of test subreddits and stuff. There\\u00b4s any way to delete unwanted subreddits?\\nJ.M.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1x40u8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"garciacarral\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1x40u8/deleting_unwanted_subreddits/\", \"locked\": false, \"name\": \"t3_1x40u8\", \"created\": 1391659640.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1x40u8/deleting_unwanted_subreddits/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Deleting unwanted subreddits.\", \"created_utc\": 1391630840.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESeems like the Reddit API \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/e59925e5d64a23b210b844e220bcd5fe4efd8ccb/r2/r2/lib/utils/utils.py#L259\\\"\\u003Edoes not allow\\u003C/a\\u003E custom url schemes (It just allows http, https, ftp or mailto) in the OAuth redirect URL. We need this for the authentication to work properly on mobile clients.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EClarifying \\u0026quot;properly\\u0026quot; - iOS apps can define custom URL schemes that you can register with the OS. If a browser receives a request to a custom URL Scheme defined this way, it opens up the application automatically and invokes a callback method that the app has defined. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI believe this will lead to a much cleaner UX for mobile clients. Will it be possible to add support for this?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi,\\n\\nSeems like the Reddit API [does not allow](https://github.com/reddit/reddit/blob/e59925e5d64a23b210b844e220bcd5fe4efd8ccb/r2/r2/lib/utils/utils.py#L259) custom url schemes (It just allows http, https, ftp or mailto) in the OAuth redirect URL. We need this for the authentication to work properly on mobile clients.\\n\\nClarifying \\\"properly\\\" - iOS apps can define custom URL schemes that you can register with the OS. If a browser receives a request to a custom URL Scheme defined this way, it opens up the application automatically and invokes a callback method that the app has defined. \\n\\nI believe this will lead to a much cleaner UX for mobile clients. Will it be possible to add support for this?\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1tmut2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"_ty\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1tmut2/oauth_redirect_url_allow_custom_url_schemes/\", \"locked\": false, \"name\": \"t3_1tmut2\", \"created\": 1387952123.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1tmut2/oauth_redirect_url_allow_custom_url_schemes/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OAuth redirect URL - Allow custom url schemes.\", \"created_utc\": 1387923323.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWould be great to allow this in-app.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECheers\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Would be great to allow this in-app.\\n\\nCheers\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1s2l79\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ljdawson\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1s2l79/is_there_an_api_to_give_gold_to_a_comment/\", \"locked\": false, \"name\": \"t3_1s2l79\", \"created\": 1386184792.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1s2l79/is_there_an_api_to_give_gold_to_a_comment/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there an API to give gold to a comment?\", \"created_utc\": 1386155992.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve added a \\u003Ccode\\u003Elink_author\\u003C/code\\u003E attribute to the comment JSON when they\\u0026#39;re listed outside of their parent submission (on userpages, \\u003Ca href=\\\"/r/subreddit/comments\\\"\\u003E/r/subreddit/comments\\u003C/a\\u003E.json, etc.). This is mostly useful for being able to determine if the user making the comment is also the OP of the submission (and will allow me to add that often-requested check to AutoModerator).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/compare/e1bb2f70...311d8768\\\"\\u003EView the code for this change on github\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've added a `link_author` attribute to the comment JSON when they're listed outside of their parent submission (on userpages, /r/subreddit/comments.json, etc.). This is mostly useful for being able to determine if the user making the comment is also the OP of the submission (and will allow me to add that often-requested check to AutoModerator).\\n\\n[View the code for this change on github](https://github.com/reddit/reddit/compare/e1bb2f70...311d8768)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1rgf6j\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Deimorz\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1rgf6j/api_change_added_link_author_attribute_for/\", \"locked\": false, \"name\": \"t3_1rgf6j\", \"created\": 1385452344.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1rgf6j/api_change_added_link_author_attribute_for/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API change: added 'link_author' attribute for comments when outside of their parent submission\", \"created_utc\": 1385423544.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere\\u0026#39;s something I noticed and I can\\u0026#39;t tell if I\\u0026#39;m going crazy or if this is a legitimate issue. I issue the following query:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehttp://www.reddit.com/r/OkCupid/search?restrict_sr=on\\u0026amp;q=(and+timestamp:1384960654..1384977313+title:\\u0026#39;critique\\u0026#39;)\\u0026amp;limit=100\\u0026amp;syntax=cloudsearch\\u0026amp;sort=new\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EWhat this should do (if I\\u0026#39;m not mistaken) is search for posts that have the word \\u0026#39;critique\\u0026#39; in their title that were posted between 1384960654..1384977313 timestamps. These timestamps correspond to:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E1384960654 : Wed Nov 20 15:17:34 UTC 2013\\n1384977313 : Wed Nov 20 19:55:13 UTC 2013\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThat is a 4-hour window. The results come back with 5 results with the following created_utc dates:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E1384942382.0 : Wed Nov 20 10:13:02 UTC 2013\\n1384937454.0 : Wed Nov 20 08:50:54 UTC 2013\\n1384935918.0 : Wed Nov 20 08:25:18 UTC 2013\\n1384932395.0 : Wed Nov 20 07:26:35 UTC 2013\\n1384931854.0 : Wed Nov 20 07:17:34 UTC 2013\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAs you can see the oldest result at the end of the list is exactly 8 hours from the second query timestamp (Nov 20, 15:17:34). When I increment that timestamp by 1, that last result disappears. None of the 5 results are within the range provided by the timestamp but they would be if I needed \\u0026quot;timestamp - 8 hours.\\u0026quot;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo, my question is - am I missing something here? Am I misinterpreting these timestamps? BTW, I believe the \\u0026quot;created\\u0026quot; field is deprecated so I use the created_utc field.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi,\\n\\nHere's something I noticed and I can't tell if I'm going crazy or if this is a legitimate issue. I issue the following query:\\n\\n    http://www.reddit.com/r/OkCupid/search?restrict_sr=on\\u0026q=(and+timestamp:1384960654..1384977313+title:'critique')\\u0026limit=100\\u0026syntax=cloudsearch\\u0026sort=new\\n\\nWhat this should do (if I'm not mistaken) is search for posts that have the word 'critique' in their title that were posted between 1384960654..1384977313 timestamps. These timestamps correspond to:\\n\\n    1384960654 : Wed Nov 20 15:17:34 UTC 2013\\n    1384977313 : Wed Nov 20 19:55:13 UTC 2013\\n\\nThat is a 4-hour window. The results come back with 5 results with the following created_utc dates:\\n\\n    1384942382.0 : Wed Nov 20 10:13:02 UTC 2013\\n    1384937454.0 : Wed Nov 20 08:50:54 UTC 2013\\n    1384935918.0 : Wed Nov 20 08:25:18 UTC 2013\\n    1384932395.0 : Wed Nov 20 07:26:35 UTC 2013\\n    1384931854.0 : Wed Nov 20 07:17:34 UTC 2013\\n\\nAs you can see the oldest result at the end of the list is exactly 8 hours from the second query timestamp (Nov 20, 15:17:34). When I increment that timestamp by 1, that last result disappears. None of the 5 results are within the range provided by the timestamp but they would be if I needed \\\"timestamp - 8 hours.\\\"\\n\\nSo, my question is - am I missing something here? Am I misinterpreting these timestamps? BTW, I believe the \\\"created\\\" field is deprecated so I use the created_utc field.\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1r5wqx\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"thunder_afternoon\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1385070150.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1r5wqx/is_the_timestamp_off_by_8_hours_in_cloudsearch/\", \"locked\": false, \"name\": \"t3_1r5wqx\", \"created\": 1385094387.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1r5wqx/is_the_timestamp_off_by_8_hours_in_cloudsearch/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is the timestamp off by 8 hours in cloudsearch syntax?\", \"created_utc\": 1385065587.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Eor do i need to do it only once(upon login)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"or do i need to do it only once(upon login)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1kla7e\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"-shenanigans-\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1kla7e/do_i_need_to_check_needs_captcha_each_and/\", \"locked\": false, \"name\": \"t3_1kla7e\", \"created\": 1376830349.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1kla7e/do_i_need_to_check_needs_captcha_each_and/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"do i need to check 'needs_captcha' each and everytime something is being posted?\", \"created_utc\": 1376801549.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m thinking about a way to gather some insightful statistics about subreddits and I\\u0026#39;d thought about analysing comments for some specific metrics. E.g. \\u003Ca href=\\\"http://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_test\\\"\\u003EFlesh-Kincaid reading level\\u003C/a\\u003E or just lengh. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there a way to obtain all comments in a stream? There is \\u003Ca href=\\\"http://www.reddit.com/comments.json\\\"\\u003Ehttp://www.reddit.com/comments.json\\u003C/a\\u003E that would be probably okay for a sample, but I\\u0026#39;d rather don\\u0026#39;t want to hit the reddit servers each second or less. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETwitter has an API endpoint that just keeps the connection opens and pushes new tweets to me: \\u003Ca href=\\\"https://dev.twitter.com/docs/api/1.1/get/statuses/sample\\\"\\u003Ehttps://dev.twitter.com/docs/api/1.1/get/statuses/sample\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks for any hints!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm thinking about a way to gather some insightful statistics about subreddits and I'd thought about analysing comments for some specific metrics. E.g. [Flesh-Kincaid reading level](http://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_test) or just lengh. \\n\\nIs there a way to obtain all comments in a stream? There is http://www.reddit.com/comments.json that would be probably okay for a sample, but I'd rather don't want to hit the reddit servers each second or less. \\n\\nTwitter has an API endpoint that just keeps the connection opens and pushes new tweets to me: https://dev.twitter.com/docs/api/1.1/get/statuses/sample\\n\\nThanks for any hints!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1dsvq5\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1dsvq5/does_reddit_have_an_equivalent_to_twitters/\", \"locked\": false, \"name\": \"t3_1dsvq5\", \"created\": 1367888704.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1dsvq5/does_reddit_have_an_equivalent_to_twitters/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Does reddit have an equivalent to Twitters \\\"Firehose\\\" API?\", \"created_utc\": 1367859904.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI wanted to explore the reddit stack so I started poking around. I enabled admin and tried to add an award, the picture of which was linked from an external source. I\\u0026#39;m now stuck with a trace back in /admin/awards with the error \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E TypeError: not all arguments converted during string formatting\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI can provide the traceback if necessary. Thanks.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I wanted to explore the reddit stack so I started poking around. I enabled admin and tried to add an award, the picture of which was linked from an external source. I'm now stuck with a trace back in /admin/awards with the error \\n\\n     TypeError: not all arguments converted during string formatting\\n\\nI can provide the traceback if necessary. Thanks.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1axt99\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"micsha\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1axt99/i_broke_my_reddit_stack_how_do_i_go_about/\", \"locked\": false, \"name\": \"t3_1axt99\", \"created\": 1364194555.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1axt99/i_broke_my_reddit_stack_how_do_i_go_about/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I broke my reddit stack, how do I go about diagnosing it and fixing it.\", \"created_utc\": 1364165755.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI really like diagrams and graphs and I was thinking about writing a viewer that arranges comments like nodes on a \\u003Ca href=\\\"http://en.wikipedia.org/wiki/Tree_(graph_theory)\\\"\\u003Etree\\u003C/a\\u003E. However, it really seems like someone would have already made something like this and it would be silly to make something that already exists. Unfortunately, I don\\u0026#39;t know how to search for it, since \\u0026quot;tree\\u0026quot; means a lot of things.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am asking in this sub-reddit because this seems like the most likely place where such a viewer would get posted.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I really like diagrams and graphs and I was thinking about writing a viewer that arranges comments like nodes on a [tree](http://en.wikipedia.org/wiki/Tree_(graph_theory\\\\) ). However, it really seems like someone would have already made something like this and it would be silly to make something that already exists. Unfortunately, I don't know how to search for it, since \\\"tree\\\" means a lot of things.\\n\\nI am asking in this sub-reddit because this seems like the most likely place where such a viewer would get posted.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1abxgt\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"conundrumer\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1abxgt/is_there_a_viewer_that_arranges_comments_as_a/\", \"locked\": false, \"name\": \"t3_1abxgt\", \"created\": 1363348442.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1abxgt/is_there_a_viewer_that_arranges_comments_as_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a viewer that arranges comments as a graphical tree?\", \"created_utc\": 1363319642.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"19yrny\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"shrayas\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/19yrny/simple_project_to_scrape_rtldr_using_praw/\", \"locked\": false, \"name\": \"t3_19yrny\", \"created\": 1362852196.0, \"url\": \"https://github.com/shrayas/reddit-tldr\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Simple project to scrape /r/tldr using PRAW (learning python)\", \"created_utc\": 1362823396.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI want to use PRAW and do some automated data collection on a regular basis. I could run this from home but I\\u0026#39;m pretty OCD and don\\u0026#39;t want an Internet hiccup, computer crash, or power outage to miss one of the polling times so ideally I could run this on a server. I have a cheap hosting account but I don\\u0026#39;t know if I can run a bot script from there. I\\u0026#39;m fine writing PHP and JavaScript and stuff but running command line type stuff gives me pause especially when it involves importing stuff. I don\\u0026#39;t want the hosting company to get mad at me if I\\u0026#39;m not releasing resources correctly.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo what are some options and best practices for writing a Reddit bot? Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I want to use PRAW and do some automated data collection on a regular basis. I could run this from home but I'm pretty OCD and don't want an Internet hiccup, computer crash, or power outage to miss one of the polling times so ideally I could run this on a server. I have a cheap hosting account but I don't know if I can run a bot script from there. I'm fine writing PHP and JavaScript and stuff but running command line type stuff gives me pause especially when it involves importing stuff. I don't want the hosting company to get mad at me if I'm not releasing resources correctly.\\n\\nSo what are some options and best practices for writing a Reddit bot? Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"18ov4m\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ipitythefoobar\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 16, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/\", \"locked\": false, \"name\": \"t3_18ov4m\", \"created\": 1361136251.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Advice on running a Reddit bot written in Python on a shared server?\", \"created_utc\": 1361107451.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am working on modifications to Reddit to support email notifications. I\\u0026#39;m doing this primarily for a clone of the Reddit source we\\u0026#39;re evaluating for part of a larger system. I find that implementing a significant feature is the best way to learn the nuts and bolts of an architecture. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI also would like to contribute something back to the community as part of our process, if possible.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo I\\u0026#39;m not too worried about doing the feature, as I\\u0026#39;m comfortable with complex systems and Reddit seems fairly approachable. I\\u0026#39;m more concerned with wasting my time on a feature that won\\u0026#39;t end up being accepted for other reasons. If I know there is no chance the feature would be accepted, I\\u0026#39;d code it differently than I might if it would likely be accepted, as there are likely Reddit-community requirements that greatly exceed ours for reasons of performance, scalability and dealing with what is likely to be a much higher flow of potential notifications for most people. For instance, people might want the option for a daily digest email on Reddit.com while on our future system, they\\u0026#39;d likely just take the full feed all the time.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESince this is a feature that seems likely to have been considered many times before, and not implemented yet, I want to make sure I understand the reasons why this hasn\\u0026#39;t yet been done.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo I\\u0026#39;d like to find out more about the informal or formal process for proposing, implementing, and then getting new features accepted into the main code line.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIn this particular case, there are likely to be considerable cost from an operations perspective for the feature. It is also likely that people within Reddit have looked at this feature and decided it wasn\\u0026#39;t worth the tradeoff in benefits versus operational costs and time required to implement the feature.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E1) Does anyone know why email notifications specifically have not yet been implemented by the full-time programming staff? A search didn\\u0026#39;t turn up any discussion.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E2) What are the operational parameters which enter into any equations about whether or not a feature like this would get accepted into the main code line?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E3) How does one obtain architectural approval for a design ahead of implementing a feature like this? Is there a defined process? Who makes these sorts of decisions?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E4) Email notification seems likely to increase the outbound email volume by several orders of magnitude. What is the specific email infrastructure I should target for sending email? It seems likely that this would change for just this feature alone as the only use of email I currently see is for confirmation of email addresses. After all, you don\\u0026#39;t even need an email address to get a Reddit account.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am working on modifications to Reddit to support email notifications. I'm doing this primarily for a clone of the Reddit source we're evaluating for part of a larger system. I find that implementing a significant feature is the best way to learn the nuts and bolts of an architecture. \\n\\nI also would like to contribute something back to the community as part of our process, if possible.\\n\\nSo I'm not too worried about doing the feature, as I'm comfortable with complex systems and Reddit seems fairly approachable. I'm more concerned with wasting my time on a feature that won't end up being accepted for other reasons. If I know there is no chance the feature would be accepted, I'd code it differently than I might if it would likely be accepted, as there are likely Reddit-community requirements that greatly exceed ours for reasons of performance, scalability and dealing with what is likely to be a much higher flow of potential notifications for most people. For instance, people might want the option for a daily digest email on Reddit.com while on our future system, they'd likely just take the full feed all the time.\\n\\nSince this is a feature that seems likely to have been considered many times before, and not implemented yet, I want to make sure I understand the reasons why this hasn't yet been done.\\n\\nSo I'd like to find out more about the informal or formal process for proposing, implementing, and then getting new features accepted into the main code line.\\n\\nIn this particular case, there are likely to be considerable cost from an operations perspective for the feature. It is also likely that people within Reddit have looked at this feature and decided it wasn't worth the tradeoff in benefits versus operational costs and time required to implement the feature.\\n\\nSo:\\n\\n1) Does anyone know why email notifications specifically have not yet been implemented by the full-time programming staff? A search didn't turn up any discussion.\\n\\n2) What are the operational parameters which enter into any equations about whether or not a feature like this would get accepted into the main code line?\\n\\n3) How does one obtain architectural approval for a design ahead of implementing a feature like this? Is there a defined process? Who makes these sorts of decisions?\\n\\n4) Email notification seems likely to increase the outbound email volume by several orders of magnitude. What is the specific email infrastructure I should target for sending email? It seems likely that this would change for just this feature alone as the only use of email I currently see is for confirmation of email addresses. After all, you don't even need an email address to get a Reddit account.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"17kwf1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"inflecto\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/17kwf1/adding_an_email_notification_feature_best_way_to/\", \"locked\": false, \"name\": \"t3_17kwf1\", \"created\": 1359605251.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/17kwf1/adding_an_email_notification_feature_best_way_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Adding an email notification feature. Best way to proceed?\", \"created_utc\": 1359576451.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHow do you get a comment given its submission.id and its comment.id ?\\u003Cbr/\\u003E\\nThis is how you do it with a submission but haven\\u0026#39;t been able to figure out how to get a comment. Im SURE its very simple and im just not seeing it.    \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esubmission = reddit.get_submission(submission_id=submissionid)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"How do you get a comment given its submission.id and its comment.id ?  \\nThis is how you do it with a submission but haven't been able to figure out how to get a comment. Im SURE its very simple and im just not seeing it.    \\n    \\n    submission = reddit.get_submission(submission_id=submissionid)\\n\\n  \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"16yu6g\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"lamerx\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/16yu6g/simple_praw_comment_question/\", \"locked\": false, \"name\": \"t3_16yu6g\", \"created\": 1358767437.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/16yu6g/simple_praw_comment_question/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Simple Praw + Comment Question\", \"created_utc\": 1358738637.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI keep getting a JSONDecodeError, anyone gotten PRAW to work on Pythonanywhere?\\n\\u003Cstrong\\u003EEDIT:\\u003C/strong\\u003E Got past the first hurdle, but then experienced this error when using PRAW login.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003ETraceback (most recent call last):\\n  File \\u0026quot;/usr/local/lib/python2.7/site-packages/flask/app.py\\u0026quot;, line 1687, in wsgi_app\\n    response = self.full_dispatch_request()\\n  File \\u0026quot;/usr/local/lib/python2.7/site-packages/flask/app.py\\u0026quot;, line 1360, in full_dispatch_request\\n    rv = self.handle_user_exception(e)\\n  File \\u0026quot;/usr/local/lib/python2.7/site-packages/flask/app.py\\u0026quot;, line 1358, in full_dispatch_request\\n    rv = self.dispatch_request()\\n  File \\u0026quot;/usr/local/lib/python2.7/site-packages/flask/app.py\\u0026quot;, line 1344, in dispatch_request\\n    return self.view_functions[rule.endpoint](**req.view_args)\\n  File \\u0026quot;/home/myapp.py\\u0026quot;, line 21, in login\\n    return saved_links_page(fetch_Links(str(form.username.data), str(form.password.data)))\\n  File \\u0026quot;/home/myapp.py\\u0026quot;, line 33, in fetch_Links\\n    r.login(str(username), str(password))\\n  File \\u0026quot;/home/.local/lib/python2.7/site-packages/praw/__init__.py\\u0026quot;, line 804, in login\\n    self.request_json(self.config[\\u0026#39;login\\u0026#39;], data=data)\\n  File \\u0026quot;/home/.local/lib/python2.7/site-packages/praw/decorators.py\\u0026quot;, line 211, in error_checked_function\\n    return_value = function(cls, *args, **kwargs)\\n  File \\u0026quot;/home/.local/lib/python2.7/site-packages/praw/__init__.py\\u0026quot;, line 375, in request_json\\n    response = self._request(url, params, data)\\n  File \\u0026quot;/home/.local/lib/python2.7/site-packages/praw/__init__.py\\u0026quot;, line 266, in _request\\n    timeout=timeout)\\n  File \\u0026quot;/home/.local/lib/python2.7/site-packages/praw/decorators.py\\u0026quot;, line 64, in __call__\\n    result = self.function(reddit_session, url, *args, **kwargs)\\n  File \\u0026quot;/home/.local/lib/python2.7/site-packages/praw/decorators.py\\u0026quot;, line 155, in __call__\\n    return self.function(*args, **kwargs)\\n  File \\u0026quot;/home/.local/lib/python2.7/site-packages/praw/helpers.py\\u0026quot;, line 149, in _request\\n    response.raise_for_status()\\n  File \\u0026quot;/home/.local/lib/python2.7/site-packages/requests/models.py\\u0026quot;, line 638, in raise_for_status\\n    raise http_error\\nHTTPError: 501 Server Error: Not Implemented\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I keep getting a JSONDecodeError, anyone gotten PRAW to work on Pythonanywhere?\\n**EDIT:** Got past the first hurdle, but then experienced this error when using PRAW login.\\n\\n    Traceback (most recent call last):\\n      File \\\"/usr/local/lib/python2.7/site-packages/flask/app.py\\\", line 1687, in wsgi_app\\n        response = self.full_dispatch_request()\\n      File \\\"/usr/local/lib/python2.7/site-packages/flask/app.py\\\", line 1360, in full_dispatch_request\\n        rv = self.handle_user_exception(e)\\n      File \\\"/usr/local/lib/python2.7/site-packages/flask/app.py\\\", line 1358, in full_dispatch_request\\n        rv = self.dispatch_request()\\n      File \\\"/usr/local/lib/python2.7/site-packages/flask/app.py\\\", line 1344, in dispatch_request\\n        return self.view_functions[rule.endpoint](**req.view_args)\\n      File \\\"/home/myapp.py\\\", line 21, in login\\n        return saved_links_page(fetch_Links(str(form.username.data), str(form.password.data)))\\n      File \\\"/home/myapp.py\\\", line 33, in fetch_Links\\n        r.login(str(username), str(password))\\n      File \\\"/home/.local/lib/python2.7/site-packages/praw/__init__.py\\\", line 804, in login\\n        self.request_json(self.config['login'], data=data)\\n      File \\\"/home/.local/lib/python2.7/site-packages/praw/decorators.py\\\", line 211, in error_checked_function\\n        return_value = function(cls, *args, **kwargs)\\n      File \\\"/home/.local/lib/python2.7/site-packages/praw/__init__.py\\\", line 375, in request_json\\n        response = self._request(url, params, data)\\n      File \\\"/home/.local/lib/python2.7/site-packages/praw/__init__.py\\\", line 266, in _request\\n        timeout=timeout)\\n      File \\\"/home/.local/lib/python2.7/site-packages/praw/decorators.py\\\", line 64, in __call__\\n        result = self.function(reddit_session, url, *args, **kwargs)\\n      File \\\"/home/.local/lib/python2.7/site-packages/praw/decorators.py\\\", line 155, in __call__\\n        return self.function(*args, **kwargs)\\n      File \\\"/home/.local/lib/python2.7/site-packages/praw/helpers.py\\\", line 149, in _request\\n        response.raise_for_status()\\n      File \\\"/home/.local/lib/python2.7/site-packages/requests/models.py\\\", line 638, in raise_for_status\\n        raise http_error\\n    HTTPError: 501 Server Error: Not Implemented\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"16y13b\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"red_foot\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1358740773.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/\", \"locked\": false, \"name\": \"t3_16y13b\", \"created\": 1358741505.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Pythonanywhere.com and PRAW\", \"created_utc\": 1358712705.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi, redditdev!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m trying to pull the first 500 posts of a subreddit in its JSON format in one call so as to limit the number of calls I send to reddit. I can append .json to the url and get the first 25 posts, but when I try to set the limit to ?limit=500 (which is supposedly the max), it will only return the first 100 posts. Since I figured this is built in, I tried to use ?count in conjunction with ?limit=100 to start at the 100th post and collect the posts from 100-200, and then repeat this for 200-300, etc, but this does not work and defaults me to the top 100 posts.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere is my code:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Edef datascrape(subreddit):\\n    import urllib2\\n    import simplejson\\n\\n\\n    temp = open(str(\\u0026quot;tempfile.txt\\u0026quot;), \\u0026#39;w\\u0026#39;)\\n\\n    url = [\\u0026quot;http://www.reddit.com/r/\\u0026quot; + subreddit + \\u0026quot;/.json?limit=100\\u0026quot;]\\n    url.append(\\u0026quot;http://www.reddit.com/r/\\u0026quot; + subreddit + \\u0026quot;/.json?count=100\\u0026amp;limit=100\\u0026quot;)\\n    url.append(\\u0026quot;http://www.reddit.com/r/\\u0026quot; + subreddit + \\u0026quot;/.json?count=200\\u0026amp;limit=100\\u0026quot;)\\n    url.append(\\u0026quot;http://www.reddit.com/r/\\u0026quot; + subreddit + \\u0026quot;/.json?count=300\\u0026amp;limit=100\\u0026quot;)\\n    url.append(\\u0026quot;http://www.reddit.com/r/\\u0026quot; + subreddit + \\u0026quot;/.json?count=400\\u0026amp;limit=100\\u0026quot;)\\n\\n    pos = 0\\n\\n    while pos \\u0026lt; 4:\\n        req = urllib2.Request(url[pos], None, {\\u0026#39;user-agent\\u0026#39;:\\u0026#39;MrFanzyPanz_Data_Test\\u0026#39;})\\n        opener = urllib2.build_opener()\\n        json_data = opener.open(req)\\n\\n        temp.write(str(json_data.read()))\\n        pos += 1\\n\\n    temp.close()\\n\\n    return\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThe \\u0026quot;subreddit\\u0026quot; variable is currently set to \\u0026quot;pics\\u0026quot;.  The API\\u0026#39;s time limitations make it really problematic to call entries individually or by lists, so I\\u0026#39;m trying to avoid the API. Is there any way to extend the limit to 500, or to use ?count and ?limit together in order to retrieve blocks of 100 posts?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks for your help!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi, redditdev!\\n\\nI'm trying to pull the first 500 posts of a subreddit in its JSON format in one call so as to limit the number of calls I send to reddit. I can append .json to the url and get the first 25 posts, but when I try to set the limit to ?limit=500 (which is supposedly the max), it will only return the first 100 posts. Since I figured this is built in, I tried to use ?count in conjunction with ?limit=100 to start at the 100th post and collect the posts from 100-200, and then repeat this for 200-300, etc, but this does not work and defaults me to the top 100 posts.\\n\\nHere is my code:\\n\\n    def datascrape(subreddit):\\n        import urllib2\\n        import simplejson\\n\\n\\n        temp = open(str(\\\"tempfile.txt\\\"), 'w')\\n\\n        url = [\\\"http://www.reddit.com/r/\\\" + subreddit + \\\"/.json?limit=100\\\"]\\n        url.append(\\\"http://www.reddit.com/r/\\\" + subreddit + \\\"/.json?count=100\\u0026limit=100\\\")\\n        url.append(\\\"http://www.reddit.com/r/\\\" + subreddit + \\\"/.json?count=200\\u0026limit=100\\\")\\n        url.append(\\\"http://www.reddit.com/r/\\\" + subreddit + \\\"/.json?count=300\\u0026limit=100\\\")\\n        url.append(\\\"http://www.reddit.com/r/\\\" + subreddit + \\\"/.json?count=400\\u0026limit=100\\\")\\n\\n        pos = 0\\n\\n        while pos \\u003C 4:\\n            req = urllib2.Request(url[pos], None, {'user-agent':'MrFanzyPanz_Data_Test'})\\n            opener = urllib2.build_opener()\\n            json_data = opener.open(req)\\n\\n            temp.write(str(json_data.read()))\\n            pos += 1\\n\\n        temp.close()\\n\\n        return\\n\\nThe \\\"subreddit\\\" variable is currently set to \\\"pics\\\".  The API's time limitations make it really problematic to call entries individually or by lists, so I'm trying to avoid the API. Is there any way to extend the limit to 500, or to use ?count and ?limit together in order to retrieve blocks of 100 posts?\\n\\nThanks for your help!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"15cfwf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MrFanzyPanz\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/15cfwf/json_problems/\", \"locked\": false, \"name\": \"t3_15cfwf\", \"created\": 1356332476.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/15cfwf/json_problems/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"JSON problems :(\", \"created_utc\": 1356303676.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ELogging in via the browser works fine.  I\\u0026#39;ve tried changing my user-agent (except for copying a browser\\u0026#39;s, since that\\u0026#39;s a no-no on reddit).  I\\u0026#39;ve tried two different accounts.  I\\u0026#39;ve even tried a remote machine a few states away.  Nothing helps.  I\\u0026#39;ve made no changes to my login code for months.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI guess I just want my \\u003Ca href=\\\"/u/moderator-bot\\\"\\u003E/u/moderator-bot\\u003C/a\\u003E to be able to come back up and help moderating.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am told that it might be an https issue.  I switched to http auth and all is fine.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Logging in via the browser works fine.  I've tried changing my user-agent (except for copying a browser's, since that's a no-no on reddit).  I've tried two different accounts.  I've even tried a remote machine a few states away.  Nothing helps.  I've made no changes to my login code for months.\\n\\nI guess I just want my /u/moderator-bot to be able to come back up and help moderating.\\n\\n\\nEdit: \\n\\nI am told that it might be an https issue.  I switched to http auth and all is fine.\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1542h3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"aperson\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1355939824.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1542h3/logging_in_via_the_api_is_503ing_for_me_ive_not/\", \"locked\": false, \"name\": \"t3_1542h3\", \"created\": 1355961136.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1542h3/logging_in_via_the_api_is_503ing_for_me_ive_not/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Logging in via the API is 503'ing for me.  I've not changed any code and I've tried machines on two different networks\", \"created_utc\": 1355932336.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIs there any documentation out there?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m particularly interested in the admin functions.  So far, I don\\u0026#39;t see much, other than adding adverts and awards.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat am I missing?  Surely there\\u0026#39;s more functionality available to admins, no?  Where\\u0026#39;s the \\u0026quot;shadowban this guy\\u0026quot; button, for example?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Is there any documentation out there?\\n\\nI'm particularly interested in the admin functions.  So far, I don't see much, other than adding adverts and awards.  \\n\\nWhat am I missing?  Surely there's more functionality available to admins, no?  Where's the \\\"shadowban this guy\\\" button, for example?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"11q6ed\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"sunshine-x\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/11q6ed/documentation_about_operating_and_administering/\", \"locked\": false, \"name\": \"t3_11q6ed\", \"created\": 1350642880.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/11q6ed/documentation_about_operating_and_administering/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Documentation about operating and administering my clone\", \"created_utc\": 1350614080.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThree things:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWith \\u003Ca href=\\\"/u/mellort\\\"\\u003E/u/mellort\\u003C/a\\u003E\\u0026#39;s support, I have moved what was previously \\u003Ca href=\\\"https://github.com/mellort/reddit_api\\\"\\u003Emellort/reddit_api\\u003C/a\\u003E to \\u003Ca href=\\\"https://github.com/praw-dev/praw\\\"\\u003Epraw-dev/praw\\u003C/a\\u003E on github. \\u003Ca href=\\\"/u/mellort\\\"\\u003E/u/mellort\\u003C/a\\u003E has made a clone from the new location, so that existing links to the repository are not broken.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAdditionally I\\u0026#39;ve official updated the name to \\u003Cstrong\\u003EPRAW\\u003C/strong\\u003E which is an acronym for \\u0026quot;Python Reddit API Wrapper\\u0026quot;. The primary impetus for the name change was to resolve the confusion I see from many people regarding what to call the package. I admit it was very confusing that the repository was called \\u003Ccode\\u003Ereddit_api\\u003C/code\\u003E and the package was called \\u003Ccode\\u003Ereddit\\u003C/code\\u003E. Hopefully it should be clear what name to use now: PRAW.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFinally, to clearly distinguish the python package from reddit\\u0026#39;s own source, I have renamed the package from \\u003Ccode\\u003Ereddit\\u003C/code\\u003E to \\u003Ccode\\u003Epraw\\u003C/code\\u003E and restarted PRAWs version number at 1.0.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAside from the readthedocs documentation, all the documentation (on github) should be updated to reflect the change.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMaking the switch is pretty simple, here are the steps:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EInstall the \\u003Ccode\\u003Epraw\\u003C/code\\u003E package (instructions on github)\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EReplace \\u003Ccode\\u003Eimport reddit\\u003C/code\\u003E with \\u003Ccode\\u003Eimport praw\\u003C/code\\u003E in your code and of course update any \\u003Ccode\\u003Ereddit.NAME\\u003C/code\\u003E references with \\u003Ccode\\u003Epraw.NAME\\u003C/code\\u003E such as \\u003Ccode\\u003Ereddit.Reddit\\u003C/code\\u003E with \\u003Ccode\\u003Epraw.Reddit\\u003C/code\\u003E.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EIf you had a user-level, or script level \\u003Ccode\\u003Ereddit_api.cfg\\u003C/code\\u003E file, please replace that with \\u003Ccode\\u003Epraw.ini\\u003C/code\\u003E. There is more information \\u003Ca href=\\\"https://github.com/praw-dev/praw/wiki/The-Configuration-Files\\\"\\u003Ehere\\u003C/a\\u003E.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EIf you have any questions / comments please don\\u0026#39;t hesitate to ask / share.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Three things:\\n\\nWith /u/mellort's support, I have moved what was previously [mellort/reddit_api](https://github.com/mellort/reddit_api) to [praw-dev/praw](https://github.com/praw-dev/praw) on github. /u/mellort has made a clone from the new location, so that existing links to the repository are not broken.\\n\\nAdditionally I've official updated the name to __PRAW__ which is an acronym for \\\"Python Reddit API Wrapper\\\". The primary impetus for the name change was to resolve the confusion I see from many people regarding what to call the package. I admit it was very confusing that the repository was called `reddit_api` and the package was called `reddit`. Hopefully it should be clear what name to use now: PRAW.\\n\\nFinally, to clearly distinguish the python package from reddit's own source, I have renamed the package from `reddit` to `praw` and restarted PRAWs version number at 1.0.\\n\\nAside from the readthedocs documentation, all the documentation (on github) should be updated to reflect the change.\\n\\nMaking the switch is pretty simple, here are the steps:\\n\\n0. Install the `praw` package (instructions on github)\\n\\n0. Replace `import reddit` with `import praw` in your code and of course update any `reddit.NAME` references with `praw.NAME` such as `reddit.Reddit` with `praw.Reddit`.\\n\\n0. If you had a user-level, or script level `reddit_api.cfg` file, please replace that with `praw.ini`. There is more information [here](https://github.com/praw-dev/praw/wiki/The-Configuration-Files).\\n\\nIf you have any questions / comments please don't hesitate to ask / share.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"vv4tg\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bboe\", \"media\": null, \"score\": 12, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/vv4tg/python_reddit_api_wrapper_package_rename_and/\", \"locked\": false, \"name\": \"t3_vv4tg\", \"created\": 1341141971.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/vv4tg/python_reddit_api_wrapper_package_rename_and/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Python Reddit API Wrapper package rename and repository move\", \"created_utc\": 1341113171.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 12}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe title says it all. I\\u0026#39;m new to developing, but am highly interested in getting involved by testing any redditdev project your working on.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have experience with the reddit bread \\u0026#39;n\\u0026#39; butter (Python, HTML, PHP...), and A LOT more. I\\u0026#39;m a second-year undergrad computer science student, but have been coding for over 6 years. I really just want to be involved with some developing.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m a 4-year-strong redditor. I know what I\\u0026#39;m developing for.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESince I don\\u0026#39;t know much, I\\u0026#39;d love to do some testing. It\\u0026#39;s simple, it\\u0026#39;ll expose me to code, and give me a chance to help out. Reply to the thread or PM me.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAwesome.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The title says it all. I'm new to developing, but am highly interested in getting involved by testing any redditdev project your working on.\\n\\nI have experience with the reddit bread 'n' butter (Python, HTML, PHP...), and A LOT more. I'm a second-year undergrad computer science student, but have been coding for over 6 years. I really just want to be involved with some developing.\\n\\nI'm a 4-year-strong redditor. I know what I'm developing for.\\n\\nSince I don't know much, I'd love to do some testing. It's simple, it'll expose me to code, and give me a chance to help out. Reply to the thread or PM me.\\n\\nAwesome.\\n\\nThanks.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"vtdwf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"AstronomerOtter\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/vtdwf/im_a_newbie_developer_i_want_to_do_testing_for/\", \"locked\": false, \"name\": \"t3_vtdwf\", \"created\": 1341035704.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/vtdwf/im_a_newbie_developer_i_want_to_do_testing_for/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I'm a newbie developer. I want to do testing for your project.\", \"created_utc\": 1341006904.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://cleanmodqueue.codeplex.com/\\\"\\u003Ehttp://cleanmodqueue.codeplex.com/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESource available. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI wrote a little tool to do moderation of reddits for me. The tool or the code may be  interesting to other devs or moderators.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe basics:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EIt\\u0026#39;s built on .NET 4.0, therefore runs on Windows only. Uses Windows Forms.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EIt is graphical - not a headless script.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003Eworks by periodically querying reddit for the contents of your aggregated modqueue, then applying rules to items in the queue. It can also monitor other more specific queues. \\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003Ethe rule flavors are: if the post is from a \\u0026quot;known bad\\u0026quot; domain, spam it.  If the post is from a \\u0026quot;known bad\\u0026quot; author, spam it. If the post is from a mod of the reddit, approve it.  If the post has N upvotes, approve it.  If the post is from a \\u0026quot;known good\\u0026quot; domain or user, approve it. \\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EYou can set the list of known bad domains and known bad authors.  Likewise known good domains and users. You can set the re-check interval, and the upvote threshold. You can set the relative priority of these constraints.  In other words, you can tell it to first delete known bad domains, then approve known good users, or vice versa. \\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EIt also presents a UI where you can tick checkboxes to explicitly specify  actions to take on particular items.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EUses the ReadAsAsync\\u0026lt;T\\u0026gt; that is part of HttpClient to de-serialize JSON responses into C# classes.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EThe reddit interaction is all wrapped up in a C# class called \\u0026quot;Reddit.Client\\u0026quot;.  It exposes methods like Login, IsLoggedIn, Logout, GetModQueue, GetNewQueue, RemovePost, ApprovePost, ReportUserAsSpammer, GetUserRecentPosts, IsUserDead (shadow-banned), and so on.  This may be useful for other .NET developers.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EAll the code is released under the New BSD license.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m offering this in hopes that it may help someone else. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"http://cleanmodqueue.codeplex.com/\\n\\nSource available. \\n\\n\\nI wrote a little tool to do moderation of reddits for me. The tool or the code may be  interesting to other devs or moderators.\\n\\nThe basics:\\n\\n- It's built on .NET 4.0, therefore runs on Windows only. Uses Windows Forms.\\n\\n- It is graphical - not a headless script.\\n\\n- works by periodically querying reddit for the contents of your aggregated modqueue, then applying rules to items in the queue. It can also monitor other more specific queues. \\n\\n- the rule flavors are: if the post is from a \\\"known bad\\\" domain, spam it.  If the post is from a \\\"known bad\\\" author, spam it. If the post is from a mod of the reddit, approve it.  If the post has N upvotes, approve it.  If the post is from a \\\"known good\\\" domain or user, approve it. \\n\\n- You can set the list of known bad domains and known bad authors.  Likewise known good domains and users. You can set the re-check interval, and the upvote threshold. You can set the relative priority of these constraints.  In other words, you can tell it to first delete known bad domains, then approve known good users, or vice versa. \\n\\n- It also presents a UI where you can tick checkboxes to explicitly specify  actions to take on particular items.\\n\\n- Uses the ReadAsAsync\\u003CT\\u003E that is part of HttpClient to de-serialize JSON responses into C# classes.\\n\\n- The reddit interaction is all wrapped up in a C# class called \\\"Reddit.Client\\\".  It exposes methods like Login, IsLoggedIn, Logout, GetModQueue, GetNewQueue, RemovePost, ApprovePost, ReportUserAsSpammer, GetUserRecentPosts, IsUserDead (shadow-banned), and so on.  This may be useful for other .NET developers.\\n\\n\\nAll the code is released under the New BSD license.\\n\\nI'm offering this in hopes that it may help someone else. \\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ufauq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"AyeMatey\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ufauq/a_working_example_in_c_cleanmodqueue_is_a_windows/\", \"locked\": false, \"name\": \"t3_ufauq\", \"created\": 1338563220.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ufauq/a_working_example_in_c_cleanmodqueue_is_a_windows/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"a working example in C# - CleanModQueue is a Windows Forms app that uses System.Net.Htttp.HttpClient to tickle reddit's API.\", \"created_utc\": 1338534420.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIn order to do backups I have been doing a simple pg_dump of the reddit database.  A couple days ago I tried restoring from this backup and ran in to a couple problems.  The frontpage and subreddit frontpages page no longer show any links.  I can confirm that the actual links exist by going to the specific link url.  Also, the moderatorion log is empty.  Everything else seems to be working fine (user logins, etc.).  I ran the cron scripts manually, though I don\\u0026#39;t think that should matter.  Any ideas?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"In order to do backups I have been doing a simple pg_dump of the reddit database.  A couple days ago I tried restoring from this backup and ran in to a couple problems.  The frontpage and subreddit frontpages page no longer show any links.  I can confirm that the actual links exist by going to the specific link url.  Also, the moderatorion log is empty.  Everything else seems to be working fine (user logins, etc.).  I ran the cron scripts manually, though I don't think that should matter.  Any ideas?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"u10w4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"RedditSrc4Research\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/u10w4/backuprestore_problem/\", \"locked\": false, \"name\": \"t3_u10w4\", \"created\": 1337811891.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/u10w4/backuprestore_problem/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Backup/Restore Problem\", \"created_utc\": 1337783091.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"aws.amazon.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"t0r6a\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"theycallmemorty\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/t0r6a/protip_run_your_bots_on_amazon_web_services_using/\", \"locked\": false, \"name\": \"t3_t0r6a\", \"created\": 1335863304.0, \"url\": \"http://aws.amazon.com/free/faqs/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"ProTip: Run your bots on Amazon Web Services using the Free Usage Tier\", \"created_utc\": 1335834504.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"phreakocious.net\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"r3pkp\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"phreakocious\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/r3pkp/i_wrote_a_gephi_plugin_to_graph_reddit_if_theres/\", \"locked\": false, \"name\": \"t3_r3pkp\", \"created\": 1332207473.0, \"url\": \"http://phreakocious.net//shreddit-20120317\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I wrote a Gephi plugin to graph reddit.. If there's interest, I will release it.\", \"created_utc\": 1332178673.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhat does the LinkedIn acquisition of IndexTank mean for new reddit clones? They very likely could never open up registrations again. Is there a patch out there to use the old search provider, or are we going to have to hack in support for other search providers since we aren\\u0026#39;t going to be able to use IndexTank?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"What does the LinkedIn acquisition of IndexTank mean for new reddit clones? They very likely could never open up registrations again. Is there a patch out there to use the old search provider, or are we going to have to hack in support for other search providers since we aren't going to be able to use IndexTank?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"pvr52\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Chairmonkey\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/pvr52/so_what_should_new_reddit_clones_do_since/\", \"locked\": false, \"name\": \"t3_pvr52\", \"created\": 1329634675.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/pvr52/so_what_should_new_reddit_clones_do_since/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"So, what should new reddit clones do since IndexTank signups are disabled?\", \"created_utc\": 1329605875.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ELet me know if this is the wrong place for this question. I am thinking of writing a reddit app, but I didn\\u0026#39;t want to deal with all the licensing stuff unless it gets decently popular. For that reason I was avoiding using the reddit name and alien logo, but can I use the orangered envelope? Is that considered a \\u0026quot;reddit logo\\u0026quot;?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Let me know if this is the wrong place for this question. I am thinking of writing a reddit app, but I didn't want to deal with all the licensing stuff unless it gets decently popular. For that reason I was avoiding using the reddit name and alien logo, but can I use the orangered envelope? Is that considered a \\\"reddit logo\\\"?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ocffg\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Killobyte\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ocffg/licensing_question_what_are_the_reddit_logos/\", \"locked\": false, \"name\": \"t3_ocffg\", \"created\": 1326325070.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ocffg/licensing_question_what_are_the_reddit_logos/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Licensing question - What are the \\\"reddit logos\\\"?\", \"created_utc\": 1326296270.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI did a google search and it appears that theere may be some \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/_pages\\\"\\u003Eundocumented\\u003C/a\\u003E API method called read_message, but that it may or may not work? And there is no one discussing how to actually use the method.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am trying to write an app that will let people check their messages, but it seems to be not worthwhile if I can\\u0026#39;t clear the orangered when they do.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: I have read through the python file and I believe I\\u0026#39;ve constructed a similar function. The server returns \\n\\u0026quot;{}\\u0026quot;  when I make the request, but my orangered stays and the message stays in unread.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere the relevant code, if anyone could help...\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EDouble Edit\\u003C/strong\\u003E I think I may not be storing the cookie properly\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EmarkMailRead: function(mailId, callback){\\n    var url = \\u0026#39;http://www.reddit.com/api/read_message/\\u0026#39;;\\n    postData =\\n    {\\n        id: mailId\\n        //uh: reddit.control.fetchModHash()\\n    };\\n    sendRequest(url, callback, postData);\\n\\n\\nfunction sendRequest(url,callback,postData) \\n{\\n    var req = createXMLHTTPObject();\\n    if (!req) return;\\n    var method = (postData) ? \\u0026quot;POST\\u0026quot; : \\u0026quot;GET\\u0026quot;;\\n    req.open(method,url,true);\\n    if (postData)\\n         req.setRequestHeader(\\u0026#39;Content-type\\u0026#39;,\\u0026#39;application/x-www-form-urlencoded\\u0026#39;);\\n    req.onreadystatechange = function () {\\n          if (req.readyState != 4) return;\\n          if (req.status != 200 \\u0026amp;\\u0026amp; req.status != 304) {\\n              errorCode = \\u0026#39;HTTP error: \\u0026#39; + req.status;\\n              callback(errorCode);\\n          }\\n          var response = JSON.parse(req.response)\\n          if(response.data \\u0026amp;\\u0026amp; response.data.modhash){\\n              reddit.control.setModHash(response.data.modhash )\\n          }\\n          if(response.data \\u0026amp;\\u0026amp; response.data.children \\u0026amp;\\u0026amp; typeof callback ==\\u0026#39;function\\u0026#39;){\\n              callback(response.data.children);\\n          }else if(response.data \\u0026amp;\\u0026amp; typeof callback==\\u0026#39;function\\u0026#39;){\\n              callback(response.data);\\n          }else if(typeof callback== \\u0026#39;function\\u0026#39;){\\n              callback(response);\\n          };\\n    }\\n    if (req.readyState == 4) return;\\n    var postString = parsePostData(postData);\\n    req.send(postString);\\n};\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I did a google search and it appears that theere may be some [undocumented](https://github.com/reddit/reddit/wiki/_pages) API method called read_message, but that it may or may not work? And there is no one discussing how to actually use the method.\\n\\nI am trying to write an app that will let people check their messages, but it seems to be not worthwhile if I can't clear the orangered when they do.\\n\\nEdit: I have read through the python file and I believe I've constructed a similar function. The server returns \\n\\\"{}\\\"  when I make the request, but my orangered stays and the message stays in unread.\\n\\nHere the relevant code, if anyone could help...\\n\\n**Double Edit** I think I may not be storing the cookie properly\\n\\n    markMailRead: function(mailId, callback){\\n        var url = 'http://www.reddit.com/api/read_message/';\\n        postData =\\n        {\\n            id: mailId\\n            //uh: reddit.control.fetchModHash()\\n        };\\n        sendRequest(url, callback, postData);\\n\\n\\n    function sendRequest(url,callback,postData) \\n    {\\n        var req = createXMLHTTPObject();\\n        if (!req) return;\\n        var method = (postData) ? \\\"POST\\\" : \\\"GET\\\";\\n        req.open(method,url,true);\\n        if (postData)\\n             req.setRequestHeader('Content-type','application/x-www-form-urlencoded');\\n        req.onreadystatechange = function () {\\n              if (req.readyState != 4) return;\\n              if (req.status != 200 \\u0026\\u0026 req.status != 304) {\\n                  errorCode = 'HTTP error: ' + req.status;\\n                  callback(errorCode);\\n              }\\n              var response = JSON.parse(req.response)\\n              if(response.data \\u0026\\u0026 response.data.modhash){\\n                  reddit.control.setModHash(response.data.modhash )\\n              }\\n              if(response.data \\u0026\\u0026 response.data.children \\u0026\\u0026 typeof callback =='function'){\\n                  callback(response.data.children);\\n              }else if(response.data \\u0026\\u0026 typeof callback=='function'){\\n                  callback(response.data);\\n              }else if(typeof callback== 'function'){\\n                  callback(response);\\n              };\\n        }\\n        if (req.readyState == 4) return;\\n        var postString = parsePostData(postData);\\n        req.send(postString);\\n    };\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"o040t\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"WiglyWorm\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/o040t/how_can_i_mark_messages_as_read_via_the_api/\", \"locked\": false, \"name\": \"t3_o040t\", \"created\": 1325563463.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/o040t/how_can_i_mark_messages_as_read_via_the_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How can I mark messages as read via the API?\", \"created_utc\": 1325534663.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ECould we get a list somewhere of implementations of the Reddit API in different programming languages?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThat way, people who just want to get developing without having to muck about with http requests can pick a project and get working, while those who want to either learn how API wrappers work or want to develop their own have some references to go off of.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EObviously the list wouldn\\u0026#39;t be limited to one single implementation per language, but here\\u0026#39;s a list of all the wrappers I\\u0026#39;ve found. I can\\u0026#39;t vouch the others, but as a user/contributor to Mellort\\u0026#39;s Python one, I can confirm that it is being actively developed:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EC#:\\u003Cbr/\\u003E\\n\\u003Ca href=\\\"http://z3rb.net/reddit-c-api/\\\"\\u003Ehttp://z3rb.net/reddit-c-api/\\u003C/a\\u003E\\u003Cbr/\\u003E\\n\\u003Ca href=\\\"http://code.google.com/p/reddit-api/source/browse/#hg%2FCSharp\\\"\\u003Ehttp://code.google.com/p/reddit-api/source/browse/#hg%2FCSharp\\u003C/a\\u003E\\u003Cbr/\\u003E\\n\\u003Ca href=\\\"https://github.com/pressf12/reddit\\\"\\u003Ehttps://github.com/pressf12/reddit\\u003C/a\\u003E (from the comments)  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EClojure:\\u003Cbr/\\u003E\\n\\u003Ca href=\\\"http://sunng.info/blog/2011/07/reddit-clj-clojure-wrapper-for-reddit-api/\\\"\\u003Ehttp://sunng.info/blog/2011/07/reddit-clj-clojure-wrapper-for-reddit-api/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPerl:\\u003Cbr/\\u003E\\n\\u003Ca href=\\\"https://github.com/three18ti/Reddit.pm\\\"\\u003Ehttps://github.com/three18ti/Reddit.pm\\u003C/a\\u003E | \\u003Ca href=\\\"http://search.cpan.org/%7Ejon/Reddit-0.11/lib/Reddit.pm\\\"\\u003ECPAN Link\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPHP:\\u003Cbr/\\u003E\\n\\u003Ca href=\\\"https://github.com/jcleblanc/reddit-php-sdk\\\"\\u003Ehttps://github.com/jcleblanc/reddit-php-sdk\\u003C/a\\u003E\\u003Cbr/\\u003E\\n\\u003Ca href=\\\"http://code.google.com/p/reddit-api/source/browse/#hg%2Fphp\\\"\\u003Ehttp://code.google.com/p/reddit-api/source/browse/#hg%2Fphp\\u003C/a\\u003E  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPython:\\u003Cbr/\\u003E\\n\\u003Ca href=\\\"https://github.com/mellort/reddit_api\\\"\\u003Ehttps://github.com/mellort/reddit_api\\u003C/a\\u003E\\u003Cbr/\\u003E\\n\\u003Ca href=\\\"https://github.com/fsuarez2005/reddit-api-wrapper-python3\\\"\\u003Ehttps://github.com/fsuarez2005/reddit-api-wrapper-python3\\u003C/a\\u003E\\u003Cbr/\\u003E\\n\\u003Ca href=\\\"https://github.com/derv82/reddiwrap\\\"\\u003Ehttps://github.com/derv82/reddiwrap\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERuby:\\u003Cbr/\\u003E\\n\\u003Ca href=\\\"https://rubygems.org/gems/ruby_reddit_api\\\"\\u003Ehttps://rubygems.org/gems/ruby_reddit_api\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EJava:\\u003Cbr/\\u003E\\n\\u003Ca href=\\\"https://bitbucket.org/_oe/jreddit\\\"\\u003Ehttps://bitbucket.org/_oe/jreddit\\u003C/a\\u003E\\u003Cbr/\\u003E\\n\\u003Ca href=\\\"https://github.com/talklittle/reddit-is-fun\\\"\\u003Ehttps://github.com/talklittle/reddit-is-fun\\u003C/a\\u003E (an app rather than a wrapper, but should contain enough to get started)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny other implementations are welcome!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Could we get a list somewhere of implementations of the Reddit API in different programming languages?\\n\\nThat way, people who just want to get developing without having to muck about with http requests can pick a project and get working, while those who want to either learn how API wrappers work or want to develop their own have some references to go off of.\\n\\nObviously the list wouldn't be limited to one single implementation per language, but here's a list of all the wrappers I've found. I can't vouch the others, but as a user/contributor to Mellort's Python one, I can confirm that it is being actively developed:\\n\\nC#:  \\nhttp://z3rb.net/reddit-c-api/  \\nhttp://code.google.com/p/reddit-api/source/browse/#hg%2FCSharp  \\nhttps://github.com/pressf12/reddit (from the comments)  \\n\\nClojure:  \\nhttp://sunng.info/blog/2011/07/reddit-clj-clojure-wrapper-for-reddit-api/\\n\\nPerl:  \\nhttps://github.com/three18ti/Reddit.pm | [CPAN Link](http://search.cpan.org/~jon/Reddit-0.11/lib/Reddit.pm)\\n\\nPHP:  \\nhttps://github.com/jcleblanc/reddit-php-sdk  \\nhttp://code.google.com/p/reddit-api/source/browse/#hg%2Fphp  \\n\\nPython:  \\nhttps://github.com/mellort/reddit_api  \\nhttps://github.com/fsuarez2005/reddit-api-wrapper-python3  \\nhttps://github.com/derv82/reddiwrap\\n\\nRuby:  \\nhttps://rubygems.org/gems/ruby_reddit_api\\n\\nJava:  \\nhttps://bitbucket.org/_oe/jreddit  \\nhttps://github.com/talklittle/reddit-is-fun (an app rather than a wrapper, but should contain enough to get started)\\n\\nAny other implementations are welcome!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"nd521\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"nemec\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 18, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1354842639.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/nd521/list_of_existing_reddit_api_wrappers/\", \"locked\": false, \"name\": \"t3_nd521\", \"created\": 1323940435.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/nd521/list_of_existing_reddit_api_wrappers/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"List of existing Reddit API Wrappers?\", \"created_utc\": 1323911635.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EBasic outcome: When viewing all the comments on a submission, the colour of the comment reflects that user\\u0026#39;s response to the submission. If they upvote, their comment has a faint green background; if they downvote, it\\u0026#39;s faint orange. No vote lodged, no alterations to the colour of the comment\\u0026#39;s background.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPossible implementation: I\\u0026#39;m not an expert on the Reddit code, so this might be totally retarded - just based on my guesswork at how the existing system works. Requires no database modification but does require a few queries that probably aren\\u0026#39;t there - esp if caching upvotes and downvotes.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhen loading a comments page...\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E$submissionUpvotes = mysql_query(\\u0026quot;SELECT * FROM votes WHERE submission = \\u0026#39;$currentSubmission\\u0026#39; AND voteType = \\u0026#39;upvote\\u0026#39;\\u0026quot;);\\nwhile($row = mysql_fetch_array($submissionUpvotes)) {\\n    $arrayUpvotes[] = $row[\\u0026#39;userID\\u0026#39;];   \\n}\\n\\n$submissionDownvotes = mysql_query(\\u0026quot;SELECT * FROM votes WHERE submission = \\u0026#39;$currentSubmission\\u0026#39; AND voteType = \\u0026#39;downvote\\u0026#39;\\u0026quot;);\\nwhile($row = mysql_fetch_array($submissionDownvotes)) {\\n    $arrayDownvotes[] = $row[\\u0026#39;userID\\u0026#39;]; \\n}\\n\\n$submissionPoints = count($arrayUpvotes) - count($arrayDownvotes);\\n\\n// here\\u0026#39;s all the other stuff to output submission points, image thumb, etc etc etc\\n\\n// when looping through comments:\\n\\nif (in_array($commentSubmitter, $arrayUpvotes)) {\\n    echo \\u0026quot;class = \\u0026#39;upvoted\\u0026#39;\\u0026quot;;\\n}\\n\\nif (in_array($commentSubmitter, $arrayDownvotes)) {\\n    echo \\u0026quot;class = \\u0026#39;downvoted\\u0026#39;\\u0026quot;;\\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAs above - I haven\\u0026#39;t had a chance to look at the Reddit code yet so not sure on the implementation part. But could this be done, and does anybody else think it would be kinda neat? Gauging feasability of code before I wander over to \\u003Ca href=\\\"/r/ideasfortheadmins\\\"\\u003E/r/ideasfortheadmins\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Basic outcome: When viewing all the comments on a submission, the colour of the comment reflects that user's response to the submission. If they upvote, their comment has a faint green background; if they downvote, it's faint orange. No vote lodged, no alterations to the colour of the comment's background.\\n\\nPossible implementation: I'm not an expert on the Reddit code, so this might be totally retarded - just based on my guesswork at how the existing system works. Requires no database modification but does require a few queries that probably aren't there - esp if caching upvotes and downvotes.\\n\\nWhen loading a comments page...\\n\\n    $submissionUpvotes = mysql_query(\\\"SELECT * FROM votes WHERE submission = '$currentSubmission' AND voteType = 'upvote'\\\");\\n    while($row = mysql_fetch_array($submissionUpvotes)) {\\n    \\t$arrayUpvotes[] = $row['userID'];\\t\\n    }\\n        \\n    $submissionDownvotes = mysql_query(\\\"SELECT * FROM votes WHERE submission = '$currentSubmission' AND voteType = 'downvote'\\\");\\n    while($row = mysql_fetch_array($submissionDownvotes)) {\\n    \\t$arrayDownvotes[] = $row['userID'];\\t\\n    }\\n    \\n    $submissionPoints = count($arrayUpvotes) - count($arrayDownvotes);\\n        \\n    // here's all the other stuff to output submission points, image thumb, etc etc etc\\n        \\n    // when looping through comments:\\n    \\n    if (in_array($commentSubmitter, $arrayUpvotes)) {\\n        echo \\\"class = 'upvoted'\\\";\\n    }\\n    \\n    if (in_array($commentSubmitter, $arrayDownvotes)) {\\n        echo \\\"class = 'downvoted'\\\";\\n    }\\n\\nAs above - I haven't had a chance to look at the Reddit code yet so not sure on the implementation part. But could this be done, and does anybody else think it would be kinda neat? Gauging feasability of code before I wander over to /r/ideasfortheadmins\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"h70v1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Sam_Mack\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 17, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/h70v1/how_much_extra_load_would_this_put_on_reddit_and/\", \"locked\": false, \"name\": \"t3_h70v1\", \"created\": 1304937149.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/h70v1/how_much_extra_load_would_this_put_on_reddit_and/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How much extra load would this put on Reddit, and would it be worthwhile?\", \"created_utc\": 1304908349.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI moderate \\u003Ca href=\\\"/r/hockey\\\"\\u003E/r/hockey\\u003C/a\\u003E and give people team crests.  There are a number of subreddits that do this, and it has become a huge hassle for big subreddits now...  Yes, we\\u0026#39;ve made that hassle ourselves by starting to offer this when traffic was lower... not complaining in that regard...\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHowever, two key concerns:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E1) It\\u0026#39;s a cumbersome process to add people\\u0026#39;s crests, and I\\u0026#39;m trying to come up with a solve (via UserJS) to at least simplify the process for moderators...\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E2) Subreddits using the a.author[href=/username]:before method are often running out of space.  The alternative is to use shorter declarations that involve the user\\u0026#39;s reddit-ID (found in their about.json) such as: .id-t2_[userid]:after  Problem with that is once you put it in you never know who the heck it is/was if you need to do some editing...\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt would be great if there were a way (maybe there is one already?) to look up a username if you know their userid... So, mine is 2539s ... can I query a URL somewhere and get back \\u0026#39;honestbleeps\\u0026#39; from that?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf so, I\\u0026#39;d build a tool for managing crests in a more efficient way...\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI was going to kill work on this because an admin (I believe ketralnis) had said it was a feature that you guys might be working on adding... but I was later informed that it had dropped off the radar due to other higher priorities (totally understandable!)...\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks...\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I moderate /r/hockey and give people team crests.  There are a number of subreddits that do this, and it has become a huge hassle for big subreddits now...  Yes, we've made that hassle ourselves by starting to offer this when traffic was lower... not complaining in that regard...\\n\\nHowever, two key concerns:\\n\\n1) It's a cumbersome process to add people's crests, and I'm trying to come up with a solve (via UserJS) to at least simplify the process for moderators...\\n\\n2) Subreddits using the a.author[href=/username]:before method are often running out of space.  The alternative is to use shorter declarations that involve the user's reddit-ID (found in their about.json) such as: .id-t2_[userid]:after  Problem with that is once you put it in you never know who the heck it is/was if you need to do some editing...\\n\\nIt would be great if there were a way (maybe there is one already?) to look up a username if you know their userid... So, mine is 2539s ... can I query a URL somewhere and get back 'honestbleeps' from that?\\n\\nIf so, I'd build a tool for managing crests in a more efficient way...\\n\\nI was going to kill work on this because an admin (I believe ketralnis) had said it was a feature that you guys might be working on adding... but I was later informed that it had dropped off the radar due to other higher priorities (totally understandable!)...\\n\\nThanks...\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"fa4w8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"honestbleeps\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/fa4w8/possible_interim_solution_for_subreddit_badges_is/\", \"locked\": false, \"name\": \"t3_fa4w8\", \"created\": 1296177610.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/fa4w8/possible_interim_solution_for_subreddit_badges_is/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Possible (interim) solution for subreddit badges: Is there a way to look up usernames by ID?\", \"created_utc\": 1296148810.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am just wondering.  As of right now its not too bad I don\\u0026#39;t really mind.  I am just interested in why this is the case.  Is there some underlying meaning, do you go through a separate server that is slower running? What do?  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso, is it a feature that everytime I try to verify I am a human the first verification never works?  Is this standard or am I just screwing up somehow every single time?  Again, not a hassle just wondering.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: Also if anyone knows please for the sake of everything good in the world let me know how to get \\u003Cstrong\\u003E\\u003Ca href=\\\"/r/politics\\\"\\u003Er/politics\\u003C/a\\u003E\\u003C/strong\\u003E off of my front page.  I have removed the frontpage status but it still shows for some reason.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am just wondering.  As of right now its not too bad I don't really mind.  I am just interested in why this is the case.  Is there some underlying meaning, do you go through a separate server that is slower running? What do?  \\n\\nAlso, is it a feature that everytime I try to verify I am a human the first verification never works?  Is this standard or am I just screwing up somehow every single time?  Again, not a hassle just wondering.\\n\\nEdit: Also if anyone knows please for the sake of everything good in the world let me know how to get **r/politics** off of my front page.  I have removed the frontpage status but it still shows for some reason.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ey6b7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"blind__man\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ey6b7/not_a_big_complaint_butwhy_does_it_take_so_long/\", \"locked\": false, \"name\": \"t3_ey6b7\", \"created\": 1294470116.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ey6b7/not_a_big_complaint_butwhy_does_it_take_so_long/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Not a big complaint but...why does it take so long to view my messages and user home page?\", \"created_utc\": 1294441316.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI would have posted a ticket for this, but I\\u0026#39;m not allowed to do so. I recall Chris saying awhile back that it had something to do with not having enough karma. Oh well. :)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve been tinkering around with the JSONP api for the first time and I\\u0026#39;ve hit a brick wall when it comes to handling errors. Here\\u0026#39;s some examples. Some apply to the JSON api as well.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Etl@devserver:~$ curl http://www.reddit.com/r/idontexistsr.json\\n \\ufffd\\ufffdL\\ufffd}P1n\\ufffd0\\n\\ufffd\\u7d99S\\ufffd\\ufffdS\\ufffd\\ufffd\\u0151\\ufffd\\ufffd\\ufffd\\\\\\ufffd\\ufffd\\ufffd\\ufffdCW\\ufffd;vu \\ufffdA8\\ufffd\\u0026quot;#\\ufffd7f8-@\\ufffd\\ufffd\\ufffdOCg\\ufffd\\ufffd\\ufffdf\\ufffde\\ufffd/_C\\ufffd4)\\ufffd\\ufffd\\ufffdY:\\ufffd?o\\n                  j1\\ufffd\\ufffd\\ufffdm\\ufffd\\ufffd\\ufffd\\ufffd \\ufffd\\ufffdk\\ufffd\\ufffd\\ufffd\\ufffdS\\ufffd \\ufffd\\ufffd`\\n                                             \\ufffdi\\ufffd$\\ufffd\\ufffdv\\ufffd\\ufffd\\ufffd\\ufffd\\ufffdJo\\ufffd{5\\ufffd\\ufffd\\ufffd/\\ufffd\\ufffd=)D\\n\\n# When viewed from a brower this will redirect to an HTML page\\n# regardless of whether its an api request\\n# Better response: {\\u0026quot;error\\u0026quot;:404}\\n\\n\\ntl@devserver:~$ curl http://www.reddit.com/r/idontexistsr.json?jsonp=fish\\n \\ufffd\\ufffd\\ufffdL\\ufffd}P1n\\ufffd0\\n\\ufffd\\u7d99S\\ufffd\\ufffdS\\ufffd\\ufffd\\u0151\\ufffd\\ufffd\\ufffd\\\\\\ufffd\\ufffd\\ufffd\\ufffdCW\\ufffd;vu \\ufffdA8\\ufffd\\u0026quot;#\\ufffd7f8-@\\ufffd\\ufffd\\ufffdOCg\\ufffd\\ufffd\\ufffdf\\ufffde\\ufffd/_C\\ufffd4)\\ufffd\\ufffd\\ufffdY:\\ufffd?o\\n                  j1\\ufffd\\ufffd\\ufffdm\\ufffd\\ufffd\\ufffd\\ufffd \\ufffd\\ufffdk\\ufffd\\ufffd\\ufffd\\ufffdS\\ufffd \\ufffd\\ufffd`\\n                                             \\ufffdi\\ufffd$\\ufffd\\ufffdv\\ufffd\\ufffd\\ufffd\\ufffd\\ufffdJo\\ufffd{5\\ufffd\\ufffd\\ufffd/\\ufffd\\ufffd=)D\\n\\n# Same as above + ignores jsonp\\n# Better response: fish({\\u0026quot;error\\u0026quot;:404})\\n\\n\\ntl@devserver:~$ curl http://www.reddit.com/user/idontexistuser/about.json\\n{error: 404}\\n\\n# Perfect :), although missing proper JSON quotes\\n\\n\\ntl@devserver:~$ curl http://www.reddit.com/user/idontexistuser/about.json?jsonp=fish\\n{error: 404}\\n\\n# Ignores jsonp\\n# Better response: fish({\\u0026quot;error\\u0026quot;:404})\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAlso, a feature request for the API would be to add awards to /user/{username}/about.json. Or at the very least, is_gold to tell if a user is a current gold subscriber. Some of the apps and visualizations I create could potentially be resource intensive for reddit. If I could easily tell if a user is a gold subscriber I can give them full[er] features while limiting normal users to less resource intensive methods.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I would have posted a ticket for this, but I'm not allowed to do so. I recall Chris saying awhile back that it had something to do with not having enough karma. Oh well. :)\\n\\nI've been tinkering around with the JSONP api for the first time and I've hit a brick wall when it comes to handling errors. Here's some examples. Some apply to the JSON api as well.\\n\\n    tl@devserver:~$ curl http://www.reddit.com/r/idontexistsr.json\\n     \\ufffd\\ufffdL\\u0002\\ufffd}P1n\\ufffd0\\n    \\ufffd\\u7d99S\\ufffd\\ufffdS\\ufffd\\u0004\\ufffd\\u0151\\ufffd\\ufffd\\ufffd\\\\\\ufffd\\ufffd\\ufffd\\ufffdCW\\ufffd;vu \\ufffdA8\\ufffd\\\"#\\ufffd7f8-\\u001a@\\ufffd\\ufffd\\ufffdOCg\\ufffd\\ufffd\\ufffdf\\ufffd\\u0019e\\ufffd/_C\\ufffd4)\\ufffd\\ufffd\\ufffdY:\\ufffd?o\\n                      j\\u00181\\ufffd\\ufffd\\ufffdm\\ufffd\\ufffd\\ufffd\\ufffd \\ufffd\\ufffdk\\ufffd\\ufffd\\ufffd\\ufffd\\u0002S\\ufffd \\ufffd\\ufffd\\u001b`\\n                                                 \\ufffdi\\ufffd$\\ufffd\\ufffdv\\ufffd\\ufffd\\ufffd\\ufffd\\ufffdJo\\u0015\\ufffd{5\\ufffd\\ufffd\\ufffd/\\ufffd\\ufffd=)D\\u0001\\n                                                                                              \\n    # When viewed from a brower this will redirect to an HTML page\\n    # regardless of whether its an api request\\n    # Better response: {\\\"error\\\":404}\\n    \\n                                                 \\n    tl@devserver:~$ curl http://www.reddit.com/r/idontexistsr.json?jsonp=fish\\n     \\ufffd\\ufffd\\ufffdL\\u0002\\ufffd}P1n\\ufffd0\\n    \\ufffd\\u7d99S\\ufffd\\ufffdS\\ufffd\\u0004\\ufffd\\u0151\\ufffd\\ufffd\\ufffd\\\\\\ufffd\\ufffd\\ufffd\\ufffdCW\\ufffd;vu \\ufffdA8\\ufffd\\\"#\\ufffd7f8-\\u001a@\\ufffd\\ufffd\\ufffdOCg\\ufffd\\ufffd\\ufffdf\\ufffd\\u0019e\\ufffd/_C\\ufffd4)\\ufffd\\ufffd\\ufffdY:\\ufffd?o\\n                      j\\u00181\\ufffd\\ufffd\\ufffdm\\ufffd\\ufffd\\ufffd\\ufffd \\ufffd\\ufffdk\\ufffd\\ufffd\\ufffd\\ufffd\\u0002S\\ufffd \\ufffd\\ufffd\\u001b`\\n                                                 \\ufffdi\\ufffd$\\ufffd\\ufffdv\\ufffd\\ufffd\\ufffd\\ufffd\\ufffdJo\\u0015\\ufffd{5\\ufffd\\ufffd\\ufffd/\\ufffd\\ufffd=)D\\n                                                 \\n    # Same as above + ignores jsonp\\n    # Better response: fish({\\\"error\\\":404})\\n    \\n    \\n    tl@devserver:~$ curl http://www.reddit.com/user/idontexistuser/about.json\\n    {error: 404}\\n    \\n    # Perfect :), although missing proper JSON quotes\\n    \\n    \\n    tl@devserver:~$ curl http://www.reddit.com/user/idontexistuser/about.json?jsonp=fish\\n    {error: 404}\\n    \\n    # Ignores jsonp\\n    # Better response: fish({\\\"error\\\":404})\\n\\n\\nAlso, a feature request for the API would be to add awards to /user/{username}/about.json. Or at the very least, is_gold to tell if a user is a current gold subscriber. Some of the apps and visualizations I create could potentially be resource intensive for reddit. If I could easily tell if a user is a gold subscriber I can give them full[er] features while limiting normal users to less resource intensive methods.\\n\\nThanks.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"e3jpm\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"tritelife\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/e3jpm/graceful_failure_with_the_reddit_jsonp_api/\", \"locked\": false, \"name\": \"t3_e3jpm\", \"created\": 1289351226.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/e3jpm/graceful_failure_with_the_reddit_jsonp_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Graceful failure with the reddit JSON[P] API\", \"created_utc\": 1289322426.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"stackoverflow.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"dpjoj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"jyper\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/dpjoj/can_anyone_track_down_any_extra_bits_of_reddit/\", \"locked\": false, \"name\": \"t3_dpjoj\", \"created\": 1286796809.0, \"url\": \"http://stackoverflow.com/questions/3900404/help-with-reddit-api-documentation\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can anyone track down any extra bits of Reddit API documentation?(self - submit)\", \"created_utc\": 1286768009.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESometimes when I\\u0026#39;m downloading the json feed of a comments view of a post, I get an HTTP result code of 200 back, but then the HTTP content is this:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E{error: 503}\\n{\\u0026#39;content-length\\u0026#39;: \\u0026#39;12\\u0026#39;, \\u0026#39;access-control\\u0026#39;: \\u0026#39;allow \\u0026lt;*\\u0026gt;\\u0026#39;, \\u0026#39;vary\\u0026#39;: \\u0026#39;Accept-Encoding\\u0026#39;, \\u0026#39;server\\u0026#39;: \\u0026quot;\\u0026#39;; DROP TABLE servertypes; --\\u0026quot;, \\u0026#39;connection\\u0026#39;: \\u0026#39;keep-alive\\u0026#39;, \\u0026#39;date\\u0026#39;: \\u0026#39;Fri, 09 Jul 2010 08:35:09 GMT\\u0026#39;, \\u0026#39;content-type\\u0026#39;: \\u0026#39;application/json; charset=UTF-8\\u0026#39;}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIs reddit trying to do a sql-injection attack on a bad json parser?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI searched through the source code and I don\\u0026#39;t see anything that is generating this on purpose. It must be some weird bug or a very obfuscated piece of code.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy bot retries the download 5 times and sometimes all 5 attempts at downloading will receive that same response. It\\u0026#39;s very weird.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThoughts?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Sometimes when I'm downloading the json feed of a comments view of a post, I get an HTTP result code of 200 back, but then the HTTP content is this:\\n\\n    {error: 503}\\n    {'content-length': '12', 'access-control': 'allow \\u003C*\\u003E', 'vary': 'Accept-Encoding', 'server': \\\"'; DROP TABLE servertypes; --\\\", 'connection': 'keep-alive', 'date': 'Fri, 09 Jul 2010 08:35:09 GMT', 'content-type': 'application/json; charset=UTF-8'}\\n\\nIs reddit trying to do a sql-injection attack on a bad json parser?\\n\\nI searched through the source code and I don't see anything that is generating this on purpose. It must be some weird bug or a very obfuscated piece of code.\\n\\nMy bot retries the download 5 times and sometimes all 5 attempts at downloading will receive that same response. It's very weird.\\n\\nThoughts?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"cnmkr\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"tayl0rs\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/cnmkr/json_feed_of_comments_view_returns_http_status/\", \"locked\": false, \"name\": \"t3_cnmkr\", \"created\": 1278693740.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/cnmkr/json_feed_of_comments_view_returns_http_status/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"json feed of comments view returns HTTP status 200 but mangled json sometimes\", \"created_utc\": 1278664940.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ECurrently, you can see all friend submissions on \\u003Ca href=\\\"/r/friends\\\"\\u003E/r/friends\\u003C/a\\u003E. But there doesn\\u0026#39;t seem to be a place where you can see all friend submissions + comments, or even a place to see all friend comments -- this limits the usefulness of its integration in clients like Gwibber, as discussed \\u003Ca href=\\\"http://www.reddit.com/r/Ubuntu/comments/bj008/upgraded_to_lucid/c0n1gf2?context=1\\\"\\u003Ehere\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo, is there a page where you can see friend submissions + comments (and possibly + liked, but I\\u0026#39;m not a fan of that one)? Would it be accepted if someone added it? Is it a feature that wasn\\u0026#39;t considered, or one ignored because of load/storage issues, like saving comments?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Currently, you can see all friend submissions on /r/friends. But there doesn't seem to be a place where you can see all friend submissions + comments, or even a place to see all friend comments -- this limits the usefulness of its integration in clients like Gwibber, as discussed [here](http://www.reddit.com/r/Ubuntu/comments/bj008/upgraded_to_lucid/c0n1gf2?context=1).\\n\\nSo, is there a page where you can see friend submissions + comments (and possibly + liked, but I'm not a fan of that one)? Would it be accepted if someone added it? Is it a feature that wasn't considered, or one ignored because of load/storage issues, like saving comments?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"bjb09\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"JoeCoT\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 18, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/bjb09/is_there_a_page_to_see_all_friend_submissions/\", \"locked\": false, \"name\": \"t3_bjb09\", \"created\": 1269813903.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/bjb09/is_there_a_page_to_see_all_friend_submissions/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a page to see all friend submissions + comments? Can one be added?\", \"created_utc\": 1269785103.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have asked the following questions at reddit-dev google group, but there was no reply.\\nI\\u0026#39;d like to get some answers here.\\u003C/p\\u003E\\n\\n\\u003Ch1\\u003E1.\\u003C/h1\\u003E\\n\\n\\u003Cp\\u003EHow can I get children id\\u0026#39;s to fetch the hidden comments in json response using \\u0026quot;morechildren\\u0026quot; api?\\nHtml responses for \\u0026quot;comments\\u0026quot; api contains all the hidden children id\\u0026#39;s but json responses don\\u0026#39;t contain the id\\u0026#39;s. \\nFor example, the response for the request \\u0026quot;\\u003Ca href=\\\"http://www.reddit.com/r/politics/comments/aoz20\\\"\\u003Ehttp://www.reddit.com/r/politics/comments/aoz20\\u003C/a\\u003E\\u0026quot; contains the following snippet for the last \\u0026quot;load more comments\\u0026quot; link.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eonclick=\\u0026quot;return morechildren(this, \\u0026#39;t3_aoz20\\u0026#39;, \\u0026#39;c0iowz2,c0iowtu,c0iowl9,c0iowhk,c0iowg5,c0iotl9,c0iorj3,c0iorhk,c0iomme,c0ip4kg,c0ioqng,c0ip00i,c0ioq2j,c0ip0r3,c0iospy,c0iovqu\\u0026#39;,0)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHowever, the response for the request \\u0026quot;\\u003Ca href=\\\"http://www.reddit.com/r/politics/comments/aoz20/.json\\\"\\u003Ehttp://www.reddit.com/r/politics/comments/aoz20/.json\\u003C/a\\u003E\\u0026quot; contains only the next snippet for the last \\u0026quot;load more comments\\u0026quot; link.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E{\\n    \\u0026quot;kind\\u0026quot;:\\u0026quot;more\\u0026quot;,\\n    \\u0026quot;data\\u0026quot;:{\\n        \\u0026quot;name\\u0026quot;:\\u0026quot;t1_c0iowz2\\u0026quot;,\\n        \\u0026quot;id\\u0026quot;:\\u0026quot;c0iowz2\\u0026quot;\\n     }\\n}\\u003C/p\\u003E\\n\\n\\u003Ch1\\u003E2.\\u003C/h1\\u003E\\n\\n\\u003Cp\\u003EIt seems that the responses for \\u0026quot;morechildren\\u0026quot; api request contain html code to insert. \\nCan I get the informations on the fetched comments in json format ?\\u003C/p\\u003E\\n\\n\\u003Ch1\\u003E3.\\u003C/h1\\u003E\\n\\n\\u003Cp\\u003EIf possible, I\\u0026#39;d like to fetch all the comments for a link at once without the \\u0026quot;load more comments\\u0026quot;.\\nIs it possible?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have asked the following questions at reddit-dev google group, but there was no reply.\\nI'd like to get some answers here.\\n\\n#1.\\nHow can I get children id's to fetch the hidden comments in json response using \\\"morechildren\\\" api?\\nHtml responses for \\\"comments\\\" api contains all the hidden children id's but json responses don't contain the id's. \\nFor example, the response for the request \\\"http://www.reddit.com/r/politics/comments/aoz20\\\" contains the following snippet for the last \\\"load more comments\\\" link.\\n\\nonclick=\\\"return morechildren(this, 't3_aoz20', 'c0iowz2,c0iowtu,c0iowl9,c0iowhk,c0iowg5,c0iotl9,c0iorj3,c0iorhk,c0iomme,c0ip4kg,c0ioqng,c0ip00i,c0ioq2j,c0ip0r3,c0iospy,c0iovqu',0)\\n\\nHowever, the response for the request \\\"http://www.reddit.com/r/politics/comments/aoz20/.json\\\" contains only the next snippet for the last \\\"load more comments\\\" link.\\n\\n{\\n    \\\"kind\\\":\\\"more\\\",\\n    \\\"data\\\":{\\n        \\\"name\\\":\\\"t1_c0iowz2\\\",\\n        \\\"id\\\":\\\"c0iowz2\\\"\\n     }\\n}\\n\\n\\n#2.\\nIt seems that the responses for \\\"morechildren\\\" api request contain html code to insert. \\nCan I get the informations on the fetched comments in json format ?\\n\\n#3.\\nIf possible, I'd like to fetch all the comments for a link at once without the \\\"load more comments\\\".\\nIs it possible?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"aql1m\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"asitdepends\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/aql1m/some_questions_on_morechildren_api_or_load_more/\", \"locked\": false, \"name\": \"t3_aql1m\", \"created\": 1263739761.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/aql1m/some_questions_on_morechildren_api_or_load_more/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Some questions on \\\"morechildren\\\" api or \\\"load more comments\\\" link.\", \"created_utc\": 1263710961.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"9kovq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"siovene\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/9kovq/i_saw_lots_of_changes_to_reddit_in_the_last_few/\", \"locked\": false, \"name\": \"t3_9kovq\", \"created\": 1253033276.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/9kovq/i_saw_lots_of_changes_to_reddit_in_the_last_few/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I saw lots of changes to Reddit in the last few weeks. When are they going to be pushed to git master?\", \"created_utc\": 1253004476.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"9hqrz\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"talklittle\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/9hqrz/android_reddit_app_open_source_under_gpl/\", \"locked\": false, \"name\": \"t3_9hqrz\", \"created\": 1252235554.0, \"url\": \"http://github.com/talklittle/reddit-is-fun/tree\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Android reddit app open source under GPL\", \"created_utc\": 1252206754.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"6oj2c\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"7oby\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/6oj2c/one_question_all_reddit_devs_need_to_ask/\", \"locked\": false, \"name\": \"t3_6oj2c\", \"created\": 1214232028.0, \"url\": \"http://www.reddit.com/info/6oi6p/comments/c04g2wf?context=1\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"One question all reddit devs need to ask themselves [bestof]\", \"created_utc\": 1214203228.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"6o2no\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"youremyjuliet\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/6o2no/pythonista_observations_on_the_reddit_source/\", \"locked\": false, \"name\": \"t3_6o2no\", \"created\": 1213913948.0, \"url\": \"http://www.reddit.com/info/6nwgk/comments/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Pythonista observations on the reddit source, reposted to redditdev\", \"created_utc\": 1213885148.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am a Redditor who is also a Voater and I\\u0026#39;ve noticed that Voat has lots of checks when you either login or create an account, especially the image CAPTCHA checks, whereas Reddit just allows one to click a few buttons and register.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHow does Reddit ensure that a crawler or a bot doesn\\u0026#39;t programmatically register all the usernames, or cause a DDOS?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am a Redditor who is also a Voater and I've noticed that Voat has lots of checks when you either login or create an account, especially the image CAPTCHA checks, whereas Reddit just allows one to click a few buttons and register.\\n\\nHow does Reddit ensure that a crawler or a bot doesn't programmatically register all the usernames, or cause a DDOS?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4oc66k\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"prahladyeri\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4oc66k/what_does_reddit_use_for_ddos_protection/\", \"locked\": false, \"name\": \"t3_4oc66k\", \"created\": 1466091056.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4oc66k/what_does_reddit_use_for_ddos_protection/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"What does Reddit use for DDOS protection?\", \"created_utc\": 1466062256.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Cstrong\\u003ESOLVED\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/trevorsenior/snoocore/blob/master/src/RedditRequest.js#L360\\\"\\u003ESnoocore splits stickied posts from normal posts.\\u003C/a\\u003E I didn\\u0026#39;t know this.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThank-you all for your time.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cdel\\u003EGood day,\\u003C/del\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cdel\\u003EI run a small data analysis project in my free time collecting data from \\u003Ca href=\\\"/r/all\\\"\\u003E/r/all\\u003C/a\\u003E. I scrape \\u003Ca href=\\\"/r/all\\\"\\u003E/r/all\\u003C/a\\u003E multiple times an hour (well with-in the rate limit) and store the raw JSONs for later processing.\\u003C/del\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cdel\\u003EHowever I\\u2019ve been noticing a pattern recently where some posts from certain subreddits (mainly \\u003Ca href=\\\"/r/The_Donald\\\"\\u003E/r/The_Donald\\u003C/a\\u003E, but it could be other subs too) have been showing up on my personal Reddit \\u003Ca href=\\\"/r/all\\\"\\u003E/r/all\\u003C/a\\u003E, but not in my scrapes. All the other posts show in the scrapes.\\u003C/del\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cdel\\u003EFor reference, I clearly recall seeing \\u003Ca href=\\\"https://www.reddit.com/r/The_Donald/comments/4kk3n1/donald_j_trump_on_twitter_how_can_crooked_hillary/\\\"\\u003Ethis post regarding one of Donald Trump\\u2019s tweets\\u003C/a\\u003E on \\u003Ca href=\\\"/r/all\\\"\\u003E/r/all\\u003C/a\\u003E, but after \\u003Ccode\\u003Egrep\\u003C/code\\u003Eing through my \\u003Ca href=\\\"/r/all\\\"\\u003E/r/all\\u003C/a\\u003E listing JSONs for the post ID, it is no-where to be found.\\u003C/del\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cdel\\u003EHas anyone else experienced this?\\u003C/del\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cdel\\u003ERegards.\\u003C/del\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"**SOLVED**\\n\\n[Snoocore splits stickied posts from normal posts.](https://github.com/trevorsenior/snoocore/blob/master/src/RedditRequest.js#L360) I didn't know this.\\n\\nThank-you all for your time.\\n\\n~~Good day,~~\\n\\n~~I run a small data analysis project in my free time collecting data from /r/all. I scrape /r/all multiple times an hour (well with-in the rate limit) and store the raw JSONs for later processing.~~\\n\\n~~However I\\u2019ve been noticing a pattern recently where some posts from certain subreddits (mainly /r/The_Donald, but it could be other subs too) have been showing up on my personal Reddit /r/all, but not in my scrapes. All the other posts show in the scrapes.~~\\n\\n~~For reference, I clearly recall seeing [this post regarding one of Donald Trump\\u2019s tweets](https://www.reddit.com/r/The_Donald/comments/4kk3n1/donald_j_trump_on_twitter_how_can_crooked_hillary/) on /r/all, but after `grep`ing through my /r/all listing JSONs for the post ID, it is no-where to be found.~~\\n\\n~~Has anyone else experienced this?~~\\n\\n~~Regards.~~\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4kmyju\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"ZwPLYLWMOX97YO\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1464007129.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4kmyju/some_posts_being_omitted_from_rall_listings/\", \"locked\": false, \"name\": \"t3_4kmyju\", \"created\": 1464019993.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4kmyju/some_posts_being_omitted_from_rall_listings/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Some posts being omitted from /r/all listings?\", \"created_utc\": 1463991193.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EReddit can produce a lot of log output, especially when something goes wrong.  Here\\u0026#39;s how to reroute reddit log output to their own logs.  Create \\u003Ccode\\u003E/etc/rsyslog.d/20-reddit.conf\\u003C/code\\u003E and populate with the following content:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E# Send reddit messages to dedicated logfiles\\n:syslogtag, contains, \\u0026quot;sutro.source\\u0026quot; /var/log/reddit/sutro.log\\n\\u0026amp;~\\n:syslogtag, contains, \\u0026quot;reddit\\u0026quot; /var/log/reddit/reddit.log\\n\\u0026amp;~\\n:syslogtag, contains, \\u0026quot;automoderator_q\\u0026quot; /var/log/reddit/queue-runners.log\\n\\u0026amp;~\\n:syslogtag, contains, \\u0026quot;butler_q\\u0026quot; /var/log/reddit/queue-runners.log\\n\\u0026amp;~\\n:syslogtag, contains, \\u0026quot;commentstree_q\\u0026quot; /var/log/reddit/queue-runners.log\\n\\u0026amp;~\\n:syslogtag, contains, \\u0026quot;del_account_q\\u0026quot; /var/log/reddit/queue-runners.log\\n\\u0026amp;~\\n:syslogtag, contains, \\u0026quot;event_collector_q\\u0026quot; /var/log/reddit/queue-runners.log\\n\\u0026amp;~\\n:syslogtag, contains, \\u0026quot;log_q\\u0026quot; /var/log/reddit/queue-runners.log\\n\\u0026amp;~\\n:syslogtag, contains, \\u0026quot;markread_q\\u0026quot; /var/log/reddit/queue-runners.log\\n\\u0026amp;~\\n:syslogtag, contains, \\u0026quot;modmail_email_q\\u0026quot; /var/log/reddit/queue-runners.log\\n\\u0026amp;~\\n:syslogtag, contains, \\u0026quot;newcomments_q\\u0026quot; /var/log/reddit/queue-runners.log\\n\\u0026amp;~\\n:syslogtag, contains, \\u0026quot;scraper_q\\u0026quot; /var/log/reddit/queue-runners.log\\n\\u0026amp;~\\n:syslogtag, contains, \\u0026quot;search_q\\u0026quot; /var/log/reddit/queue-runners.log\\n\\u0026amp;~\\n:syslogtag, contains, \\u0026quot;vote_comment_q\\u0026quot; /var/log/reddit/queue-runners.log\\n\\u0026amp;~\\n:syslogtag, contains, \\u0026quot;vote_fastlane_q\\u0026quot; /var/log/reddit/queue-runners.log\\n\\u0026amp;~\\n:syslogtag, contains, \\u0026quot;vote_link_q\\u0026quot; /var/log/reddit/queue-runners.log\\n\\u0026amp;~\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENB: I\\u0026#39;m not using HAProxy so if that generates its own log messages (I\\u0026#39;m guessing it doesn\\u0026#39;t), they\\u0026#39;ll still end up wherever the currently go.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThen, as root, execute:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Emkdir /var/log/reddit\\nchown syslog.adm /var/log/reddit\\nchmod 775 /var/log/reddit\\nservice rsyslog restart\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EYou probably should consider creating a suitable entry in \\u003Ccode\\u003E/etc/logrotate.d\\u003C/code\\u003E.  I haven\\u0026#39;t done this yet, and will update the post when I have.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELogs should now appear in \\u003Ccode\\u003E/var/log/syslog\\u003C/code\\u003E.  This may be helpful, especially when you\\u0026#39;re getting weird errors from reddit.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Reddit can produce a lot of log output, especially when something goes wrong.  Here's how to reroute reddit log output to their own logs.  Create `/etc/rsyslog.d/20-reddit.conf` and populate with the following content:\\n\\n    # Send reddit messages to dedicated logfiles\\n    :syslogtag, contains, \\\"sutro.source\\\" /var/log/reddit/sutro.log\\n    \\u0026~\\n    :syslogtag, contains, \\\"reddit\\\" /var/log/reddit/reddit.log\\n    \\u0026~\\n    :syslogtag, contains, \\\"automoderator_q\\\" /var/log/reddit/queue-runners.log\\n    \\u0026~\\n    :syslogtag, contains, \\\"butler_q\\\" /var/log/reddit/queue-runners.log\\n    \\u0026~\\n    :syslogtag, contains, \\\"commentstree_q\\\" /var/log/reddit/queue-runners.log\\n    \\u0026~\\n    :syslogtag, contains, \\\"del_account_q\\\" /var/log/reddit/queue-runners.log\\n    \\u0026~\\n    :syslogtag, contains, \\\"event_collector_q\\\" /var/log/reddit/queue-runners.log\\n    \\u0026~\\n    :syslogtag, contains, \\\"log_q\\\" /var/log/reddit/queue-runners.log\\n    \\u0026~\\n    :syslogtag, contains, \\\"markread_q\\\" /var/log/reddit/queue-runners.log\\n    \\u0026~\\n    :syslogtag, contains, \\\"modmail_email_q\\\" /var/log/reddit/queue-runners.log\\n    \\u0026~\\n    :syslogtag, contains, \\\"newcomments_q\\\" /var/log/reddit/queue-runners.log\\n    \\u0026~\\n    :syslogtag, contains, \\\"scraper_q\\\" /var/log/reddit/queue-runners.log\\n    \\u0026~\\n    :syslogtag, contains, \\\"search_q\\\" /var/log/reddit/queue-runners.log\\n    \\u0026~\\n    :syslogtag, contains, \\\"vote_comment_q\\\" /var/log/reddit/queue-runners.log\\n    \\u0026~\\n    :syslogtag, contains, \\\"vote_fastlane_q\\\" /var/log/reddit/queue-runners.log\\n    \\u0026~\\n    :syslogtag, contains, \\\"vote_link_q\\\" /var/log/reddit/queue-runners.log\\n    \\u0026~\\n\\nNB: I'm not using HAProxy so if that generates its own log messages (I'm guessing it doesn't), they'll still end up wherever the currently go.\\n\\nThen, as root, execute:\\n\\n    mkdir /var/log/reddit\\n    chown syslog.adm /var/log/reddit\\n    chmod 775 /var/log/reddit\\n    service rsyslog restart\\n\\nYou probably should consider creating a suitable entry in `/etc/logrotate.d`.  I haven't done this yet, and will update the post when I have.\\n\\nLogs should now appear in `/var/log/syslog`.  This may be helpful, especially when you're getting weird errors from reddit.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4k2lhm\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"StrixTechnica\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4k2lhm/howto_separate_reddit_logging_from_varlogsyslog/\", \"locked\": false, \"name\": \"t3_4k2lhm\", \"created\": 1463695413.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4k2lhm/howto_separate_reddit_logging_from_varlogsyslog/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"HOWTO: Separate reddit logging from /var/log/syslog\", \"created_utc\": 1463666613.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4ipws0\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"iBzOtaku\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4ipws0/why_doesnt_reddits_mail_inbox_use_ajax_for_live/\", \"locked\": false, \"name\": \"t3_4ipws0\", \"created\": 1462924936.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4ipws0/why_doesnt_reddits_mail_inbox_use_ajax_for_live/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why doesn't reddit's mail inbox use AJAX for live notifications?\", \"created_utc\": 1462896136.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi everyone,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAwhile ago I was trying to find a reddit API wrapper for Node.js, but I wasn\\u0026#39;t really satisfied with any of the existing options. So for the past few months I\\u0026#39;ve been working on \\u003Ca href=\\\"https://github.com/not-an-aardvark/snoowrap\\\"\\u003Esnoowrap\\u003C/a\\u003E. snoowrap is a fully-featured reddit API wrapper supporting OAuth authentication, every API endpoint, and a handful of other things under the hood to make interaction with the API simple and consistent. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAfter lots of testing in alpha, I just released snoowrap v1.0.0 earlier today. It can be installed using \\u003Ccode\\u003Enpm install snoowrap\\u003C/code\\u003E. The source code, as well as further info and documentation, can be found \\u003Ca href=\\\"https://github.com/not-an-aardvark/snoowrap\\\"\\u003Ehere\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEnjoy!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi everyone,\\n\\nAwhile ago I was trying to find a reddit API wrapper for Node.js, but I wasn't really satisfied with any of the existing options. So for the past few months I've been working on [snoowrap](https://github.com/not-an-aardvark/snoowrap). snoowrap is a fully-featured reddit API wrapper supporting OAuth authentication, every API endpoint, and a handful of other things under the hood to make interaction with the API simple and consistent. \\n\\nAfter lots of testing in alpha, I just released snoowrap v1.0.0 earlier today. It can be installed using `npm install snoowrap`. The source code, as well as further info and documentation, can be found [here](https://github.com/not-an-aardvark/snoowrap).\\n\\nEnjoy!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4inmqc\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"not_an_aardvark\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4inmqc/i_made_a_reddit_api_wrapper_for_nodejs_and/\", \"locked\": false, \"name\": \"t3_4inmqc\", \"created\": 1462884086.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4inmqc/i_made_a_reddit_api_wrapper_for_nodejs_and/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I made a reddit API wrapper for Node.js and browsers\", \"created_utc\": 1462855286.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"477py7\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/477py7/redditresub_export_import_subreddits_from_a/\", \"locked\": false, \"name\": \"t3_477py7\", \"created\": 1456277795.0, \"url\": \"https://github.com/x89/reddit-resub\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"reddit-resub - export / import subreddits from a Reddit account\", \"created_utc\": 1456248995.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am interested in getting into writing bots.  I have been a full time developer of Java since the 1.1 days.  I have messed around with Python, but I am certainly no expert.  From what I can tell, the reddit bot community is strongly slanted towards Python.  So I have a couple questions:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003EIs the Java Reddit bot community active?\\u003C/li\\u003E\\n\\u003Cli\\u003EAre there API\\u0026#39;s for Java that are as good as PRAW?\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EIf No, then it\\u0026#39;ll be a good opportunity to learn Python.  If that is the case, should I use Python 2.x or 3.x ?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks all.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am interested in getting into writing bots.  I have been a full time developer of Java since the 1.1 days.  I have messed around with Python, but I am certainly no expert.  From what I can tell, the reddit bot community is strongly slanted towards Python.  So I have a couple questions:\\n\\n1. Is the Java Reddit bot community active?\\n2. Are there API's for Java that are as good as PRAW?\\n\\nIf No, then it'll be a good opportunity to learn Python.  If that is the case, should I use Python 2.x or 3.x ?\\n\\nThanks all.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"41otyb\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"elihusmails\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 19, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/41otyb/python_or_java_for_bot_development/\", \"locked\": false, \"name\": \"t3_41otyb\", \"created\": 1453241580.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Python or Java for bot development\", \"created_utc\": 1453212780.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EJust out of curiosity since Google has seemingly failed me.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am thinking running two different Reddit bots (both via PRAW, in case it matters) from the same machine - they are completely different bots and will be using different accounts. Do these bots have different caps on the number of requests, or do they have to share the bandwidth?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Just out of curiosity since Google has seemingly failed me.\\n\\nI am thinking running two different Reddit bots (both via PRAW, in case it matters) from the same machine - they are completely different bots and will be using different accounts. Do these bots have different caps on the number of requests, or do they have to share the bandwidth?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3z3ff3\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"sufficiency\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3z3ff3/two_bots_one_ip/\", \"locked\": false, \"name\": \"t3_3z3ff3\", \"created\": 1451733331.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3z3ff3/two_bots_one_ip/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Two bots, one IP?\", \"created_utc\": 1451704531.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EEDIT: I have figured out the solution: Reddit actually includes the comment object in its reply. I\\u0026#39;ll leave this post up for posterity.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003ESuppose I want a bot to make a comment and then distinguish it with an [M]. As far as I know, there\\u0026#39;s no batch script to do this, so it requires two requests: one to \\u003Ccode\\u003E/api/comment\\u003C/code\\u003E and another to \\u003Ccode\\u003E/api/distinguish\\u003C/code\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHowever, the \\u003Ccode\\u003E/api/distinguish\\u003C/code\\u003E endpoint requires the ID of the comment that just got posted, and there doesn\\u0026#39;t seem to be a good way to figure that out.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI suppose the bot could make a separate request to \\u003Ccode\\u003E/user/the_bots_username/comments\\u003C/code\\u003E, figure out which comment was just created, and get its ID from there. However, this method is sort of undesirable because (a) it requires additional scope, and (b) getting the correct comment can be somewhat complicated if multiple comments are being created simultaneously.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cdel\\u003E(It would be nice if Reddit sent back the ID along with the 200 response after creating a comment, but it doesn\\u0026#39;t currently do that.)\\u003C/del\\u003E (edit: Whoops, it does. There\\u0026#39;s the solution, then.)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Etl;dr: Is there an easy way to get the ID of a comment immediately after it\\u0026#39;s created?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"EDIT: I have figured out the solution: Reddit actually includes the comment object in its reply. I'll leave this post up for posterity.\\n___\\nSuppose I want a bot to make a comment and then distinguish it with an [M]. As far as I know, there's no batch script to do this, so it requires two requests: one to `/api/comment` and another to `/api/distinguish`.\\n\\nHowever, the `/api/distinguish` endpoint requires the ID of the comment that just got posted, and there doesn't seem to be a good way to figure that out.\\n\\nI suppose the bot could make a separate request to `/user/the_bots_username/comments`, figure out which comment was just created, and get its ID from there. However, this method is sort of undesirable because (a) it requires additional scope, and (b) getting the correct comment can be somewhat complicated if multiple comments are being created simultaneously.\\n\\n~~(It would be nice if Reddit sent back the ID along with the 200 response after creating a comment, but it doesn't currently do that.)~~ (edit: Whoops, it does. There's the solution, then.)\\n\\ntl;dr: Is there an easy way to get the ID of a comment immediately after it's created?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3vmq4j\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"not_an_aardvark\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1449388021.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3vmq4j/how_to_make_a_comment_and_immediately_get_its_id/\", \"locked\": false, \"name\": \"t3_3vmq4j\", \"created\": 1449409364.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3vmq4j/how_to_make_a_comment_and_immediately_get_its_id/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to make a comment and immediately get its ID?\", \"created_utc\": 1449380564.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://i.imgur.com/2BK1O45.jpg\\\"\\u003EHere\\u0026#39;s an example of the regex working on a comment\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI want to detect imgur links which do not have a file extension:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003Eimgur.com/abc123\\u003C/code\\u003E should match, but\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003Eimgur.com/abc123.jpg\\u003C/code\\u003E should not match.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s a fairly simple regex as they go, but I\\u0026#39;m having a problem because some links have a 6-character ID while some have a 7-character ID, eg:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003Eimgur.com/abcdef\\u003C/code\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003Eimgur.com/abcdefg\\u003C/code\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eare both valid. My regex initially looked for exactly 6 characters in the Id, which worked fine until I found out that some 7-character IDs exist, so I changed the {6} (not pictured above) to {6,7}, but that seemed to omit the final character, so I changed it to {6,8}, which still didn\\u0026#39;t work. I finally tried {0,8}, but still the last character was omitted, even though I\\u0026#39;m using greedy matching, and maybe you see why: I\\u0026#39;m only looking for matches which are not immediately followed by a file extension such as .jpg.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003Eimgur.com/abcdefg.jpg\\u003C/code\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe full 7-character ID above does not match because it ends in \\u0026quot;.jpg\\u0026quot;. However the 6-character ID is not followed by .jpg because it is followed by the 7th character of the ID, so the match is found:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003Eimgur.com/abcdef\\u003C/code\\u003E | \\u003Ccode\\u003Eg.jpg\\u003C/code\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe left group above is the match because it doesn\\u0026#39;t end with .jpg, whereas if you include the final character of the ID you are forced to also include the .jpg, which makes the group fail to match.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m not sure how well I explained that so feel free to tell me that I\\u0026#39;ve just confused you. Otherwise, any idea how I can match the whole ID if it is followed by an extension? It needs to prioritise matching the full ID, and \\u003Cstrong\\u003Ethen\\u003C/strong\\u003E check whether it is followed by an extension, rather than trying to match anything it can. I think I need it to keep consuming ID characters until it hits a space or a dot and then stop to check the extension?\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003Etl;dr I need a regex to match \\u003Ccode\\u003Eimgur.com/abcdefg\\u003C/code\\u003E but not \\u003Ccode\\u003Eimgur.com/abcdef.png\\u003C/code\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"[Here's an example of the regex working on a comment](http://i.imgur.com/2BK1O45.jpg)\\n\\nI want to detect imgur links which do not have a file extension:\\n\\n`imgur.com/abc123` should match, but\\n\\n`imgur.com/abc123.jpg` should not match.\\n\\nIt's a fairly simple regex as they go, but I'm having a problem because some links have a 6-character ID while some have a 7-character ID, eg:\\n\\n`imgur.com/abcdef`\\n\\n`imgur.com/abcdefg`\\n\\nare both valid. My regex initially looked for exactly 6 characters in the Id, which worked fine until I found out that some 7-character IDs exist, so I changed the {6} (not pictured above) to {6,7}, but that seemed to omit the final character, so I changed it to {6,8}, which still didn't work. I finally tried {0,8}, but still the last character was omitted, even though I'm using greedy matching, and maybe you see why: I'm only looking for matches which are not immediately followed by a file extension such as .jpg.\\n\\n`imgur.com/abcdefg.jpg`\\n\\nThe full 7-character ID above does not match because it ends in \\\".jpg\\\". However the 6-character ID is not followed by .jpg because it is followed by the 7th character of the ID, so the match is found:\\n\\n`imgur.com/abcdef` | `g.jpg`\\n\\nThe left group above is the match because it doesn't end with .jpg, whereas if you include the final character of the ID you are forced to also include the .jpg, which makes the group fail to match.\\n\\nI'm not sure how well I explained that so feel free to tell me that I've just confused you. Otherwise, any idea how I can match the whole ID if it is followed by an extension? It needs to prioritise matching the full ID, and **then** check whether it is followed by an extension, rather than trying to match anything it can. I think I need it to keep consuming ID characters until it hits a space or a dot and then stop to check the extension?\\n\\n---\\n\\ntl;dr I need a regex to match `imgur.com/abcdefg` but not `imgur.com/abcdef.png`\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3vlrnz\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"theonefoster\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1449362905.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3vlrnz/can_anyone_help_with_this_regex_which_tries_to/\", \"locked\": false, \"name\": \"t3_3vlrnz\", \"created\": 1449391076.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3vlrnz/can_anyone_help_with_this_regex_which_tries_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can anyone help with this regex which tries to detect indirect imgur links? It's failing to detect the last character, and I know why but can't prevent it.\", \"created_utc\": 1449362276.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}], \"after\": \"t3_3vlrnz\", \"before\": null}}"
        },
        "headers": {
          "CF-RAY": "2b59b9aef35a20c6-LAX",
          "Connection": "keep-alive",
          "Content-Type": "application/json; charset=UTF-8",
          "Date": "Sun, 19 Jun 2016 20:28:05 GMT",
          "Server": "cloudflare-nginx",
          "Strict-Transport-Security": "max-age=15552000; includeSubDomains; preload",
          "Transfer-Encoding": "chunked",
          "Vary": "accept-encoding",
          "X-Moose": "majestic",
          "cache-control": "private, s-maxage=0, max-age=0, must-revalidate",
          "expires": "-1",
          "x-content-type-options": "nosniff",
          "x-frame-options": "SAMEORIGIN",
          "x-ratelimit-remaining": "583.0",
          "x-ratelimit-reset": "116",
          "x-ratelimit-used": "17",
          "x-reddit-tracking": "https://pixel.redditmedia.com/pixel/of_destiny.png?v=LVU3XlI1JH3QUuQug5gv2p4V0pc0aepdbIhzl7QVNoVaj%2FFszq%2BUTAj1qqxABSB3m2f1%2BIwJbS0L2Ym4yEB4zzlJKZfflNmW",
          "x-ua-compatible": "IE=edge",
          "x-xss-protection": "1; mode=block"
        },
        "status": {
          "code": 200,
          "message": "OK"
        },
        "url": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_270bnk&raw_json=1"
      }
    },
    {
      "recorded_at": "2016-06-19T20:28:06",
      "request": {
        "body": {
          "encoding": "utf-8",
          "string": ""
        },
        "headers": {
          "Accept": "*/*",
          "Accept-Encoding": "identity",
          "Authorization": "bearer YE4ESXZ9lTjIP0ukcopwXM6361g",
          "Connection": "keep-alive",
          "Cookie": "loid=SdAZIdDQoUDzjNqfRk; loidcreated=2016-06-19T20%3A27%3A59.507Z; __cfduid=da3dbedd6bfa050f4f12fea14395b830f1466368079",
          "User-Agent": "prawtools/0.19 PRAW/4.0.0b4 prawcore/0.0.8"
        },
        "method": "GET",
        "uri": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_3vlrnz&raw_json=1"
      },
      "response": {
        "body": {
          "encoding": "UTF-8",
          "string": "{\"kind\": \"Listing\", \"data\": {\"modhash\": null, \"children\": [{\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m loading posts from the frontpage and was wondering whether there is a simple solution to mark posts as seen so that if I were to load the frontpage again from the API they are either not included or are but with some way to filter them out.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm loading posts from the frontpage and was wondering whether there is a simple solution to mark posts as seen so that if I were to load the frontpage again from the API they are either not included or are but with some way to filter them out.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3upa2w\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"VylarLtd\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3upa2w/is_it_possible_to_mark_posts_as_seen/\", \"locked\": false, \"name\": \"t3_3upa2w\", \"created\": 1448823965.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3upa2w/is_it_possible_to_mark_posts_as_seen/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is it possible to mark posts as 'seen'?\", \"created_utc\": 1448795165.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ucsn4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Vermilion\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ucsn4/is_the_installredditsh_script_broken_right_now_i/\", \"locked\": false, \"name\": \"t3_3ucsn4\", \"created\": 1448580022.0, \"url\": \"https://github.com/reddit/reddit/issues/1480\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is the install-reddit.sh script broken right now? I can't get it to work on Ubuntu 14.04.3\", \"created_utc\": 1448551222.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAll you need to do is add the \\u003Ccode\\u003Eviewport\\u003C/code\\u003E meta tag that controls responsive rendering of your content:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E  \\u0026lt;meta name=\\u0026quot;viewport\\u0026quot; content=\\u0026quot;width=device-width, initial-scale=1\\u0026quot;\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EOkay, maybe I am missing something. If it were that obvious, you would have done it by now. After all, you guys are expert web designers and far more skilled than a blabbing motormouth like me. But surely, you can do something to sort this out?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"All you need to do is add the `viewport` meta tag that controls responsive rendering of your content:\\n\\n      \\u003Cmeta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1\\\"\\u003E\\n\\nOkay, maybe I am missing something. If it were that obvious, you would have done it by now. After all, you guys are expert web designers and far more skilled than a blabbing motormouth like me. But surely, you can do something to sort this out?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3txfq3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"rms_returns\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3txfq3/why_dont_you_make_redditcom_pages_responsive_so/\", \"locked\": false, \"name\": \"t3_3txfq3\", \"created\": 1448304025.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3txfq3/why_dont_you_make_redditcom_pages_responsive_so/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why don't you make Reddit.com pages responsive so they can be viewed on smaller devices without having to zoom/scroll?\", \"created_utc\": 1448275225.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey friends.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m currently looking for a new maintainer for a reddit API wrapper written in JavaScript:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/trevorsenior/snoocore\\\"\\u003Ehttps://github.com/trevorsenior/snoocore\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt has full tests, documentation, etc.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI don\\u0026#39;t have the time needed to maintain this project properly. If anyone is interested in taking it over you can reach me on reddit, email, etc. I\\u0026#39;ve never transferred a project to a new maintainer, but  I will work with you on setting up development (running test, etc.) and making the transition as smooth as possible should anyone step up.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey friends.\\n\\nI'm currently looking for a new maintainer for a reddit API wrapper written in JavaScript:\\n\\nhttps://github.com/trevorsenior/snoocore\\n\\nIt has full tests, documentation, etc.\\n\\nI don't have the time needed to maintain this project properly. If anyone is interested in taking it over you can reach me on reddit, email, etc. I've never transferred a project to a new maintainer, but  I will work with you on setting up development (running test, etc.) and making the transition as smooth as possible should anyone step up.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3s8b4l\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"tsenior\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1447162604.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3s8b4l/looking_for_a_new_maintainer_for_snoocore_a/\", \"locked\": false, \"name\": \"t3_3s8b4l\", \"created\": 1447157442.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3s8b4l/looking_for_a_new_maintainer_for_snoocore_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Looking for a new maintainer for Snoocore - A JavaScript Reddit API wrapper.\", \"created_utc\": 1447128642.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cpre\\u003E\\u003Ccode\\u003Edef replyToMessages():\\nmessages = r.get_unread()\\n\\nfor message in messages:\\n    user = message.author.name\\n    print \\u0026quot;Message dectected from \\u0026quot; + user\\n    if \\u0026quot;!recommend\\u0026quot; in message.body.lower():\\n        messageToSend = createReply()\\n        message.reply(messageToSend)\\n    message.mark_as_read()\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThat\\u0026#39;s my code, which is called every 10 seconds to check for new messages. I tried using the unset_has_mail=True parameter for get get_unread function, but yielded the same results. I also tried using fetch=true on get_unread, but again my bot replied three times.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat\\u0026#39;s weird is that it replies three times - once per 10-second call for three loops - then stops, so it\\u0026#39;s not as if it\\u0026#39;s failing to be marked as read. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny ideas?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"    def replyToMessages():\\n    messages = r.get_unread()\\n\\n    for message in messages:\\n        user = message.author.name\\n        print \\\"Message dectected from \\\" + user\\n        if \\\"!recommend\\\" in message.body.lower():\\n            messageToSend = createReply()\\n            message.reply(messageToSend)\\n        message.mark_as_read()\\n\\nThat's my code, which is called every 10 seconds to check for new messages. I tried using the unset_has_mail=True parameter for get get_unread function, but yielded the same results. I also tried using fetch=true on get_unread, but again my bot replied three times.\\n\\nWhat's weird is that it replies three times - once per 10-second call for three loops - then stops, so it's not as if it's failing to be marked as read. \\n\\nAny ideas?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3s2h3m\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"theonefoster\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3s2h3m/my_bot_is_replying_to_messages_three_times_any/\", \"locked\": false, \"name\": \"t3_3s2h3m\", \"created\": 1447056352.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3s2h3m/my_bot_is_replying_to_messages_three_times_any/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"My bot is replying to messages three times. Any idea why?\", \"created_utc\": 1447027552.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo if I use this endpoint:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E/subreddits/mine/subscriber\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThe only dates I get is the date the subreddit was created, and \\u0026quot;user_is_subscriber\\u0026quot; is just boolean.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there a way to get the date the user subscribed to a particular subreddit, or at least how many days they\\u0026#39;ve been subscribed to it for?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: I\\u0026#39;ll give a bit more context.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe user has authorized my app, so I can get this information. My app is a tool for subreddits to use (to do with polling), but I want to add a way to prevent abuse from people external to the subreddit in question. i.e. \\u0026quot;User must be a subscriber to /r/... before this poll was created\\u0026quot; sort of thing.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So if I use this endpoint:\\n\\n    /subreddits/mine/subscriber\\n\\nThe only dates I get is the date the subreddit was created, and \\\"user_is_subscriber\\\" is just boolean.\\n\\nIs there a way to get the date the user subscribed to a particular subreddit, or at least how many days they've been subscribed to it for?\\n\\nEdit: I'll give a bit more context.\\n\\nThe user has authorized my app, so I can get this information. My app is a tool for subreddits to use (to do with polling), but I want to add a way to prevent abuse from people external to the subreddit in question. i.e. \\\"User must be a subscriber to /r/... before this poll was created\\\" sort of thing.\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3p2vh4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"jb2386\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1445132642.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3p2vh4/is_there_a_way_to_get_the_date_a_user_subscribed/\", \"locked\": false, \"name\": \"t3_3p2vh4\", \"created\": 1445086159.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3p2vh4/is_there_a_way_to_get_the_date_a_user_subscribed/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a way to get the date a user subscribed to a subreddit?\", \"created_utc\": 1445057359.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EDon\\u0026#39;t know if I should ask here...\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Don't know if I should ask here...\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ntk51\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"jokoon\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 14, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ntk51/there_is_a_1000_limit_of_number_of_posts_i_can/\", \"locked\": false, \"name\": \"t3_3ntk51\", \"created\": 1444238793.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ntk51/there_is_a_1000_limit_of_number_of_posts_i_can/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"There is a 1000 limit of number of posts I can get when using PRAW to get all my comments or upvotes. Is it possible to ask somewhere to get a whole data dump of my reddit data ?\", \"created_utc\": 1444209993.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThere are several inconsistencies, but today I noticed quite a massive one with comments. If you\\u0026#39;d look \\u003Ca href=\\\"http://i.imgur.com/f3DBuHR.png\\\"\\u003Ehere\\u003C/a\\u003E, from left to right, it\\u0026#39;s your mentions, then your inbox, then using /api/info, then from the user listing, then form the \\u003Ca href=\\\"/r/subreddit/comments\\\"\\u003E/r/subreddit/comments\\u003C/a\\u003E listing, and there\\u0026#39;s even more discrepancy with the Submission listing, but, I couldn\\u0026#39;t get it to load in Diffuse, the program you are seeing now (Don\\u0026#39;t ask me why it\\u0026#39;s a giraffe. Go fucking figure).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis massive inconsistency makes it a pain for devs to do things. If possible, it should be a lot more along the same path and idea, with all possible info there.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso, there\\u0026#39;s an inconsistency with \\u003Ccode\\u003Ereplies\\u003C/code\\u003E. Both if there are replies but it\\u0026#39;s on a listing that doesn\\u0026#39;t show them, and that there actually are no replies, it is \\u003Ccode\\u003E\\u0026quot;\\u0026quot;\\u003C/code\\u003E, which is very vague. The way it \\u003Cem\\u003Eshould be\\u003C/em\\u003E is that if it has replies, list them. If it doesn\\u0026#39;t, make it \\u003Ccode\\u003Enull\\u003C/code\\u003E, not \\u003Ccode\\u003E\\u0026quot;\\u0026quot;\\u003C/code\\u003E. If it does have replies but they are empty, such as they are in a non submission listing, it should be \\u003Ccode\\u003E\\u0026quot;\\u0026quot;\\u003C/code\\u003E. This gives the largest amount of info gathering and error handling, with the minimum amount of requests.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;d try to fix these myself on \\u003Ca href=\\\"http://www.GitHub.com/reddit/reddit\\\"\\u003Ehttp://www.GitHub.com/reddit/reddit\\u003C/a\\u003E, but I can\\u0026#39;t make sense over some of the naming schemes.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"There are several inconsistencies, but today I noticed quite a massive one with comments. If you'd look [here](http://i.imgur.com/f3DBuHR.png), from left to right, it's your mentions, then your inbox, then using /api/info, then from the user listing, then form the /r/subreddit/comments listing, and there's even more discrepancy with the Submission listing, but, I couldn't get it to load in Diffuse, the program you are seeing now (Don't ask me why it's a giraffe. Go fucking figure).\\n\\nThis massive inconsistency makes it a pain for devs to do things. If possible, it should be a lot more along the same path and idea, with all possible info there.\\n\\nAlso, there's an inconsistency with `replies`. Both if there are replies but it's on a listing that doesn't show them, and that there actually are no replies, it is `\\\"\\\"`, which is very vague. The way it *should be* is that if it has replies, list them. If it doesn't, make it `null`, not `\\\"\\\"`. If it does have replies but they are empty, such as they are in a non submission listing, it should be `\\\"\\\"`. This gives the largest amount of info gathering and error handling, with the minimum amount of requests.\\n\\nI'd try to fix these myself on http://www.GitHub.com/reddit/reddit, but I can't make sense over some of the naming schemes.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3iaaf2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"13steinj\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3iaaf2/massive_inconsistency_in_the_api_especially_with/\", \"locked\": false, \"name\": \"t3_3iaaf2\", \"created\": 1440499945.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3iaaf2/massive_inconsistency_in_the_api_especially_with/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Massive Inconsistency in the API, especially with comments.\", \"created_utc\": 1440471145.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIf anyone can offer a way to improve this, I\\u0026#39;d appreciate it. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EWARNING\\u003C/strong\\u003E:  Some people may post \\u003Cstrong\\u003ENSFW content\\u003C/strong\\u003E.  I wouldn\\u0026#39;t open this at work.  But it\\u0026#39;s fun to watch.  We could add links to the pics to go directly to the comments / threads themselves.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EJSFiddle here\\u003C/strong\\u003E:  \\u003Ca href=\\\"http://jsfiddle.net/wyktd9r6/embedded/result/\\\"\\u003Ehttp://jsfiddle.net/wyktd9r6/embedded/result/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cem\\u003ETo view the code example: \\u003Ca href=\\\"http://jsfiddle.net/wyktd9r6/\\\"\\u003Ehttp://jsfiddle.net/wyktd9r6/\\u003C/a\\u003E\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E(You may have to give it a few seconds before you start seeing content)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks to \\u003Ca href=\\\"/u/SkarsgardAbraxis\\\"\\u003E/u/SkarsgardAbraxis\\u003C/a\\u003E for the suggestions -- you can click the picture to get the full picture.  Or click the subreddit name to go straight to the thread / comment.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EFuture Enhancements?\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EHook this up to t3 events (submissions) as they come in.  We could show videos and pictures from submission objects as they come in.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003ERestrict subreddits and comments within certain subreddits -- don\\u0026#39;t allow connect from certain subreddits (wtf, etc.)\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"If anyone can offer a way to improve this, I'd appreciate it. \\n\\n**WARNING**:  Some people may post **NSFW content**.  I wouldn't open this at work.  But it's fun to watch.  We could add links to the pics to go directly to the comments / threads themselves.\\n\\n**JSFiddle here**:  http://jsfiddle.net/wyktd9r6/embedded/result/\\n\\n*To view the code example: http://jsfiddle.net/wyktd9r6/*\\n\\n(You may have to give it a few seconds before you start seeing content)\\n\\nEdit:\\n\\nThanks to /u/SkarsgardAbraxis for the suggestions -- you can click the picture to get the full picture.  Or click the subreddit name to go straight to the thread / comment.\\n\\n**Future Enhancements?**\\n\\n* Hook this up to t3 events (submissions) as they come in.  We could show videos and pictures from submission objects as they come in.\\n\\n* Restrict subreddits and comments within certain subreddits -- don't allow connect from certain subreddits (wtf, etc.)\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3gnkbi\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1439364435.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3gnkbi/viewing_links_to_pictures_posted_to_reddit_in/\", \"locked\": false, \"name\": \"t3_3gnkbi\", \"created\": 1439363180.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3gnkbi/viewing_links_to_pictures_posted_to_reddit_in/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Viewing links to pictures posted to reddit in near real-time\", \"created_utc\": 1439334380.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m currently implementing OAuth authentication into my android app, and Ive got it working, but the flow is...pretty bad. It\\u0026#39;s possible to wander outside of the OAuth page into reddit proper. Am I missing something or is it really that bad? Is there something I can do about it?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm currently implementing OAuth authentication into my android app, and Ive got it working, but the flow is...pretty bad. It's possible to wander outside of the OAuth page into reddit proper. Am I missing something or is it really that bad? Is there something I can do about it?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3bpd1t\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"okmkz\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3bpd1t/is_there_anything_i_can_do_to_make_the_oauth_flow/\", \"locked\": false, \"name\": \"t3_3bpd1t\", \"created\": 1435744888.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3bpd1t/is_there_anything_i_can_do_to_make_the_oauth_flow/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there anything I can do to make the OAuth flow less terrible for my mobile users?\", \"created_utc\": 1435716088.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EReferring to this: \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example\\\"\\u003Ehttps://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Referring to this: https://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"36zbd0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"seriouslulz\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/36zbd0/can_we_get_nonexpiring_tokens_for_script_apps/\", \"locked\": false, \"name\": \"t3_36zbd0\", \"created\": 1432411120.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/36zbd0/can_we_get_nonexpiring_tokens_for_script_apps/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can we get non-expiring tokens for script apps?\", \"created_utc\": 1432382320.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAn example of how it works (at time of post):\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"/u/redditor\\\"\\u003E/u/redditor\\u003C/a\\u003E comments:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E\\u0026quot;I hate the way I never have enough leg room on the airplane!\\u0026quot;\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"/u/_nsa_bot_\\\"\\u003E/u/_nsa_bot_\\u003C/a\\u003E replies:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E\\u0026quot;\\u003Cem\\u003EI\\u0026#39;m in beta still! Message me if you have feedback.\\u003C/em\\u003E As per \\u003Ca href=\\\"http://www.scribd.com/doc/82701103/Analyst-Desktop-Binder-REDACTED\\\"\\u003Ethe NSA\\u0026#39;s Social Media Reference Guide for DHS Analyst\\u003C/a\\u003E: It is likely the NSA noted your use of the word or phrase \\u0026#39;airplane\\u0026#39;.\\u0026quot;\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EThis is the basic idea behind nsa_bot: to mirror when people are using NSA\\u0026#39;s Social Media monitoring keywords, and make it known that they are using these words. It can be funny at times, but it can also be eye opening.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENow, the NSA isn\\u0026#39;t necessarily keeping track (...or are they?) of people using the word \\u0026#39;team\\u0026#39; or \\u0026#39;power\\u0026#39; or \\u0026#39;airplane\\u0026#39;, but they are listed key words in \\u003Ca href=\\\"http://www.scribd.com/doc/82701103/Analyst-Desktop-Binder-REDACTED\\\"\\u003Ethe NSA\\u0026#39;s Social Media Reference Guide for DHS Analyst\\u003C/a\\u003E. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWith so many day to day words on the list, the bot can easily comment on several (5-15) posts per minute. I would like to reduce that amount to without removing words from the list.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EThings I am considering\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003Ereorganizing the list - this would allow for the less day to day words to be encountered first, at least showing more exciting words more often. (if a user uses \\u0026#39;team\\u0026#39; and \\u0026#39;explosive\\u0026#39; in the same comment, \\u0026#39;explosive\\u0026#39; is first on the list and is therefore the noted word)\\u003C/li\\u003E\\n\\u003Cli\\u003Ecount all instances of words on the list and give the comment a score. only reply to comments with a score above a certain threshold. (if a user comments with \\u0026#39;team\\u0026#39;, they have a low score. if they comment with \\u0026#39;power\\u0026#39;, \\u0026#39;drug\\u0026#39;, \\u0026#39;cartel\\u0026#39;, \\u0026#39;explosive\\u0026#39;, \\u0026#39;airplane\\u0026#39;, they have a high score and merit a reply.)\\u003C/li\\u003E\\n\\u003Cli\\u003Ewords may have different values. (I\\u0026#39;d probably use a dictionary/hash instead of a list in that case) - similar to above option\\u003C/li\\u003E\\n\\u003Cli\\u003E... others?\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EI want this to be informative, yet non-spammy and appropriate. All suggestions will be appreciated.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESee source code \\u003Ca href=\\\"https://github.com/underscorejho/nsa_bot\\\"\\u003Ehere\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"An example of how it works (at time of post):\\n\\n/u/redditor comments:\\n\\n\\u003E\\\"I hate the way I never have enough leg room on the airplane!\\\"\\n\\n/u/_nsa_bot_ replies:\\n\\n\\u003E\\\"*I'm in beta still! Message me if you have feedback.* As per [the NSA's Social Media Reference Guide for DHS Analyst](http://www.scribd.com/doc/82701103/Analyst-Desktop-Binder-REDACTED): It is likely the NSA noted your use of the word or phrase 'airplane'.\\\"\\n\\nThis is the basic idea behind nsa_bot: to mirror when people are using NSA's Social Media monitoring keywords, and make it known that they are using these words. It can be funny at times, but it can also be eye opening.\\n\\nNow, the NSA isn't necessarily keeping track (...or are they?) of people using the word 'team' or 'power' or 'airplane', but they are listed key words in [the NSA's Social Media Reference Guide for DHS Analyst](http://www.scribd.com/doc/82701103/Analyst-Desktop-Binder-REDACTED). \\n\\nWith so many day to day words on the list, the bot can easily comment on several (5-15) posts per minute. I would like to reduce that amount to without removing words from the list.\\n\\n**Things I am considering**\\n\\n* reorganizing the list - this would allow for the less day to day words to be encountered first, at least showing more exciting words more often. (if a user uses 'team' and 'explosive' in the same comment, 'explosive' is first on the list and is therefore the noted word)\\n* count all instances of words on the list and give the comment a score. only reply to comments with a score above a certain threshold. (if a user comments with 'team', they have a low score. if they comment with 'power', 'drug', 'cartel', 'explosive', 'airplane', they have a high score and merit a reply.)\\n* words may have different values. (I'd probably use a dictionary/hash instead of a list in that case) - similar to above option\\n* ... others?\\n\\nI want this to be informative, yet non-spammy and appropriate. All suggestions will be appreciated.\\n\\nSee source code [here](https://github.com/underscorejho/nsa_bot).\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"36pdk2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"_nsa_bot_\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/36pdk2/looking_for_a_better_search_algorithm_for_nsa_bot/\", \"locked\": false, \"name\": \"t3_36pdk2\", \"created\": 1432201504.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/36pdk2/looking_for_a_better_search_algorithm_for_nsa_bot/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Looking for a better search algorithm for nsa_bot\", \"created_utc\": 1432172704.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EBrowser clients get left in the cold again.  Is there any way to get a JSON response without a redirect here?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://reddit.com/r/all/random.json\\\"\\u003Ehttp://reddit.com/r/all/random.json\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Browser clients get left in the cold again.  Is there any way to get a JSON response without a redirect here?\\n\\nhttp://reddit.com/r/all/random.json\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"36cdtx\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"go1dfish\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/36cdtx/rsubredditrandomjson_is_a_redirect_this_breaks/\", \"locked\": false, \"name\": \"t3_36cdtx\", \"created\": 1431960345.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/36cdtx/rsubredditrandomjson_is_a_redirect_this_breaks/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"/r/subreddit/random.json is a redirect, this breaks CORS\", \"created_utc\": 1431931545.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EVersion 2.1.21 of praw now gives me a warning if my user_agent string contains the word \\u0026#39;bot\\u0026#39; Is there something going on with reddit that I should know about? Are they cracking down on bots?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Version 2.1.21 of praw now gives me a warning if my user_agent string contains the word 'bot' Is there something going on with reddit that I should know about? Are they cracking down on bots?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"30vp0e\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"IAMA_YOU_AMA\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/30vp0e/warning_in_praw_about_using_the_keyword_bot/\", \"locked\": false, \"name\": \"t3_30vp0e\", \"created\": 1427796447.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/30vp0e/warning_in_praw_about_using_the_keyword_bot/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Warning in praw about using the keyword 'bot'\", \"created_utc\": 1427767647.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EGreetings \\u003Ca href=\\\"/r/redditdev\\\"\\u003E/r/redditdev\\u003C/a\\u003E,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve recently made a few small changes to the multireddit APIs:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003ESeveral new fields added to multireddit responses.\\u003C/li\\u003E\\n\\u003Cli\\u003EThe multireddit\\u0026#39;s \\u003Ccode\\u003Edescription\\u003C/code\\u003E fields have been pulled into the \\u0026quot;main\\u0026quot; model. (Existing description endpoints continue to work).\\u003C/li\\u003E\\n\\u003Cli\\u003EWhen updating a multireddit, not-including a field is no longer an error; it will simply not get updated. This change was to avoid issues with adding the new fields, since we have no existing PATCH endpoint.\\u003C/li\\u003E\\n\\u003Cli\\u003EYou can now list any user\\u0026#39;s public multis, at \\u003Ccode\\u003E/api/multi/user/\\u0026lt;name\\u0026gt;\\u003C/code\\u003E.\\u003C/li\\u003E\\n\\u003Cli\\u003EYou can set a multireddit\\u0026#39;s visibility to \\u0026quot;hidden\\u0026quot;. A \\u0026quot;hidden\\u0026quot; multi is the same as a \\u0026quot;private\\u0026quot; one, except it will no longer show up in your sidebar on the website.\\u003C/li\\u003E\\n\\u003Cli\\u003EYou can now create/copy/\\u0026quot;rename\\u0026quot; a multi and have reddit generate a slug/ID for you. Set the \\u003Ccode\\u003Edisplay_name\\u003C/code\\u003E (on create) or use the \\u003Ccode\\u003Edisplay_name\\u003C/code\\u003E parameter instead of the \\u003Ccode\\u003Eto\\u003C/code\\u003E param. \\u003Ccode\\u003Edisplay_name\\u003C/code\\u003E, as a multi attribute, can be any unicode text; the slug will be an ascii-derivation of the \\u003Ccode\\u003Edisplay_name\\u003C/code\\u003E.\\u003C/li\\u003E\\n\\u003Cli\\u003EWhen retrieving multis, set \\u003Ccode\\u003E?expand_srs=on\\u003C/code\\u003E to get some additional metadata about the subreddits in the multireddit.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Greetings /r/redditdev,\\n\\nI've recently made a few small changes to the multireddit APIs:\\n\\n* Several new fields added to multireddit responses.\\n* The multireddit's `description` fields have been pulled into the \\\"main\\\" model. (Existing description endpoints continue to work).\\n* When updating a multireddit, not-including a field is no longer an error; it will simply not get updated. This change was to avoid issues with adding the new fields, since we have no existing PATCH endpoint.\\n* You can now list any user's public multis, at `/api/multi/user/\\u003Cname\\u003E`.\\n* You can set a multireddit's visibility to \\\"hidden\\\". A \\\"hidden\\\" multi is the same as a \\\"private\\\" one, except it will no longer show up in your sidebar on the website.\\n* You can now create/copy/\\\"rename\\\" a multi and have reddit generate a slug/ID for you. Set the `display_name` (on create) or use the `display_name` parameter instead of the `to` param. `display_name`, as a multi attribute, can be any unicode text; the slug will be an ascii-derivation of the `display_name`.\\n* When retrieving multis, set `?expand_srs=on` to get some additional metadata about the subreddits in the multireddit.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2x9nzv\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2x9nzv/multireddit_api_changes/\", \"locked\": false, \"name\": \"t3_2x9nzv\", \"created\": 1425009593.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2x9nzv/multireddit_api_changes/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Multireddit API changes\", \"created_utc\": 1424980793.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;d really love to be able to put a birthday cake icon by a user\\u0026#39;s name when it\\u0026#39;s their cakeday, like how Reddit.com does. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis doesn\\u0026#39;t appear possible without making queries for every single user in a thread (hundreds or thousands of requests), right? Could it be?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'd really love to be able to put a birthday cake icon by a user's name when it's their cakeday, like how Reddit.com does. \\n\\nThis doesn't appear possible without making queries for every single user in a thread (hundreds or thousands of requests), right? Could it be?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2x1khz\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"iamthatis\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2x1khz/would_it_be_possible_to_add_a_cakedayaccount/\", \"locked\": false, \"name\": \"t3_2x1khz\", \"created\": 1424846506.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2x1khz/would_it_be_possible_to_add_a_cakedayaccount/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Would it be possible to add a cakeday/account creation date in comments?\", \"created_utc\": 1424817706.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI put together \\u003Ca href=\\\"http://redd.it/2slz58\\\"\\u003Esome code\\u003C/a\\u003E (it\\u0026#39;s not pretty) to scrape the Snoovatar PNG data, but it consumes a lot of resources on the client because it has to load the whole page and render the javascript, especially if there\\u0026#39;s a list of users and it\\u0026#39;s happening asynchronously (rate limited and resource limited to within reason, of course).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI was hoping there could be an API call that returns the base64 PNG string.  \\u003Cdel\\u003E/api/v1/me/snoo\\u003C/del\\u003E or something.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf there\\u0026#39;s a better way to accomplish it, I\\u0026#39;d certainly take the advice :)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E// Edit: I\\u0026#39;m not sure why I put /api/v1/me as the function should operate globally on all users.  Think I meant to say /user/\\u003Cem\\u003Eusername\\u003C/em\\u003E/snoo.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBut now that I\\u0026#39;m implementing rate limiting and such, I can see this would put an additional load on the reddit servers and potentially exhaust my API queue limit.  If you were to do server-side generated Snoovatars, it seems that it would work better by creating a PNG for your imaging caching outlet because:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EZero load after you create the initial image (it\\u0026#39;s offloaded to the CDN).\\u003C/li\\u003E\\n\\u003Cli\\u003EDoes not exhaust the queue pool for API clients.\\u003C/li\\u003E\\n\\u003Cli\\u003EEasier imaging caching in the client.  Better to check HTTP HEAD than to download the entire Snoovatar base64 payload and compare strings.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I put together [some code](http://redd.it/2slz58) (it's not pretty) to scrape the Snoovatar PNG data, but it consumes a lot of resources on the client because it has to load the whole page and render the javascript, especially if there's a list of users and it's happening asynchronously (rate limited and resource limited to within reason, of course).\\n\\n\\nI was hoping there could be an API call that returns the base64 PNG string.  ~~/api/v1/me/snoo~~ or something.\\n\\n\\nIf there's a better way to accomplish it, I'd certainly take the advice :)\\n\\n\\n\\nThanks!\\n\\n\\n// Edit: I'm not sure why I put /api/v1/me as the function should operate globally on all users.  Think I meant to say /user/*username*/snoo.\\n\\n\\nBut now that I'm implementing rate limiting and such, I can see this would put an additional load on the reddit servers and potentially exhaust my API queue limit.  If you were to do server-side generated Snoovatars, it seems that it would work better by creating a PNG for your imaging caching outlet because:\\n\\n\\n* Zero load after you create the initial image (it's offloaded to the CDN).\\n* Does not exhaust the queue pool for API clients.\\n* Easier imaging caching in the client.  Better to check HTTP HEAD than to download the entire Snoovatar base64 payload and compare strings.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2t2rru\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"vswr\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1421862447.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2t2rru/suggestion_api_access_for_snoovatars/\", \"locked\": false, \"name\": \"t3_2t2rru\", \"created\": 1421808250.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2t2rru/suggestion_api_access_for_snoovatars/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Suggestion] API access for Snoovatars\", \"created_utc\": 1421779450.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESimilar to this, \\u003Ca href=\\\"http://www.reddit.com/r/reddithax/comments/2gurqs/update_the_report_box_report_reasons/\\\"\\u003Ehttp://www.reddit.com/r/reddithax/comments/2gurqs/update_the_report_box_report_reasons/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI noticed that the structure of the report box has changed, and possibly to make that text in the label tag hard to target. I could sit there and hack around it, but I wanted to check with the admins - are certain reports tied to specific events in the back end in such a way that it would be bad if people were reporting based on different rules? Are there other reasons that the admins don\\u0026#39;t want us fiddling with this box? Or were the changes simply a matter of updating the coding style?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E-srsly\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Similar to this, http://www.reddit.com/r/reddithax/comments/2gurqs/update_the_report_box_report_reasons/\\n\\nI noticed that the structure of the report box has changed, and possibly to make that text in the label tag hard to target. I could sit there and hack around it, but I wanted to check with the admins - are certain reports tied to specific events in the back end in such a way that it would be bad if people were reporting based on different rules? Are there other reasons that the admins don't want us fiddling with this box? Or were the changes simply a matter of updating the coding style?\\n\\nThanks!\\n\\n-srsly\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2s7bko\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2s7bko/is_using_css_to_modify_the_report_button_text/\", \"locked\": false, \"name\": \"t3_2s7bko\", \"created\": 1421124344.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2s7bko/is_using_css_to_modify_the_report_button_text/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is using CSS to modify the report button text frowned upon?\", \"created_utc\": 1421095544.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m trying to implement a \\u0026quot;Login with reddit\\u0026quot; system. Currently I\\u0026#39;m successfully authenticating the user and getting their identity and my app is being authorized with \\u0026quot;permanent\\u0026quot; duration.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe issue occurs when the user is logged out of my site and wants to log in, and they\\u0026#39;re logged in on reddit. When sending them through the oauth flow, reddit prompts them to authorize the application again.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETwitter, facebook and various other services automatically redirect the user back to the application if they\\u0026#39;ve already approved it, providing a one-click login experience. The additional prompt from reddit may cause the user to think they signed in through a different social network and is not a great experience.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there something I\\u0026#39;m missing here to get reddit to automatically redirect the client? Or is this a known issue / bug?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm trying to implement a \\\"Login with reddit\\\" system. Currently I'm successfully authenticating the user and getting their identity and my app is being authorized with \\\"permanent\\\" duration.\\n\\nThe issue occurs when the user is logged out of my site and wants to log in, and they're logged in on reddit. When sending them through the oauth flow, reddit prompts them to authorize the application again.\\n\\nTwitter, facebook and various other services automatically redirect the user back to the application if they've already approved it, providing a one-click login experience. The additional prompt from reddit may cause the user to think they signed in through a different social network and is not a great experience.\\n\\nIs there something I'm missing here to get reddit to automatically redirect the client? Or is this a known issue / bug?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2gnjjg\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"notR1CH\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2gnjjg/problem_with_oauth2_flow_when_the_user_has/\", \"locked\": false, \"name\": \"t3_2gnjjg\", \"created\": 1410985524.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2gnjjg/problem_with_oauth2_flow_when_the_user_has/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Problem with oauth2 flow when the user has already approved the app\", \"created_utc\": 1410956724.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI understand why in some situations this type of thing could be beneficial, limiting the amount of posts you have to keep cached in memory, but it seems to me like it couldn\\u0026#39;t be that huge of a performance impact, especially if they only applied vote changes to archived content once a day.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I understand why in some situations this type of thing could be beneficial, limiting the amount of posts you have to keep cached in memory, but it seems to me like it couldn't be that huge of a performance impact, especially if they only applied vote changes to archived content once a day.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2f4sg1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"echocage\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 14, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2f4sg1/why_does_reddit_disallow_voting_on_archived/\", \"locked\": false, \"name\": \"t3_2f4sg1\", \"created\": 1409565328.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2f4sg1/why_does_reddit_disallow_voting_on_archived/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why does reddit disallow voting on archived content? (legitimately curious)\", \"created_utc\": 1409536528.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EMaybe you guys will like this. Let me know if it sucks or if it can be improved. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m not a fan of the Facebook trends panel, it seems like is PR and advertising disguised as social media trends. So I made a Chrome extension that replaces it with stories from Reddit.\\nYou can install the extension from here: \\u003Ca href=\\\"https://chrome.google.com/webstore/detail/reddit-facebook-trends/gbjbbjnmjelanjckpnebpbbgkilpgilp\\\"\\u003Ehttps://chrome.google.com/webstore/detail/reddit-facebook-trends/gbjbbjnmjelanjckpnebpbbgkilpgilp\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s open source (Contributors welcome): \\u003Ca href=\\\"https://github.com/dslounge/fbRedditTrends\\\"\\u003Ehttps://github.com/dslounge/fbRedditTrends\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Maybe you guys will like this. Let me know if it sucks or if it can be improved. \\n\\nI'm not a fan of the Facebook trends panel, it seems like is PR and advertising disguised as social media trends. So I made a Chrome extension that replaces it with stories from Reddit.\\nYou can install the extension from here: https://chrome.google.com/webstore/detail/reddit-facebook-trends/gbjbbjnmjelanjckpnebpbbgkilpgilp\\n\\nIt's open source (Contributors welcome): https://github.com/dslounge/fbRedditTrends\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2dlnym\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"grndctrl\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2dlnym/replacing_fb_trends_with_reddit_stories/\", \"locked\": false, \"name\": \"t3_2dlnym\", \"created\": 1408100087.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2dlnym/replacing_fb_trends_with_reddit_stories/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Replacing FB trends with reddit stories.\", \"created_utc\": 1408071287.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIs there an api call to return a list of today\\u0026#39;s trending subreddits? The only way I currently see to do this is by calling \\u003Ca href=\\\"http://www.reddit.com/r/trendingsubreddits.json\\\"\\u003Ehttp://www.reddit.com/r/trendingsubreddits.json\\u003C/a\\u003E and then extracting the subreddits out of the title parameter of the first link.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis seems a little hacky, is there a nicer way of obtaining them?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Is there an api call to return a list of today's trending subreddits? The only way I currently see to do this is by calling [http://www.reddit.com/r/trendingsubreddits.json](http://www.reddit.com/r/trendingsubreddits.json) and then extracting the subreddits out of the title parameter of the first link.\\n\\nThis seems a little hacky, is there a nicer way of obtaining them?\\n\\nThanks\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"26damo\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Miloco\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/26damo/todays_trending_subreddits/\", \"locked\": false, \"name\": \"t3_26damo\", \"created\": 1400958329.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/26damo/todays_trending_subreddits/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Today's trending subreddits\", \"created_utc\": 1400929529.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Cstrong\\u003ETL;DR\\u003C/strong\\u003E The mobile reddit site (the currnet \\u0026quot;modern\\u0026quot; blue one) is missing basic reddit features and has bugs. It has been like this for years. Are there plans to fix them?\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cem\\u003EI\\u0026#39;ve PM\\u0026#39;d the official \\u003Ca href=\\\"/u/reddit\\\"\\u003E/u/reddit\\u003C/a\\u003E account about this a few times and have never gotten a reply. \\u003Ca href=\\\"/r/compact\\\"\\u003E/r/compact\\u003C/a\\u003E seems abandoned, so I figured I\\u0026#39;d try here since it seems the admins actually read it:\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECurrently, the mobile site (\\u003Ca href=\\\"http://m.reddit.com/\\\"\\u003Ehttp://m.reddit.com/\\u003C/a\\u003E) has two main issues: it\\u0026#39;s missing some basic reddit features, and  it has some bugs. There\\u0026#39;s a partial and possibly outdated list of these issues in \\u003Ca href=\\\"/r/compact\\\"\\u003E/r/compact\\u003C/a\\u003E. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBy \\u0026quot;basic features\\u0026quot;, I don\\u0026#39;t mean the fancy stuff that the dedicated mobile apps like AlienBlue do. I\\u0026#39;m referring to the base reddit features like being able to edit a comment without having to be in landscape mode, or being able to save a comment. In addition to missing these expected features, there\\u0026#39;s also some prominent display bugs (\\u003Ca href=\\\"http://i.imgur.com/he5Dm9M.png\\\"\\u003Esuch as this one where bulleted lists are compressed\\u003C/a\\u003E), and functionality bugs (such as the lack of pagination). I could make an entire list of these, but for the sake of keeping this post short, I\\u0026#39;ll omit it for now.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAt this point, I\\u0026#39;m so disappointed by these issues that I\\u0026#39;m willing to start contributing to it in a non-programming manner (by doing things like compiling organized lists of the previously mentioned bugs/missing features), but before doing so, I would need to know if the admins even care about the mobile site any more. To me, it seems to have been abandoned ever since former admin \\u003Ca href=\\\"/u/Paradox\\\"\\u003E/u/Paradox\\u003C/a\\u003E stopped working on it a few years ago.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"**TL;DR** The mobile reddit site (the currnet \\\"modern\\\" blue one) is missing basic reddit features and has bugs. It has been like this for years. Are there plans to fix them?\\n\\n--------------\\n\\n*I've PM'd the official /u/reddit account about this a few times and have never gotten a reply. /r/compact seems abandoned, so I figured I'd try here since it seems the admins actually read it:*\\n\\nCurrently, the mobile site (http://m.reddit.com/) has two main issues: it's missing some basic reddit features, and  it has some bugs. There's a partial and possibly outdated list of these issues in /r/compact. \\n\\nBy \\\"basic features\\\", I don't mean the fancy stuff that the dedicated mobile apps like AlienBlue do. I'm referring to the base reddit features like being able to edit a comment without having to be in landscape mode, or being able to save a comment. In addition to missing these expected features, there's also some prominent display bugs ([such as this one where bulleted lists are compressed](http://i.imgur.com/he5Dm9M.png)), and functionality bugs (such as the lack of pagination). I could make an entire list of these, but for the sake of keeping this post short, I'll omit it for now.\\n\\nAt this point, I'm so disappointed by these issues that I'm willing to start contributing to it in a non-programming manner (by doing things like compiling organized lists of the previously mentioned bugs/missing features), but before doing so, I would need to know if the admins even care about the mobile site any more. To me, it seems to have been abandoned ever since former admin /u/Paradox stopped working on it a few years ago.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"262hdd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"alphanovember\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1400625399.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/262hdd/will_compact_mredditcom_ever_be_fixed_its_missing/\", \"locked\": false, \"name\": \"t3_262hdd\", \"created\": 1400653038.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/262hdd/will_compact_mredditcom_ever_be_fixed_its_missing/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Will compact (m.reddit.com) ever be fixed? It's missing a lot of basic features and has some annoying bugs\", \"created_utc\": 1400624238.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ypwo4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"llimllib\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ypwo4/a_script_to_back_up_all_of_a_users_comments_and/\", \"locked\": false, \"name\": \"t3_1ypwo4\", \"created\": 1393208196.0, \"url\": \"https://github.com/llimllib/backupreddituser\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"A script to back up all of a users' comments and submissions\", \"created_utc\": 1393179396.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello again devs! I\\u0026#39;ve just exposed more api endpoints over OAuth:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cstrong\\u003E\\u003Ca href=\\\"http://www.reddit.com/dev/api/oauth#GET_api_submit_text.json\\\"\\u003E/api/submit_text\\u003C/a\\u003E\\u003C/strong\\u003E is now available if you have \\u003Cstrong\\u003Esubmit\\u003C/strong\\u003E scope\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cstrong\\u003E\\u003Ca href=\\\"http://www.reddit.com/dev/api/oauth#GET_user_%7Busername%7D_about.json\\\"\\u003E/user/\\u0026lt;username\\u0026gt;/about\\u003C/a\\u003E\\u003C/strong\\u003E is now available if you have the \\u003Cstrong\\u003Eread\\u003C/strong\\u003E scope\\u003C/li\\u003E\\n\\u003Cli\\u003ENew scope: \\u003Cstrong\\u003E\\u003Ca href=\\\"http://www.reddit.com/dev/api/oauth#scope_report\\\"\\u003Ereport\\u003C/a\\u003E\\u003C/strong\\u003E. Exposes /api/hide, /api/unhide, and /api/report.\\u003C/li\\u003E\\n\\u003Cli\\u003ENew scope: \\u003Cstrong\\u003E\\u003Ca href=\\\"http://www.reddit.com/dev/api/oauth#scope_flair\\\"\\u003Eflair\\u003C/a\\u003E\\u003C/strong\\u003E. Exposes /api/selectflair, /api/setflairenabled, and /api/flairselector. Note: /api/flairselector isn\\u0026#39;t showing up on the page just yet; expect a documentation update in a little bit to make that more available.\\u003C/li\\u003E\\n\\u003Cli\\u003EFinally, \\u003Ca href=\\\"http://www.reddit.com/dev/api/oauth#GET_api_v1_me\\\"\\u003E/api/v1/me\\u003C/a\\u003E now has attributes for \\u0026quot;has_mail\\u0026quot; and \\u0026quot;has_mod_mail\\u0026quot; \\u003Cstrong\\u003Eif\\u003C/strong\\u003E your token includes the \\u003Cem\\u003Eprivatemessages\\u003C/em\\u003E scope.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/compare/3511b08110ce6d10214ece6e782340ad23a76c2d...1a117fffbdf94ed764ffdc8284d3453324b0a49d\\\"\\u003ESee the code on github\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello again devs! I've just exposed more api endpoints over OAuth:\\n\\n* **[/api/submit_text](http://www.reddit.com/dev/api/oauth#GET_api_submit_text.json)** is now available if you have **submit** scope\\n* **[/user/\\u003Cusername\\u003E/about](http://www.reddit.com/dev/api/oauth#GET_user_{username}_about.json)** is now available if you have the **read** scope\\n* New scope: **[report](http://www.reddit.com/dev/api/oauth#scope_report)**. Exposes /api/hide, /api/unhide, and /api/report.\\n* New scope: **[flair](http://www.reddit.com/dev/api/oauth#scope_flair)**. Exposes /api/selectflair, /api/setflairenabled, and /api/flairselector. Note: /api/flairselector isn't showing up on the page just yet; expect a documentation update in a little bit to make that more available.\\n* Finally, [/api/v1/me](http://www.reddit.com/dev/api/oauth#GET_api_v1_me) now has attributes for \\\"has_mail\\\" and \\\"has_mod_mail\\\" **if** your token includes the *privatemessages* scope.\\n\\n[See the code on github](https://github.com/reddit/reddit/compare/3511b08110ce6d10214ece6e782340ad23a76c2d...1a117fffbdf94ed764ffdc8284d3453324b0a49d)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1xuk43\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1xuk43/oauth2_more_endpoints_available_new_scopes_new/\", \"locked\": false, \"name\": \"t3_1xuk43\", \"created\": 1392363465.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1xuk43/oauth2_more_endpoints_available_new_scopes_new/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[OAuth2] More endpoints available, new scopes, new identity data\", \"created_utc\": 1392334665.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey guys, a bit new to Reddit development.  I started using Reddit a couple years ago on the iReddit app, which I guess was the official one, and now I realized that it\\u0026#39;s off the app store and the project seems to be dead.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI found the source code here \\u003Ca href=\\\"https://github.com/reddit/iReddit\\\"\\u003Ehttps://github.com/reddit/iReddit\\u003C/a\\u003E and I was wondering what the status of the project was.  If nobody is working on it right now, would it be acceptable to fork it, make my own changes, and submit to the app store? A bit unsure about the licensing.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey guys, a bit new to Reddit development.  I started using Reddit a couple years ago on the iReddit app, which I guess was the official one, and now I realized that it's off the app store and the project seems to be dead.\\n\\nI found the source code here https://github.com/reddit/iReddit and I was wondering what the status of the project was.  If nobody is working on it right now, would it be acceptable to fork it, make my own changes, and submit to the app store? A bit unsure about the licensing.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1xeg7b\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"eaglelion\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1xeg7b/ireddit_app/\", \"locked\": false, \"name\": \"t3_1xeg7b\", \"created\": 1391939149.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1xeg7b/ireddit_app/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"iReddit App\", \"created_utc\": 1391910349.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI would appreciate if someone could clarify. If I ensure that no more than 30 requests per minute are made by my app, is it OK if it happens that more than 1 request is made every two seconds? Or does it have to be necessarily 1 request every two seconds, thus with a delay between requests? Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I would appreciate if someone could clarify. If I ensure that no more than 30 requests per minute are made by my app, is it OK if it happens that more than 1 request is made every two seconds? Or does it have to be necessarily 1 request every two seconds, thus with a delay between requests? Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1v1qea\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"VitoBotta\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1v1qea/how_does_burstiness_of_the_api_work_exactly/\", \"locked\": false, \"name\": \"t3_1v1qea\", \"created\": 1389584232.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1v1qea/how_does_burstiness_of_the_api_work_exactly/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How does 'burstiness' of the API work exactly?\", \"created_utc\": 1389555432.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey, I wanted to ask, in the future can we please get a heads up when Reddit is making a change to an API which will result in removal of a property?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m the developer of ReddHub (\\u003Ca href=\\\"/r/reddhub\\\"\\u003E/r/reddhub\\u003C/a\\u003E) and yesterday it seems Reddit removed the isClicked property on a link item. This basically broke the app for the many, many, users of the app. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;d be great if Reddit did not do many changes that remove properties, and if so, gave a bit of a headsup.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks again for the awesome API, I jus thought I\\u0026#39;d make this request, since it\\u0026#39;s created a bit of an issue for me.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey, I wanted to ask, in the future can we please get a heads up when Reddit is making a change to an API which will result in removal of a property?\\n\\nI'm the developer of ReddHub (/r/reddhub) and yesterday it seems Reddit removed the isClicked property on a link item. This basically broke the app for the many, many, users of the app. \\n\\nIt'd be great if Reddit did not do many changes that remove properties, and if so, gave a bit of a headsup.\\n\\nThanks again for the awesome API, I jus thought I'd make this request, since it's created a bit of an issue for me.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1p7iv0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"csmaster2005\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1p7iv0/getting_heads_up_when_api_changes_remove/\", \"locked\": false, \"name\": \"t3_1p7iv0\", \"created\": 1382753778.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1p7iv0/getting_heads_up_when_api_changes_remove/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Getting heads up when API changes remove properties?\", \"created_utc\": 1382724978.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am a senior in high school who is very amateur at coding, with very little experience in API. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am working on a network science project which will be mapping the top Reddit users to the subreddits they are posting in, but I have no idea how to get the top users!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI know karmawhores shows the top 20 but I\\u0026#39;m not sure if they just know the top users. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI would not like to manually look for the top 1000 users, so if someone can give me a prod in the right direction I would appreciate it!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am a senior in high school who is very amateur at coding, with very little experience in API. \\n\\nI am working on a network science project which will be mapping the top Reddit users to the subreddits they are posting in, but I have no idea how to get the top users!\\n\\nI know karmawhores shows the top 20 but I'm not sure if they just know the top users. \\n\\nI would not like to manually look for the top 1000 users, so if someone can give me a prod in the right direction I would appreciate it!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1o83z7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"PointsOutTrains\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1381523833.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1o83z7/would_it_be_possible_to_make_a_list_of_reddits/\", \"locked\": false, \"name\": \"t3_1o83z7\", \"created\": 1381534062.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1o83z7/would_it_be_possible_to_make_a_list_of_reddits/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Would it be possible to make a list of Reddit's top karma users using python and Reddit's API?\", \"created_utc\": 1381505262.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EYour \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/install-reddit.sh\\\"\\u003Einstall-reddit.sh\\u003C/a\\u003E set up haproxy with paster to serve up the both static and dynamic urls.  What do you do in production?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve been looking at nginx / uswgi for my single server deployment.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Your [install-reddit.sh](https://github.com/reddit/reddit/blob/master/install-reddit.sh) set up haproxy with paster to serve up the both static and dynamic urls.  What do you do in production?\\n\\nI've been looking at nginx / uswgi for my single server deployment.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1mn8kf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"myerscarpenter\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1mn8kf/what_does_reddit_use_in_production_as_your/\", \"locked\": false, \"name\": \"t3_1mn8kf\", \"created\": 1379544838.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1mn8kf/what_does_reddit_use_in_production_as_your/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"What does reddit use in production as your webserver\", \"created_utc\": 1379516038.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey all, I\\u0026#39;m just learning c++ right now and thought it would be fun to make a reddit bot for fun. I was wondering if it is possible to do it in c++, or if it is only possible to do it in scripting languages like python? Thanks.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey all, I'm just learning c++ right now and thought it would be fun to make a reddit bot for fun. I was wondering if it is possible to do it in c++, or if it is only possible to do it in scripting languages like python? Thanks.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1kux24\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"tryingtoprogram\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1kux24/possible_to_make_a_bot_in_c/\", \"locked\": false, \"name\": \"t3_1kux24\", \"created\": 1377180326.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1kux24/possible_to_make_a_bot_in_c/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Possible to make a bot in C++\", \"created_utc\": 1377151526.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWas trying to find a tool or app for windows to automatically cycle my desktop background through images from subreddits such as \\u0026quot;EarthPorn\\u0026quot; or \\u0026quot;wallpapers\\u0026quot; etc.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECouldn\\u0026#39;t find anything that did what I was looking for, so wrote a small Java program to do it for me.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt turned out alright so I added a GUI and exe wrapper and thought I would see what others think.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECheck out the code and download it if you feel like it. \\u003Ca href=\\\"https://github.com/Exote/RedditWallpaper\\\"\\u003Ehttps://github.com/Exote/RedditWallpaper\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELet me know what you think and if anyone has any idea how to handle some of the other image types/hosts such as flickr (who hide the .jpg behind all sorts of javascript) let me know.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Was trying to find a tool or app for windows to automatically cycle my desktop background through images from subreddits such as \\\"EarthPorn\\\" or \\\"wallpapers\\\" etc.\\n\\nCouldn't find anything that did what I was looking for, so wrote a small Java program to do it for me.\\n\\nIt turned out alright so I added a GUI and exe wrapper and thought I would see what others think.\\n\\nCheck out the code and download it if you feel like it. [https://github.com/Exote/RedditWallpaper](https://github.com/Exote/RedditWallpaper)\\n\\nLet me know what you think and if anyone has any idea how to handle some of the other image types/hosts such as flickr (who hide the .jpg behind all sorts of javascript) let me know.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1i0mu9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Exote\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1i0mu9/a_small_tool_to_pull_images_from_any_subreddit/\", \"locked\": false, \"name\": \"t3_1i0mu9\", \"created\": 1373503775.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1i0mu9/a_small_tool_to_pull_images_from_any_subreddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"A small tool to pull images from any subreddit and set them as user's desktop background. Any critique?\", \"created_utc\": 1373474975.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi. Just decided I wanna write a reddit bot today. What I want to be able to do is from the API go to a subreddit like askreddit and get every single post that is on there. Basically what I would get if I went to the subreddit and clicked next until that ran out.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt looks like I should be able to do it with the \\u0026quot;fullname\\u0026quot; feature of the API but not sure how to work that. And can\\u0026#39;t find anyone that has done this.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny thoughts?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi. Just decided I wanna write a reddit bot today. What I want to be able to do is from the API go to a subreddit like askreddit and get every single post that is on there. Basically what I would get if I went to the subreddit and clicked next until that ran out.\\n\\nIt looks like I should be able to do it with the \\\"fullname\\\" feature of the API but not sure how to work that. And can't find anyone that has done this.\\n\\nAny thoughts?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1gqown\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Jonovono\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1gqown/can_i_get_every_single_possible_submission_from_a/\", \"locked\": false, \"name\": \"t3_1gqown\", \"created\": 1371780311.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1gqown/can_i_get_every_single_possible_submission_from_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can I get every single possible submission from a subreddit?\", \"created_utc\": 1371751511.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1d3bkl\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"karangoeluw\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1d3bkl/jreddit_java_wrapper_for_reddit_api_continuing/\", \"locked\": false, \"name\": \"t3_1d3bkl\", \"created\": 1366936900.0, \"url\": \"https://github.com/thekarangoel/jReddit\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"jReddit - Java Wrapper for Reddit API (Continuing work)\", \"created_utc\": 1366908100.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhen my bot submits something, I usually get a result like this:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026quot;{\\u0026quot;json\\u0026quot;: {\\u0026quot;errors\\u0026quot;: [], \\u0026quot;data\\u0026quot;: {\\u0026quot;url\\u0026quot;:\\n\\u0026quot;http://www.reddit.com/tb/187xxx.json\\u0026quot;, \\u0026quot;id\\u0026quot;: \\u0026quot;187xxx\\u0026quot;, \\u0026quot;name\\u0026quot;:\\n\\u0026quot;t3_187xxx\\u0026quot;}}}\\u0026quot;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EWhich is fine and dandy.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBut sometimes, when the exact same running code during the exact same session\\nsubmits a post, I get this:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026quot;{\\u0026quot;jquery\\u0026quot;: [[0, 1, \\u0026quot;call\\u0026quot;, [\\u0026quot;body\\u0026quot;]], [1, 2, \\u0026quot;attr\\u0026quot;, \\u0026quot;find\\u0026quot;], [2, 3,\\n\\u0026quot;call\\u0026quot;, [\\u0026quot;.status\\u0026quot;]], [3, 4, \\u0026quot;attr\\u0026quot;, \\u0026quot;hide\\u0026quot;], [4, 5, \\u0026quot;call\\u0026quot;, []], [5, 6,\\n\\u0026quot;attr\\u0026quot;, \\u0026quot;html\\u0026quot;], [6, 7, \\u0026quot;call\\u0026quot;, [\\u0026quot;\\u0026quot;]], [7, 8, \\u0026quot;attr\\u0026quot;, \\u0026quot;end\\u0026quot;], [8, 9, \\u0026quot;call\\u0026quot;,\\n[]], [1, 10, \\u0026quot;attr\\u0026quot;, \\u0026quot;attr\\u0026quot;], [10, 11, \\u0026quot;call\\u0026quot;, [\\u0026quot;target\\u0026quot;, \\u0026quot;_top\\u0026quot;]], [1, 12,\\n\\u0026quot;attr\\u0026quot;, \\u0026quot;redirect\\u0026quot;], [12, 13, \\u0026quot;call\\u0026quot;,\\n[\\u0026quot;http://www.reddit.com/tb/187xxx.json\\u0026quot;]]]}\\u0026quot;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENote: My post request has the following:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E extension =\\u0026gt; json\\n kind =\\u0026gt; self\\n sr =\\u0026gt; subreddit \\n text =\\u0026gt; text \\n then =\\u0026gt; tb\\n title =\\u0026gt; title \\n uh =\\u0026gt; modhash \\n api_type =\\u0026gt; json\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENow I would rather not have the Jquery bullshit wrapping the post result, but I\\ncan deal with it if needed.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHowever it\\u0026#39;s really annoying to get inconsistent results.  Sure I can work my\\nway around it, but what\\u0026#39;s the problem here?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT (2013-02-16): Turns out this was a bug in the libcurl bindings I was using.\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"When my bot submits something, I usually get a result like this:\\n\\n    \\\"{\\\"json\\\": {\\\"errors\\\": [], \\\"data\\\": {\\\"url\\\":\\n    \\\"http://www.reddit.com/tb/187xxx.json\\\", \\\"id\\\": \\\"187xxx\\\", \\\"name\\\":\\n    \\\"t3_187xxx\\\"}}}\\\"\\n\\nWhich is fine and dandy.\\n\\nBut sometimes, when the exact same running code during the exact same session\\nsubmits a post, I get this:\\n\\n    \\\"{\\\"jquery\\\": [[0, 1, \\\"call\\\", [\\\"body\\\"]], [1, 2, \\\"attr\\\", \\\"find\\\"], [2, 3,\\n    \\\"call\\\", [\\\".status\\\"]], [3, 4, \\\"attr\\\", \\\"hide\\\"], [4, 5, \\\"call\\\", []], [5, 6,\\n    \\\"attr\\\", \\\"html\\\"], [6, 7, \\\"call\\\", [\\\"\\\"]], [7, 8, \\\"attr\\\", \\\"end\\\"], [8, 9, \\\"call\\\",\\n    []], [1, 10, \\\"attr\\\", \\\"attr\\\"], [10, 11, \\\"call\\\", [\\\"target\\\", \\\"_top\\\"]], [1, 12,\\n    \\\"attr\\\", \\\"redirect\\\"], [12, 13, \\\"call\\\",\\n    [\\\"http://www.reddit.com/tb/187xxx.json\\\"]]]}\\\"\\n\\nNote: My post request has the following:\\n\\n     extension =\\u003E json\\n     kind =\\u003E self\\n     sr =\\u003E subreddit \\n     text =\\u003E text \\n     then =\\u003E tb\\n     title =\\u003E title \\n     uh =\\u003E modhash \\n     api_type =\\u003E json\\n\\nNow I would rather not have the Jquery bullshit wrapping the post result, but I\\ncan deal with it if needed.\\n\\nHowever it's really annoying to get inconsistent results.  Sure I can work my\\nway around it, but what's the problem here?\\n\\n**EDIT (2013-02-16): Turns out this was a bug in the libcurl bindings I was using.**\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"187o5m\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"maplesyrupballs\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1361028820.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/187o5m/results_of_submit_post_queries_sometimes_get/\", \"locked\": false, \"name\": \"t3_187o5m\", \"created\": 1360475150.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/187o5m/results_of_submit_post_queries_sometimes_get/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Results of submit POST queries sometimes get wrapped in Jquery crap, sometimes they don't.  Why?\", \"created_utc\": 1360446350.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI wrote this because of the top 500 ama post the other week and how I thought it was difficult to go through and just find responses to questions by the OP\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://rnm.gauzza.com\\\"\\u003Ehttp://rnm.gauzza.com\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt uses bootstrap and the c# reddit api library that I had to tinker with a bit to get 100% working for what I needed.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I wrote this because of the top 500 ama post the other week and how I thought it was difficult to go through and just find responses to questions by the OP\\n\\nhttp://rnm.gauzza.com\\n\\nIt uses bootstrap and the c# reddit api library that I had to tinker with a bit to get 100% working for what I needed.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"185e3c\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"gauzzastrip\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/185e3c/little_site_to_attempt_to_parse_out_op_answers_to/\", \"locked\": false, \"name\": \"t3_185e3c\", \"created\": 1360382147.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/185e3c/little_site_to_attempt_to_parse_out_op_answers_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Little site to attempt to parse out OP answers to questions\", \"created_utc\": 1360353347.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m working on a bot that will automate many of the moderation functions of a -swap style subreddit. I\\u0026#39;d really like to not have all the bot\\u0026#39;s command-and-control messages sitting out in public threads, and would prefer to use the private messaging system to pass information back and forth and control authentication. I can receive messages to the bot, but I can\\u0026#39;t have the bot reply to them because the compose API demands a CAPTCHA.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there anything else I can do to get the same effect? Is there a way to get a bot certified as not-spammy to be allowed to bypass that? Any other advice?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm working on a bot that will automate many of the moderation functions of a -swap style subreddit. I'd really like to not have all the bot's command-and-control messages sitting out in public threads, and would prefer to use the private messaging system to pass information back and forth and control authentication. I can receive messages to the bot, but I can't have the bot reply to them because the compose API demands a CAPTCHA.\\n\\nIs there anything else I can do to get the same effect? Is there a way to get a bot certified as not-spammy to be allowed to bypass that? Any other advice?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"16qju9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"JaedenStormes\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/16qju9/any_way_to_authorize_a_bot_to_send_private/\", \"locked\": false, \"name\": \"t3_16qju9\", \"created\": 1358430162.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/16qju9/any_way_to_authorize_a_bot_to_send_private/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Any way to authorize a bot to send private messages?\", \"created_utc\": 1358401362.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI turned off all reddit-related firefox extensions. Clicking load more comments just yields the red text \\u0026quot;loading...\\u0026quot;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPasting from firebug:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERequest Headers:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EPOST /api/morechildren HTTP/1.1\\nHost: www.reddit.com\\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:18.0) Gecko/20100101 Firefox/18.0\\nAccept: application/json, text/javascript, */*; q=0.01\\nAccept-Language: en-US,en;q=0.5\\nAccept-Encoding: gzip, deflate\\nDNT: 1\\nContent-Type: application/x-www-form-urlencoded; charset=UTF-8\\nX-Requested-With: XMLHttpRequest\\nReferer: http://www.reddit.com/r/AskReddit/comments/16fe4d/what_company_has_forever_lost_your_business_why/\\nContent-Length: 207\\nCookie: \\u0026lt;noneofyourbusiness\\u0026gt;\\nConnection: keep-alive\\nPragma: no-cache\\nCache-Control: no-cache\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EResponse Headers:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EHTTP/1.1 403 Forbidden\\nCache-Control: no-cache\\nContent-Type: text/html\\nVary: Accept-Encoding\\nDate: Sat, 12 Jan 2013 12:20:14 GMT\\nTransfer-Encoding: chunked\\nConnection: close, Transfer-Encoding\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I turned off all reddit-related firefox extensions. Clicking load more comments just yields the red text \\\"loading...\\\"\\n\\nPasting from firebug:\\n\\nRequest Headers:\\n\\n    POST /api/morechildren HTTP/1.1\\n    Host: www.reddit.com\\n    User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:18.0) Gecko/20100101 Firefox/18.0\\n    Accept: application/json, text/javascript, */*; q=0.01\\n    Accept-Language: en-US,en;q=0.5\\n    Accept-Encoding: gzip, deflate\\n    DNT: 1\\n    Content-Type: application/x-www-form-urlencoded; charset=UTF-8\\n    X-Requested-With: XMLHttpRequest\\n    Referer: http://www.reddit.com/r/AskReddit/comments/16fe4d/what_company_has_forever_lost_your_business_why/\\n    Content-Length: 207\\n    Cookie: \\u003Cnoneofyourbusiness\\u003E\\n    Connection: keep-alive\\n    Pragma: no-cache\\n    Cache-Control: no-cache\\n\\nResponse Headers:\\n\\n    HTTP/1.1 403 Forbidden\\n    Cache-Control: no-cache\\n    Content-Type: text/html\\n    Vary: Accept-Encoding\\n    Date: Sat, 12 Jan 2013 12:20:14 GMT\\n    Transfer-Encoding: chunked\\n    Connection: close, Transfer-Encoding\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"16fnly\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"RafiTheMage447\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/16fnly/load_more_comments_apimorechildren_returns_403/\", \"locked\": false, \"name\": \"t3_16fnly\", \"created\": 1358022718.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/16fnly/load_more_comments_apimorechildren_returns_403/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"load more comments (/api/morechildren) returns 403 Forbidden - Firefox 18\", \"created_utc\": 1357993918.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello Reddit Developer Community!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have what should be a simple question, I\\u0026#39;m the founder of \\u003Ca href=\\\"/r/DiaryOfEarth\\\"\\u003E/r/DiaryOfEarth\\u003C/a\\u003E, which requires a new post at midnight each night. So far, I\\u0026#39;ve been manually creating this post. I\\u0026#39;m wondering what, in your professional opinions, you believe to be the best way to automate this process. I imagine I must leave a machine on somewhere which runs a job at 11:55p each night, that logs into an account and posts the thread via the Reddit API. Correct? Can you point me in the right direction?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny help is appreciated, much thanks!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E-Eric\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello Reddit Developer Community!\\n\\nI have what should be a simple question, I'm the founder of /r/DiaryOfEarth, which requires a new post at midnight each night. So far, I've been manually creating this post. I'm wondering what, in your professional opinions, you believe to be the best way to automate this process. I imagine I must leave a machine on somewhere which runs a job at 11:55p each night, that logs into an account and posts the thread via the Reddit API. Correct? Can you point me in the right direction?\\n\\nAny help is appreciated, much thanks!\\n\\n-Eric\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"15dsju\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"epheterson\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 14, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/15dsju/automate_new_thread_at_midnight_every_night/\", \"locked\": false, \"name\": \"t3_15dsju\", \"created\": 1356401485.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/15dsju/automate_new_thread_at_midnight_every_night/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Automate new thread at midnight, every night.\", \"created_utc\": 1356372685.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"145sxg\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ugart\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 16, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/145sxg/is_this_doing_what_i_think_it_is_doing/\", \"locked\": false, \"name\": \"t3_145sxg\", \"created\": 1354504174.0, \"url\": \"https://github.com/reddit/reddit/commit/db7e5e9ac66ae4b5527c606f32819c9d9fa930e5\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is this doing what I think it is doing?\", \"created_utc\": 1354475374.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1375la\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"DaGoodBoy\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1375la/simple_reddit_app_for_google_chrome/\", \"locked\": false, \"name\": \"t3_1375la\", \"created\": 1352953121.0, \"url\": \"https://github.com/DaGoodBoy/reddit-chrome-app\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Simple Reddit App for Google Chrome\", \"created_utc\": 1352924321.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m toying with a Chrome plugin. I\\u0026#39;m grabbing JSON API data, but I can\\u0026#39;t change the user agent because of browser restrictions. Trying to change it triggers the error \\u0026quot;Refused to set unsafe header \\u0026#39;User-Agent\\u0026#39;\\u0026quot;. Item two in the API wiki states \\u0026quot;Change your client\\u0026#39;s User-Agent string to something unique and descriptive, preferably referencing your reddit username.\\u0026quot;.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor now I\\u0026#39;m setting a header value for X-User-Agent, which the browser does permit. What is the \\u0026quot;right thing\\u0026quot; that I should do? Has anyone thought of this from a browser security context?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm toying with a Chrome plugin. I'm grabbing JSON API data, but I can't change the user agent because of browser restrictions. Trying to change it triggers the error \\\"Refused to set unsafe header 'User-Agent'\\\". Item two in the API wiki states \\\"Change your client's User-Agent string to something unique and descriptive, preferably referencing your reddit username.\\\".\\n\\nFor now I'm setting a header value for X-User-Agent, which the browser does permit. What is the \\\"right thing\\\" that I should do? Has anyone thought of this from a browser security context?\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"11ebmc\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"badmonkey0001\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/11ebmc/when_user_agent_cant_be_changed_for_api_calls/\", \"locked\": false, \"name\": \"t3_11ebmc\", \"created\": 1350123374.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/11ebmc/when_user_agent_cant_be_changed_for_api_calls/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"When user agent can't be changed for API calls?\", \"created_utc\": 1350094574.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi all,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m trying to implement global ratelimiting for my bot so I can run multiple PRAW bots in parallel, but apparently I\\u0026#39;m not catching every time PRAW makes a request to reddit. Can someone please clarify how many requests are being made in the following code snippets?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI count this as 1 request:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Er.get_subreddit(\\u0026quot;all\\u0026quot;).get_top(limit=100)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI count r.get_front_page() as 1 request, then each submission.comments as another request.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eposts = []\\n\\nfor submission in r.get_front_page():\\n    for comment in submission.comments:\\n        posts.append(comment)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI count this as 1 request:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Er.get_redditor(user).get_overview(limit=2000)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi all,\\n\\nI'm trying to implement global ratelimiting for my bot so I can run multiple PRAW bots in parallel, but apparently I'm not catching every time PRAW makes a request to reddit. Can someone please clarify how many requests are being made in the following code snippets?\\n\\nI count this as 1 request:\\n\\n    r.get_subreddit(\\\"all\\\").get_top(limit=100)\\n\\nI count r.get_front_page() as 1 request, then each submission.comments as another request.\\n\\n    posts = []\\n\\n    for submission in r.get_front_page():\\n        for comment in submission.comments:\\n            posts.append(comment)\\n\\nI count this as 1 request:\\n\\n    r.get_redditor(user).get_overview(limit=2000)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"10omtd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"rhiever\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/10omtd/praw_when_are_requests_being_made_code_included/\", \"locked\": false, \"name\": \"t3_10omtd\", \"created\": 1348983747.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/10omtd/praw_when_are_requests_being_made_code_included/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"PRAW: when are requests being made? (code included)\", \"created_utc\": 1348954947.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m currently developing a client for Chrome and could use a partner who really knew what they were doing with JavaScript and the reddit API. I\\u0026#39;m an interface designer and while I\\u0026#39;ve been practicing my hand at js development, I feel a little out of my depth at times.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have the basic site up. Currently you can view stories from any subreddit and switch subreddits. Here is a screenshot: \\u003Ca href=\\\"http://i.imgur.com/ACyWq.png\\\"\\u003Ehttp://i.imgur.com/ACyWq.png\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s slow going but I intend to stick to it. There is a lot of work to be done, comments page, ability to login and post, for example. It would be fantastic to have a js developer to work with on this project.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI don\\u0026#39;t believe a reddit client for a browser is pointless. It\\u0026#39;s always nice to give the user a choice. I created a reddit \\u0026quot;app\\u0026quot; for the Chrome webstore, which was basically a redirect to reddit.com. It was the first reddit app on the webstore and as such, it generated quite a few thousand installs. Unfortunately, this past week it was taken down. I can resubmit the app, but only if it does more than redirect a user. I figure this is a great time to release a reddit client, because we have a guaranteed userbase there. As soon as the app is resubmitted, it will update automatically for all users.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESorry if I posted this in the wrong subreddit. I was looking to speak with people who have reddit API experience. Thanks for reading. I welcome your feedback.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: Link title fail. Would anybody be \\u003Cem\\u003Einterested\\u003C/em\\u003E...\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm currently developing a client for Chrome and could use a partner who really knew what they were doing with JavaScript and the reddit API. I'm an interface designer and while I've been practicing my hand at js development, I feel a little out of my depth at times.\\n\\nI have the basic site up. Currently you can view stories from any subreddit and switch subreddits. Here is a screenshot: http://i.imgur.com/ACyWq.png\\n\\nIt's slow going but I intend to stick to it. There is a lot of work to be done, comments page, ability to login and post, for example. It would be fantastic to have a js developer to work with on this project.\\n\\nI don't believe a reddit client for a browser is pointless. It's always nice to give the user a choice. I created a reddit \\\"app\\\" for the Chrome webstore, which was basically a redirect to reddit.com. It was the first reddit app on the webstore and as such, it generated quite a few thousand installs. Unfortunately, this past week it was taken down. I can resubmit the app, but only if it does more than redirect a user. I figure this is a great time to release a reddit client, because we have a guaranteed userbase there. As soon as the app is resubmitted, it will update automatically for all users.\\n\\nSorry if I posted this in the wrong subreddit. I was looking to speak with people who have reddit API experience. Thanks for reading. I welcome your feedback.\\n\\nEdit: Link title fail. Would anybody be *interested*...\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"y9vz3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"muppethead\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 18, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/y9vz3/would_anybody_be_interesting_in_helping_me_create/\", \"locked\": false, \"name\": \"t3_y9vz3\", \"created\": 1345084776.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/y9vz3/would_anybody_be_interesting_in_helping_me_create/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Would anybody be interesting in helping me create a reddit client for Google Chrome?\", \"created_utc\": 1345055976.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI kinda need to find a comment I posted perhaps a year or so ago.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve posted quite a bit, so going \\u0026quot;next\\u0026quot;, \\u0026quot;next\\u0026quot; is possible if tedious, but I\\u0026#39;m curious if it will give them all to me, or will cut out after a while.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETheoretically I could write a web crawler to do that \\u0026quot;next\\u0026quot; thing for me - but the:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E Disallow: /*after=\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Ein robots.txt tells me not to.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI tried the search feature, but it seems near useless for comments.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there any other way?   Any existing API for this?  If it doesn\\u0026#39;t exist already, would reddit accept a patch that said \\u0026quot;let me download all my own comments\\u0026quot;?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I kinda need to find a comment I posted perhaps a year or so ago.\\n\\nI've posted quite a bit, so going \\\"next\\\", \\\"next\\\" is possible if tedious, but I'm curious if it will give them all to me, or will cut out after a while.\\n\\nTheoretically I could write a web crawler to do that \\\"next\\\" thing for me - but the:\\n\\n     Disallow: /*after=\\n\\nin robots.txt tells me not to.\\n\\nI tried the search feature, but it seems near useless for comments.\\n\\n\\nIs there any other way?   Any existing API for this?  If it doesn't exist already, would reddit accept a patch that said \\\"let me download all my own comments\\\"?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"wvrc1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"rmxz\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/wvrc1/is_there_a_way_i_can_download_every_comment_i/\", \"locked\": false, \"name\": \"t3_wvrc1\", \"created\": 1342833012.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/wvrc1/is_there_a_way_i_can_download_every_comment_i/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a way I can download every comment I ever posted?\", \"created_utc\": 1342804212.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m setting up my own Reddit instance on Ubuntu 12.04 LTS using \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/Install-guide\\\"\\u003Ethis guide\\u003C/a\\u003E. Right now I\\u0026#39;ve gotten to the point where all the \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/Dependencies\\\"\\u003Edependencies\\u003C/a\\u003E are installed, and I\\u0026#39;ve started configuring everything.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhen setting up Cassandra, I found Cassandra wasn\\u0026#39;t actually running and eventually found out the 512 MB I allocated to this VM wasn\\u0026#39;t enough. I kicked it up to 2 GB and it runs now, but \\u003Ca href=\\\"http://i.imgur.com/pCFEU.png\\\"\\u003ECassandra is uing over half of the available ram\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis seems ridiculous considering there isn\\u0026#39;t even any information in the database. Setting up Cassandra was a little sketch since the ppa listed in the dependencies guide doesn\\u0026#39;t seem to work anymore.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDid I set up Cassandra wrong or is this just something I have to deal with? Since Reddit primarily uses Cassandra for caching (according to the Wiki), can I just disable caching and therefore disable Cassandra?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm setting up my own Reddit instance on Ubuntu 12.04 LTS using [this guide](https://github.com/reddit/reddit/wiki/Install-guide). Right now I've gotten to the point where all the [dependencies](https://github.com/reddit/reddit/wiki/Dependencies) are installed, and I've started configuring everything.\\n\\nWhen setting up Cassandra, I found Cassandra wasn't actually running and eventually found out the 512 MB I allocated to this VM wasn't enough. I kicked it up to 2 GB and it runs now, but [Cassandra is uing over half of the available ram](http://i.imgur.com/pCFEU.png).\\n\\nThis seems ridiculous considering there isn't even any information in the database. Setting up Cassandra was a little sketch since the ppa listed in the dependencies guide doesn't seem to work anymore.\\n\\nDid I set up Cassandra wrong or is this just something I have to deal with? Since Reddit primarily uses Cassandra for caching (according to the Wiki), can I just disable caching and therefore disable Cassandra?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"tuq5l\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Pathogen-David\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/tuq5l/is_cassandra_always_this_fat_or_did_i_do/\", \"locked\": false, \"name\": \"t3_tuq5l\", \"created\": 1337466882.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/tuq5l/is_cassandra_always_this_fat_or_did_i_do/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is Cassandra always this fat or did I do something wrong?\", \"created_utc\": 1337438082.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe attribute is stored as \\u0026quot;edited\\u0026quot; and the value is same as for created_utc - seconds since epoch.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EImportant note: For older posts, the value returned will be a boolean true/false, since those posts did not store the edit time.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESee also: \\u003Ca href=\\\"http://www.reddit.com/r/changelog/comments/toj9e/reddit_change_minor_selfposts_now_have_asterisks/\\\"\\u003Ehttp://www.reddit.com/r/changelog/comments/toj9e/reddit_change_minor_selfposts_now_have_asterisks/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E(And you can use that post as an example: \\u003Ca href=\\\"http://www.reddit.com/r/changelog/comments/toj9e/reddit_change_minor_selfposts_now_have_asterisks/.json\\\"\\u003Ehttp://www.reddit.com/r/changelog/comments/toj9e/reddit_change_minor_selfposts_now_have_asterisks/.json\\u003C/a\\u003E)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The attribute is stored as \\\"edited\\\" and the value is same as for created_utc - seconds since epoch.\\n\\nImportant note: For older posts, the value returned will be a boolean true/false, since those posts did not store the edit time.\\n\\nSee also: http://www.reddit.com/r/changelog/comments/toj9e/reddit_change_minor_selfposts_now_have_asterisks/\\n\\n(And you can use that post as an example: http://www.reddit.com/r/changelog/comments/toj9e/reddit_change_minor_selfposts_now_have_asterisks/.json)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"toto8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/toto8/api_last_edited_time_of_postcomment_available/\", \"locked\": false, \"name\": \"t3_toto8\", \"created\": 1337144906.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/toto8/api_last_edited_time_of_postcomment_available/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API: Last edited time of post/comment available\", \"created_utc\": 1337116106.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIs it possible to make the frontpage sorted by a different method (e.g. top) by default?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat about not even listing any links at all, rather just a static home page that links to subreddits?  I\\u0026#39;m half tempted to look in to doing this on another level (e.g. apache mod_rewrite) and just directing it to a different page, but I\\u0026#39;d rather keep it within the reddit code so that it works regardless of rendering method.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt seemed like this should be simple, but I seem to be missing how the controller actually works...\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Is it possible to make the frontpage sorted by a different method (e.g. top) by default?\\n\\nWhat about not even listing any links at all, rather just a static home page that links to subreddits?  I'm half tempted to look in to doing this on another level (e.g. apache mod_rewrite) and just directing it to a different page, but I'd rather keep it within the reddit code so that it works regardless of rendering method.\\n\\nIt seemed like this should be simple, but I seem to be missing how the controller actually works...\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"t4841\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"RedditSrc4Research\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/t4841/modify_frontpage/\", \"locked\": false, \"name\": \"t3_t4841\", \"created\": 1336032992.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/t4841/modify_frontpage/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Modify frontpage?\", \"created_utc\": 1336004192.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey everyone,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m in the midst of developing an app for Reddit and have been using the API extensively. I am trying to pull the list of subreddits that you see in the subreddit bar up on the top of the page. After scouring the internet, I could not find anything. Does anyone know how I can do this? Without crawling the page of course.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey everyone,\\n\\nI'm in the midst of developing an app for Reddit and have been using the API extensively. I am trying to pull the list of subreddits that you see in the subreddit bar up on the top of the page. After scouring the internet, I could not find anything. Does anyone know how I can do this? Without crawling the page of course.\\n\\nThanks\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"sep1w\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kortank\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/sep1w/creating_a_reddit_app_hit_a_little_speed_bump/\", \"locked\": false, \"name\": \"t3_sep1w\", \"created\": 1334720653.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/sep1w/creating_a_reddit_app_hit_a_little_speed_bump/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Creating a Reddit App, hit a little speed bump\", \"created_utc\": 1334691853.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"blog.fruiapps.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"s2ll2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"fruiapps\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/s2ll2/howwhy_i_extended_reddits_ranking_system_to_rate/\", \"locked\": false, \"name\": \"t3_s2ll2\", \"created\": 1334098762.0, \"url\": \"http://www.blog.fruiapps.com/2012/04/Chose-a-mathematical-model-to-rate-players\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How/Why i extended reddit's ranking system to rate players for my multiplayer game. \", \"created_utc\": 1334069962.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am trying to write a web app that supports users logging in, upvoting, commenting, submitting new self posts, edits user flair, a few mod actions too.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWell I got the logging in working and submitting new threads, however when it does work no JSON gets returned with the thread url like the API Doc says.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cdel\\u003EThen if it fails requiring captcha, there is no doc saying how to do that.\\u003C/del\\u003E Figured that out thanks to the freenode channel. The API Returns captcha with a value you construct a url with. \\u003Ca href=\\\"http://reddit.com/captcha/xxxxxxx.png\\\"\\u003Ehttp://reddit.com/captcha/xxxxxxx.png\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo for the guys who do know the api, please spend a weekend improving the doc, provide detailed explanations and code samples in various languages or even pseudo code.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEverytime I try to do something that involves the api it takes me hours to get it working because lack of documentation and I am about to rip my damn hair out.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere is my OSS code if anyone cares: \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://code.google.com/p/playitforward/source/list\\\"\\u003Ehttp://code.google.com/p/playitforward/source/list\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am trying to write a web app that supports users logging in, upvoting, commenting, submitting new self posts, edits user flair, a few mod actions too.\\n\\nWell I got the logging in working and submitting new threads, however when it does work no JSON gets returned with the thread url like the API Doc says.\\n\\n~~Then if it fails requiring captcha, there is no doc saying how to do that.~~ Figured that out thanks to the freenode channel. The API Returns captcha with a value you construct a url with. http://reddit.com/captcha/xxxxxxx.png\\n\\nSo for the guys who do know the api, please spend a weekend improving the doc, provide detailed explanations and code samples in various languages or even pseudo code.\\n\\nEverytime I try to do something that involves the api it takes me hours to get it working because lack of documentation and I am about to rip my damn hair out.\\n\\nHere is my OSS code if anyone cares: \\n\\nhttp://code.google.com/p/playitforward/source/list\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"qh192\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"sevenalive\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/qh192/api_documentation_it_sucks_hard/\", \"locked\": false, \"name\": \"t3_qh192\", \"created\": 1330882980.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/qh192/api_documentation_it_sucks_hard/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API Documentation - It sucks hard.\", \"created_utc\": 1330854180.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"pmdtt\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MDY\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/pmdtt/some_documentation_for_sciteit_code_and_by/\", \"locked\": false, \"name\": \"t3_pmdtt\", \"created\": 1329106013.0, \"url\": \"https://github.com/constantAmateur/sciteit/wiki\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Some documentation for sciteit code (and by extension reddit)\", \"created_utc\": 1329077213.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been trying to set up reddit on a test machine using Ubuntu 11.04. The installation script runs fine without any errors (save for the PostgreSQL encoding, that defaults to ANSI and has to be changed).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe site runs fine for a while, but then Cassandra freezes (can\\u0026#39;t even restart the service):\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003Epycassa.pool.AllServersUnavailable: An attempt was made to connect to each of the servers twice, but none of the attempts succeeded. The last failure was timeout: timed out\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EThe logs in /var/log/cassandra show no errors before the freeze. Rebooting the machine makes it work again for a few minutes.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny ideas on how to troubleshoot this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been trying to set up reddit on a test machine using Ubuntu 11.04. The installation script runs fine without any errors (save for the PostgreSQL encoding, that defaults to ANSI and has to be changed).\\n\\nThe site runs fine for a while, but then Cassandra freezes (can't even restart the service):\\n\\n\\u003E pycassa.pool.AllServersUnavailable: An attempt was made to connect to each of the servers twice, but none of the attempts succeeded. The last failure was timeout: timed out\\n\\nThe logs in /var/log/cassandra show no errors before the freeze. Rebooting the machine makes it work again for a few minutes.\\n\\n\\nAny ideas on how to troubleshoot this?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"phycj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"thrazz\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/phycj/cassandra_problems_setting_up_a_reddit_clone/\", \"locked\": false, \"name\": \"t3_phycj\", \"created\": 1328835210.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/phycj/cassandra_problems_setting_up_a_reddit_clone/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Cassandra problems setting up a reddit clone\", \"created_utc\": 1328806410.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI can\\u0026#39;t find it in the json returned by a by_id request and it\\u0026#39;s not in the listing at \\u003Ca href=\\\"/r/whatever/about/spam\\\"\\u003E/r/whatever/about/spam\\u003C/a\\u003E.json. I suppose I could scrape HTML for the \\u0026quot;removed by\\u0026quot; text but that\\u0026#39;s a bit hackish.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EGrepping through the source tree, it looks like that info isn\\u0026#39;t publicly exposed anywhere except the HTML interface, but I might be wrong.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I can't find it in the json returned by a by_id request and it's not in the listing at /r/whatever/about/spam.json. I suppose I could scrape HTML for the \\\"removed by\\\" text but that's a bit hackish.\\n\\nGrepping through the source tree, it looks like that info isn't publicly exposed anywhere except the HTML interface, but I might be wrong.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ou1xv\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ou1xv/is_there_any_way_to_find_out_which_user_is/\", \"locked\": false, \"name\": \"t3_ou1xv\", \"created\": 1327410296.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ou1xv/is_there_any_way_to_find_out_which_user_is/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there any way to find out which user is responsible for banning an item using the API?\", \"created_utc\": 1327381496.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHow can I get the reddit ID of a subreddit if I have the url? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor example \\u003Ca href=\\\"/r/aww\\\"\\u003E/r/aww\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"How can I get the reddit ID of a subreddit if I have the url? \\n\\nFor example /r/aww\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"omlmg\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"quinbd\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/omlmg/how_can_i_get_the_reddit_id_of_a_subreddit_if_i/\", \"locked\": false, \"name\": \"t3_omlmg\", \"created\": 1326969850.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/omlmg/how_can_i_get_the_reddit_id_of_a_subreddit_if_i/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How can I get the reddit ID of a subreddit if I have the url? for example /r/aww\", \"created_utc\": 1326941050.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT:\\u003C/strong\\u003E Working on \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/o6v24/php_flair_bot_guy_here_i_did_it_let_me_know_what/c3ev5zy\\\"\\u003Ea better solution\\u003C/a\\u003E that doesn\\u0026#39;t require the user\\u0026#39;s password.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT2:\\u003C/strong\\u003E I did it again! Same links as before. Now when you fill out the form it sends a message to the user with a link. I had to use tinyurl because for some reason the reddit api for composing messages was cutting of my php variables after the first one. Check it out and let me know what you think, and thanks for all the help!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo I had the problem about verifying a user is who they say they are. \\u003Ca href=\\\"http://www.reddit.com/user/zjs\\\"\\u003Ezjs\\u003C/a\\u003E had \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/o5vi3/creating_a_web_form_to_let_users_edit_their_flair/c3em9qa\\\"\\u003Esome good ideas\\u003C/a\\u003E but I decided that for now I will just take the user\\u0026#39;s name and password, and if people don\\u0026#39;t like it I\\u0026#39;ll change it in the future. Here is the \\u003Ca href=\\\"http://pastebin.com/Z1WQZ0Tq\\\"\\u003Eweb form\\u003C/a\\u003E, and the \\u003Ca href=\\\"http://pastebin.com/NxgQH93F\\\"\\u003Ephp script\\u003C/a\\u003E that does all the work. If you\\u0026#39;re interested in something like this it should be really easy to just modify that to do what you need to do. Thanks to those who responded to my questions over the past few days, and a special shout out to \\u003Ca href=\\\"http://www.reddit.com/user/bboe\\\"\\u003Ebboe\\u003C/a\\u003E who \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/o523l/anyone_want_to_help_me_figure_out_why_my_php/c3egg80\\\"\\u003Ehelped me solve a problem that had be banging my head against the wall for a few hours\\u003C/a\\u003E. If you feel like testing it out, you can sub to \\u003Ca href=\\\"/r/killobyte\\\"\\u003E/r/killobyte\\u003C/a\\u003E and get to the form \\u003Ca href=\\\"http://www.quicklookbusy.com/flairbot/flairform.php\\\"\\u003Ehere\\u003C/a\\u003E. Right now it\\u0026#39;s all about Drexel because I want to use it there, but I\\u0026#39;m testing it in \\u003Ca href=\\\"/r/killobyte\\\"\\u003E/r/killobyte\\u003C/a\\u003E first.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"**EDIT:** Working on [a better solution](http://www.reddit.com/r/redditdev/comments/o6v24/php_flair_bot_guy_here_i_did_it_let_me_know_what/c3ev5zy) that doesn't require the user's password.\\n\\n**EDIT2:** I did it again! Same links as before. Now when you fill out the form it sends a message to the user with a link. I had to use tinyurl because for some reason the reddit api for composing messages was cutting of my php variables after the first one. Check it out and let me know what you think, and thanks for all the help!\\n\\nSo I had the problem about verifying a user is who they say they are. [zjs](http://www.reddit.com/user/zjs) had [some good ideas](http://www.reddit.com/r/redditdev/comments/o5vi3/creating_a_web_form_to_let_users_edit_their_flair/c3em9qa) but I decided that for now I will just take the user's name and password, and if people don't like it I'll change it in the future. Here is the [web form](http://pastebin.com/Z1WQZ0Tq), and the [php script](http://pastebin.com/NxgQH93F) that does all the work. If you're interested in something like this it should be really easy to just modify that to do what you need to do. Thanks to those who responded to my questions over the past few days, and a special shout out to [bboe](http://www.reddit.com/user/bboe) who [helped me solve a problem that had be banging my head against the wall for a few hours](http://www.reddit.com/r/redditdev/comments/o523l/anyone_want_to_help_me_figure_out_why_my_php/c3egg80). If you feel like testing it out, you can sub to /r/killobyte and get to the form [here](http://www.quicklookbusy.com/flairbot/flairform.php). Right now it's all about Drexel because I want to use it there, but I'm testing it in /r/killobyte first.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"o6v24\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Killobyte\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 24, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/o6v24/php_flair_bot_guy_here_i_did_it_let_me_know_what/\", \"locked\": false, \"name\": \"t3_o6v24\", \"created\": 1325984404.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/o6v24/php_flair_bot_guy_here_i_did_it_let_me_know_what/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"PHP flair bot guy here... I did it! Let me know what you think\", \"created_utc\": 1325955604.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThere are a few \\u0026quot;special\\u0026quot; subreddits that aren\\u0026#39;t actually subreddits- you can browse them but not submit to them, forexample \\u003Ca href=\\\"/r/all\\\"\\u003E/r/all\\u003C/a\\u003E and \\u003Ca href=\\\"/r/random\\\"\\u003E/r/random\\u003C/a\\u003E. What are these called in the reddit code base? \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"There are a few \\\"special\\\" subreddits that aren't actually subreddits- you can browse them but not submit to them, forexample /r/all and /r/random. What are these called in the reddit code base? \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"kpcui\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/kpcui/fixing_a_bug_in_the_reddit_is_fun_android_app/\", \"locked\": false, \"name\": \"t3_kpcui\", \"created\": 1316834496.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/kpcui/fixing_a_bug_in_the_reddit_is_fun_android_app/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Fixing a bug in the reddit is fun android app, need help naming a variable :p\", \"created_utc\": 1316805696.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m not sure if this is the /r/ to post this to, if not, please send me the right way. :)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m using mellort\\u0026#39;s reddit \\u003Ca href=\\\"https://github.com/mellort/reddit_api\\\"\\u003EAPI\\u003C/a\\u003E for python as a wrapper.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat I\\u0026#39;m trying to do: Given an id for a submission, retrieve all comments for that submission. But the problem is that it\\u0026#39;s slow... I just tried to retrieve 400 comments from a submission, and it took close to a minute to complete.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt seems the way the reddit api works is through links to \\u0026quot;more\\u0026quot; results for each comment, and the API wrapper has to retrieve all of those links over multiple requests. Is there a way to tell the reddit API to give me all the results, or more results in one go, rather than having to enumerate through all of the \\u0026quot;more\\u0026quot; links?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am using this small project as a way to learn Python, so please be nice, but here\\u0026#39;s the code that i\\u0026#39;m working with (snipped):\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport reddit\\n\\ndef get_comments(submission):\\n    all_comments_list = []\\n\\n    comments = submission.comments\\n    for comment in comments:\\n        _get_all_comments(comment, all_comments_list)\\n\\n    return all_comments_list\\n\\ndef _get_all_comments(cur_comment, list):\\n    replies = None\\n\\n    if type(cur_comment) == reddit.comment.Comment:\\n        list.append(cur_comment)\\n\\n        if hasattr(cur_comment, \\u0026#39;replies\\u0026#39;):\\n            replies = cur_comment.replies\\n    else:       # It\\u0026#39;s a MoreComments object then.\\n        if hasattr(cur_comment, \\u0026#39;comments\\u0026#39;):\\n            replies = cur_comment.comments\\n\\n    if replies:\\n        for comment in replies:\\n            _get_all_comments(comment, list)\\n\\nr = reddit.Reddit(user_agent=\\u0026#39;niothiel\\u0026#39;)\\nsubmission = r.get_submission_by_id(\\u0026#39;dnjoe\\u0026#39;)\\nprint get_comments(submission)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EMy information for this problem comes from the reddit API docs located \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/API\\\"\\u003Ehere\\u003C/a\\u003E. Specifically under the \\u003Cstrong\\u003EFetching More\\u003C/strong\\u003E section.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo, is there a better way of doing this? And thank you in advance for your answers. :)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm not sure if this is the /r/ to post this to, if not, please send me the right way. :)\\n\\nI'm using mellort's reddit [API](https://github.com/mellort/reddit_api) for python as a wrapper.\\n\\nWhat I'm trying to do: Given an id for a submission, retrieve all comments for that submission. But the problem is that it's slow... I just tried to retrieve 400 comments from a submission, and it took close to a minute to complete.\\n\\nIt seems the way the reddit api works is through links to \\\"more\\\" results for each comment, and the API wrapper has to retrieve all of those links over multiple requests. Is there a way to tell the reddit API to give me all the results, or more results in one go, rather than having to enumerate through all of the \\\"more\\\" links?\\n\\nI am using this small project as a way to learn Python, so please be nice, but here's the code that i'm working with (snipped):\\n\\n    import reddit\\n    \\n    def get_comments(submission):\\n        all_comments_list = []\\n        \\n        comments = submission.comments\\n        for comment in comments:\\n            _get_all_comments(comment, all_comments_list)\\n            \\n        return all_comments_list\\n    \\n    def _get_all_comments(cur_comment, list):\\n        replies = None\\n        \\n        if type(cur_comment) == reddit.comment.Comment:\\n            list.append(cur_comment)\\n            \\n            if hasattr(cur_comment, 'replies'):\\n                replies = cur_comment.replies\\n        else:       # It's a MoreComments object then.\\n            if hasattr(cur_comment, 'comments'):\\n                replies = cur_comment.comments\\n        \\n        if replies:\\n            for comment in replies:\\n                _get_all_comments(comment, list)\\n\\n    r = reddit.Reddit(user_agent='niothiel')\\n    submission = r.get_submission_by_id('dnjoe')\\n    print get_comments(submission)\\n                \\n\\nMy information for this problem comes from the reddit API docs located [here](https://github.com/reddit/reddit/wiki/API). Specifically under the **Fetching More** section.\\n\\nSo, is there a better way of doing this? And thank you in advance for your answers. :)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"k61iq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"niothiel\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/k61iq/reddit_comment_enumeration/\", \"locked\": false, \"name\": \"t3_k61iq\", \"created\": 1315309342.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/k61iq/reddit_comment_enumeration/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Comment Enumeration\", \"created_utc\": 1315280542.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;d like to set up a local clone running on my linux box just to satisfy my curiosity on how it all works. There must be some sort of guide to do this? I\\u0026#39;ve already forked the git and have the code sitting here, I just don\\u0026#39;t know what to do next.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'd like to set up a local clone running on my linux box just to satisfy my curiosity on how it all works. There must be some sort of guide to do this? I've already forked the git and have the code sitting here, I just don't know what to do next.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"j7373\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Scurry\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/j7373/is_there_a_howto_guide_on_setting_up_a_reddit/\", \"locked\": false, \"name\": \"t3_j7373\", \"created\": 1312349557.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/j7373/is_there_a_howto_guide_on_setting_up_a_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a howto guide on setting up a reddit clone?\", \"created_utc\": 1312320757.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo I was looking to make an app that would consume the reddit API, when it dawned on me that if my app somehow got up into a significant number of users, the requests would easily be flowing in more than once per 2 seconds.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy question is, then, what are the boundaries of once per 2 seconds? If I distribute a client app, is it once per 2 seconds per user?  What about a centralized web app? Is that still user based, or is the entire web app restricted from making more than 1 request per 2 seconds?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So I was looking to make an app that would consume the reddit API, when it dawned on me that if my app somehow got up into a significant number of users, the requests would easily be flowing in more than once per 2 seconds.\\n\\nMy question is, then, what are the boundaries of once per 2 seconds? If I distribute a client app, is it once per 2 seconds per user?  What about a centralized web app? Is that still user based, or is the entire web app restricted from making more than 1 request per 2 seconds?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"i7bxj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"StupidLorbie\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/i7bxj/clarification_of_the_1_request_per_2_seconds_rule/\", \"locked\": false, \"name\": \"t3_i7bxj\", \"created\": 1308879327.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/i7bxj/clarification_of_the_1_request_per_2_seconds_rule/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Clarification of the 1 request per 2 seconds rule\", \"created_utc\": 1308850527.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"htfnq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"reseph\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/htfnq/designing_the_tagging_feature_for_subreddits/\", \"locked\": false, \"name\": \"t3_htfnq\", \"created\": 1307446793.0, \"url\": \"http://www.reddit.com/r/modhelp/comments/ht45x/dear_mods_help_me_design_a_new_feature_for/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Designing the 'tagging' feature for subreddits (cross-post)\", \"created_utc\": 1307417993.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI built a collection of comments scraped from reddit.com/comments as they were posted. My goal is to look them up 48 hours or more later and get updated karma details for them. However, the json for /comments doesn\\u0026#39;t give the article id.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWell, if it\\u0026#39;s a root comment for an article, it gives the article id as the parent_id. However, if it\\u0026#39;s a child to another comment, there doesn\\u0026#39;t appear to be any way to look it up.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHave I missed something? Is there a way to do this lookup? Even /api/morechildren requires link_id with the article\\u0026#39;s ID.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny help would be much appreciated. :)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: I\\u0026#39;m a dumbass. /comments has link information. I repurposed code and didn\\u0026#39;t verify its absence on the page. Thanks chime.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I built a collection of comments scraped from reddit.com/comments as they were posted. My goal is to look them up 48 hours or more later and get updated karma details for them. However, the json for /comments doesn't give the article id.\\n\\nWell, if it's a root comment for an article, it gives the article id as the parent_id. However, if it's a child to another comment, there doesn't appear to be any way to look it up.\\n\\nHave I missed something? Is there a way to do this lookup? Even /api/morechildren requires link_id with the article's ID.\\n\\nAny help would be much appreciated. :)\\n\\nEDIT: I'm a dumbass. /comments has link information. I repurposed code and didn't verify its absence on the page. Thanks chime.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"fouzs\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"nexted\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/fouzs/is_it_possible_to_lookup_comments_using_only_the/\", \"locked\": false, \"name\": \"t3_fouzs\", \"created\": 1298208414.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/fouzs/is_it_possible_to_lookup_comments_using_only_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is it possible to lookup comments using only the base-36 id for the comment, and no article name/id?\", \"created_utc\": 1298179614.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EA while ago, I had whipped up a small greasemonkey script to fulfill the role of \\u003Ca href=\\\"http://daringfireball.net/projects/smartypants/\\\"\\u003ESmartyPants\\u003C/a\\u003E for my reddit needs. Among many other issues with Microsoft\\u0026#39;s fine operating system, there\\u0026#39;s no easy way to type in en-dashes and em-dashes\\u2014their solution is to fake it in their products by substituting instances of \\u003Ccode\\u003E--\\u003C/code\\u003E surrounded by words with em-dashes. As you might imagine, this is kind of irritating as the em-dash is an incredibly common form of punctuation. SmartyPants also adds many other features, including TeX-like quotation marks, (c) and (r) symbol replacement, proper ellipsis support, etc. And there are really no downsides! It was \\u003Cem\\u003Edesigned\\u003C/em\\u003E to be integrated in with markdown.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI found myself with a dearth of  projects to do and I was bored, so I thought about giving back to the community by integrating some form of SmartyPants\\u0026#39;s functionality into reddit\\u0026#39;s markdown implementation. When I looked at the source, however, I discovered that there was already support there, but reddit had \\u003Ca href=\\\"http://code.reddit.com/browser/r2/r2/lib/c/reddit-discount-wrapper.c#L70\\\"\\u003Eintentionally disabled it\\u003C/a\\u003E! (\\u003Cem\\u003Egasp here\\u003C/em\\u003E.)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis isn\\u0026#39;t really a complaint so much as my wondering why you disabled it. I can\\u0026#39;t imagine that it\\u0026#39;s considered intrusive, like images or HTML, or that it\\u0026#39;s a safety concern, like the \\u003Ccode\\u003ESAFELINK\\u003C/code\\u003E flag. Are you guys planning on releasing the individual features (like the superscript feature that is in SmartyPants!) as gold features slowly? seeing \\u003Ccode\\u003E\\u0026amp;emdash;\\u003C/code\\u003E littered throughout my normally beautiful comments makes me a little sad.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnyway, here\\u0026#39;s a convenient patch to reenable it:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ediff --git a/r2/r2/lib/c/reddit-discount-wrapper.c b/r2/r2/lib/c/reddit-discount-wrapper.c\\nindex d95b46f..a852315 100644\\n--- a/r2/r2/lib/c/reddit-discount-wrapper.c\\n+++ b/r2/r2/lib/c/reddit-discount-wrapper.c\\n@@ -67,7 +67,7 @@ reddit_discount_wrap(const char * text, int nofollow, const char * target,\\n\\n   mmiot = mkd_string((char *) text, strlen(text), 0);\\n\\n-  mkd_compile(mmiot, MKD_NOHTML | MKD_NOIMAGE | MKD_NOPANTS | MKD_NOHEADER |\\n+  mkd_compile(mmiot, MKD_NOHTML | MKD_NOIMAGE | MKD_NOHEADER |\\n                      MKD_NO_EXT | MKD_AUTOLINK | MKD_SAFELINK);\\n\\n   mkd_e_flags (mmiot, \\u0026amp;cb_flagmaker);\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EA man can hope.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"A while ago, I had whipped up a small greasemonkey script to fulfill the role of [SmartyPants](http://daringfireball.net/projects/smartypants/) for my reddit needs. Among many other issues with Microsoft's fine operating system, there's no easy way to type in en-dashes and em-dashes\\u2014their solution is to fake it in their products by substituting instances of `--` surrounded by words with em-dashes. As you might imagine, this is kind of irritating as the em-dash is an incredibly common form of punctuation. SmartyPants also adds many other features, including TeX-like quotation marks, (c) and (r) symbol replacement, proper ellipsis support, etc. And there are really no downsides! It was *designed* to be integrated in with markdown.\\n\\nI found myself with a dearth of  projects to do and I was bored, so I thought about giving back to the community by integrating some form of SmartyPants's functionality into reddit's markdown implementation. When I looked at the source, however, I discovered that there was already support there, but reddit had [intentionally disabled it](http://code.reddit.com/browser/r2/r2/lib/c/reddit-discount-wrapper.c#L70)! (*gasp here*.)\\n\\nThis isn't really a complaint so much as my wondering why you disabled it. I can't imagine that it's considered intrusive, like images or HTML, or that it's a safety concern, like the `SAFELINK` flag. Are you guys planning on releasing the individual features (like the superscript feature that is in SmartyPants!) as gold features slowly? seeing `\\u0026emdash;` littered throughout my normally beautiful comments makes me a little sad.\\n\\nAnyway, here's a convenient patch to reenable it:\\n\\n\\tdiff --git a/r2/r2/lib/c/reddit-discount-wrapper.c b/r2/r2/lib/c/reddit-discount-wrapper.c\\n\\tindex d95b46f..a852315 100644\\n\\t--- a/r2/r2/lib/c/reddit-discount-wrapper.c\\n\\t+++ b/r2/r2/lib/c/reddit-discount-wrapper.c\\n\\t@@ -67,7 +67,7 @@ reddit_discount_wrap(const char * text, int nofollow, const char * target,\\n\\t \\n\\t   mmiot = mkd_string((char *) text, strlen(text), 0);\\n\\t \\n\\t-  mkd_compile(mmiot, MKD_NOHTML | MKD_NOIMAGE | MKD_NOPANTS | MKD_NOHEADER |\\n\\t+  mkd_compile(mmiot, MKD_NOHTML | MKD_NOIMAGE | MKD_NOHEADER |\\n\\t                      MKD_NO_EXT | MKD_AUTOLINK | MKD_SAFELINK);\\n\\t \\n\\t   mkd_e_flags (mmiot, \\u0026cb_flagmaker);\\n\\n\\nA man can hope.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"e4ezp\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/e4ezp/smartypants_support/\", \"locked\": false, \"name\": \"t3_e4ezp\", \"created\": 1289478021.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/e4ezp/smartypants_support/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Smartypants support?\", \"created_utc\": 1289449221.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have a low-activity reddit installation that sees 10-30 new links a day.  This reddit was running fine on a VM on a provider but about a week ago I transferred the entire installation (code + db) to another VM running on a different provider.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEver since I did the move I\\u0026#39;ve started to get random reddit crashes (about once every 12 hours) and seem to be caused when people add links.  What happens is that a row for the link is created in the reddit_thing_link table but an incomplete or no row is created in the reddit_data_link table.  As a consequence, the link has no sr_id and I get a hard crash with no further pages served until I manually delete the rows.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am pretty sure this is a problem with my configuration as this installation has run fine for over 9 months on the first provider.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have vacuumed the database but the only discernible differences in the configuration are that my original VM was running a 64-bit Ubuntu (postgres version 8.4.4) while the new one is running a 32-bit Ubuntu (postgres version 8.4.5).  I see no clues in the postgres log but obviously data corruption/loss is occurring.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDoes anybody have any ideas how I could proceed on this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have a low-activity reddit installation that sees 10-30 new links a day.  This reddit was running fine on a VM on a provider but about a week ago I transferred the entire installation (code + db) to another VM running on a different provider.\\n\\nEver since I did the move I've started to get random reddit crashes (about once every 12 hours) and seem to be caused when people add links.  What happens is that a row for the link is created in the reddit_thing_link table but an incomplete or no row is created in the reddit_data_link table.  As a consequence, the link has no sr_id and I get a hard crash with no further pages served until I manually delete the rows.\\n\\nI am pretty sure this is a problem with my configuration as this installation has run fine for over 9 months on the first provider.\\n\\nI have vacuumed the database but the only discernible differences in the configuration are that my original VM was running a 64-bit Ubuntu (postgres version 8.4.4) while the new one is running a 32-bit Ubuntu (postgres version 8.4.5).  I see no clues in the postgres log but obviously data corruption/loss is occurring.  \\n\\nDoes anybody have any ideas how I could proceed on this?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"dxn4z\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"gadhaboy\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/dxn4z/any_idea_why_ive_started_losing_rows/\", \"locked\": false, \"name\": \"t3_dxn4z\", \"created\": 1288291005.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/dxn4z/any_idea_why_ive_started_losing_rows/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Any idea why I've started losing rows?\", \"created_utc\": 1288262205.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAlthough I have only just begun to look through the reddit codebase to determine\\nexactly how to do this I\\u0026#39;d like to get some imput on it, and ideally some\\npointers at where the changes might be made to make this happen (I\\u0026#39;ve tracked it\\nas fare as r2.models.account.Account.friends, but I can\\u0026#39;t find where\\nAccount.friend_ids() is defined)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnyway, here\\u0026#39;s the idea\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhere the current comment tagline contains something like\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026lt;a href=\\u0026quot;http://www.reddit.com/user/ketralnis\\u0026quot; class=\\u0026quot;author submitter id-t2_nn0q friend\\u0026quot;\\u0026gt;ketralnis\\u0026lt;/a\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eand reddit.css contains \\n    .tagline .friend   { color:orangered }\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhether or not \\u003Ccode\\u003Efriend\\u003C/code\\u003E ends up in the class is AFAICT related to a test in\\n r2.models.builder.wrap_items line 128, this test would be entirely removed if\\n this proposal or something like it were implemented.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe idea is to take advantage of CSS caching in browsers. That is, supply a custom\\nCSS file per user (in a manner which can be invalidated to force a recache) e.g.\\nfrom reddit.com/bjartr.css?\\u0026lt;some sort of uniquifier\\u0026gt; which would contain e.g\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E.tagline .id-t2_nn0q { color:orangered }\\n.tagline .\\u0026lt;another friend\\u0026#39;s id\\u0026gt; { color:orangered }\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eand so on.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI still have to research where the CSS generation routine would be appropriate\\nand how to do it in such a way that fits the style of the rest of reddit\\u0026#39;s code.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe reason to use CSS directly instead of JS to achevie this is to leverage the\\nrendering engine of the client\\u0026#39;s browser which will be faster than using JS to\\nmodify a page and force subsequent re-renderings.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWorst case is a browser doesn\\u0026#39;t cache CSS at all (rare AFAIK) and the CSS file\\nis served from a reddit side cache (flag on account says whether or not friends\\nlist has changed since it was last generated) and so the actual list of friends\\nwould only ever be queried on first visit or when the friends list changes or\\nthey view their friends list in their preferences. Even if the CSS file is reserved \\nin its entirety every time it only causes two queries (one to see if it \\nneeds to be regenerated and one to get the CSS text) which would mean for\\na logged in user with zero friends that is in the quite unusual position of having \\nbrowser which does not cache CSS there may be an extra query compared to\\nthat same user in the current system. I do not believe this is problematic due to\\nthe rarity of such a browser configuration.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EComments, advice, and critiques are welcome and encouraged. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Although I have only just begun to look through the reddit codebase to determine\\nexactly how to do this I'd like to get some imput on it, and ideally some\\npointers at where the changes might be made to make this happen (I've tracked it\\nas fare as r2.models.account.Account.friends, but I can't find where\\nAccount.friend_ids() is defined)\\n\\nAnyway, here's the idea\\n\\nWhere the current comment tagline contains something like\\n\\n    \\u003Ca href=\\\"http://www.reddit.com/user/ketralnis\\\" class=\\\"author submitter id-t2_nn0q friend\\\"\\u003Eketralnis\\u003C/a\\u003E\\n\\nand reddit.css contains \\n    .tagline .friend   { color:orangered }\\n\\nWhether or not `friend` ends up in the class is AFAICT related to a test in\\n r2.models.builder.wrap_items line 128, this test would be entirely removed if\\n this proposal or something like it were implemented.\\n\\n\\nThe idea is to take advantage of CSS caching in browsers. That is, supply a custom\\nCSS file per user (in a manner which can be invalidated to force a recache) e.g.\\nfrom reddit.com/bjartr.css?\\u003Csome sort of uniquifier\\u003E which would contain e.g\\n\\n    .tagline .id-t2_nn0q { color:orangered }\\n    .tagline .\\u003Canother friend's id\\u003E { color:orangered }\\n\\nand so on.\\n\\nI still have to research where the CSS generation routine would be appropriate\\nand how to do it in such a way that fits the style of the rest of reddit's code.\\n\\nThe reason to use CSS directly instead of JS to achevie this is to leverage the\\nrendering engine of the client's browser which will be faster than using JS to\\nmodify a page and force subsequent re-renderings.\\n\\nWorst case is a browser doesn't cache CSS at all (rare AFAIK) and the CSS file\\nis served from a reddit side cache (flag on account says whether or not friends\\nlist has changed since it was last generated) and so the actual list of friends\\nwould only ever be queried on first visit or when the friends list changes or\\nthey view their friends list in their preferences. Even if the CSS file is reserved \\nin its entirety every time it only causes two queries (one to see if it \\nneeds to be regenerated and one to get the CSS text) which would mean for\\na logged in user with zero friends that is in the quite unusual position of having \\nbrowser which does not cache CSS there may be an extra query compared to\\nthat same user in the current system. I do not believe this is problematic due to\\nthe rarity of such a browser configuration.\\n\\n\\nComments, advice, and critiques are welcome and encouraged. \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"dr81o\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Bjartr\", \"media\": null, \"score\": 11, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/dr81o/looking_into_improving_friends_list_impact_on/\", \"locked\": false, \"name\": \"t3_dr81o\", \"created\": 1287102805.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/dr81o/looking_into_improving_friends_list_impact_on/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Looking into improving friends list impact on performance\", \"created_utc\": 1287074005.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 11}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"imgur.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"dr2h9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"1von\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/dr2h9/uhm_whats_this/\", \"locked\": false, \"name\": \"t3_dr2h9\", \"created\": 1287071535.0, \"url\": \"http://imgur.com/chsuy.png\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"uhm what's this?\", \"created_utc\": 1287042735.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESay I wanted to try to write, for example, a reddit search engine.  I would want the title of each post (ignoring comments) and some additional information (such as submission time, subreddit, etc.) to populate my own local database.  Is there any way to get this information without spidering reddit at a rate of one request every two seconds (which would never finish)?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYes, I\\u0026#39;m aware that this dataset would probably be at least tens of gigabytes uncompressed (hundreds, maybe?).  I\\u0026#39;m just guesstimating the order of magnitude based on the size of Wikipedia dumps.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Say I wanted to try to write, for example, a reddit search engine.  I would want the title of each post (ignoring comments) and some additional information (such as submission time, subreddit, etc.) to populate my own local database.  Is there any way to get this information without spidering reddit at a rate of one request every two seconds (which would never finish)?\\n\\nYes, I'm aware that this dataset would probably be at least tens of gigabytes uncompressed (hundreds, maybe?).  I'm just guesstimating the order of magnitude based on the size of Wikipedia dumps.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"dn556\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"master_rahl\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/dn556/getting_a_large_amount_of_data_from_reddit/\", \"locked\": false, \"name\": \"t3_dn556\", \"created\": 1286322796.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/dn556/getting_a_large_amount_of_data_from_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Getting a large amount of data from reddit\", \"created_utc\": 1286293996.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EYesterday came the great news of the \\u003Ca href=\\\"http://blog.reddit.com/2010/06/weve-open-sourced-ireddit.html\\\"\\u003EReddit iphone app going open source\\u003C/a\\u003E.  That being the case, we fellow redditors over at \\u003Ca href=\\\"http://www.radioreddit.com\\\"\\u003ERadio Reddit\\u003C/a\\u003E wanted to jump on the opportunity to use this to build a Radio Reddit app for the iphone, as well as a foundation for other mobile environments.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EA general concept is as follows:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOn RadioReddit.com, users that are artists or are parts of bands upload their music into our site with all relevant information going into our database.  We then have playlists generated automatically based on genre and \\u003Ca href=\\\"http://www.radioreddit.com/schedule\\\"\\u003Eschedule\\u003C/a\\u003E of said genre.  Users can see what song is playing at the moment on the website and vote it up or down.  An example of the outcome can be seen at \\u003Ca href=\\\"/r/radioreddit\\\"\\u003E/r/radioreddit\\u003C/a\\u003E.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe are thinking that we can adapt the code to incorporate a stream of Radio Reddit and information of what is currently playing while allowing for voting in real time. Our site and database also holds tags, all id3 info, etc so maybe something unique can be done there(thank you \\u003Ca href=\\\"http://www.reddit.com/user/octatone\\\"\\u003Eoctatone\\u003C/a\\u003E and \\u003Ca href=\\\"http://www.reddit.com/user/johncub\\\"\\u003Ejohncub\\u003C/a\\u003E, you are awesome). As we also offer \\u003Ca href=\\\"http://www.radioreddit.com/podcasts\\\"\\u003Epodcasts\\u003C/a\\u003E of every show, we think this might be worth incorporating as well.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you are interested in joining in the fun, you can find us in our \\u003Ca href=\\\"http://www.radioreddit.com/chat\\\"\\u003Echat room\\u003C/a\\u003E or on irc at irc.makethemusic.org in #radioreddit-dev and #radioreddit.  \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Yesterday came the great news of the [Reddit iphone app going open source](http://blog.reddit.com/2010/06/weve-open-sourced-ireddit.html).  That being the case, we fellow redditors over at [Radio Reddit](http://www.radioreddit.com) wanted to jump on the opportunity to use this to build a Radio Reddit app for the iphone, as well as a foundation for other mobile environments.  \\n\\nA general concept is as follows:\\n\\nOn RadioReddit.com, users that are artists or are parts of bands upload their music into our site with all relevant information going into our database.  We then have playlists generated automatically based on genre and [schedule](http://www.radioreddit.com/schedule) of said genre.  Users can see what song is playing at the moment on the website and vote it up or down.  An example of the outcome can be seen at [/r/radioreddit](/r/radioreddit).  \\n\\nWe are thinking that we can adapt the code to incorporate a stream of Radio Reddit and information of what is currently playing while allowing for voting in real time. Our site and database also holds tags, all id3 info, etc so maybe something unique can be done there(thank you [octatone](http://www.reddit.com/user/octatone) and [johncub](http://www.reddit.com/user/johncub), you are awesome). As we also offer [podcasts](http://www.radioreddit.com/podcasts) of every show, we think this might be worth incorporating as well.\\n\\nIf you are interested in joining in the fun, you can find us in our [chat room](http://www.radioreddit.com/chat) or on irc at irc.makethemusic.org in #radioreddit-dev and #radioreddit.  \\n\\n \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"cgkig\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"HarryMuffin\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/cgkig/we_are_building_a_team_of_developers_and_testers/\", \"locked\": false, \"name\": \"t3_cgkig\", \"created\": 1276928422.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/cgkig/we_are_building_a_team_of_developers_and_testers/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"We are building a team of developers and testers for Radio Reddit mobile apps. Inquire within.\", \"created_utc\": 1276899622.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EProbably old news and part of ongoing pattern but perhaps of interest.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve got the dev version of Chrome and had an update to ver 5.0.396.0 with the result that the \\u0026quot;reply\\u0026quot; to comment link no longer opens a comment box.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe error was reported \\u0026amp; I expect it\\u0026#39;ll be addressed.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Probably old news and part of ongoing pattern but perhaps of interest.  \\n  \\nI've got the dev version of Chrome and had an update to ver 5.0.396.0 with the result that the \\\"reply\\\" to comment link no longer opens a comment box.  \\n  \\nThe error was reported \\u0026 I expect it'll be addressed.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"c27ez\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"defrost\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 16, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/c27ez/chrome_broke_reddit_again/\", \"locked\": false, \"name\": \"t3_c27ez\", \"created\": 1273540861.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/c27ez/chrome_broke_reddit_again/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Chrome broke reddit (again)\", \"created_utc\": 1273512061.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIt seems the award system currently checks comments in the last 24 hours and awards to the best ones. However if a comment is posted say one hour before this check it is completely dismissed in the shadow of the winning comment for that day and ignored in the next day.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECan anyone point to where the code that calculates this is so I can have a go at working on a fairer method?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"It seems the award system currently checks comments in the last 24 hours and awards to the best ones. However if a comment is posted say one hour before this check it is completely dismissed in the shadow of the winning comment for that day and ignored in the next day.\\n\\nCan anyone point to where the code that calculates this is so I can have a go at working on a fairer method?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"bc165\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"James_dude\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/bc165/theres_a_small_problem_with_how_awards_are/\", \"locked\": false, \"name\": \"t3_bc165\", \"created\": 1268336925.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/bc165/theres_a_small_problem_with_how_awards_are/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"There's a small problem with how awards are calculated\", \"created_utc\": 1268308125.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi redditdev, I just tried to install a local version of reddit on a fresh install ubuntu-server 9.10. As a disclaimer, I\\u0026#39;ll be honest, as far as linux dev work is concerned, I am a beginner. \\nI used the guide found at \\u003Ca href=\\\"http://code.reddit.com/wiki/RedditStartToFinishIntrepid\\\"\\u003ERedditStartToFinish\\u003C/a\\u003E however I ran into problems so I then used some of SystemicPlural\\u0026#39;s suggestions found in this redditdev \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/ajk0j/problem_with_beautifulsoup_when_setting_reddit_up/\\\"\\u003Epost\\u003C/a\\u003E \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERight after a fresh install of ubuntu-server 9.10, here is my entire command history:\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esudo apt-get install curl gcc gettext git-core libfreetype6 libfreetype6-dev -y\\nsudo apt-get install libjpeg62 libjpeg62-dev libpng12-0 libpq-dev memcached -y\\nsudo apt-get install postgresql-8.3 python python-dev python-setuptools subversion -y\\ngit clone http://code.reddit.com/repo/reddit.git\\nsudo chmod 777 -R ~/reddit\\ncd reddit\\ngit pull\\ncd r2\\nsudo apt-get install libxslt1-dev -y\\nsudo python setup.py develop --find-links http://www.crummy.com/software/BeautifulSoup/download/3.x/\\nmake\\nsudo mkdir /usr/local/pgsql\\nsudo mkdir /usr/local/pgsql/data\\nsudo chown postgres /usr/local/pgsql/data\\nsudo passwd postgres\\nsu postgres\\ncd /home/gadgetsolo/reddit/r2/\\n/usr/lib/postgresql/8.3/bin/initdb -D /usr/local/pgsql/data\\ncreatedb -E utf8 newreddit\\ncreatedb -E utf8 changed\\ncreatedb -E utf8 email\\ncreatedb -E utf8 query_queue\\ncreatedb -E utf8 awards\\ncreatedb -E utf8 award\\ncreatedb -E utf8 authorize\\npsql newreddit \\u0026lt; ../sql/functions.sql\\ncreateuser -P ri\\npaster shell example.ini\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EAfter running the last command, I get this error:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esqlalchemy.exc.ProgrammingError: (ProgrammingError) function hot(integer, integer, timestamp with time zone) does not exist\\nLINE 1: ...idx_hot_reddit_thing_award on reddit_thing_award (hot(ups, d...\\nHINT:  No function matches the given name and argument types. You might need to add explicit type casts.\\n\\u0026#39;create index idx_hot_reddit_thing_award on reddit_thing_award (hot(ups, downs, date), date)\\u0026#39; {}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EIf anyone can spot any errors in my command history, I would really appreciate any feedback. \\nAlso, if anyone has a working version of local reddit running on a fresh install of ubuntu 9.10, it would be great if you could share your command history from start to end. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT1\\u003C/strong\\u003E, Fixed formatting of command history\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi redditdev, I just tried to install a local version of reddit on a fresh install ubuntu-server 9.10. As a disclaimer, I'll be honest, as far as linux dev work is concerned, I am a beginner. \\nI used the guide found at [RedditStartToFinish](http://code.reddit.com/wiki/RedditStartToFinishIntrepid) however I ran into problems so I then used some of SystemicPlural's suggestions found in this redditdev [post](http://www.reddit.com/r/redditdev/comments/ajk0j/problem_with_beautifulsoup_when_setting_reddit_up/) \\n\\nRight after a fresh install of ubuntu-server 9.10, here is my entire command history:\\n\\n----------\\n\\n    sudo apt-get install curl gcc gettext git-core libfreetype6 libfreetype6-dev -y\\n    sudo apt-get install libjpeg62 libjpeg62-dev libpng12-0 libpq-dev memcached -y\\n    sudo apt-get install postgresql-8.3 python python-dev python-setuptools subversion -y\\n    git clone http://code.reddit.com/repo/reddit.git\\n    sudo chmod 777 -R ~/reddit\\n    cd reddit\\n    git pull\\n    cd r2\\n    sudo apt-get install libxslt1-dev -y\\n    sudo python setup.py develop --find-links http://www.crummy.com/software/BeautifulSoup/download/3.x/\\n    make\\n    sudo mkdir /usr/local/pgsql\\n    sudo mkdir /usr/local/pgsql/data\\n    sudo chown postgres /usr/local/pgsql/data\\n    sudo passwd postgres\\n    su postgres\\n    cd /home/gadgetsolo/reddit/r2/\\n    /usr/lib/postgresql/8.3/bin/initdb -D /usr/local/pgsql/data\\n    createdb -E utf8 newreddit\\n    createdb -E utf8 changed\\n    createdb -E utf8 email\\n    createdb -E utf8 query_queue\\n    createdb -E utf8 awards\\n    createdb -E utf8 award\\n    createdb -E utf8 authorize\\n    psql newreddit \\u003C ../sql/functions.sql\\n    createuser -P ri\\n    paster shell example.ini\\n\\n\\n----------\\n\\nAfter running the last command, I get this error:\\n\\n    sqlalchemy.exc.ProgrammingError: (ProgrammingError) function hot(integer, integer, timestamp with time zone) does not exist\\n    LINE 1: ...idx_hot_reddit_thing_award on reddit_thing_award (hot(ups, d...\\n    HINT:  No function matches the given name and argument types. You might need to add explicit type casts.\\n    'create index idx_hot_reddit_thing_award on reddit_thing_award (hot(ups, downs, date), date)' {}\\n\\n***\\nIf anyone can spot any errors in my command history, I would really appreciate any feedback. \\nAlso, if anyone has a working version of local reddit running on a fresh install of ubuntu 9.10, it would be great if you could share your command history from start to end. \\n\\n**EDIT1**, Fixed formatting of command history\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ap45m\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"gadgetSolo\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ap45m/setting_up_reddit_on_fresh_install_of/\", \"locked\": false, \"name\": \"t3_ap45m\", \"created\": 1263421856.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ap45m/setting_up_reddit_on_fresh_install_of/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Setting up reddit on fresh install of ubuntu-server 9.10? i'm stuck. Command history shown in below:\", \"created_utc\": 1263393056.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey Reddit Dev people. Sorry if this isn\\u0026#39;t the right place to post this, but I\\u0026#39;m just getting this started and don\\u0026#39;t know the meta side of Reddit too well yet. I\\u0026#39;m looking to create a Reddit inbox notifier for the Mac, ideally as a menu bar item. I see that you can access \\u003Ca href=\\\"http://www.reddit.com/message/inbox/.json\\\"\\u003Ehttp://www.reddit.com/message/inbox/.json\\u003C/a\\u003E or .xml to view inbox data. My question is if there\\u0026#39;s a good way to programmatically log in to Reddit and fetch this while logged in. I suppose I can always use WebKit and scrape the loginbox, but I was hoping for something a bit more elegant.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey Reddit Dev people. Sorry if this isn't the right place to post this, but I'm just getting this started and don't know the meta side of Reddit too well yet. I'm looking to create a Reddit inbox notifier for the Mac, ideally as a menu bar item. I see that you can access http://www.reddit.com/message/inbox/.json or .xml to view inbox data. My question is if there's a good way to programmatically log in to Reddit and fetch this while logged in. I suppose I can always use WebKit and scrape the loginbox, but I was hoping for something a bit more elegant.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"anq3l\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"SlaunchaMan\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/anq3l/authenticating_to_reddit_api/\", \"locked\": false, \"name\": \"t3_anq3l\", \"created\": 1263125877.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/anq3l/authenticating_to_reddit_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Authenticating to Reddit API\", \"created_utc\": 1263097077.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"7c2e7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ifatree\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/7c2e7/my_reddits_this_is_why_i_love_you_guys/\", \"locked\": false, \"name\": \"t3_7c2e7\", \"created\": 1226130112.0, \"url\": \"http://www.reddit.com/reddits/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"\\\"My Reddits\\\"?! This is why I love you guys.\", \"created_utc\": 1226101312.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"6okvi\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"thehigherlife\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/6okvi/im_really_new_to_all_of_this_where_is_the_color/\", \"locked\": false, \"name\": \"t3_6okvi\", \"created\": 1214261008.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/6okvi/im_really_new_to_all_of_this_where_is_the_color/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"i'm really new to all of this.  where is the color scheme and logos defined in the live code once i have the website running?\", \"created_utc\": 1214232208.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EReproduce:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003Ecurl http://i.redd.it/zldsan2mcq0x.jpg\\u003C/code\\u003E\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EHTTP/1.1 404 Not Found\\nDate: Thu, 02 Jun 2016 00:47:36 GMT\\nContent-Type: application/xml\\nConnection: keep-alive\\nSet-Cookie: __cfduid=dcd4dbef859c4e07cea9ca66e29b7cbfa1464828456; expires=Fri, 02-Jun-17 00:47:36 GMT; path=/; domain=.redd.it; HttpOnly\\nx-amz-request-id: 88EF73A7E117A4C4\\nx-amz-id-2: j1HBaojloU3j3E2wYbL0Oex66M6Ay8PPxlp5CFGdcotrtzJWK/ZcZeflR7YlOmtKT53JssIRdmc=\\nCF-Cache-Status: HIT\\nX-Content-Type-Options: nosniff\\nServer: cloudflare-nginx\\nCF-RAY: 2ac6e51bacc407b5-MIA\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThis url works on chrome and gets redirected to \\u003Ccode\\u003EHTTPS\\u003C/code\\u003E: \\u003Ca href=\\\"http://i.redd.it/zldsan2mcq0x.jpg\\\"\\u003Ehttp://i.redd.it/zldsan2mcq0x.jpg\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Reproduce:\\n\\n`curl http://i.redd.it/zldsan2mcq0x.jpg`\\n\\n    HTTP/1.1 404 Not Found\\n    Date: Thu, 02 Jun 2016 00:47:36 GMT\\n    Content-Type: application/xml\\n    Connection: keep-alive\\n    Set-Cookie: __cfduid=dcd4dbef859c4e07cea9ca66e29b7cbfa1464828456; expires=Fri, 02-Jun-17 00:47:36 GMT; path=/; domain=.redd.it; HttpOnly\\n    x-amz-request-id: 88EF73A7E117A4C4\\n    x-amz-id-2: j1HBaojloU3j3E2wYbL0Oex66M6Ay8PPxlp5CFGdcotrtzJWK/ZcZeflR7YlOmtKT53JssIRdmc=\\n    CF-Cache-Status: HIT\\n    X-Content-Type-Options: nosniff\\n    Server: cloudflare-nginx\\n    CF-RAY: 2ac6e51bacc407b5-MIA\\n    \\nThis url works on chrome and gets redirected to `HTTPS`: http://i.redd.it/zldsan2mcq0x.jpg\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4m4jeb\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"amleszk\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1464828534.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4m4jeb/http_for_ireddit_should_redirect_to_https_instead/\", \"locked\": false, \"name\": \"t3_4m4jeb\", \"created\": 1464856969.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4m4jeb/http_for_ireddit_should_redirect_to_https_instead/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"HTTP for i.redd.it should redirect to HTTPS, instead returns 404\", \"created_utc\": 1464828169.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI know it\\u0026#39;s brand-spanking new.  But can we have the API for \\u003Ca href=\\\"https://www.reddit.com/r/changelog/comments/4kuk2j/reddit_change_introducing_image_uploading_beta/\\\"\\u003Ethis\\u003C/a\\u003E please? Obviously not the actual endpoints before it\\u0026#39;s out of beta, but the doxygen/docstring/whatever-generated description?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I know it's brand-spanking new.  But can we have the API for [this](https://www.reddit.com/r/changelog/comments/4kuk2j/reddit_change_introducing_image_uploading_beta/) please? Obviously not the actual endpoints before it's out of beta, but the doxygen/docstring/whatever-generated description?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4l7okc\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"CogitoErgoReddit\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4l7okc/new_image_uploading_api/\", \"locked\": false, \"name\": \"t3_4l7okc\", \"created\": 1464327851.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4l7okc/new_image_uploading_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New image uploading API?\", \"created_utc\": 1464299051.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m making a bot that submits images. As of now, I upload to imgur and then submit the link, but I\\u0026#39;d like to start using the reddit in house image hosting. How do I do this using praw? I don\\u0026#39;t think I saw anything in the documentation\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm making a bot that submits images. As of now, I upload to imgur and then submit the link, but I'd like to start using the reddit in house image hosting. How do I do this using praw? I don't think I saw anything in the documentation\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4l17dd\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"GoddammitJosh\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4l17dd/is_there_a_reddit_image_upload_api_for_using_the/\", \"locked\": false, \"name\": \"t3_4l17dd\", \"created\": 1464230854.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4l17dd/is_there_a_reddit_image_upload_api_for_using_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a reddit image upload API? (for using the i.reddituploads links)\", \"created_utc\": 1464202054.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello, I am a reddit noob and I\\u0026#39;m trying to build an app that scrapes comments on reddit posts. I want to play by the rules (\\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/API\\\"\\u003Ehttps://github.com/reddit/reddit/wiki/API\\u003C/a\\u003E) , which says \\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EClients must authenticate with OAuth2\\u003C/li\\u003E\\n\\u003Cli\\u003EClients connecting via OAuth2 may make up to 60 requests per minute. \\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EI have successfully got authentication token through plain http request in java.\\nHowever, when I type a url on my browser, appending .json after a post name,  it responds with a json blob of the page, without having to go through OAuth2.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere is my question:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EAm I supposed to embed my token in the request header, even though it seems like it doesn\\u0026#39;t require an authentication to scrape the webpage?  ( for example, \\u003Ca href=\\\"https://www.reddit.com/r/redditdev/4fx4so.json\\\"\\u003Ehttps://www.reddit.com/r/redditdev/4fx4so.json\\u003C/a\\u003E)\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EFor each request for getting comments of a post, does it count as 1 request in the rate limit, or something else?\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EWhat could be the best way to scrape comments of a post?\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EIf there is any rules/info that I missed, kindly let me know. Thanks\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello, I am a reddit noob and I'm trying to build an app that scrapes comments on reddit posts. I want to play by the rules (https://github.com/reddit/reddit/wiki/API) , which says \\n\\n- Clients must authenticate with OAuth2\\n- Clients connecting via OAuth2 may make up to 60 requests per minute. \\n\\nI have successfully got authentication token through plain http request in java.\\nHowever, when I type a url on my browser, appending .json after a post name,  it responds with a json blob of the page, without having to go through OAuth2.\\n\\nHere is my question:\\n\\n1. Am I supposed to embed my token in the request header, even though it seems like it doesn't require an authentication to scrape the webpage?  ( for example, https://www.reddit.com/r/redditdev/4fx4so.json)\\n\\n2. For each request for getting comments of a post, does it count as 1 request in the rate limit, or something else?\\n\\n3. What could be the best way to scrape comments of a post?\\n\\nIf there is any rules/info that I missed, kindly let me know. Thanks\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4fx4so\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"seungkim11\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1461305413.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4fx4so/question_about_reddit_scraping_rules/\", \"locked\": false, \"name\": \"t3_4fx4so\", \"created\": 1461332441.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4fx4so/question_about_reddit_scraping_rules/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"question about reddit scraping rules\", \"created_utc\": 1461303641.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://savvit.soon.it/\\\"\\u003Ehttp://savvit.soon.it/\\u003C/a\\u003E (or if you don\\u0026#39;t mind self-signed SSL, \\u003Ca href=\\\"https://savvit.soon.it/\\\"\\u003Ehttps://savvit.soon.it/\\u003C/a\\u003E)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m not even sure this is the right place to share this app, but I wanted to share it somewhere because I know that at least a few people were looking for something that does this.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis is a very primitive/basic release (and not a very permanent url or ssl), so I\\u0026#39;d love any comments or feedback.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: Here\\u0026#39;s the link to the source: \\u003Ca href=\\\"https://github.com/1npo/savvit\\\"\\u003Ehttps://github.com/1npo/savvit\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"http://savvit.soon.it/ (or if you don't mind self-signed SSL, https://savvit.soon.it/)\\n\\nI'm not even sure this is the right place to share this app, but I wanted to share it somewhere because I know that at least a few people were looking for something that does this.\\n\\nThis is a very primitive/basic release (and not a very permanent url or ssl), so I'd love any comments or feedback.\\n\\nEDIT: Here's the link to the source: https://github.com/1npo/savvit\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4fvx8n\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"1npo\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1461888099.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4fvx8n/i_made_a_webapp_that_downloads_your_saved_links/\", \"locked\": false, \"name\": \"t3_4fvx8n\", \"created\": 1461312429.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4fvx8n/i_made_a_webapp_that_downloads_your_saved_links/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I made a webapp that downloads your 'saved' links as a bookmarks file\", \"created_utc\": 1461283629.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/r/redditdev/comments/3g3hu2/is_there_any_way_to_accurately_determine_whether/\\\"\\u003EI found this thread on the topic but it\\u0026#39;s archived.\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDetermining \\u0026quot;private\\u0026quot; is not a problem because the API will return a type of \\u0026quot;private\\u0026quot;.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve been determining banned/quarantined by looking at the subscriber count, which will be undefined. (Private\\u0026#39;s subscriber count is undefined too, but it returns a type of private. Banned/quarantined return \\u0026quot;public\\u0026quot;.)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"[I found this thread on the topic but it's archived.](https://www.reddit.com/r/redditdev/comments/3g3hu2/is_there_any_way_to_accurately_determine_whether/)\\n\\nDetermining \\\"private\\\" is not a problem because the API will return a type of \\\"private\\\".\\n\\nI've been determining banned/quarantined by looking at the subscriber count, which will be undefined. (Private's subscriber count is undefined too, but it returns a type of private. Banned/quarantined return \\\"public\\\".)\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4dlu24\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"lecherous_hump\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1459951613.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4dlu24/is_there_a_way_to_tell_whether_a_sub_is_banned_or/\", \"locked\": false, \"name\": \"t3_4dlu24\", \"created\": 1459979803.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4dlu24/is_there_a_way_to_tell_whether_a_sub_is_banned_or/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a way to tell whether a sub is banned or quarantined?\", \"created_utc\": 1459951003.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EJust realized that I haven\\u0026#39;t moved a long-running bot (that wakes up every 15 minutes to look at a subreddit) over to use OAuth (\\u003Ca href=\\\"https://www.reddit.com/r/redditdev/comments/3xdf11/introducing_new_api_terms/\\\"\\u003Eas required\\u003C/a\\u003E). It\\u0026#39;s still working after the 3/17 changeover, however.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs this something that I can count on continuing to function, or am I in a grace period?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Just realized that I haven't moved a long-running bot (that wakes up every 15 minutes to look at a subreddit) over to use OAuth ([as required](https://www.reddit.com/r/redditdev/comments/3xdf11/introducing_new_api_terms/)). It's still working after the 3/17 changeover, however.\\n\\nIs this something that I can count on continuing to function, or am I in a grace period?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4brbi2\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"MrSpontaneous\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4brbi2/anonymous_api_access_still_working/\", \"locked\": false, \"name\": \"t3_4brbi2\", \"created\": 1458850862.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4brbi2/anonymous_api_access_still_working/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Anonymous API access still working?\", \"created_utc\": 1458822062.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhile this is great for not having to store the Client secret on the device, how is the API authenticating with just the Client ID? Also, how can I ensure no one uses my refresh token with the client id to obtain an access token?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"While this is great for not having to store the Client secret on the device, how is the API authenticating with just the Client ID? Also, how can I ensure no one uses my refresh token with the client id to obtain an access token?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"472nfh\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"atioxx\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/472nfh/installed_apps_have_no_client_secret_how_does_the/\", \"locked\": false, \"name\": \"t3_472nfh\", \"created\": 1456200010.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/472nfh/installed_apps_have_no_client_secret_how_does_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Installed apps have no client secret. How does the API identifies access from a device to the authentication server without it?\", \"created_utc\": 1456171210.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am writing an app that is going to hook into a users saved, upvoted etc listings and display them, but I don\\u0026#39;t want to be taxing the api by calling it too often. I\\u0026#39;ve searched but been unable to find any webhook or pubsubhubub kind of access so that I can move to a more reactive approach instead of constantly polling every x minutes or hours etc.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there anything that could give me the per user granularity while not being to taxing?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am writing an app that is going to hook into a users saved, upvoted etc listings and display them, but I don't want to be taxing the api by calling it too often. I've searched but been unable to find any webhook or pubsubhubub kind of access so that I can move to a more reactive approach instead of constantly polling every x minutes or hours etc.\\n\\nIs there anything that could give me the per user granularity while not being to taxing?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"46hgd4\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"how_do_i_land\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/46hgd4/pubsubhubub_or_webhook_for_certain_data/\", \"locked\": false, \"name\": \"t3_46hgd4\", \"created\": 1455864245.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/46hgd4/pubsubhubub_or_webhook_for_certain_data/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Pubsubhubub or webhook for certain data?\", \"created_utc\": 1455835445.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;d love to read it if anyone has it. I\\u0026#39;ve been working in it for some time now and am having trouble finding larger sites that are written in it. I\\u0026#39;d like to see how they work.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'd love to read it if anyone has it. I've been working in it for some time now and am having trouble finding larger sites that are written in it. I'd like to see how they work.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3tzotw\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"busterroni\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3tzotw/will_the_code_from_when_reddit_was_written_in/\", \"locked\": false, \"name\": \"t3_3tzotw\", \"created\": 1448341803.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3tzotw/will_the_code_from_when_reddit_was_written_in/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Will the code from when reddit was written in web.py ever be open-sourced?\", \"created_utc\": 1448313003.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m of course aware of PRAW, but it would be much more useful to me to have something using something like the template method pattern to provide more high level behavior. For example having listeners for events like:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003ESubmission Added\\u003C/li\\u003E\\n\\u003Cli\\u003EComment Added\\u003C/li\\u003E\\n\\u003Cli\\u003EComment Edited\\u003C/li\\u003E\\n\\u003Cli\\u003EKeyword Mentioned\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EBasically, is there something that takes the grunt work out of making a Reddit Bot?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf not would there be interest from others to use such a framework if I were to develop it as part of my current project. I\\u0026#39;d probably be coding in Python just because it has the best libraries for what I need, but what could other bot makers use?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm of course aware of PRAW, but it would be much more useful to me to have something using something like the template method pattern to provide more high level behavior. For example having listeners for events like:\\n\\n* Submission Added\\n* Comment Added\\n* Comment Edited\\n* Keyword Mentioned\\n\\nBasically, is there something that takes the grunt work out of making a Reddit Bot?\\n\\nIf not would there be interest from others to use such a framework if I were to develop it as part of my current project. I'd probably be coding in Python just because it has the best libraries for what I need, but what could other bot makers use?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3sk8sy\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"IAPark\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/\", \"locked\": false, \"name\": \"t3_3sk8sy\", \"created\": 1447381618.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is There an Existing Framework for Reddit Bots?\", \"created_utc\": 1447352818.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been transitioning a bot from modhashes to OAuth2.  Unfortunately I\\u0026#39;ve been receiving this message for about 8 out of 10 requests when using the OAuth2 API on EC2 today.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs this something I should expect longterm with OAuth2?  Is EC2 being silently rate limited in a way I should know about?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s a tad frustrating.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EEdit: Here\\u0026#39;s a gist of what I\\u0026#39;m up to, as requested.  \\u003C/p\\u003E\\n\\n\\u003Ch4\\u003E\\u003Cstrong\\u003EStep 1:\\u003C/strong\\u003E Here\\u0026#39;s how I get my access token (grant_type: password) for my \\u003Cstrong\\u003Escript app:\\u003C/strong\\u003E\\u003C/h4\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E$accessTokenUrl = \\u0026#39;https://www.reddit.com/api/v1/access_token\\u0026#39;;\\n//altered id and secret obviously\\n$clientId = \\u0026#39;M5rIsMl2....\\u0026#39;;  \\n$clientSecret = \\u0026#39;5lF3a5aXKnSOBfsfXQZ.....\\u0026#39;;\\n\\n$params = array(\\n  \\u0026quot;username\\u0026quot; =\\u0026gt; \\u0026quot;myalbatross\\u0026quot;,\\n  \\u0026quot;password\\u0026quot; =\\u0026gt; \\u0026quot;hunter2\\u0026quot;,\\n  \\u0026quot;grant_type\\u0026quot; =\\u0026gt; \\u0026quot;password\\u0026quot;,\\n  \\u0026quot;scope\\u0026quot;=\\u0026gt; \\u0026quot;identity,edit,flair,history,modconfig,modflair,modlog,modposts,modwiki,mysubreddits,privatemessages,read,report,save,submit,subscribe,vote,wikiedit,wikiread\\u0026quot;);\\n\\n$endpoint = \\u0026quot;https://www.reddit.com/api/v1/access_token\\u0026quot;;\\n$postData = \\u0026quot;\\u0026quot;;\\n\\n//This is needed to properly form post the credentials object\\nforeach($params as $k =\\u0026gt; $v) {\\n   $postData .= $k . \\u0026#39;=\\u0026#39;.urlencode($v).\\u0026#39;\\u0026amp;\\u0026#39;;\\n}\\n\\n$postData = rtrim($postData, \\u0026#39;\\u0026amp;\\u0026#39;);\\n\\n\\n$curl = curl_init();\\ncurl_setopt($curl, CURLOPT_URL, $accessTokenUrl);\\ncurl_setopt($curl, CURLOPT_USERPWD, $clientId . \\u0026quot;:\\u0026quot; . $clientSecret);  \\ncurl_setopt($curl, CURLOPT_POSTFIELDS, $postData);\\ncurl_setopt($curl, CURLOPT_HEADER, true);\\ncurl_setopt($curl, CURLOPT_RETURNTRANSFER, true);\\ncurl_setopt($curl, CURLOPT_POST, true);\\ncurl_setopt($curl, CURLOPT_USERAGENT, \\u0026#39;User-Agent: Example Bot/0.1 by /u/myalbatross\\u0026#39;);\\ncurl_setopt($curl, CURLOPT_HEADER,\\u0026#39;Content-Type: application/x-www-form-urlencoded\\u0026#39;);\\n\\n$json_response = curl_exec($curl);\\n\\n\\n$info = curl_getinfo($curl);\\nif(!empty($json_response))\\n    $response = json_decode($json_response, true);\\nif(empty($response[\\u0026#39;access_token\\u0026#39;])) \\n    die(\\u0026#39;Failed to get access token\\u0026#39;);\\n\\n$access_token = $response[\\u0026#39;access_token\\u0026#39;];\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch4\\u003E\\u003Cstrong\\u003EStep 2:\\u003C/strong\\u003E And here\\u0026#39;s where I try and \\u003Cem\\u003Euse\\u003C/em\\u003E my access token to update my own flair on a subreddit I moderate.\\u003C/h4\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E$subreddit = \\u0026#39;pics\\u0026#39;;\\n$url = \\u0026quot;https://oauth.reddit.com/r/\\u0026quot;. $subreddit .\\u0026quot;/api/flair.json\\u0026quot;;\\n$authorization = \\u0026quot;Authorization: Bearer \\u0026quot; . $access_token;\\n\\n$userData = array(\\n    \\u0026#39;api_type\\u0026#39; =\\u0026gt; \\u0026#39;json\\u0026#39;,\\n    \\u0026#39;css_class\\u0026#39;=\\u0026gt; \\u0026#39;testCSSClass\\u0026#39;,\\n    \\u0026#39;link\\u0026#39;=\\u0026gt; \\u0026#39;t5_Som1hi1ng\\u0026#39;,\\n    \\u0026#39;name\\u0026#39;=\\u0026gt; \\u0026#39;myalbatross\\u0026#39;,\\n    \\u0026#39;text\\u0026#39;=\\u0026gt; \\u0026#39;TestFlairText\\u0026#39;,\\n    // \\u0026#39;uh\\u0026#39; =\\u0026gt; null // shouldn\\u0026#39;t need this with OAuth2, right?...\\n);  \\n\\n$curl = curl_init();\\n    curl_setopt($curl, CURLOPT_HTTPHEADER, array(\\u0026#39;Content-Type: application/json\\u0026#39; , $authorization ));\\n    curl_setopt($curl, CURLOPT_RETURNTRANSFER, 1);\\n    curl_setopt($curl, CURLOPT_HEADER, 1);\\n    curl_setopt($curl, CURLOPT_URL, $url); \\n    curl_setopt($curl, CURLOPT_POSTFIELDS, http_build_query($userData));\\n    curl_setopt($curl, CURLOPT_CONNECTTIMEOUT, 30);\\n    curl_setopt($curl, CURLOPT_USERAGENT, \\u0026#39;User-Agent: Example Bot 0.1 by /u/myalbatross\\u0026#39;);\\n    curl_setopt($curl, CURLOPT_RETURNTRANSFER, 1);\\n\\n    $contents = curl_exec($curl);\\n\\n    $http_status = curl_getinfo($curl, CURLINFO_HTTP_CODE);\\n    if($http_status==403) {\\n        echo \\u0026quot;\\u0026lt;hr\\u0026gt;\\u0026lt;pre\\u0026gt;Contents:\\n        \\u0026quot;;\\n        print_r($contents);\\n        echo \\u0026quot;\\u0026lt;-End of Contents\\u0026lt;/pre\\u0026gt;\\u0026quot;;\\n    }\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch4\\u003EResult:\\u003C/h4\\u003E\\n\\n\\u003Cp\\u003EMost of the time, I get an \\u0026#39;Our CDN was unable to reach our servers\\u0026#39; along with a picture of sad Snoo.  But, every 10th try or so it \\u003Cem\\u003Edoes\\u003C/em\\u003E work, and I\\u0026#39;ll get a 200 code response complaining about the parameters I\\u0026#39;m passing on this call.  I\\u0026#39;m following the API\\u0026#39;s spec\\u0026#39;s on the parameters as best I can see.  I\\u0026#39;m using code that worked fine with modhashes.  So I\\u0026#39;m baffled.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been transitioning a bot from modhashes to OAuth2.  Unfortunately I've been receiving this message for about 8 out of 10 requests when using the OAuth2 API on EC2 today.\\n\\nIs this something I should expect longterm with OAuth2?  Is EC2 being silently rate limited in a way I should know about?\\n\\nIt's a tad frustrating.\\n\\n\\n---- \\nEdit: Here's a gist of what I'm up to, as requested.  \\n\\n#### **Step 1:** Here's how I get my access token (grant_type: password) for my **script app:**\\n\\n    $accessTokenUrl = 'https://www.reddit.com/api/v1/access_token';\\n    //altered id and secret obviously\\n    $clientId = 'M5rIsMl2....';  \\n    $clientSecret = '5lF3a5aXKnSOBfsfXQZ.....';\\n\\n\\t$params = array(\\n\\t  \\\"username\\\" =\\u003E \\\"myalbatross\\\",\\n\\t  \\\"password\\\" =\\u003E \\\"hunter2\\\",\\n\\t  \\\"grant_type\\\" =\\u003E \\\"password\\\",\\n\\t  \\\"scope\\\"=\\u003E \\\"identity,edit,flair,history,modconfig,modflair,modlog,modposts,modwiki,mysubreddits,privatemessages,read,report,save,submit,subscribe,vote,wikiedit,wikiread\\\");\\n\\t  \\n\\t$endpoint = \\\"https://www.reddit.com/api/v1/access_token\\\";\\n\\t$postData = \\\"\\\";\\n\\n\\t//This is needed to properly form post the credentials object\\n\\tforeach($params as $k =\\u003E $v) {\\n\\t   $postData .= $k . '='.urlencode($v).'\\u0026';\\n\\t}\\n\\n\\t$postData = rtrim($postData, '\\u0026');\\n\\n\\n    $curl = curl_init();\\n\\tcurl_setopt($curl, CURLOPT_URL, $accessTokenUrl);\\n\\tcurl_setopt($curl, CURLOPT_USERPWD, $clientId . \\\":\\\" . $clientSecret);  \\n\\tcurl_setopt($curl, CURLOPT_POSTFIELDS, $postData);\\n\\tcurl_setopt($curl, CURLOPT_HEADER, true);\\n\\tcurl_setopt($curl, CURLOPT_RETURNTRANSFER, true);\\n\\tcurl_setopt($curl, CURLOPT_POST, true);\\n\\tcurl_setopt($curl, CURLOPT_USERAGENT, 'User-Agent: Example Bot/0.1 by /u/myalbatross');\\n\\tcurl_setopt($curl, CURLOPT_HEADER,'Content-Type: application/x-www-form-urlencoded');\\n\\n\\t$json_response = curl_exec($curl);\\n\\n\\n\\t$info = curl_getinfo($curl);\\n\\tif(!empty($json_response))\\n\\t\\t$response = json_decode($json_response, true);\\n\\tif(empty($response['access_token'])) \\n        die('Failed to get access token');\\n\\n    $access_token = $response['access_token'];\\n\\n#### **Step 2:** And here's where I try and *use* my access token to update my own flair on a subreddit I moderate.\\n\\n    $subreddit = 'pics';\\n    $url = \\\"https://oauth.reddit.com/r/\\\". $subreddit .\\\"/api/flair.json\\\";\\n    $authorization = \\\"Authorization: Bearer \\\" . $access_token;\\n\\t\\n\\t$userData = array(\\n\\t\\t'api_type' =\\u003E 'json',\\n\\t\\t'css_class'=\\u003E 'testCSSClass',\\n\\t\\t'link'=\\u003E 't5_Som1hi1ng',\\n\\t\\t'name'=\\u003E 'myalbatross',\\n\\t\\t'text'=\\u003E 'TestFlairText',\\n\\t\\t// 'uh' =\\u003E null // shouldn't need this with OAuth2, right?...\\n\\t);\\t\\n\\n\\t$curl = curl_init();\\n\\t\\tcurl_setopt($curl, CURLOPT_HTTPHEADER, array('Content-Type: application/json' , $authorization ));\\n\\t\\tcurl_setopt($curl, CURLOPT_RETURNTRANSFER, 1);\\n\\t\\tcurl_setopt($curl, CURLOPT_HEADER, 1);\\n\\t\\tcurl_setopt($curl, CURLOPT_URL, $url); \\n\\t\\tcurl_setopt($curl, CURLOPT_POSTFIELDS, http_build_query($userData));\\n\\t\\tcurl_setopt($curl, CURLOPT_CONNECTTIMEOUT, 30);\\n\\t\\tcurl_setopt($curl, CURLOPT_USERAGENT, 'User-Agent: Example Bot 0.1 by /u/myalbatross');\\n\\t\\tcurl_setopt($curl, CURLOPT_RETURNTRANSFER, 1);\\n\\t\\t\\n\\t\\t$contents = curl_exec($curl);\\n\\t\\t\\n\\t\\t$http_status = curl_getinfo($curl, CURLINFO_HTTP_CODE);\\n\\t\\tif($http_status==403) {\\n\\t\\t\\techo \\\"\\u003Chr\\u003E\\u003Cpre\\u003EContents:\\n\\t\\t\\t\\\";\\n\\t\\t\\tprint_r($contents);\\n\\t\\t\\techo \\\"\\u003C-End of Contents\\u003C/pre\\u003E\\\";\\n        }\\n\\n#### Result: \\n\\nMost of the time, I get an 'Our CDN was unable to reach our servers' along with a picture of sad Snoo.  But, every 10th try or so it *does* work, and I'll get a 200 code response complaining about the parameters I'm passing on this call.  I'm following the API's spec's on the parameters as best I can see.  I'm using code that worked fine with modhashes.  So I'm baffled.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3sfq65\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"myalbatross\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 14, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1447315160.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3sfq65/our_cdn_was_unable_to_reach_our_servers/\", \"locked\": false, \"name\": \"t3_3sfq65\", \"created\": 1447296430.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3sfq65/our_cdn_was_unable_to_reach_our_servers/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Our CDN was unable to reach our servers...\", \"created_utc\": 1447267630.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m building a reddit app and was about to implement some error handling when a user tries to vote on an archived post through my app. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENot only was there no error received from the api but checking on reddit.com I was able to actually upvote and change the score of an archived post.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm building a reddit app and was about to implement some error handling when a user tries to vote on an archived post through my app. \\n\\nNot only was there no error received from the api but checking on reddit.com I was able to actually upvote and change the score of an archived post.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3nujcg\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"xCavemanNinjax\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3nujcg/i_can_vote_on_archived_posts_through_reddit_api/\", \"locked\": false, \"name\": \"t3_3nujcg\", \"created\": 1444258933.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3nujcg/i_can_vote_on_archived_posts_through_reddit_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I can vote on archived posts through reddit api\", \"created_utc\": 1444230133.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ECan you guys help me out?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Can you guys help me out?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3jqj22\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"raymestalez\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3jqj22/how_can_i_make_a_list_of_the_top_comments_in_a/\", \"locked\": false, \"name\": \"t3_3jqj22\", \"created\": 1441484441.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3jqj22/how_can_i_make_a_list_of_the_top_comments_in_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How can I make a list of the top comments in a subreddit with PRAW?\", \"created_utc\": 1441455641.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ETitles says it all. Some user contribute a lot but I could be interested in only a subset and this could save bandwidth.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENot much hope of a positive answer for this (otherwise Google would have found it). Unless the same info (at least submissions and corresponding points) is available using a completely different call?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Titles says it all. Some user contribute a lot but I could be interested in only a subset and this could save bandwidth.\\n\\nNot much hope of a positive answer for this (otherwise Google would have found it). Unless the same info (at least submissions and corresponding points) is available using a completely different call?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3j2tom\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ofnuts\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3j2tom/is_there_a_way_to_restrict_uusersubmitted_to/\", \"locked\": false, \"name\": \"t3_3j2tom\", \"created\": 1441053176.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3j2tom/is_there_a_way_to_restrict_uusersubmitted_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a way to restrict /u/{user}/submitted to specific subreddit(s).\", \"created_utc\": 1441024376.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe app is finished and we are very excited to release it so we just wondered if anyone else who has gone through the same process could let us know! Thank you.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: Just to avoid confusion, we are referring to a reddit license: \\u003Ca href=\\\"https://www.reddit.com/wiki/licensing\\\"\\u003Ehttps://www.reddit.com/wiki/licensing\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT 08/09/2015: Still no response :(\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The app is finished and we are very excited to release it so we just wondered if anyone else who has gone through the same process could let us know! Thank you.\\n\\nEDIT: Just to avoid confusion, we are referring to a reddit license: https://www.reddit.com/wiki/licensing\\n\\nEDIT 08/09/2015: Still no response :(\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ic0r6\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"VylarLtd\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 20, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1441702404.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ic0r6/we_applied_for_an_android_license_a_few_days_ago/\", \"locked\": false, \"name\": \"t3_3ic0r6\", \"created\": 1440539709.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ic0r6/we_applied_for_an_android_license_a_few_days_ago/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"We applied for an Android license a few days ago and were wondering how long it usually takes to get a response?\", \"created_utc\": 1440510909.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIn the irc channel someone said that it only worked on Precise Pangolin, I thought it was just the install script that only worked on Precise Pangolin though. Otherwise should it work on other versions of Ubuntu? How about Mint?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI ask because I put Precise Pangolin on a live usb and it kept breaking after about a day of use.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"In the irc channel someone said that it only worked on Precise Pangolin, I thought it was just the install script that only worked on Precise Pangolin though. Otherwise should it work on other versions of Ubuntu? How about Mint?\\n\\nI ask because I put Precise Pangolin on a live usb and it kept breaking after about a day of use.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3gt8fd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"myusernameranoutofsp\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3gt8fd/does_reddit_only_work_on_ubuntu/\", \"locked\": false, \"name\": \"t3_3gt8fd\", \"created\": 1439468784.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3gt8fd/does_reddit_only_work_on_ubuntu/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Does reddit only work on Ubuntu?\", \"created_utc\": 1439439984.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m writing a Drupal module that will integrate some functions with reddit, and ideally I\\u0026#39;d be able to set up a queue in the event that too many requests get made, which is a possibility given my site. I\\u0026#39;ll be using the jcleblanc PHP wrapper for the reddit API.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm writing a Drupal module that will integrate some functions with reddit, and ideally I'd be able to set up a queue in the event that too many requests get made, which is a possibility given my site. I'll be using the jcleblanc PHP wrapper for the reddit API.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3glyfq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"RetroTheft\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3glyfq/whats_the_best_way_to_stay_within_reddits_30/\", \"locked\": false, \"name\": \"t3_3glyfq\", \"created\": 1439339453.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3glyfq/whats_the_best_way_to_stay_within_reddits_30/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"What's the best way to stay within Reddit's 30 requests per minute when writing in PHP?\", \"created_utc\": 1439310653.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EBecause i remember hearing that somewhere, but can\\u0026#39;t find from where. And the whole OAuth thing is so complicated i might as well give up if that\\u0026#39;s needed.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Because i remember hearing that somewhere, but can't find from where. And the whole OAuth thing is so complicated i might as well give up if that's needed.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3dm9af\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MysticPing\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3dm9af/is_praw_login_function_going_to_be_removed/\", \"locked\": false, \"name\": \"t3_3dm9af\", \"created\": 1437164853.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3dm9af/is_praw_login_function_going_to_be_removed/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"is praw login function going to be removed?\", \"created_utc\": 1437136053.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"paste.org\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3d8f0p\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"nakilon\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3d8f0p/why_does_this_get_only_4442_comments_of_7621/\", \"locked\": false, \"name\": \"t3_3d8f0p\", \"created\": 1436893009.0, \"url\": \"http://www.paste.org/flat/79342\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why does this get only 4442 comments of 7621?\", \"created_utc\": 1436864209.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"https://modlog.github.io/#/monitor\\\"\\u003Ehttps://modlog.github.io/#/monitor\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you authorize the bot, it will (for 1 hour at most) run in a browser tab searching for SFW removals and posting them to \\u003Ca href=\\\"/r/modlog\\\"\\u003E/r/modlog\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis project is quite new but I wanted to share it with \\u003Ca href=\\\"/r/redditdev\\\"\\u003E/r/redditdev\\u003C/a\\u003E because I think this type of bot/tool could be a very powerful concept.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s built as a \\u003Ca href=\\\"/r/Emberjs\\\"\\u003E/r/Emberjs\\u003C/a\\u003E app using Snoocore\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"https://modlog.github.io/#/monitor\\n\\nIf you authorize the bot, it will (for 1 hour at most) run in a browser tab searching for SFW removals and posting them to /r/modlog\\n\\nThis project is quite new but I wanted to share it with /r/redditdev because I think this type of bot/tool could be a very powerful concept.\\n\\nIt's built as a /r/Emberjs app using Snoocore\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"36co61\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"go1dfish\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/36co61/modloggithubio_is_a_new_type_of_reddit_bot_it/\", \"locked\": false, \"name\": \"t3_36co61\", \"created\": 1431969677.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/36co61/modloggithubio_is_a_new_type_of_reddit_bot_it/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"modlog.github.io is a new type of reddit bot. It runs in your browser to crowdsource moderation transparency.\", \"created_utc\": 1431940877.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ECurrently when you bring up /api/v1/authorize.compact you get taken to a page that lets you easily navigate away from it using the button in the top right corner. Any navigation using this button breaks the flow and the user will not be redirected back into the application to complete the process to obtain an OAuth token.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso if this button needs to stay then it would be helpful if the user navigates to the create user option with this button that it would be able to carry along the original URL parameters from the first request so that after the create user option completes the user could be redirected back into the application to finish up the OAuth token process.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt would be even nicer to have a totally separate registration OAuth page that would also be able to redirect back into the application for a \\u0026quot;mostly\\u0026quot; seamless way to create users without having to have them type their credentials into the application.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAre there any plans around this considering OAuth will now be required and more important than ever for apps that plan to support it?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Currently when you bring up /api/v1/authorize.compact you get taken to a page that lets you easily navigate away from it using the button in the top right corner. Any navigation using this button breaks the flow and the user will not be redirected back into the application to complete the process to obtain an OAuth token.\\n\\nAlso if this button needs to stay then it would be helpful if the user navigates to the create user option with this button that it would be able to carry along the original URL parameters from the first request so that after the create user option completes the user could be redirected back into the application to finish up the OAuth token process.\\n\\nIt would be even nicer to have a totally separate registration OAuth page that would also be able to redirect back into the application for a \\\"mostly\\\" seamless way to create users without having to have them type their credentials into the application.\\n\\nAre there any plans around this considering OAuth will now be required and more important than ever for apps that plan to support it?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"33fh5p\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"frantic_apparatus\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/33fh5p/request_for_improvements_to_the_oauth_compact/\", \"locked\": false, \"name\": \"t3_33fh5p\", \"created\": 1429698210.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/33fh5p/request_for_improvements_to_the_oauth_compact/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Request for improvements to the OAuth compact login page for mobile applications\", \"created_utc\": 1429669410.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Ehi there. sorry if this isn\\u0026#39;t the best place for troubleshooting the install script.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI ran the script successfully and reddit is running on an AWS instance. However, when visit the site, I get a generic nginx page that says \\u0026quot;Welcome to nginx. If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\\u0026quot; When I visit port 80, I get the same page.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny ideas what could be wrong? Troubleshooting ideas? Or where I can go for help?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: I was able to fix the listening issue. There\\u0026#39;s a line in the default nginx.conf include \\u0026quot;/etc/nginx/conf.d/*.conf;\\u0026quot; which was overriding the listening. Once I corrected that and restarted haproxy,  I was able to match the port listening configuration that I observed on my successful local VM installation.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHowever, I can\\u0026#39;t get reddit to actually launch. Every step seemed to complete successfully except for actually starting reddit.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eusername:~/src/reddit/scripts$ sudo initctl emit reddit-start\\ninitctl: Event failed\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny ideas for troubleshooting \\u0026quot;initctl: Event failed\\u0026quot;?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"hi there. sorry if this isn't the best place for troubleshooting the install script.\\n\\nI ran the script successfully and reddit is running on an AWS instance. However, when visit the site, I get a generic nginx page that says \\\"Welcome to nginx. If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\\\" When I visit port 80, I get the same page.\\n\\nAny ideas what could be wrong? Troubleshooting ideas? Or where I can go for help?\\n\\nEDIT: I was able to fix the listening issue. There's a line in the default nginx.conf include \\\"/etc/nginx/conf.d/*.conf;\\\" which was overriding the listening. Once I corrected that and restarted haproxy,  I was able to match the port listening configuration that I observed on my successful local VM installation.\\n\\nHowever, I can't get reddit to actually launch. Every step seemed to complete successfully except for actually starting reddit.\\n\\nusername:~/src/reddit/scripts$ sudo initctl emit reddit-start\\ninitctl: Event failed\\n\\nAny ideas for troubleshooting \\\"initctl: Event failed\\\"?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2dcv3c\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"lets_get_hyrule\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1407964987.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2dcv3c/newb_question_on_reddit_clone_installed/\", \"locked\": false, \"name\": \"t3_2dcv3c\", \"created\": 1407896363.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2dcv3c/newb_question_on_reddit_clone_installed/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Newb question on reddit clone - installed successfully but not able to reach reddit on port 80\", \"created_utc\": 1407867563.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2d3kw9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Mustermind\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2d3kw9/over_the_past_few_weeks_i_wrote_a_reddit_api/\", \"locked\": false, \"name\": \"t3_2d3kw9\", \"created\": 1407649243.0, \"url\": \"https://github.com/avidw/redd#readme\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Over the past few weeks, I wrote a reddit API wrapper in Ruby. Any thoughts so far?\", \"created_utc\": 1407620443.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi, I\\u0026#39;ve written a script using PRAW to collect user data, but when I fetched the data of \\u003Ca href=\\\"/u/stuff_and_crap\\\"\\u003E/u/stuff_and_crap\\u003C/a\\u003E AVG showed me this popup: \\u003Ca href=\\\"http://i.imgur.com/BOLwFtt.png\\\"\\u003Ehttp://i.imgur.com/BOLwFtt.png\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs this a false positive? As far as I know, I\\u0026#39;m only fetching text.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi, I've written a script using PRAW to collect user data, but when I fetched the data of /u/stuff_and_crap AVG showed me this popup: http://i.imgur.com/BOLwFtt.png\\n\\nIs this a false positive? As far as I know, I'm only fetching text.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2cex9y\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Theemuts\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2cex9y/virus_detected_in_data_returned_by_reddit/\", \"locked\": false, \"name\": \"t3_2cex9y\", \"created\": 1407002745.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2cex9y/virus_detected_in_data_returned_by_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Virus detected in data returned by reddit\", \"created_utc\": 1406973945.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2ak90p\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"calebkeith\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2ak90p/can_we_please_get_messageunread_fixed_for_the/\", \"locked\": false, \"name\": \"t3_2ak90p\", \"created\": 1405253732.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2ak90p/can_we_please_get_messageunread_fixed_for_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can we please get message/unread fixed for the OAuth API?\", \"created_utc\": 1405224932.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}], \"after\": \"t3_2ak90p\", \"before\": null}}"
        },
        "headers": {
          "CF-RAY": "2b59b9b7144120c6-LAX",
          "Connection": "keep-alive",
          "Content-Type": "application/json; charset=UTF-8",
          "Date": "Sun, 19 Jun 2016 20:28:06 GMT",
          "Server": "cloudflare-nginx",
          "Strict-Transport-Security": "max-age=15552000; includeSubDomains; preload",
          "Transfer-Encoding": "chunked",
          "Vary": "accept-encoding",
          "X-Moose": "majestic",
          "cache-control": "private, s-maxage=0, max-age=0, must-revalidate",
          "expires": "-1",
          "x-content-type-options": "nosniff",
          "x-frame-options": "SAMEORIGIN",
          "x-ratelimit-remaining": "582.0",
          "x-ratelimit-reset": "115",
          "x-ratelimit-used": "18",
          "x-reddit-tracking": "https://pixel.redditmedia.com/pixel/of_destiny.png?v=yheHfyicLZVUgELll%2FZ2QYGmMFFiBGx3Wui4q%2F8g6G7B89r88HIhymLNq5nnuUpZEB9khu%2B%2Be52NWmI0e%2Bb%2Bu42Td%2FKCDF4w",
          "x-ua-compatible": "IE=edge",
          "x-xss-protection": "1; mode=block"
        },
        "status": {
          "code": 200,
          "message": "OK"
        },
        "url": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_3vlrnz&raw_json=1"
      }
    },
    {
      "recorded_at": "2016-06-19T20:28:08",
      "request": {
        "body": {
          "encoding": "utf-8",
          "string": ""
        },
        "headers": {
          "Accept": "*/*",
          "Accept-Encoding": "identity",
          "Authorization": "bearer YE4ESXZ9lTjIP0ukcopwXM6361g",
          "Connection": "keep-alive",
          "Cookie": "loid=SdAZIdDQoUDzjNqfRk; loidcreated=2016-06-19T20%3A27%3A59.507Z; __cfduid=da3dbedd6bfa050f4f12fea14395b830f1466368079",
          "User-Agent": "prawtools/0.19 PRAW/4.0.0b4 prawcore/0.0.8"
        },
        "method": "GET",
        "uri": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_2ak90p&raw_json=1"
      },
      "response": {
        "body": {
          "encoding": "UTF-8",
          "string": "{\"kind\": \"Listing\", \"data\": {\"modhash\": null, \"children\": [{\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EOver the past few days I\\u0026#39;ve been getting a large number of 504 errors, including lots of duplicate comments when attempting to retrieve a subreddit\\u0026#39;s comments.  I\\u0026#39;ve put in some retries and the script can eventually make it past, but I\\u0026#39;ve also started getting gateway timeouts that exceed my retry limit.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Over the past few days I've been getting a large number of 504 errors, including lots of duplicate comments when attempting to retrieve a subreddit's comments.  I've put in some retries and the script can eventually make it past, but I've also started getting gateway timeouts that exceed my retry limit.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"25nan4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MrSpontaneous\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1400181404.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/25nan4/praw_reddit_api_504s/\", \"locked\": false, \"name\": \"t3_25nan4\", \"created\": 1400208100.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/25nan4/praw_reddit_api_504s/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[PRAW] Reddit API 504's?\", \"created_utc\": 1400179300.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m just learning how to play with reddit\\u0026#39;s JSON data right now by building an image collector. It\\u0026#39;s going well, but I\\u0026#39;ve run into a block. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m using regex to find every url that ends in a gif/jpg/png/whatever, but this leaves out all links that are of the form \\u0026quot;\\u003Ca href=\\\"http://imgur.com/k8Sq4ms\\\"\\u003Ehttp://imgur.com/k8Sq4ms\\u003C/a\\u003E\\u0026quot;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENow, I could just append \\u0026quot;.jpg\\u0026quot; to all of these links, but that won\\u0026#39;t cover the cases in which they are gifs/pngs/etc. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDoes anyone know a way that I can figure out what the file type is so that I can then append it?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm just learning how to play with reddit's JSON data right now by building an image collector. It's going well, but I've run into a block. \\n\\nI'm using regex to find every url that ends in a gif/jpg/png/whatever, but this leaves out all links that are of the form \\\"http://imgur.com/k8Sq4ms\\\"\\n\\nNow, I could just append \\\".jpg\\\" to all of these links, but that won't cover the cases in which they are gifs/pngs/etc. \\n\\nDoes anyone know a way that I can figure out what the file type is so that I can then append it?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"24e1um\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Chronic8888\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/24e1um/how_to_find_imgur_filetype_with_a_url_not/\", \"locked\": false, \"name\": \"t3_24e1um\", \"created\": 1398912992.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/24e1um/how_to_find_imgur_filetype_with_a_url_not/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to find imgur file-type with a URL not containing one.\", \"created_utc\": 1398884192.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESome API endpoints (like the modlog) contain fullnames for users instead of usernames. Is there some endpoint I can use to convert a fullname for a user into their username? I know about /by_id/ but that appears to be for links only.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Some API endpoints (like the modlog) contain fullnames for users instead of usernames. Is there some endpoint I can use to convert a fullname for a user into their username? I know about /by_id/ but that appears to be for links only.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"23okx5\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Doctor_McKay\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/23okx5/converting_fullnames_to_usernames/\", \"locked\": false, \"name\": \"t3_23okx5\", \"created\": 1398207293.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/23okx5/converting_fullnames_to_usernames/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Converting fullnames to usernames\", \"created_utc\": 1398178493.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis bot will soon become your definitive source for reddit gold stats, and  will help people find exactly how much gold makes up the daily goal. Soon, a website will be set up that gives you graphs and such on reddit gold, but for now its just the bot!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYou can use it by saying- +\\u003Ca href=\\\"/u/goldstatsbot\\\"\\u003E/u/goldstatsbot\\u003C/a\\u003E in your comment! Try it out!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This bot will soon become your definitive source for reddit gold stats, and  will help people find exactly how much gold makes up the daily goal. Soon, a website will be set up that gives you graphs and such on reddit gold, but for now its just the bot!\\n\\nYou can use it by saying- +/u/goldstatsbot in your comment! Try it out!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"21ro51\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Healdb\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 36, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/21ro51/introducing_ugoldstatsbot/\", \"locked\": false, \"name\": \"t3_21ro51\", \"created\": 1396237570.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/21ro51/introducing_ugoldstatsbot/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Introducing /u/goldstatsbot!\", \"created_utc\": 1396208770.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve updated the /prefs/apps view of what apps have access to your account. Now, you\\u0026#39;ll see just one entry on that page per-app. Previously, you\\u0026#39;d see one entry for each active access \\u0026amp; refresh token, which often cluttered the page (particularly when refresh tokens were involved).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFeedback is welcome and desired, both from a user standpoint and a developer standpoint.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAdditionally, I\\u0026#39;m considering putting a restriction on the maximum number of active refresh tokens an app can have per-user. (Google, for example, restricts to \\u003Ca href=\\\"https://developers.google.com/accounts/docs/OAuth2InstalledApp#refresh\\\"\\u003Eone refresh token per client-user combination\\u003C/a\\u003E). If you have any feedback along those lines - particularly with respect to \\u0026quot;how many\\u0026quot; should be allowed - please add that here.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've updated the /prefs/apps view of what apps have access to your account. Now, you'll see just one entry on that page per-app. Previously, you'd see one entry for each active access \\u0026 refresh token, which often cluttered the page (particularly when refresh tokens were involved).\\n\\nFeedback is welcome and desired, both from a user standpoint and a developer standpoint.\\n\\nAdditionally, I'm considering putting a restriction on the maximum number of active refresh tokens an app can have per-user. (Google, for example, restricts to [one refresh token per client-user combination](https://developers.google.com/accounts/docs/OAuth2InstalledApp#refresh)). If you have any feedback along those lines - particularly with respect to \\\"how many\\\" should be allowed - please add that here.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1vvatw\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kemitche\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1vvatw/oauth_prefsapps_now_shows_just_1_entry_per_app/\", \"locked\": false, \"name\": \"t3_1vvatw\", \"created\": 1390444112.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1vvatw/oauth_prefsapps_now_shows_just_1_entry_per_app/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[OAuth] /prefs/apps now shows just 1 entry per app. Refresh token feedback desired.\", \"created_utc\": 1390415312.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI occasionally get people asking if they can browse private subs on my site, but I\\u0026#39;m stumped as to how to make it happen.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAssuming a user has logged in via OAuth, is it possible?  Or would it require something server-side to pass the modhash cookie as well?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I occasionally get people asking if they can browse private subs on my site, but I'm stumped as to how to make it happen.\\n\\nAssuming a user has logged in via OAuth, is it possible?  Or would it require something server-side to pass the modhash cookie as well?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1tyfur\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"radd_it\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1tyfur/is_there_a_way_to_get_json_from_private/\", \"locked\": false, \"name\": \"t3_1tyfur\", \"created\": 1388372978.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1tyfur/is_there_a_way_to_get_json_from_private/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a way to get JSON from private subreddits via Javascript?\", \"created_utc\": 1388344178.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditanalytics.com/searchRecent?query=NSA\\\"\\u003EExample\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EGets the previous 100 comments that contain a certain word or term.\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditanalytics.com/searchRecent?query=NSA\\\"\\u003Ehttp://api.redditanalytics.com/searchRecent?query=NSA\\u003C/a\\u003E  (last 100 comments that contain NSA -- not case-sensitive)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditanalytics.com/searchRecent?query=%23Paul%20Walker%23\\\"\\u003Ehttp://api.redditanalytics.com/searchRecent?query=%23Paul%20Walker%23\\u003C/a\\u003E (last 100 comments about Paul Walker)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditanalytics.com/searchRecent?query=Sun+black+hole\\\"\\u003Ehttp://api.redditanalytics.com/searchRecent?query=Sun+black+hole\\u003C/a\\u003E (last 100 comments with the word sun or black or hole)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditanalytics.com/searchRecent?query=christmas\\\"\\u003Ehttp://api.redditanalytics.com/searchRecent?query=christmas\\u003C/a\\u003E (last 100 comments with the word christmas)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://api.redditanalytics.com/searchRecent?query=python\\\"\\u003Ehttp://api.redditanalytics.com/searchRecent?query=python\\u003C/a\\u003E (last 100 comments with the word Python)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso includes subreddit rankings for the query term.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"[Example](http://api.redditanalytics.com/searchRecent?query=NSA)\\n\\n**Gets the previous 100 comments that contain a certain word or term.**\\n\\nhttp://api.redditanalytics.com/searchRecent?query=NSA  (last 100 comments that contain NSA -- not case-sensitive)\\n\\nhttp://api.redditanalytics.com/searchRecent?query=%23Paul%20Walker%23 (last 100 comments about Paul Walker)\\n\\nhttp://api.redditanalytics.com/searchRecent?query=Sun+black+hole (last 100 comments with the word sun or black or hole)\\n\\nhttp://api.redditanalytics.com/searchRecent?query=christmas (last 100 comments with the word christmas)\\n\\nhttp://api.redditanalytics.com/searchRecent?query=python (last 100 comments with the word Python)\\n\\n\\n\\nAlso includes subreddit rankings for the query term.\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1t3cd0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1387293902.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1t3cd0/new_api_call_search_comments/\", \"locked\": false, \"name\": \"t3_1t3cd0\", \"created\": 1387322470.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1t3cd0/new_api_call_search_comments/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New API call: Search comments\", \"created_utc\": 1387293670.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHow do bots keep up with scanning all the comments on Reddit? It seems like if you can only make a call every 2 seconds some would slip through the cracks, right? Reddit is a big place... Do the big bots just selectively scan certain subreddits and block others?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEspecially for a bot that isn\\u0026#39;t super popular, it would suck if the few instances it was summoned the calls fell in the 2 second down-time.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: I\\u0026#39;ve answered the question on my own and feel a little dumb thinking that Reddit experiences more than 50 comments a second. If you go \\u003Ca href=\\\"http://www.reddit.com/comments\\\"\\u003Ehere\\u003C/a\\u003E it\\u0026#39;s pretty obvious that anyone should be fine making a call even as slowly as every 30 seconds or a minute.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit2: I\\u0026#39;m dumb and \\u003Ca href=\\\"/u/aglidden\\\"\\u003E/u/aglidden\\u003C/a\\u003E showed me how... \\u003Ca href=\\\"http://www.reddit.com/r/all/comments\\\"\\u003Ehere\\u003C/a\\u003E is the real list of all comments being made (the previous link was just your personal frontpage). Some napkin math works it out to about 100 comments every 30 seconds.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"How do bots keep up with scanning all the comments on Reddit? It seems like if you can only make a call every 2 seconds some would slip through the cracks, right? Reddit is a big place... Do the big bots just selectively scan certain subreddits and block others?\\n\\nEspecially for a bot that isn't super popular, it would suck if the few instances it was summoned the calls fell in the 2 second down-time.\\n\\nEdit: I've answered the question on my own and feel a little dumb thinking that Reddit experiences more than 50 comments a second. If you go [here](http://www.reddit.com/comments) it's pretty obvious that anyone should be fine making a call even as slowly as every 30 seconds or a minute.\\n\\nEdit2: I'm dumb and /u/aglidden showed me how... [here](http://www.reddit.com/r/all/comments) is the real list of all comments being made (the previous link was just your personal frontpage). Some napkin math works it out to about 100 comments every 30 seconds.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1raoy2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Bluesroo\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1385238446.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1raoy2/dealing_with_the_1_call_2_seconds_rule/\", \"locked\": false, \"name\": \"t3_1raoy2\", \"created\": 1385258221.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1raoy2/dealing_with_the_1_call_2_seconds_rule/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Dealing with the 1 call/ 2 seconds rule.\", \"created_utc\": 1385229421.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EJust wondering if Reddit plans to index comments for search at some point. Some of the most interesting activity on Reddit happens in the comments, and it would be really cool to be able to analyze that data directly via the API.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Just wondering if Reddit plans to index comments for search at some point. Some of the most interesting activity on Reddit happens in the comments, and it would be really cool to be able to analyze that data directly via the API.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1o32t2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"tacobellscannon\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1o32t2/any_plans_to_make_comments_searchable/\", \"locked\": false, \"name\": \"t3_1o32t2\", \"created\": 1381375218.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1o32t2/any_plans_to_make_comments_searchable/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Any plans to make comments searchable?\", \"created_utc\": 1381346418.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ns6f9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"klassobanieras\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ns6f9/any_interest_in_a_posttoreddit_uiactivity_xpost/\", \"locked\": false, \"name\": \"t3_1ns6f9\", \"created\": 1381005924.0, \"url\": \"http://www.reddit.com/r/iOSProgramming/comments/1ns66q/any_interest_in_a_posttoreddit_uiactivity/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Any interest in a post-to-Reddit UIActivity? [X-post iOSProgramming]\", \"created_utc\": 1380977124.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m looking to develop an addon for user tagging, as \\u003Ca href=\\\"http://www.reddit.com/r/ideasfortheadmins/comments/1nibox/include_original_commenter_label/\\\"\\u003Esuggested here\\u003C/a\\u003E (though it will have a bit more functionality than what he outliend), but I want it to work even after hitting \\u0026quot;continue this thread\\u0026quot;.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI could save the top level commenter\\u0026#39;s user id when the thread initially loads, but then it wouldn\\u0026#39;t work if linked deep into a comment chain.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo is there a way of doing this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm looking to develop an addon for user tagging, as [suggested here](http://www.reddit.com/r/ideasfortheadmins/comments/1nibox/include_original_commenter_label/) (though it will have a bit more functionality than what he outliend), but I want it to work even after hitting \\\"continue this thread\\\".\\n\\nI could save the top level commenter's user id when the thread initially loads, but then it wouldn't work if linked deep into a comment chain.\\n\\nSo is there a way of doing this?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1nragz\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"RainbowCrash\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1380932219.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1nragz/is_there_a_way_to_get_the_top_level_comment_of_a/\", \"locked\": false, \"name\": \"t3_1nragz\", \"created\": 1380960729.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1nragz/is_there_a_way_to_get_the_top_level_comment_of_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a way to get the top level comment of a given comment id with an API call?\", \"created_utc\": 1380931929.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cul\\u003E\\n\\u003Cli\\u003EI know Perl best. I\\u0026#39;d like to use that. \\u003C/li\\u003E\\n\\u003Cli\\u003EI\\u0026#39;d like to learn about the Reddit API. I have no plans to post to Reddit.\\u003C/li\\u003E\\n\\u003Cli\\u003EI\\u0026#39;d like to just count how many users on Reddit have logged in in the last 30 days. \\u003C/li\\u003E\\n\\u003Cli\\u003EI found the API docs but don\\u0026#39;t even know where to start. I guess i need a Perl wrapper? Which I also found. None of the Perl wrappers look mature though based on the version numbers. \\u003C/li\\u003E\\n\\u003Cli\\u003EI also found the API rules.\\u003C/li\\u003E\\n\\u003Cli\\u003EI read I have to register as a Reddit developer? How do I do that?\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EThanks.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"* I know Perl best. I'd like to use that. \\n* I'd like to learn about the Reddit API. I have no plans to post to Reddit.\\n* I'd like to just count how many users on Reddit have logged in in the last 30 days. \\n* I found the API docs but don't even know where to start. I guess i need a Perl wrapper? Which I also found. None of the Perl wrappers look mature though based on the version numbers. \\n* I also found the API rules.\\n* I read I have to register as a Reddit developer? How do I do that?\\n\\nThanks.\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1nl1wd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ta1901\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1nl1wd/programmer_new_to_reddit_api_where_do_i_start/\", \"locked\": false, \"name\": \"t3_1nl1wd\", \"created\": 1380753728.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1nl1wd/programmer_new_to_reddit_api_where_do_i_start/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Programmer, new to Reddit API, where do I start?\", \"created_utc\": 1380724928.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EDissatisfied with the filtering capabilities of a mailing list I\\u0026#39;m on, I got to thinking about a bridge that would synchronise the contents of a mailling list with a dedicated subreddit. In other words, a post or reply on the list would show up as a new topic or a comment in the subreddit, and vice versa.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis prompts a few questions:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EHas someone done this already? I couldn\\u0026#39;t find anything, but perhaps my google-fu was not strong enough.\\u003C/li\\u003E\\n\\u003Cli\\u003EIs there any reason this might be considered against the rules?\\u003C/li\\u003E\\n\\u003Cli\\u003EIs there a way of getting push notifications from reddit.com when a new post or comment appears?\\u003C/li\\u003E\\n\\u003Cli\\u003EDoes anyone else think this might be useful?\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Dissatisfied with the filtering capabilities of a mailing list I'm on, I got to thinking about a bridge that would synchronise the contents of a mailling list with a dedicated subreddit. In other words, a post or reply on the list would show up as a new topic or a comment in the subreddit, and vice versa.\\n\\nThis prompts a few questions:\\n\\n* Has someone done this already? I couldn't find anything, but perhaps my google-fu was not strong enough.\\n* Is there any reason this might be considered against the rules?\\n* Is there a way of getting push notifications from reddit.com when a new post or comment appears?\\n* Does anyone else think this might be useful?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1m8rwn\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"the_imp\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1m8rwn/building_a_subreddit_mailing_list_bridge/\", \"locked\": false, \"name\": \"t3_1m8rwn\", \"created\": 1379019202.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1m8rwn/building_a_subreddit_mailing_list_bridge/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Building a subreddit \\u003C-\\u003E mailing list bridge\", \"created_utc\": 1378990402.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ETalking about the redirect URI on this page:\\n\\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/OAuth2\\\"\\u003Ehttps://github.com/reddit/reddit/wiki/OAuth2\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am using a MEAN stack.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Talking about the redirect URI on this page:\\nhttps://github.com/reddit/reddit/wiki/OAuth2\\n\\nI am using a MEAN stack.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1m55fs\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kevinmrr\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1378862095.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1m55fs/creating_my_first_app_and_planning_to_use_reddit/\", \"locked\": false, \"name\": \"t3_1m55fs\", \"created\": 1378888790.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1m55fs/creating_my_first_app_and_planning_to_use_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Creating my first app and planning to use reddit API. I need to use oauth, so I'm registering for an application id and secret... it's asking for a redirect URI. What should I fill in, since I haven't gone live with a web host?\", \"created_utc\": 1378859990.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m trying to use the JSON API to fetch post listings from subreddits, and there are a lot of fields coming back. Some of them, like \\u0026quot;subreddit\\u0026quot; or \\u0026quot;title\\u0026quot; are obvious, but others like \\u0026quot;distinguished\\u0026quot; or \\u0026quot;media_embed\\u0026quot;, not so much, especially as they seem to return null or empty values most of the time. I\\u0026#39;d really like to know what these fields are used for, so I can figure out what data types they should be converted to (I\\u0026#39;m using a statically typed language, so it\\u0026#39;s kinda important).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI can\\u0026#39;t seem to find a list anywhere. The API page only has function definitions, and I\\u0026#39;d rather not dig through the source code if possible. Does anyone have a full list?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm trying to use the JSON API to fetch post listings from subreddits, and there are a lot of fields coming back. Some of them, like \\\"subreddit\\\" or \\\"title\\\" are obvious, but others like \\\"distinguished\\\" or \\\"media_embed\\\", not so much, especially as they seem to return null or empty values most of the time. I'd really like to know what these fields are used for, so I can figure out what data types they should be converted to (I'm using a statically typed language, so it's kinda important).\\n\\nI can't seem to find a list anywhere. The API page only has function definitions, and I'd rather not dig through the source code if possible. Does anyone have a full list?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ktaz4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ktaz4/is_there_a_list_of_all_the_fields_returned_by_the/\", \"locked\": false, \"name\": \"t3_1ktaz4\", \"created\": 1377132561.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1ktaz4/is_there_a_list_of_all_the_fields_returned_by_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a list of all the fields returned by the JSON API, along with their meanings and (preferably) data types?\", \"created_utc\": 1377103761.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have been having troubles finding out what is necessary to run the reddit source code. What I found on various sites is that (obviously) root access is required, and it is easier to implement with a Linux OS. But as far as the actual hardware goes, how much RAM and CPU is needed?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOne VPS host I found has a cheap option with only a 500Mhz CPU limit, and 512MB dedicated/1GB burst RAM. But it is obviously upgradable. I just don\\u0026#39;t want to over spend.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI tried using the search feature to find out these details, but I couldn\\u0026#39;t find what I was looking for. I apologize if this has actually been asked before and I missed it.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have been having troubles finding out what is necessary to run the reddit source code. What I found on various sites is that (obviously) root access is required, and it is easier to implement with a Linux OS. But as far as the actual hardware goes, how much RAM and CPU is needed?\\n\\nOne VPS host I found has a cheap option with only a 500Mhz CPU limit, and 512MB dedicated/1GB burst RAM. But it is obviously upgradable. I just don't want to over spend.\\n\\nI tried using the search feature to find out these details, but I couldn't find what I was looking for. I apologize if this has actually been asked before and I missed it.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1kco9e\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"sillymod\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1kco9e/systemhosting_requirements_for_a_reddit_clone/\", \"locked\": false, \"name\": \"t3_1kco9e\", \"created\": 1376521958.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1kco9e/systemhosting_requirements_for_a_reddit_clone/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"System/hosting requirements for a reddit clone?\", \"created_utc\": 1376493158.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have an idea for a comment response bot (providing basic facts for a specific subject).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have a little coding background (aka beginner).  Where should I start and what steps do I need to take?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have an idea for a comment response bot (providing basic facts for a specific subject).\\n\\nI have a little coding background (aka beginner).  Where should I start and what steps do I need to take?\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1iktjd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"socioevo\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 23, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1iktjd/how_would_i_start_setting_up_and_coding_a_comment/\", \"locked\": false, \"name\": \"t3_1iktjd\", \"created\": 1374203356.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1iktjd/how_would_i_start_setting_up_and_coding_a_comment/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How would I start setting up and coding a comment response bot?\", \"created_utc\": 1374174556.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;d like to offer users the ability to \\u0026quot;sign in with reddit,\\u0026quot; as opposed to facebook/twitter/google.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs reddit\\u0026#39;s implementation intended to be used that way? Regarding rate limiting, would it be 30/minute globally or per user? I\\u0026#39;d only need the 1 /me.json after auth for each login.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'd like to offer users the ability to \\\"sign in with reddit,\\\" as opposed to facebook/twitter/google.\\n\\nIs reddit's implementation intended to be used that way? Regarding rate limiting, would it be 30/minute globally or per user? I'd only need the 1 /me.json after auth for each login.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1hs5j9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"stickytruth\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1hs5j9/reddit_oauth_as_identity_provider/\", \"locked\": false, \"name\": \"t3_1hs5j9\", \"created\": 1373192391.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1hs5j9/reddit_oauth_as_identity_provider/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit OAuth as identity provider?\", \"created_utc\": 1373163591.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello, Redditdev!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am a UCLA student, and my roommates and I are conducting a large research project on reddit communities. We are planning on taking data hourly from reddit using praw and about 10 VMs for 1 month. The code will not upvote or downvote anything, or interact with reddit in any way other than simply pulling data.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe user-agent names will follow the format\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Euser_agent = \\u0026#39;UCLA Reddit Research Project: __\\u0026#39;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Ewith the __ being filled by the VM number.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI just wanted to let you guys know that these bots are harmless, and also to say thank you so much for your help over the last 6 months as I worked on getting this off the ground! You guys are the best!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello, Redditdev!\\n\\nI am a UCLA student, and my roommates and I are conducting a large research project on reddit communities. We are planning on taking data hourly from reddit using praw and about 10 VMs for 1 month. The code will not upvote or downvote anything, or interact with reddit in any way other than simply pulling data.\\n\\nThe user-agent names will follow the format\\n\\n    user_agent = 'UCLA Reddit Research Project: __'\\n\\nwith the __ being filled by the VM number.\\n\\nI just wanted to let you guys know that these bots are harmless, and also to say thank you so much for your help over the last 6 months as I worked on getting this off the ground! You guys are the best!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1hptt3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MrFanzyPanz\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1hptt3/large_reddit_research_project/\", \"locked\": false, \"name\": \"t3_1hptt3\", \"created\": 1373092695.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Large Reddit Research Project\", \"created_utc\": 1373063895.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"vimeo.com\", \"banned_by\": null, \"media_embed\": {\"content\": \"\\u003Ciframe src=\\\"http://player.vimeo.com/video/10506751\\\" width=\\\"600\\\" height=\\\"338\\\" frameborder=\\\"0\\\" webkitallowfullscreen mozallowfullscreen allowfullscreen\\u003E\\u003C/iframe\\u003E\", \"width\": 600, \"scrolling\": false, \"height\": 338}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1hdqny\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"maqarg\", \"media\": {\"type\": \"vimeo.com\", \"oembed\": {\"provider_url\": \"http://vimeo.com/\", \"description\": \"Steve Huffman at the Future of Web Apps Miami 2010\", \"title\": \"Lessons Learned while at Reddit\", \"type\": \"video\", \"thumbnail_width\": 640, \"height\": 338, \"width\": 600, \"html\": \"\\u003Ciframe src=\\\"http://player.vimeo.com/video/10506751\\\" width=\\\"600\\\" height=\\\"338\\\" frameborder=\\\"0\\\" webkitallowfullscreen mozallowfullscreen allowfullscreen\\u003E\\u003C/iframe\\u003E\", \"author_name\": \"Carsonified\", \"version\": \"1.0\", \"provider_name\": \"Vimeo\", \"thumbnail_url\": \"http://b.vimeocdn.com/ts/552/066/55206655_640.jpg\", \"thumbnail_height\": 360, \"author_url\": \"http://vimeo.com/carsonified\"}}, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1hdqny/lessons_learned_while_at_reddit/\", \"locked\": false, \"name\": \"t3_1hdqny\", \"created\": 1372651374.0, \"url\": \"http://vimeo.com/10506751\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Lessons Learned while at Reddit\", \"created_utc\": 1372622574.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhen getting specific submissions by id, the entire fetch fails if just one id is ungettable. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor instance, the reddit id \\u0026quot;1ghhps\\u0026quot; is not present on the system, and including that id with a list of others in a call to \\u003Ca href=\\\"http://www.reddit.com/by_id/t3_1ghhps,t3_1ghhpt\\\"\\u003Ehttp://www.reddit.com/by_id/t3_1ghhps,t3_1ghhpt\\u003C/a\\u003E will cause that call to fail even though the second id is present in the system.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe system should either fail silently on the missing id and not include it in the returned JSON or include in the JSON wrapper that the id is not present or has been deleted.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT:  The most efficient approach that I can come up with in the event that an error 404 is returned when requesting a batch of 100 id\\u0026#39;s is \\u0026quot;divide and conquer.\\u0026quot;  That means splicing the original request array of ID\\u0026#39;s in half and making a new request for those ID\\u0026#39;s.  Then continue to split the array until the bad ID is the only one left in a request.  The performance for such a situation is probably O(log N) if there is one bad ID out of 100.  That means making a dozen or so requests to get the original batch of 100 ID\\u0026#39;s successfully instead of just making 1 request.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThank you again!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"When getting specific submissions by id, the entire fetch fails if just one id is ungettable. \\n\\nFor instance, the reddit id \\\"1ghhps\\\" is not present on the system, and including that id with a list of others in a call to http://www.reddit.com/by_id/t3_1ghhps,t3_1ghhpt will cause that call to fail even though the second id is present in the system.\\n\\nThe system should either fail silently on the missing id and not include it in the returned JSON or include in the JSON wrapper that the id is not present or has been deleted.\\n\\nThanks!\\n\\nEDIT:  The most efficient approach that I can come up with in the event that an error 404 is returned when requesting a batch of 100 id's is \\\"divide and conquer.\\\"  That means splicing the original request array of ID's in half and making a new request for those ID's.  Then continue to split the array until the bad ID is the only one left in a request.  The performance for such a situation is probably O(log N) if there is one bad ID out of 100.  That means making a dozen or so requests to get the original batch of 100 ID's successfully instead of just making 1 request.\\n\\nThank you again!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1h65v0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1372336789.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1h65v0/request_dont_abandon_the_returned_results_because/\", \"locked\": false, \"name\": \"t3_1h65v0\", \"created\": 1372357782.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1h65v0/request_dont_abandon_the_returned_results_because/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Request: Don't abandon the returned results because one ID is missing in a /by_id/ request.\", \"created_utc\": 1372328982.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"dev.redditanalytics.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1h3ba4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1h3ba4/using_javascript_and_the_reddit_api_to_bring/\", \"locked\": false, \"name\": \"t3_1h3ba4\", \"created\": 1372254740.0, \"url\": \"http://dev.redditanalytics.com/hottest.php\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Using Javascript and the Reddit API to bring interaction to you users via JSONP requests.\", \"created_utc\": 1372225940.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m currently writing a script to allow a user to log in and list their subreddits (basic, I know) but as I\\u0026#39;m very new to Python and PRAW I\\u0026#39;m having to re-run my script a lot to debug my code. The problem I\\u0026#39;m having is that Reddit is detecting too many login requests and locking me out for periods of time (just had to wait an hour to post this) by restricting my IP address. Is there anyway I can get around this by somehow signalling that I am developing and not a spammer?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm currently writing a script to allow a user to log in and list their subreddits (basic, I know) but as I'm very new to Python and PRAW I'm having to re-run my script a lot to debug my code. The problem I'm having is that Reddit is detecting too many login requests and locking me out for periods of time (just had to wait an hour to post this) by restricting my IP address. Is there anyway I can get around this by somehow signalling that I am developing and not a spammer?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1gzdxj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1gzdxj/keep_getting_locked_out_for_too_many_login/\", \"locked\": false, \"name\": \"t3_1gzdxj\", \"created\": 1372126404.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1gzdxj/keep_getting_locked_out_for_too_many_login/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Keep getting locked out for too many login requests. Anything I can do to get around this whilst I'm developing?\", \"created_utc\": 1372097604.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Eusing this url : \\u003Ca href=\\\"https://oauth.reddit.com/api/vote\\\"\\u003Ehttps://oauth.reddit.com/api/vote\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ewith these parameters  : id=[a link id]\\u0026amp;dir=1 \\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eand a POST request\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI get a \\u0026quot;{}\\u0026quot; response with an OK http status but the vote is not getting registered with reddit.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have successfully logged on, grabbed listings etc. I can\\u0026#39;t vote however. The vote endpoint mentions a modhash that I can\\u0026#39;t grab from anywhere using oauth. Is it required?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"using this url : https://oauth.reddit.com/api/vote\\n\\nwith these parameters  : id=[a link id]\\u0026dir=1 \\n\\nand a POST request\\n\\nI get a \\\"{}\\\" response with an OK http status but the vote is not getting registered with reddit.\\n\\nI have successfully logged on, grabbed listings etc. I can't vote however. The vote endpoint mentions a modhash that I can't grab from anywhere using oauth. Is it required?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1dov8h\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Vrokolos\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1dov8h/cant_vote_using_oauth_should_i_just_abandon_oauth/\", \"locked\": false, \"name\": \"t3_1dov8h\", \"created\": 1367721889.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1dov8h/cant_vote_using_oauth_should_i_just_abandon_oauth/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can't vote using oauth. Should I just abandon oauth for reddit? Is it not ready yet?\", \"created_utc\": 1367693089.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m writing a small bot to monitor selected threads and then send me an email when a new comment/post is made. ATM, it\\u0026#39;s running as a cron job that checks every 5 minutes, prints to standard out which then gets emailed to me. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI get error like these, which kills the thread and then sends me email of the error dumps. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eraise HTTPError(http_error_msg, response=self)\\nrequests.exceptions.HTTPError: 504 Server Error: Gateway Time-out\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eraise HTTPError(http_error_msg, response=self)\\nrequests.exceptions.HTTPError: 502 Server Error: Bad Gateway\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eand the like\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat I want\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ea)At least a way to quiet these errors so my inbox doesn\\u0026#39;t get flooded.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eb)Even better would be to somehow handle the error and tell it to retry the request.  I don\\u0026#39;t know how to send a signal back down to the appropriate function/class(?) that is doing the work. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve tried putting stuff in try: and except: and \\u0026quot;pass\\u0026quot;-ing on errors. It seems to work for some errors, but it seems that /some/ exceptions occur anyway. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm writing a small bot to monitor selected threads and then send me an email when a new comment/post is made. ATM, it's running as a cron job that checks every 5 minutes, prints to standard out which then gets emailed to me. \\n\\n \\n\\nI get error like these, which kills the thread and then sends me email of the error dumps. \\n\\n\\n\\nraise HTTPError(http_error_msg, response=self)\\nrequests.exceptions.HTTPError: 504 Server Error: Gateway Time-out\\n\\n\\nraise HTTPError(http_error_msg, response=self)\\nrequests.exceptions.HTTPError: 502 Server Error: Bad Gateway\\n\\n\\nand the like\\n\\n\\nWhat I want\\n\\n\\na)At least a way to quiet these errors so my inbox doesn't get flooded.\\n\\nb)Even better would be to somehow handle the error and tell it to retry the request.  I don't know how to send a signal back down to the appropriate function/class(?) that is doing the work. \\n\\n\\n\\n\\nI've tried putting stuff in try: and except: and \\\"pass\\\"-ing on errors. It seems to work for some errors, but it seems that /some/ exceptions occur anyway. \\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1dmdcz\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"macmouse4\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1367637901.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1dmdcz/praw_and_httperror_handling/\", \"locked\": false, \"name\": \"t3_1dmdcz\", \"created\": 1367621248.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1dmdcz/praw_and_httperror_handling/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"PRAW and HTTPError handling\", \"created_utc\": 1367592448.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello friends,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor a university project, we are trying to retrieve a lot of reddit data from \\u003Ca href=\\\"/r/all\\\"\\u003E/r/all\\u003C/a\\u003E. Using this data, we want to apply machine learning and text mining techniques to hopefully come to some fun conclusions and possibly predict whether or not a post will become popular. ;)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt does, however, seem impractical to get a big database of posts.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe experimented with the csharp wrapper \\u0026#39;RedditSharp\\u0026#39; for the reddit API, but we cannot figure out how to retrieve multiple pages.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOur next step was to write our own fetcher, but we still struggle with the retrieval of multiple pages.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny advice you guys can give would be appreciated.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello friends,\\n\\nFor a university project, we are trying to retrieve a lot of reddit data from /r/all. Using this data, we want to apply machine learning and text mining techniques to hopefully come to some fun conclusions and possibly predict whether or not a post will become popular. ;)\\n\\nIt does, however, seem impractical to get a big database of posts.\\n\\nWe experimented with the csharp wrapper 'RedditSharp' for the reddit API, but we cannot figure out how to retrieve multiple pages.\\n\\nOur next step was to write our own fetcher, but we still struggle with the retrieval of multiple pages.\\n\\nAny advice you guys can give would be appreciated.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1djxf4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"DriesDr\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1djxf4/using_reddit_data_for_textmining_project/\", \"locked\": false, \"name\": \"t3_1djxf4\", \"created\": 1367535059.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1djxf4/using_reddit_data_for_textmining_project/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Using reddit data for text-mining project\", \"created_utc\": 1367506259.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAre there any official rules for bots on top of the \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/API#wiki-rules\\\"\\u003EAPI rules\\u003C/a\\u003E and the general \\u003Ca href=\\\"http://www.reddit.com/rules\\\"\\u003Erules of reddit\\u003C/a\\u003E?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Are there any official rules for bots on top of the [API rules] (https://github.com/reddit/reddit/wiki/API#wiki-rules) and the general [rules of reddit](http://www.reddit.com/rules)?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1dbjvx\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"zlozlozlozlozlozlo\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1dbjvx/what_are_the_rules_for_bots/\", \"locked\": false, \"name\": \"t3_1dbjvx\", \"created\": 1367241027.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1dbjvx/what_are_the_rules_for_bots/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"What are the rules for bots?\", \"created_utc\": 1367212227.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;d like to be able to grab the CAPTCHA image whenever it shows up, present it to the user, and submit the user\\u0026#39;s response back to reddit.  I can\\u0026#39;t seem to find any relevant entries in the official docs -- can someone please point me in the right direction?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI went to the PRAW source code for some insight and found \\u003Ca href=\\\"https://github.com/praw-dev/praw/blob/master/praw/decorators.py#L178\\\"\\u003Ethis\\u003C/a\\u003E decorator function.  I\\u0026#39;m a bit confused as to where \\u003Ccode\\u003Ecaptcha_id\\u003C/code\\u003E is coming from.  I assume it\\u0026#39;s an argument from the wrapped function, but which one?  And who sets it?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI guess this question now boils down to \\u0026quot;how do I get a CAPTCHA id\\u0026quot;? \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'd like to be able to grab the CAPTCHA image whenever it shows up, present it to the user, and submit the user's response back to reddit.  I can't seem to find any relevant entries in the official docs -- can someone please point me in the right direction?\\n\\nI went to the PRAW source code for some insight and found [this](https://github.com/praw-dev/praw/blob/master/praw/decorators.py#L178) decorator function.  I'm a bit confused as to where `captcha_id` is coming from.  I assume it's an argument from the wrapped function, but which one?  And who sets it?\\n\\nI guess this question now boils down to \\\"how do I get a CAPTCHA id\\\"? \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1crgln\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1366494553.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1crgln/how_can_i_grab_a_captcha_image_to_present_to_a/\", \"locked\": false, \"name\": \"t3_1crgln\", \"created\": 1366520955.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1crgln/how_can_i_grab_a_captcha_image_to_present_to_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How can I grab a CAPTCHA image to present to a user?\", \"created_utc\": 1366492155.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been using PRAW to crawl through all of the subreddits, but I\\u0026#39;ve found that some subreddits have been banned (thereby not existing), abruptly stopping the crawling process via a 404 error. Is there a way to test whether a subreddit exists?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been using PRAW to crawl through all of the subreddits, but I've found that some subreddits have been banned (thereby not existing), abruptly stopping the crawling process via a 404 error. Is there a way to test whether a subreddit exists?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1cq7g1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ben444422\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/\", \"locked\": false, \"name\": \"t3_1cq7g1\", \"created\": 1366465641.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to know if subreddit exists?\", \"created_utc\": 1366436841.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1brr3o\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"magnanamos\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1brr3o/using_the_api_to_mark_a_message_as_read_doesnt/\", \"locked\": false, \"name\": \"t3_1brr3o\", \"created\": 1365238144.0, \"url\": \"http://www.reddit.com/r/redditdev/comments/1022k3/using_the_api_to_mark_a_message_as_read_doesnt/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Using the api to mark a message as read doesn't seem to change the has_mail attribute from me.json. Any way around this?\", \"created_utc\": 1365209344.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EElse what about using \\u003Ca href=\\\"https://github.com/praw-dev/praw\\\"\\u003Ehttps://github.com/praw-dev/praw\\u003C/a\\u003E as a Jabber bot like \\u003Ca href=\\\"https://github.com/fritzy/SleekXMPP/wiki\\\"\\u003Ehttps://github.com/fritzy/SleekXMPP/wiki\\u003C/a\\u003E ?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Else what about using https://github.com/praw-dev/praw as a Jabber bot like https://github.com/fritzy/SleekXMPP/wiki ?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1bqlqv\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"utopiah\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1bqlqv/is_there_a_solution_eg_irssi_script_or_bitblee/\", \"locked\": false, \"name\": \"t3_1bqlqv\", \"created\": 1365205480.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1bqlqv/is_there_a_solution_eg_irssi_script_or_bitblee/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a solution (e.g. irssi script or bitblee plugin) to have Reddit inbox and messages in an IRC client?\", \"created_utc\": 1365176680.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1bhyg8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ares623\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1bhyg8/is_the_30_requests_per_minute_calculated_by_user/\", \"locked\": false, \"name\": \"t3_1bhyg8\", \"created\": 1364909349.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1bhyg8/is_the_30_requests_per_minute_calculated_by_user/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is the 30 requests per minute calculated by user? Or by IP Address?\", \"created_utc\": 1364880549.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have \\u003Ca href=\\\"https://github.com/JKirchartz/yinzbot\\\"\\u003Ea bot\\u003C/a\\u003E that sends links to reddit posts, it checks for updates every 10 minutes on a json feed (\\u003Ca href=\\\"http://reddit.com/r/redditdev/.json\\\"\\u003Ehttp://reddit.com/r/redditdev/.json\\u003C/a\\u003E, for example) but it keeps giving me \\u003Ca href=\\\"http://httpstatus.es/429\\\"\\u003E429 errors\\u003C/a\\u003E. I saw that reddit prefers a username in the user-agent, so I added mine and I still get 429 errors. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDo I really need to use oAuth just to get this feed? Or is there another way to avoid 429\\u0026#39;s?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have [a bot](https://github.com/JKirchartz/yinzbot) that sends links to reddit posts, it checks for updates every 10 minutes on a json feed (http://reddit.com/r/redditdev/.json, for example) but it keeps giving me [429 errors](http://httpstatus.es/429). I saw that reddit prefers a username in the user-agent, so I added mine and I still get 429 errors. \\n\\nDo I really need to use oAuth just to get this feed? Or is there another way to avoid 429's?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1b94zr\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"JKirchartz\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1b94zr/hitting_feed_every_10_minutes_still_it_gives_me/\", \"locked\": false, \"name\": \"t3_1b94zr\", \"created\": 1364607922.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1b94zr/hitting_feed_every_10_minutes_still_it_gives_me/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Hitting feed every 10 minutes, still it gives me 429 error most of the time.\", \"created_utc\": 1364579122.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ECurl error: Couldn\\u0026#39;t resolve host \\u0026#39;\\u003Ca href=\\\"http://www.reddit.com\\\"\\u003Ewww.reddit.com\\u003C/a\\u003E\\u0026#39;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECalling reddit with php api wrapper.  I\\u0026#39;ve modified the user-agent call to be unique.  I\\u0026#39;m using the api to get the saved.json data.  The call goes through sometimes but then stops for a while.  Is there a limit that I\\u0026#39;m surpassing?  Any advice would be appreciated.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Curl error: Couldn't resolve host 'www.reddit.com'\\n\\nCalling reddit with php api wrapper.  I've modified the user-agent call to be unique.  I'm using the api to get the saved.json data.  The call goes through sometimes but then stops for a while.  Is there a limit that I'm surpassing?  Any advice would be appreciated.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1b4ywk\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"talaqen\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1b4ywk/calling_api_but_getting_curl_error_couldnt/\", \"locked\": false, \"name\": \"t3_1b4ywk\", \"created\": 1364450380.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1b4ywk/calling_api_but_getting_curl_error_couldnt/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Calling API, but getting Curl Error: \\\"Couldn't resolve host\\\".  Any thoughts\", \"created_utc\": 1364421580.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo I\\u0026#39;m reallllllly new to dev work. I want to just do a simple HTTP GET and search based on a keyword. Does this require Auth or should I be able to do it from a \\u0026quot;public\\u0026quot; point-of-view? What would the proper call be?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So I'm reallllllly new to dev work. I want to just do a simple HTTP GET and search based on a keyword. Does this require Auth or should I be able to do it from a \\\"public\\\" point-of-view? What would the proper call be?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1aixtk\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"stroker351w\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1aixtk/reddit_api_keyword/\", \"locked\": false, \"name\": \"t3_1aixtk\", \"created\": 1363646612.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1aixtk/reddit_api_keyword/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit API Keyword\", \"created_utc\": 1363617812.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am getting started w/ praw-dev and from other posts it looks like there is a limit of 1000 results that cannot be violated. What I am trying to do is to crawl for posts with title string containing key word (say \\u0026quot;happy dog\\u0026quot;) along with an image. I don\\u0026#39;t need the comments. Can anyone please recommend (or point me to sample code) for this ? I am wondering if there is a way to get just the title + image associated without burdening the backend (and hopefully somehow get past the 1000 limit). Thanks! \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI don\\u0026#39;t need to do this real-time - so pacing requests are perfectly fine.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am getting started w/ praw-dev and from other posts it looks like there is a limit of 1000 results that cannot be violated. What I am trying to do is to crawl for posts with title string containing key word (say \\\"happy dog\\\") along with an image. I don't need the comments. Can anyone please recommend (or point me to sample code) for this ? I am wondering if there is a way to get just the title + image associated without burdening the backend (and hopefully somehow get past the 1000 limit). Thanks! \\n\\nI don't need to do this real-time - so pacing requests are perfectly fine.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"19tiyz\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"mrg3_2013\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1362625676.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/19tiyz/using_praw_to_search_posts_with_a_title_keyword/\", \"locked\": false, \"name\": \"t3_19tiyz\", \"created\": 1362653000.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/19tiyz/using_praw_to_search_posts_with_a_title_keyword/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Using PRAW to search posts with a title keyword\", \"created_utc\": 1362624200.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello, Redditdev!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI want to pull the score and author of the comments attached to a link. Preferably on a timely basis. Can get_comments be used on a submission? Is there a way to do this using PRAW that I\\u0026#39;m missing?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks! :D\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EP.S.\\u003C/strong\\u003E Shout out to bboe and Deimorz for all your help!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello, Redditdev!\\n\\nI want to pull the score and author of the comments attached to a link. Preferably on a timely basis. Can get_comments be used on a submission? Is there a way to do this using PRAW that I'm missing?\\n\\nThanks! :D\\n\\n**P.S.** Shout out to bboe and Deimorz for all your help!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"19g943\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MrFanzyPanz\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/19g943/any_way_to_glean_comments_from_a_link/\", \"locked\": false, \"name\": \"t3_19g943\", \"created\": 1362150450.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/19g943/any_way_to_glean_comments_from_a_link/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Any way to glean comments from a link?\", \"created_utc\": 1362121650.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m trying to build an OAuth library for Android and I\\u0026#39;m starting off OAuth2 with Reddit. I have successfully gotten an access token but I can not seem to figure out how to send this to validate requests. I\\u0026#39;m starting on the simple \\u003Ca href=\\\"https://oauth.reddit.com/api/me.json\\\"\\u003Ehttps://oauth.reddit.com/api/me.json\\u003C/a\\u003E endpoint and I\\u0026#39;m putting as Post Data access_token [my access token]. I always receive a 403 forbidden \\u0026quot;request forbidden by administrative rules\\u0026quot;. Are there other parameters I need to send along or am I doing something completely wrong?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm trying to build an OAuth library for Android and I'm starting off OAuth2 with Reddit. I have successfully gotten an access token but I can not seem to figure out how to send this to validate requests. I'm starting on the simple https://oauth.reddit.com/api/me.json endpoint and I'm putting as Post Data access_token [my access token]. I always receive a 403 forbidden \\\"request forbidden by administrative rules\\\". Are there other parameters I need to send along or am I doing something completely wrong?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"197x36\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"schamp557\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/197x36/using_oauth_to_send_valid_requests/\", \"locked\": false, \"name\": \"t3_197x36\", \"created\": 1361860959.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/197x36/using_oauth_to_send_valid_requests/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Using OAuth to send valid requests\", \"created_utc\": 1361832159.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey all,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe want to do some experiments with a fresh installation of reddit, and our sysadmin is asking if we have an equivalent of this script: \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu\\\"\\u003Ehttps://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E... but for CentOS?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m guessing it can be adapted but maybe someone has a script that is already written/tested?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECheers,\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey all,\\n\\nWe want to do some experiments with a fresh installation of reddit, and our sysadmin is asking if we have an equivalent of this script: https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu\\n\\n... but for CentOS?\\n\\nI'm guessing it can be adapted but maybe someone has a script that is already written/tested?\\n\\nCheers,\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"182ggq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"tppiel\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/182ggq/installation_script_for_centos/\", \"locked\": false, \"name\": \"t3_182ggq\", \"created\": 1360279761.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/182ggq/installation_script_for_centos/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Installation script for CentOS?\", \"created_utc\": 1360250961.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhere do i find a log of my gold credit spending, i could see sent messages and all sorts up to my ip address but couldn\\u0026#39;t find it, Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Where do i find a log of my gold credit spending, i could see sent messages and all sorts up to my ip address but couldn't find it, Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"17pbiv\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/17pbiv/where_do_i_find_my_history_or_gold_credit_spending/\", \"locked\": false, \"name\": \"t3_17pbiv\", \"created\": 1359767853.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/17pbiv/where_do_i_find_my_history_or_gold_credit_spending/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Where do i find my History or Gold Credit spending? \", \"created_utc\": 1359739053.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi - I\\u0026#39;m working on a website and I\\u0026#39;d like to allow users to log in with reddit.  I\\u0026#39;m using the ruby library jackdempsey/omniauth-reddit (github).  I get to the login screen ok, but after I log in, I see \\u0026quot;invalid redirect_uri parameter\\u0026quot;.  The URL itself seems to work fine: \\u003Ca href=\\\"http://comments-enabled.com/auth/reddit/callback\\\"\\u003Ehttp://comments-enabled.com/auth/reddit/callback\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe only thing I can think of is that the URL is not https.  Is that a requirement?  I don\\u0026#39;t see that documented anywhere.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi - I'm working on a website and I'd like to allow users to log in with reddit.  I'm using the ruby library jackdempsey/omniauth-reddit (github).  I get to the login screen ok, but after I log in, I see \\\"invalid redirect_uri parameter\\\".  The URL itself seems to work fine: http://comments-enabled.com/auth/reddit/callback\\n\\nThe only thing I can think of is that the URL is not https.  Is that a requirement?  I don't see that documented anywhere.\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"17ka6l\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"datsa\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/17ka6l/oauth_invalid_redirect_uri_parameter/\", \"locked\": false, \"name\": \"t3_17ka6l\", \"created\": 1359586989.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/17ka6l/oauth_invalid_redirect_uri_parameter/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"oauth: invalid redirect_uri parameter\", \"created_utc\": 1359558189.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI want to test my iOS application (a whisky app that can submit reviews in the format expected on \\u003Ca href=\\\"/r/scotch\\\"\\u003E/r/scotch\\u003C/a\\u003E) and was looking for a subreddit that I could test my posts on. Is there such an animal?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI hope this is the right place to post this and if not is there a suggestion for a better subreddit to target?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I want to test my iOS application (a whisky app that can submit reviews in the format expected on /r/scotch) and was looking for a subreddit that I could test my posts on. Is there such an animal?\\n\\nI hope this is the right place to post this and if not is there a suggestion for a better subreddit to target?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"17ibuu\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"speywatch\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/17ibuu/is_there_a_subreddit_to_test_posts_using_the_api/\", \"locked\": false, \"name\": \"t3_17ibuu\", \"created\": 1359513389.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/17ibuu/is_there_a_subreddit_to_test_posts_using_the_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a subreddit to test posts using the API?\", \"created_utc\": 1359484589.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ECould the subreddit stylesheet validation be relaxed to permit \\u003Ca href=\\\"http://www.css3.info/preview/multiple-backgrounds/\\\"\\u003Emultiple background images\\u003C/a\\u003E?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThat would allow a small number of sprites to be combined in a single fake link to form many possible arrangements. Each arrangement would need a single CSS declaration but would not need its own unique sprite.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EA simple example, with 2 layers built selected from 3 sprites (a, b, c) in a single PNG:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ea[href=\\u0026quot;#ab\\u0026quot;]{\\n  background-image: url(%%sprite-t1%%), url(%%sprite-t1%%);\\n  background-position: -100px -100px, -200px -100px;\\n}\\na[href=\\u0026quot;#ac\\u0026quot;]{\\n  background-image: url(%%sprite-t1%%), url(%%sprite-t1%%);\\n  background-position: -100px -100px, -300px -100px;\\n}\\na[href=\\u0026quot;#bc\\u0026quot;]{\\n  background-image: url(%%sprite-t1%%), url(%%sprite-t1%%);\\n  background-position: -200px -100px, -300px -100px;\\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EWith corresponding definitions for \\u003Ccode\\u003E#a\\u003C/code\\u003E, \\u003Ccode\\u003E#b\\u003C/code\\u003E, \\u003Ccode\\u003E#c\\u003C/code\\u003E, just 3 base sprites could be composed into 6 combinations, instead of needing 6 different precomposed sprites. (If the layer ordering matters, then 3 permuted sprites would enable up to 9 distinct effects.)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe flexibility and relative efficiency would be even greater with more sprites or more background layers.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis would allow commenters to build complex characters from simple fake links.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Could the subreddit stylesheet validation be relaxed to permit [multiple background images](http://www.css3.info/preview/multiple-backgrounds/)?\\n\\nThat would allow a small number of sprites to be combined in a single fake link to form many possible arrangements. Each arrangement would need a single CSS declaration but would not need its own unique sprite.\\n\\nA simple example, with 2 layers built selected from 3 sprites (a, b, c) in a single PNG:\\n\\n    a[href=\\\"#ab\\\"]{\\n      background-image: url(%%sprite-t1%%), url(%%sprite-t1%%);\\n      background-position: -100px -100px, -200px -100px;\\n    }\\n    a[href=\\\"#ac\\\"]{\\n      background-image: url(%%sprite-t1%%), url(%%sprite-t1%%);\\n      background-position: -100px -100px, -300px -100px;\\n    }\\n    a[href=\\\"#bc\\\"]{\\n      background-image: url(%%sprite-t1%%), url(%%sprite-t1%%);\\n      background-position: -200px -100px, -300px -100px;\\n    }\\n\\nWith corresponding definitions for `#a`, `#b`, `#c`, just 3 base sprites could be composed into 6 combinations, instead of needing 6 different precomposed sprites. (If the layer ordering matters, then 3 permuted sprites would enable up to 9 distinct effects.)\\n\\nThe flexibility and relative efficiency would be even greater with more sprites or more background layers.\\n\\nThis would allow commenters to build complex characters from simple fake links.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"17h0al\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Quintysential\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/17h0al/enable_multiple_backgroundimage_urls_in_css/\", \"locked\": false, \"name\": \"t3_17h0al\", \"created\": 1359456908.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/17h0al/enable_multiple_backgroundimage_urls_in_css/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Enable multiple background-image URLs in CSS\", \"created_utc\": 1359428108.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi guys,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve just started using PRAW today and I\\u0026#39;m absolutely loving it.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI did run into some issues while writing a bot. It\\u0026#39;s supposed to check a specific subreddit for the newest posts, read their contents and alert me whenever it finds something that matches a certain regular expression within them.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m checking for new submissions every 10 seconds, by calling s.get_new_by_date(limit=5) on my subreddit\\u0026#39;s Subreddit object. The problem is that after the initial call, new posts are usually only fetched after anywhere from 30 seconds to a few minutes AFTER they were created, where I expect them to show up within a maximal amount of 10 seconds from the moment they were created. They do appear when I refresh the page in my browser.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThings I tried doing to resolve this:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EMy user agent follows the API guidelines and isn\\u0026#39;t generic.\\u003C/li\\u003E\\n\\u003Cli\\u003ESetting my local praw.ini file so that cache_timeout=5 to ensure I\\u0026#39;m not reading from the cache.\\u003C/li\\u003E\\n\\u003Cli\\u003ECalling s.refresh() on my Subreddit object after every sleep(10).\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EDo you guys have any idea what might be causing this to happen? Is it because pages are cached server-side, and so I cannot force a refresh on the user\\u0026#39;s side? If so, why do pages usually take more than a minute (which is far more than the forced 30-second limitation if there is in fact one) to be detected by my algorithm?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks for reading, I appreciate your time.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIcy\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi guys,\\n\\nI've just started using PRAW today and I'm absolutely loving it.\\n\\nI did run into some issues while writing a bot. It's supposed to check a specific subreddit for the newest posts, read their contents and alert me whenever it finds something that matches a certain regular expression within them.\\n\\nI'm checking for new submissions every 10 seconds, by calling s.get_new_by_date(limit=5) on my subreddit's Subreddit object. The problem is that after the initial call, new posts are usually only fetched after anywhere from 30 seconds to a few minutes AFTER they were created, where I expect them to show up within a maximal amount of 10 seconds from the moment they were created. They do appear when I refresh the page in my browser.\\n\\nThings I tried doing to resolve this:\\n\\n- My user agent follows the API guidelines and isn't generic.\\n- Setting my local praw.ini file so that cache_timeout=5 to ensure I'm not reading from the cache.\\n- Calling s.refresh() on my Subreddit object after every sleep(10).\\n\\nDo you guys have any idea what might be causing this to happen? Is it because pages are cached server-side, and so I cannot force a refresh on the user's side? If so, why do pages usually take more than a minute (which is far more than the forced 30-second limitation if there is in fact one) to be detected by my algorithm?\\n\\n\\nThanks for reading, I appreciate your time.\\n\\nIcy\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"17aiml\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"IcyRespawn\", \"media\": null, \"score\": 10, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/17aiml/praw_getting_newest_posts_to_show_up_as_quickly/\", \"locked\": false, \"name\": \"t3_17aiml\", \"created\": 1359192492.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/17aiml/praw_getting_newest_posts_to_show_up_as_quickly/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[PRAW] Getting newest posts to show up as quickly as  possible\", \"created_utc\": 1359163692.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 10}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe confidence sort seem to rank comments, dose \\u0026#39;top\\u0026#39; item use the same algorithm? If not, how does \\u0026#39;top\\u0026#39; item sort? \\nI\\u0026#39;m not familiar with python, your reply can save my time. so thanks in advance\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The confidence sort seem to rank comments, dose 'top' item use the same algorithm? If not, how does 'top' item sort? \\nI'm not familiar with python, your reply can save my time. so thanks in advance\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"16v5yg\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"zhou533\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/16v5yg/how_does_top_item_sort/\", \"locked\": false, \"name\": \"t3_16v5yg\", \"created\": 1358606046.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/16v5yg/how_does_top_item_sort/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"how does 'top' item sort?\", \"created_utc\": 1358577246.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis feature is available in about/stylesheet, but afaik it\\u0026#39;s CSS only and entirely server side. It would be nice if we could enable this on ALL code blocks.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have noticed there is some remnants of adding this functionality in older commits in the Snudown repository, but it\\u0026#39;s been removed for whatever reason. Is it worth discussing how we might do this?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe simplest solution would be to include a client-side JS highlighter in this repo. in the Snudown repo, the changes will be very minimal involving the inclusion of a class directive in the \\u0026lt;pre\\u0026gt; or \\u0026lt;code\\u0026gt; blocks \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026lt;code class=\\u0026quot;prettyprint\\u0026quot;\\u0026gt;...\\u0026lt;/code\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThe big issue is the impact the extra overhead incurred by loading a client side highlighter.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThoughts?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This feature is available in about/stylesheet, but afaik it's CSS only and entirely server side. It would be nice if we could enable this on ALL code blocks.\\n\\nI have noticed there is some remnants of adding this functionality in older commits in the Snudown repository, but it's been removed for whatever reason. Is it worth discussing how we might do this?\\n\\nThe simplest solution would be to include a client-side JS highlighter in this repo. in the Snudown repo, the changes will be very minimal involving the inclusion of a class directive in the \\u003Cpre\\u003E or \\u003Ccode\\u003E blocks \\n    \\n    \\u003Ccode class=\\\"prettyprint\\\"\\u003E...\\u003C/code\\u003E\\n\\nThe big issue is the impact the extra overhead incurred by loading a client side highlighter.\\n\\nThoughts?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"16esdb\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"domlebo70\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1357951000.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/16esdb/language_specific_syntax_highlighting_in_code/\", \"locked\": false, \"name\": \"t3_16esdb\", \"created\": 1357978889.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/16esdb/language_specific_syntax_highlighting_in_code/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Language specific syntax highlighting in \\u003Ccode\\u003E blocks. Options?\", \"created_utc\": 1357950089.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m trying to add in an automated fashion ~2000 approved wiki contributors to our wiki.  (At askscience we want all ~2000 or so panelists to be able to edit the wiki, but not have general users edited it.)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFirst, I have tried doing it through the API, but couldn\\u0026#39;t figure out how to do it to add as wikicontributor to a subreddit not to tied to specific wiki page and was getting 403 errors.  Details on my API attempt at the bottom.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENext, I tried doing this through client-side javascript.  Go to the add wiki contributors page manually, logged in as myself, populate an array of users to add (which we maintain in a spreadsheet) and then use the following to add the users (with a delay of adding 1 user every 2 seconds originally; and 5 seconds on my second attempt).  (Yes jQuery could do this more concisely, but basically this just types the name into each form and then simulates a click of the button with a delay of two seconds - using an IIFE and setTimeout).\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Efor(i=0; i\\u0026lt; users.length; i++) {\\nsetTimeout(\\n   (function(user) {\\n       return function() {\\n          var a = document.getElementById(\\u0026#39;name\\u0026#39;);\\n          a.value = user;\\n          var b = document.getElementsByClassName(\\u0026#39;btn\\u0026#39;)[0];\\n          var event = document.createEvent(\\u0026#39;UIEvents\\u0026#39;);\\n          event.initUIEvent(\\u0026#39;click\\u0026#39;, true, true, window, 1);\\n          b.dispatchEvent(event);\\n      }\\n   })(users[i]), 80000*i);\\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EHowever, after I added about ~80 contributors last night (at a rate of 1 every 2seconds), I got SUBREDDIT_RATELIMIT error message.  So this morning I tried again, and was able to get another ~80 or so in before the same error message popped up.  This time adding at a rate of 1 every \\u003Cdel\\u003E5 seconds\\u003C/del\\u003E, \\u003Cdel\\u003E10 seconds\\u003C/del\\u003E, \\u003Cdel\\u003E30 seconds\\u003C/del\\u003E, \\u003Cem\\u003E80 seconds is fine, as is everything over 72 seconds\\u003C/em\\u003E,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there any published limits to avoid this SUBREDDIT_RATELIMIT?  I really want to add ~2000 or so users.\\u003C/p\\u003E\\n\\n\\u003Ch3\\u003EAPI Attempt:\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EUsing \\u003Ca href=\\\"http://www.reddit.com/dev/api#POST_api_wiki_alloweditor_:act\\\"\\u003Ehttp://www.reddit.com/dev/api#POST_api_wiki_alloweditor_:act\\u003C/a\\u003E\\nI tried something like (using python requests module):\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E import requests\\n headers = {\\u0026#39;User-Agent\\u0026#39;: \\u0026#39;djimbob setting adding ask science panelists\\u0026#39;}\\n requests.post(\\u0026#39;http://www.reddit.com/api/wiki/alloweditor/add\\u0026#39;, data=dict(act=\\u0026#39;add\\u0026#39;, page=\\u0026#39;/r/askscience/wiki/\\u0026#39;, uh=modhash, username=\\u0026#39;asksci_throwaway\\u0026#39;), headers=headers)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Ebut kept getting 403\\u0026#39;s \\u0026quot;forbidden - forbidden (reddit.com); you are not allowed to do that - wiki_disabled.\\u0026quot; (even though if I went to \\u003Ca href=\\\"http://www.reddit.com/r/askscience/about/wikicontributors/\\\"\\u003Ehttp://www.reddit.com/r/askscience/about/wikicontributors/\\u003C/a\\u003E\\nI could add contributors).   I think this is because it was trying to add editors only to a specific page; which is not what I want.  We want the index only editable by mods; the other pages edited by all panelists.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm trying to add in an automated fashion ~2000 approved wiki contributors to our wiki.  (At askscience we want all ~2000 or so panelists to be able to edit the wiki, but not have general users edited it.)\\n\\nFirst, I have tried doing it through the API, but couldn't figure out how to do it to add as wikicontributor to a subreddit not to tied to specific wiki page and was getting 403 errors.  Details on my API attempt at the bottom.\\n\\nNext, I tried doing this through client-side javascript.  Go to the add wiki contributors page manually, logged in as myself, populate an array of users to add (which we maintain in a spreadsheet) and then use the following to add the users (with a delay of adding 1 user every 2 seconds originally; and 5 seconds on my second attempt).  (Yes jQuery could do this more concisely, but basically this just types the name into each form and then simulates a click of the button with a delay of two seconds - using an IIFE and setTimeout).\\n\\n    for(i=0; i\\u003C users.length; i++) {\\n    setTimeout(\\n       (function(user) {\\n           return function() {\\n              var a = document.getElementById('name');\\n              a.value = user;\\n              var b = document.getElementsByClassName('btn')[0];\\n              var event = document.createEvent('UIEvents');\\n              event.initUIEvent('click', true, true, window, 1);\\n              b.dispatchEvent(event);\\n          }\\n       })(users[i]), 80000*i);\\n    }\\n\\nHowever, after I added about ~80 contributors last night (at a rate of 1 every 2seconds), I got SUBREDDIT_RATELIMIT error message.  So this morning I tried again, and was able to get another ~80 or so in before the same error message popped up.  This time adding at a rate of 1 every ~~5 seconds~~, ~~10 seconds~~, ~~30 seconds~~, *80 seconds is fine, as is everything over 72 seconds*,\\n\\nIs there any published limits to avoid this SUBREDDIT_RATELIMIT?  I really want to add ~2000 or so users.\\n\\n### API Attempt:\\n\\nUsing http://www.reddit.com/dev/api#POST_api_wiki_alloweditor_:act\\nI tried something like (using python requests module):\\n\\n     import requests\\n     headers = {'User-Agent': 'djimbob setting adding ask science panelists'}\\n     requests.post('http://www.reddit.com/api/wiki/alloweditor/add', data=dict(act='add', page='/r/askscience/wiki/', uh=modhash, username='asksci_throwaway'), headers=headers)\\n\\nbut kept getting 403's \\\"forbidden - forbidden (reddit.com); you are not allowed to do that - wiki_disabled.\\\" (even though if I went to http://www.reddit.com/r/askscience/about/wikicontributors/\\nI could add contributors).   I think this is because it was trying to add editors only to a specific page; which is not what I want.  We want the index only editable by mods; the other pages edited by all panelists.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"15olbr\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"djimbob\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1356974905.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/15olbr/adding_2000_wikicontributors_cant_use_api_simple/\", \"locked\": false, \"name\": \"t3_15olbr\", \"created\": 1356923047.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/15olbr/adding_2000_wikicontributors_cant_use_api_simple/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Adding ~2000 WikiContributors - Can't use API; simple client side script running into \\\"SUBREDDIT_RATELIMIT\\\"\", \"created_utc\": 1356894247.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ECould someone be so kind as to direct me to any documentation that may exist for using praw to create a new or even write to an existing CSS of a subreddit?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Could someone be so kind as to direct me to any documentation that may exist for using praw to create a new or even write to an existing CSS of a subreddit?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"15nuau\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"lamerx\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/15nuau/praw_css/\", \"locked\": false, \"name\": \"t3_15nuau\", \"created\": 1356875433.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/15nuau/praw_css/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Praw + CSS\", \"created_utc\": 1356846633.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESurely this isn\\u0026#39;t that hard to do? I need to receive the images a subreddit\\u0026#39;s using and without OAuth that goes fine (trough \\u003Ca href=\\\"/r/subreddit/about/stylesheet\\\"\\u003E/r/subreddit/about/stylesheet\\u003C/a\\u003E) but when trying to get it trough oauth (\\u003Ca href=\\\"https://oauth.reddit.com/r/subreddit/about/stylesheet.json\\\"\\u003Ehttps://oauth.reddit.com/r/subreddit/about/stylesheet.json\\u003C/a\\u003E) results in a 400 error (which seems to be the 404 of the OAuth domain)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Surely this isn't that hard to do? I need to receive the images a subreddit's using and without OAuth that goes fine (trough /r/subreddit/about/stylesheet) but when trying to get it trough oauth (https://oauth.reddit.com/r/subreddit/about/stylesheet.json) results in a 400 error (which seems to be the 404 of the OAuth domain)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"15ku6v\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MoederPoeder\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/15ku6v/please_add_rsubredditaboutstylesheet_to_oauth/\", \"locked\": false, \"name\": \"t3_15ku6v\", \"created\": 1356742402.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/15ku6v/please_add_rsubredditaboutstylesheet_to_oauth/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Please add /r/subreddit/about/stylesheet to OAuth!\", \"created_utc\": 1356713602.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhen submitting a youtube, soundcloud, or bandcamp link reddit parses the site\\u0026#39;s meta tags to scrape thumbnail and video url info.  Is it possible for me to implement this on my own website? \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"When submitting a youtube, soundcloud, or bandcamp link reddit parses the site's meta tags to scrape thumbnail and video url info.  Is it possible for me to implement this on my own website? \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"15eydf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"RinseIt\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/15eydf/video_embed_meta_tags_that_reddit_can_parse_such/\", \"locked\": false, \"name\": \"t3_15eydf\", \"created\": 1356454798.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/15eydf/video_embed_meta_tags_that_reddit_can_parse_such/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Video embed meta tags that reddit can parse (such as done with youtube, soundcloud, bandcamp)\", \"created_utc\": 1356425998.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Cem\\u003ENot sure if this is where to put it, but I found a coding issue in the site\\u0026#39;s HTML:\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIn the awards help page, the table in the bottom just under:\\n\\u0026quot;There are also some special trophies that are rarer/better than awards:\\u0026quot; is broken.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERemoving these:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cpre\\u003E\\u003Ccode\\u003E \\u0026lt;pre class=\\u0026quot;wiki\\u0026quot;\\u0026gt;#\\n \\u0026lt;/pre\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EThen fixing the table\\u0026#39;s \\u0026lt;tr\\u0026gt;\\u0026#39;s and \\u0026lt;td\\u0026gt;\\u0026#39;s will fix the issue with the table not displaying.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"*Not sure if this is where to put it, but I found a coding issue in the site's HTML:*\\n\\nIn the awards help page, the table in the bottom just under:\\n\\\"There are also some special trophies that are rarer/better than awards:\\\" is broken.\\n\\nRemoving these:\\n\\u003E      \\u003Cpre class=\\\"wiki\\\"\\u003E#\\n\\u003E      \\u003C/pre\\u003E\\n\\nThen fixing the table's \\u003Ctr\\u003E's and \\u003Ctd\\u003E's will fix the issue with the table not displaying.\\n\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"150sws\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Crosswavc\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1355789476.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/150sws/awards_page_fix/\", \"locked\": false, \"name\": \"t3_150sws\", \"created\": 1355817572.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/150sws/awards_page_fix/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Awards Page Fix\", \"created_utc\": 1355788772.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi Devs, forgive me if this is an imprecise question:  I\\u0026#39;m posing a question for a more technical colleague from whom English is a second language:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E How frequently are subreddits are updated?\\u003C/li\\u003E\\n\\u003Cli\\u003E Does the update frequency vary by the sort function?  For example, does \\u201cnew\\u201d update more frequently than \\u201cwhat\\u2019s hot\\u201d?\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EWe\\u0026#39;ve been hunting around for documentation but haven\\u0026#39;t found any.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi Devs, forgive me if this is an imprecise question:  I'm posing a question for a more technical colleague from whom English is a second language:\\n\\n1.  How frequently are subreddits are updated?\\n2.  Does the update frequency vary by the sort function?  For example, does \\u201cnew\\u201d update more frequently than \\u201cwhat\\u2019s hot\\u201d?\\n\\nWe've been hunting around for documentation but haven't found any.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"131k4a\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"onelouderdude\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/131k4a/subreddit_rates_of_update/\", \"locked\": false, \"name\": \"t3_131k4a\", \"created\": 1352718388.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/131k4a/subreddit_rates_of_update/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Subreddit rates of update \", \"created_utc\": 1352689588.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m currently in the process of building a content aggregator somewhat like Reddit but a bit more specialised, however at the moment there\\u0026#39;s a huge problem with the number of bots that are on the site. I\\u0026#39;ve implemented time between registration and captchas much like Reddit does but I\\u0026#39;m still having the same problem.\\nI\\u0026#39;m not sure this is the right subreddit for this question, if it\\u0026#39;s not, then please point me in the direction where I can hopefully can some answers :) Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm currently in the process of building a content aggregator somewhat like Reddit but a bit more specialised, however at the moment there's a huge problem with the number of bots that are on the site. I've implemented time between registration and captchas much like Reddit does but I'm still having the same problem.\\nI'm not sure this is the right subreddit for this question, if it's not, then please point me in the direction where I can hopefully can some answers :) Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"zs41u\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"facebookdotcom\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/zs41u/how_does_reddit_deal_with_bots/\", \"locked\": false, \"name\": \"t3_zs41u\", \"created\": 1347506344.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/zs41u/how_does_reddit_deal_with_bots/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How does Reddit deal with bots?\", \"created_utc\": 1347477544.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello, Dev!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m an intern for CenSoC (The Center for the Study of Choice) at the University of Technology, Sydney. We are currently doing a project involving various structures that implement \\u0026quot;Follow the Leader\\u0026quot; type dynamics. We look to see if the choices that people make are dependent on the influence of \\u0026quot;leaders\\u0026quot; in their community, and try to analyze why this is/is not an attribute of the community.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe would like to do some simple data mining through the Reddit API, since Reddit is already effectually a simple choice experiment (upvotes/downvotes). We were planning on collecting content and user data to see how they influence each other, however, we hit a snag:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf we try to request user info one at a time, with a 2 second delay between each, the time taken to collect all of the data would be overdrawn. We have been considering restructuring the data mining approach, but we also wanted to know if a patch would be simple enough to allow GET requests by_id. Something of this nature:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ein the file which implements GET username about API:    \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E/r2/r2/controllers/listingcontroller.py, line ~679\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eto go from something like this:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Edef GET_about(self, vuser):\\n\\n    ...\\n\\n    return Reddit(content = Wrapped(vuser)).render()\\n\\n    ...\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eto something like the following, where the username(s) url portion is split using regex (much like in the \\u0026#39;by_id\\u0026#39; api code):\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ereddit.com/user/userA,userB,userC/about.json\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E... and then looped on the server-side appending to the returned http response without having to re-initiate the http request/response every time:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Edef GET_about(self, usernames):\\n\\n    ...    \\n    splitter = re.compile(\\u0026#39;[ ,]+\\u0026#39;)    \\n    return Reddit(content = [Wrapped(x) for x in in splitter.split(usernames)]).render()    \\n    ...\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eobviously, the correct Python-syntax and various variable-validation in reddit arch. will be necessary, but the logic of the above, albeit incorrect Python-syntax-wise, remains.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWould something like this be feasible? We currently don\\u0026#39;t have any team members who are well-versed in Python, and we also don\\u0026#39;t know how things look on Reddit\\u0026#39;s end.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks, Reddit! :D\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello, Dev!\\n\\nI'm an intern for CenSoC (The Center for the Study of Choice) at the University of Technology, Sydney. We are currently doing a project involving various structures that implement \\\"Follow the Leader\\\" type dynamics. We look to see if the choices that people make are dependent on the influence of \\\"leaders\\\" in their community, and try to analyze why this is/is not an attribute of the community.\\n\\nWe would like to do some simple data mining through the Reddit API, since Reddit is already effectually a simple choice experiment (upvotes/downvotes). We were planning on collecting content and user data to see how they influence each other, however, we hit a snag:\\n\\nIf we try to request user info one at a time, with a 2 second delay between each, the time taken to collect all of the data would be overdrawn. We have been considering restructuring the data mining approach, but we also wanted to know if a patch would be simple enough to allow GET requests by_id. Something of this nature:\\n    \\n\\n\\nin the file which implements GET username about API:    \\n\\n    /r2/r2/controllers/listingcontroller.py, line ~679\\n\\nto go from something like this:\\n    \\n\\n    def GET_about(self, vuser):\\n\\n        ...\\n\\n        return Reddit(content = Wrapped(vuser)).render()\\n\\n        ...\\n\\nto something like the following, where the username(s) url portion is split using regex (much like in the 'by_id' api code):\\n\\n    reddit.com/user/userA,userB,userC/about.json\\n\\n... and then looped on the server-side appending to the returned http response without having to re-initiate the http request/response every time:\\n\\n    def GET_about(self, usernames):\\n\\n        ...    \\n        splitter = re.compile('[ ,]+')    \\n        return Reddit(content = [Wrapped(x) for x in in splitter.split(usernames)]).render()    \\n        ...\\n    \\n\\nobviously, the correct Python-syntax and various variable-validation in reddit arch. will be necessary, but the logic of the above, albeit incorrect Python-syntax-wise, remains.\\n\\n    \\n\\nWould something like this be feasible? We currently don't have any team members who are well-versed in Python, and we also don't know how things look on Reddit's end.\\n\\nThanks, Reddit! :D\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"zf378\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MrFanzyPanz\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1346886921.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/zf378/possible_api_patch_batch_get_command_by_id/\", \"locked\": false, \"name\": \"t3_zf378\", \"created\": 1346915340.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/zf378/possible_api_patch_batch_get_command_by_id/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Possible API patch: Batch GET command by_id?\", \"created_utc\": 1346886540.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EMy primary usage for the reddit source would be via the fantastic API features, but I recall reading about denied service from the reddit.com API when used regularly (I think it was over 30 calls a minute from the same address, maybe even much less).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDoes anybody know if the source API is also restricted in such a way? I sure hope not, or if there is a workaround.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"My primary usage for the reddit source would be via the fantastic API features, but I recall reading about denied service from the reddit.com API when used regularly (I think it was over 30 calls a minute from the same address, maybe even much less).\\n\\nDoes anybody know if the source API is also restricted in such a way? I sure hope not, or if there is a workaround.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ywevx\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bttf2at29\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ywevx/does_the_reddit_source_api_deny_heavy_usage_like/\", \"locked\": false, \"name\": \"t3_ywevx\", \"created\": 1346098692.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ywevx/does_the_reddit_source_api_deny_heavy_usage_like/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Does the reddit source API deny heavy usage, like reddit.com?\", \"created_utc\": 1346069892.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAs the rankings depend on time, number of votes, they change constantly over time. So I was wondering if Reddit is updating the rankings constantly over time, in a background process?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"As the rankings depend on time, number of votes, they change constantly over time. So I was wondering if Reddit is updating the rankings constantly over time, in a background process?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"xv08c\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"reddit_code_q\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/xv08c/is_reddit_updating_post_rankings_in_realtime_or/\", \"locked\": false, \"name\": \"t3_xv08c\", \"created\": 1344424221.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/xv08c/is_reddit_updating_post_rankings_in_realtime_or/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is Reddit updating post rankings in real-time or is it using a background job to update them?\", \"created_utc\": 1344395421.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI took a look around the API docs and the usage guidelines, but couldn\\u0026#39;t find any information addressing what I\\u0026#39;d like to do.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m working on a site that allows users to create bits of content. Aside from that I\\u0026#39;d like the site to have every piece of reddit functionality (voting, commenting). I was thinking it might be neat to create a subreddit, and then use the API to leverage reddits voting and commenting engine.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs this sort of usage discouraged? Is there a better way to do it than using the API?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I took a look around the API docs and the usage guidelines, but couldn't find any information addressing what I'd like to do.\\n\\nI'm working on a site that allows users to create bits of content. Aside from that I'd like the site to have every piece of reddit functionality (voting, commenting). I was thinking it might be neat to create a subreddit, and then use the API to leverage reddits voting and commenting engine.\\n\\nIs this sort of usage discouraged? Is there a better way to do it than using the API?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"wud8k\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"cambridgemike\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/wud8k/use_api_to_create_white_label_site/\", \"locked\": false, \"name\": \"t3_wud8k\", \"created\": 1342767681.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/wud8k/use_api_to_create_white_label_site/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Use API to create white label site?\", \"created_utc\": 1342738881.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve posted queries on this subreddit before, but haven\\u0026#39;t got much replies. I know this is a subreddit for the reddit API client and the reddit source code.  But my first assumption was that we could post queries here if we had problems with our reddit installation. I\\u0026#39;m not cribbing but that\\u0026#39;s what I assumed. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnyway, I went through \\u003Ca href=\\\"http://www.reddit.com/r/raerth/comments/f2xrr/raerths_moderation_guide/\\\"\\u003ERaerth\\u0026#39;s Moderation Guide\\u003C/a\\u003E but couldn\\u0026#39;t find any mention of a subreddit where you could post queries related to installing reddit - stuck with an installation, installation gone wrong, features not working after a new installation - a place where you could say, \\u003Cem\\u003E\\u0026quot;Help reddit, I just installed my reddit site, but I\\u0026#39;m stuck with this problem\\u0026quot;\\u003C/em\\u003E etc. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo, getting back to the topic, is there such a subreddit where fellow redditors help out each other with reddit installation problems?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've posted queries on this subreddit before, but haven't got much replies. I know this is a subreddit for the reddit API client and the reddit source code.  But my first assumption was that we could post queries here if we had problems with our reddit installation. I'm not cribbing but that's what I assumed. \\n\\nAnyway, I went through [Raerth's Moderation Guide] (http://www.reddit.com/r/raerth/comments/f2xrr/raerths_moderation_guide/) but couldn't find any mention of a subreddit where you could post queries related to installing reddit - stuck with an installation, installation gone wrong, features not working after a new installation - a place where you could say, *\\\"Help reddit, I just installed my reddit site, but I'm stuck with this problem\\\"* etc. \\n\\nSo, getting back to the topic, is there such a subreddit where fellow redditors help out each other with reddit installation problems?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"wfqr9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"pencil_the_anus\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1342103339.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/wfqr9/is_there_a_subreddit_for_posting_reddit/\", \"locked\": false, \"name\": \"t3_wfqr9\", \"created\": 1342121929.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/wfqr9/is_there_a_subreddit_for_posting_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a subreddit for posting reddit installation-related queries?\", \"created_utc\": 1342093129.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m trying to get started with open source development and I figured, since I use reddit every day, it would probably be a good project to start with.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/issues/457\\\"\\u003EIssue #457\\u003C/a\\u003E seems like it should be pretty easy to knock out, but I can\\u0026#39;t find the relevant source code. What tips do you have for learning the codebase and tracking down relevant code?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm trying to get started with open source development and I figured, since I use reddit every day, it would probably be a good project to start with.\\n\\n[Issue #457](https://github.com/reddit/reddit/issues/457) seems like it should be pretty easy to knock out, but I can't find the relevant source code. What tips do you have for learning the codebase and tracking down relevant code?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"w3vmb\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"BrewerHimself\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/w3vmb/any_tips_for_a_new_rredditdever_trying_to/\", \"locked\": false, \"name\": \"t3_w3vmb\", \"created\": 1341568258.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/w3vmb/any_tips_for_a_new_rredditdever_trying_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Any tips for a new r/redditdev-er trying to understand the codebase?\", \"created_utc\": 1341539458.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIt seems that you can no longer access the json or xml version of /new for any subreddit. It works fine in the browser, but I have tested multiple scripts from multiple domains with no luck.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAm I the only one having this issue?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"It seems that you can no longer access the json or xml version of /new for any subreddit. It works fine in the browser, but I have tested multiple scripts from multiple domains with no luck.\\n\\nAm I the only one having this issue?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ua3gk\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"andrewguenther\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ua3gk/api_access_to_newjsonxml_disabled/\", \"locked\": false, \"name\": \"t3_ua3gk\", \"created\": 1338311885.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ua3gk/api_access_to_newjsonxml_disabled/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API access to /new{.json/.xml} disabled?\", \"created_utc\": 1338283085.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI currently have Opera set up so that one of my custom search engines is \\u003Ca href=\\\"http://www.reddit.com/r/%s\\\"\\u003Ehttp://www.reddit.com/r/%s\\u003C/a\\u003E - this means if I type \\u0026quot;r redditdev\\u0026quot; in my address bar I get straight to \\u003Ca href=\\\"http://www.reddit.com/r/redditdev\\\"\\u003Ehttp://www.reddit.com/r/redditdev\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI thought it might be cool to extend this to have autocomplete, so I could type \\u0026quot;r Ask\\u0026quot; it pops up AskReddit, AskScience, etc. To do this, I need a URL which when passed \\u0026quot;Ask\\u0026quot; returns a JSON list of subreddits whose names start with \\u0026quot;Ask\\u0026quot;. However, AFAIK, no such URL exists in the API.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI could host the URL myself, but first I need a way to get a machine readable list of \\u003Cstrong\\u003Eall\\u003C/strong\\u003E subreddits. I could grab them from \\u003Ca href=\\\"http://www.reddit.com/reddits\\\"\\u003Ehttp://www.reddit.com/reddits\\u003C/a\\u003E, but that only returns them 25 at a time, and includes a whole lot of extra data besides the name (which is all I need), so it\\u0026#39;s a bit impractical, especially to keep the list up-to-date.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EUPDATE: I\\u0026#39;m running a perl script to scrape \\u003Ca href=\\\"http://www.reddit.com/reddits.json\\\"\\u003Ehttp://www.reddit.com/reddits.json\\u003C/a\\u003E, but you can only get 25 results per request. It\\u0026#39;s been running for over 15 minutes now, and 15,600 reddits. I might be able to run it as a cron job, but probably no more than once a day. I guess the list doesn\\u0026#39;t need to be up-to-the-minute. I\\u0026#39;ll publish a link to the list in case anyone else needs it, to save reddit from being scraped by too many robots.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EUPDATE2: OK, got it working eventually, thanks to ketralnis. I was able to get the data from /api/search_reddit_names.json, but it wasn\\u0026#39;t directly compatible with Opera search suggestions because a) It requires POSTed params and b) the JSON object wasn\\u0026#39;t quite in the correct format. I wrote a perl script to act as a gateway, which you can see \\u003Ca href=\\\"http://pastebin.com/83RkaNtX\\\"\\u003Ehere\\u003C/a\\u003E if you are interested. \\u003Ca href=\\\"http://pastebin.com/WXZjKbrD\\\"\\u003EHere\\u003C/a\\u003E is the relevant section of %APPDATA%\\\\Opera\\\\search.ini\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I currently have Opera set up so that one of my custom search engines is http://www.reddit.com/r/%s - this means if I type \\\"r redditdev\\\" in my address bar I get straight to http://www.reddit.com/r/redditdev.\\n\\nI thought it might be cool to extend this to have autocomplete, so I could type \\\"r Ask\\\" it pops up AskReddit, AskScience, etc. To do this, I need a URL which when passed \\\"Ask\\\" returns a JSON list of subreddits whose names start with \\\"Ask\\\". However, AFAIK, no such URL exists in the API.\\n\\nI could host the URL myself, but first I need a way to get a machine readable list of **all** subreddits. I could grab them from http://www.reddit.com/reddits, but that only returns them 25 at a time, and includes a whole lot of extra data besides the name (which is all I need), so it's a bit impractical, especially to keep the list up-to-date.\\n\\nUPDATE: I'm running a perl script to scrape http://www.reddit.com/reddits.json, but you can only get 25 results per request. It's been running for over 15 minutes now, and 15,600 reddits. I might be able to run it as a cron job, but probably no more than once a day. I guess the list doesn't need to be up-to-the-minute. I'll publish a link to the list in case anyone else needs it, to save reddit from being scraped by too many robots.\\n\\nUPDATE2: OK, got it working eventually, thanks to ketralnis. I was able to get the data from /api/search_reddit_names.json, but it wasn't directly compatible with Opera search suggestions because a) It requires POSTed params and b) the JSON object wasn't quite in the correct format. I wrote a perl script to act as a gateway, which you can see [here](http://pastebin.com/83RkaNtX) if you are interested. [Here](http://pastebin.com/WXZjKbrD) is the relevant section of %APPDATA%\\\\Opera\\\\search.ini\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"t9r5i\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"mishagale\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/t9r5i/is_there_a_way_to_do_nameglobs_for_subreddits/\", \"locked\": false, \"name\": \"t3_t9r5i\", \"created\": 1336346898.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/t9r5i/is_there_a_way_to_do_nameglobs_for_subreddits/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a way to do nameglobs for subreddits? \\nTrying to figure out a way to do autocomplete for \\nsubreddit search\", \"created_utc\": 1336318098.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIf I wanted to delete everything (all submissions, users, etc.), would the best way be to just delete the postgresql \\u0026quot;reddit\\u0026quot; database, then recreate it and set it up using the sql/functions.sql file?  Is that safe / complete?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESimilarly, if I wanted to back up everything then should I just dump or replicate the \\u0026quot;reddit\\u0026quot; database?  Would that miss anything?  (Presuming my code/config is already saved elsewhere.)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"If I wanted to delete everything (all submissions, users, etc.), would the best way be to just delete the postgresql \\\"reddit\\\" database, then recreate it and set it up using the sql/functions.sql file?  Is that safe / complete?\\n\\nSimilarly, if I wanted to back up everything then should I just dump or replicate the \\\"reddit\\\" database?  Would that miss anything?  (Presuming my code/config is already saved elsewhere.)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"t8dbj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"new2rsrc\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/t8dbj/cleaning_and_backing_up_database/\", \"locked\": false, \"name\": \"t3_t8dbj\", \"created\": 1336258181.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/t8dbj/cleaning_and_backing_up_database/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Cleaning and Backing up Database?\", \"created_utc\": 1336229381.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo I have (mostly) completed my modifications and would like to deploy my reddit to a production server.  Ideally, I would like to have it running on an existing apache/php server (since it is serving the domain I would like to use with related content).  It looks like the best? way to do this is use mod_wsgi, then configure the virtualhost to serve both.  I\\u0026#39;ve run in to a couple questions trying to do this:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E1) Is there a good way to package the code for production on a different machine?  I used the \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu\\\"\\u003EUbuntu 11.04 install script\\u003C/a\\u003E which worked great.  I was thinking I may want to just use that again, then drop in my modified code...  is that the best way to go?  I know typically you can package it as an egg, but it seems like that would be more difficult (and may not be optimized).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E2)  What all does the \\u003Ccode\\u003Emake\\u003C/code\\u003E do?  It looks like it is compiling the python to c for optimization?  How do I ensure that this is the optimized/compiled code being used?  It appears that the uwsgi server that was configured by the install script uses the same run.ini as my development paster server (which I can tell uses the regular .py files).  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E3)  Will reddit have problems running in a sub-directory on the apache2/mod_wsgi server?  Will any of the internal links, etc. break?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E4)  Does anyone have any tips for running php and wsgi on the same apache server?  The closest I could find was \\u003Ca href=\\\"http://serverfault.com/questions/226449/how-can-django-wsgi-and-php-share-on-apache\\\"\\u003Ethis\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E5)  What exactly are the SECRET, MODSECRET, FEEDSECRET, ADMINSECRET, and traffic_secret in the .ini?  They look important, and like I should change them...\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E6) Any other tips or suggestions?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So I have (mostly) completed my modifications and would like to deploy my reddit to a production server.  Ideally, I would like to have it running on an existing apache/php server (since it is serving the domain I would like to use with related content).  It looks like the best? way to do this is use mod_wsgi, then configure the virtualhost to serve both.  I've run in to a couple questions trying to do this:\\n\\n1) Is there a good way to package the code for production on a different machine?  I used the [Ubuntu 11.04 install script](https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu) which worked great.  I was thinking I may want to just use that again, then drop in my modified code...  is that the best way to go?  I know typically you can package it as an egg, but it seems like that would be more difficult (and may not be optimized).\\n\\n2)  What all does the `make` do?  It looks like it is compiling the python to c for optimization?  How do I ensure that this is the optimized/compiled code being used?  It appears that the uwsgi server that was configured by the install script uses the same run.ini as my development paster server (which I can tell uses the regular .py files).  \\n\\n3)  Will reddit have problems running in a sub-directory on the apache2/mod_wsgi server?  Will any of the internal links, etc. break?\\n\\n4)  Does anyone have any tips for running php and wsgi on the same apache server?  The closest I could find was [this](http://serverfault.com/questions/226449/how-can-django-wsgi-and-php-share-on-apache).\\n\\n5)  What exactly are the SECRET, MODSECRET, FEEDSECRET, ADMINSECRET, and traffic_secret in the .ini?  They look important, and like I should change them...\\n\\n6) Any other tips or suggestions?\\n\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"t8ctu\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"RedditSrc4Research\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/t8ctu/reddit_deployment/\", \"locked\": false, \"name\": \"t3_t8ctu\", \"created\": 1336257223.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/t8ctu/reddit_deployment/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Deployment\", \"created_utc\": 1336228423.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi, I\\u0026#39;m developing an browser extension to help us AskScience Mods and have a few questions.\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EHow often do session cookies / modhashes expire?  Could I login once (or once an hour/day/week/month) and then continue to use the same modhash/cookie?  Looking at the python source the session cookie doesn\\u0026#39;t appear to ever expire (the check of the cookie \\u003Ccode\\u003Edatetime+user+sha(datetime+user+secret)\\u003C/code\\u003E reads datetime+user from the cookie and then compares to the secret).  However, the modhash for CSRF protection seems to just be a placeholder in the reddit source.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EI plan on giving mods a few browser buttons that will combine a couple separate requests into a single action; e.g., mod presses a button that uses the API to \\u0026#39;post a (brief) comment\\u0026#39;, \\u0026#39;send a (brief) message\\u0026#39;, and \\u0026#39;change user\\u0026#39;s flair\\u0026#39;.  Should I wait 2 seconds between the three separate requests or can I send request 1, wait for response 1; then send request 2, wait for response 2; then send request 3, wait for response 3)?  I expect these actions to happen rarely after testing finishes, less than 20 times a day (60 total requests daily) -- nowhere near the limit of 60 reqs/minute.  So, I\\u0026#39;d prefer not to have to wait 4+ seconds to finish the combined requests, so they can get quicker feedback of success/failure.  Is say 3 requests in say a second bad if the next request isn\\u0026#39;t for tens of minutes later?  (This is also part of the reason I want to login once; so its not four requests).\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi, I'm developing an browser extension to help us AskScience Mods and have a few questions.\\n\\n1. How often do session cookies / modhashes expire?  Could I login once (or once an hour/day/week/month) and then continue to use the same modhash/cookie?  Looking at the python source the session cookie doesn't appear to ever expire (the check of the cookie `datetime+user+sha(datetime+user+secret)` reads datetime+user from the cookie and then compares to the secret).  However, the modhash for CSRF protection seems to just be a placeholder in the reddit source.\\n\\n2. I plan on giving mods a few browser buttons that will combine a couple separate requests into a single action; e.g., mod presses a button that uses the API to 'post a (brief) comment', 'send a (brief) message', and 'change user's flair'.  Should I wait 2 seconds between the three separate requests or can I send request 1, wait for response 1; then send request 2, wait for response 2; then send request 3, wait for response 3)?  I expect these actions to happen rarely after testing finishes, less than 20 times a day (60 total requests daily) -- nowhere near the limit of 60 reqs/minute.  So, I'd prefer not to have to wait 4+ seconds to finish the combined requests, so they can get quicker feedback of success/failure.  Is say 3 requests in say a second bad if the next request isn't for tens of minutes later?  (This is also part of the reason I want to login once; so its not four requests).\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"rq4ok\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"djimbob\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/rq4ok/login_cookiemodhash_expiration_time_and_is/\", \"locked\": false, \"name\": \"t3_rq4ok\", \"created\": 1333431430.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/rq4ok/login_cookiemodhash_expiration_time_and_is/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Login cookie/modhash expiration time and is consistently having ~3 small reqs in a second bad if they are spaced out by ~hr plus?\", \"created_utc\": 1333402630.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EEdit: 1.3.0 is official now. \\u003Ccode\\u003Epip install reddit\\u003C/code\\u003E or \\u003Ccode\\u003Epip install --upgrade reddit\\u003C/code\\u003E will install six if it is not already installed.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI recently finished adding hybrid support for both Python 2.6+ and 3.2+ and I would really appreciate if some PRAW users would upgrade to the 1.3.0dev version of PRAW to ensure everything works as expected. I have extended PRAW\\u0026#39;s tests to include unicode testing and thus given that all the tests pass on both 2.6, and 3.2 I don\\u0026#39;t expect there to be any issues. Nevertheless it doesn\\u0026#39;t hurt to be sure.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E1.3.0dev is not available via pip, thus you\\u0026#39;ll need to manually fetch it from \\u003Ca href=\\\"https://github.com/bboe/reddit_api/tree/python3\\\"\\u003Egithub\\u003C/a\\u003E. Going forward PRAW will depend on the \\u003Ccode\\u003Esix\\u003C/code\\u003E module which can be obtained via \\u003Ccode\\u003Epip install six\\u003C/code\\u003E. To make sure you are running the correct version you can run:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003Epython -c \\u0026#39;import reddit; print(reddit.VERSION)\\u0026#39;\\u003C/code\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe output should be \\u003Ccode\\u003E1.3.0dev\\u003C/code\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you run into any issues, please follow up either here, or on #reddit-dev in irc.freenode.net. Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Edit: 1.3.0 is official now. `pip install reddit` or `pip install --upgrade reddit` will install six if it is not already installed.\\n\\nI recently finished adding hybrid support for both Python 2.6+ and 3.2+ and I would really appreciate if some PRAW users would upgrade to the 1.3.0dev version of PRAW to ensure everything works as expected. I have extended PRAW's tests to include unicode testing and thus given that all the tests pass on both 2.6, and 3.2 I don't expect there to be any issues. Nevertheless it doesn't hurt to be sure.\\n\\n1.3.0dev is not available via pip, thus you'll need to manually fetch it from [github](https://github.com/bboe/reddit_api/tree/python3). Going forward PRAW will depend on the `six` module which can be obtained via `pip install six`. To make sure you are running the correct version you can run:\\n\\n`python -c 'import reddit; print(reddit.VERSION)'`\\n\\nThe output should be `1.3.0dev`.\\n\\nIf you run into any issues, please follow up either here, or on #reddit-dev in irc.freenode.net. Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"r5e5l\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bboe\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/r5e5l/python_reddit_api_developers_python_23_hybrid/\", \"locked\": false, \"name\": \"t3_r5e5l\", \"created\": 1332296027.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/r5e5l/python_reddit_api_developers_python_23_hybrid/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Python Reddit API Developers: Python 2/3 hybrid beta testing needed\", \"created_utc\": 1332267227.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo, just for fun, I want to make a reddit profile that is subscribed to all of the subreddits.  You\\u0026#39;re probably asking \\u0026quot;Why don\\u0026#39;t you just use the \\u0026quot;All\\u0026quot; tab?\\u0026quot; Well my goal is to manually unsubscribe from boring/bad/big subreddits as I browse so I can find some hidden gems.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI understand that I need to use subscribe(\\u0026lt;subreddit id\\u0026gt;), so to subscribe to Pics would be \\u0026quot;subscribe(\\u0026#39;t5_2cneq\\u0026#39;)\\u0026quot;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETwo problems I see so far is the request limit, and how to compile a list of all the public subreddits.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe 1 request every two seconds wouldn\\u0026#39;t be hard to program, but will just mean the script will take a long time. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EGetting a list of every subreddit seems like it could be very hard.  Luckily metareddit, \\u003Ca href=\\\"https://github.com/modemuser/metareddit\\\"\\u003Epost the source code of their spider\\u003C/a\\u003E, so I might be able to modify and simplify that.  Or I could just parse every page of [\\u003Ca href=\\\"http://www.reddit.com/reddits%5D(www.reddit.com/reddits)\\\"\\u003Ewww.reddit.com/reddits](www.reddit.com/reddits)\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo I was looking for some input on my idea, has it been done before?  Any ideas about compiling a list of subreddits?  Thanks a lot.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So, just for fun, I want to make a reddit profile that is subscribed to all of the subreddits.  You're probably asking \\\"Why don't you just use the \\\"All\\\" tab?\\\" Well my goal is to manually unsubscribe from boring/bad/big subreddits as I browse so I can find some hidden gems.\\n\\nI understand that I need to use subscribe(\\u003Csubreddit id\\u003E), so to subscribe to Pics would be \\\"subscribe('t5_2cneq')\\\"\\n\\nTwo problems I see so far is the request limit, and how to compile a list of all the public subreddits.\\n\\nThe 1 request every two seconds wouldn't be hard to program, but will just mean the script will take a long time. \\n\\nGetting a list of every subreddit seems like it could be very hard.  Luckily metareddit, [post the source code of their spider](https://github.com/modemuser/metareddit), so I might be able to modify and simplify that.  Or I could just parse every page of [www.reddit.com/reddits](www.reddit.com/reddits).\\n\\nSo I was looking for some input on my idea, has it been done before?  Any ideas about compiling a list of subreddits?  Thanks a lot.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"p2rqf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"sudosandwich3\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 16, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/p2rqf/i_want_to_see_all_of_the_subreddits/\", \"locked\": false, \"name\": \"t3_p2rqf\", \"created\": 1327938002.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/p2rqf/i_want_to_see_all_of_the_subreddits/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I want to see all of the subreddits!\", \"created_utc\": 1327909202.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve been trying to understand how reddit renders the comments tree and was hoping someone could help me out.  What I\\u0026#39;m specifically trying to do is to have a comment not be displayed when a flag is set.  This isn\\u0026#39;t for any specific purpose, I\\u0026#39;m just trying to understand the code at this point.  I\\u0026#39;m not familiar with pylons/mako (although I know the basics), but am pretty experienced with using python.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe best i\\u0026#39;ve been able to do so far is a crude hack of the template comment_skeleton.html, where I define blocks as empty if the flag is set.  When I\\u0026#39;ve tried digging deeper into how things are done, I end up tumbling down the rabbit hole of inherited classes without much insight.  For example, I tried following the simple route from the /c/COMMENT_ID to the comment_by_id action of the front controller.  If you follow it far enough, you end up at the _render function of Templated class, which I think probably calls something else to do the rendering depending on the context.  In principal, the right way to implement my if flag, no print action would be to change the python code rather than the template (at least I think it is), but I\\u0026#39;m utterly lost as to how to do it.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Etl;dr  If anyone can explain to me how comments are rendered and where the best place to change how they\\u0026#39;re rendered is, I would be eternally gratefu\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello,\\n\\nI've been trying to understand how reddit renders the comments tree and was hoping someone could help me out.  What I'm specifically trying to do is to have a comment not be displayed when a flag is set.  This isn't for any specific purpose, I'm just trying to understand the code at this point.  I'm not familiar with pylons/mako (although I know the basics), but am pretty experienced with using python.\\n\\nThe best i've been able to do so far is a crude hack of the template comment_skeleton.html, where I define blocks as empty if the flag is set.  When I've tried digging deeper into how things are done, I end up tumbling down the rabbit hole of inherited classes without much insight.  For example, I tried following the simple route from the /c/COMMENT_ID to the comment_by_id action of the front controller.  If you follow it far enough, you end up at the _render function of Templated class, which I think probably calls something else to do the rendering depending on the context.  In principal, the right way to implement my if flag, no print action would be to change the python code rather than the template (at least I think it is), but I'm utterly lost as to how to do it.\\n\\ntl;dr  If anyone can explain to me how comments are rendered and where the best place to change how they're rendered is, I would be eternally gratefu\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"n4y4v\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MDY\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/n4y4v/understanding_how_comments_are_rendered/\", \"locked\": false, \"name\": \"t3_n4y4v\", \"created\": 1323389377.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/n4y4v/understanding_how_comments_are_rendered/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Understanding how comments are rendered...\", \"created_utc\": 1323360577.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"mujsd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ayoformayo\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/mujsd/is_reddits_api_open_and_where_can_i_find_it/\", \"locked\": false, \"name\": \"t3_mujsd\", \"created\": 1322671857.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/mujsd/is_reddits_api_open_and_where_can_i_find_it/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is reddit's API open and where can I find it?\", \"created_utc\": 1322643057.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m guessing you could skin a custom reddit install to look like a blog, then use self posts as your blog posts and let people comment/vote as usual.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBut could you somehow integrate just the comment/voting system used by reddit into a wordpress site?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm guessing you could skin a custom reddit install to look like a blog, then use self posts as your blog posts and let people comment/vote as usual.\\n\\nBut could you somehow integrate just the comment/voting system used by reddit into a wordpress site?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"lk3n2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"crimson117\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/lk3n2/is_it_possible_to_use_the_reddit_comment_system/\", \"locked\": false, \"name\": \"t3_lk3n2\", \"created\": 1319245549.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/lk3n2/is_it_possible_to_use_the_reddit_comment_system/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is it possible to use the reddit comment system as the comment system for a wordpress blog?\", \"created_utc\": 1319216749.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"k1cy1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"gehsekky\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/k1cy1/teaching_myself_python_made_a_script_to_sync/\", \"locked\": false, \"name\": \"t3_k1cy1\", \"created\": 1314918297.0, \"url\": \"https://github.com/gehsekky/reddit-scripts/blob/master/reddit_friend_sync.py\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Teaching myself Python. Made a script to sync friends across multiple reddit accounts. Constructive criticism appreciated.\", \"created_utc\": 1314889497.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe translations for \\u0026quot;what\\u0026#39;s new\\u0026quot; and \\u0026quot;what\\u0026#39;s hot\\u0026quot; are exchanged in the german translation. I forked \\u003Ca href=\\\"https://github.com/reddit/reddit-i18n\\\"\\u003Ereddit-i18n\\u003C/a\\u003E and fixed it. Should I just issue a pull request or is there another way to fix the translations? (\\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/i635r/i_want_to_help_you_out_with_the_german/\\\"\\u003Ethis\\u003C/a\\u003E suggests that there is a translations interface somewhere, but my google-fu seems weak)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The translations for \\\"what's new\\\" and \\\"what's hot\\\" are exchanged in the german translation. I forked [reddit-i18n](https://github.com/reddit/reddit-i18n) and fixed it. Should I just issue a pull request or is there another way to fix the translations? ([this](http://www.reddit.com/r/redditdev/comments/i635r/i_want_to_help_you_out_with_the_german/) suggests that there is a translations interface somewhere, but my google-fu seems weak)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"jbmph\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"calaveraDeluxe\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/jbmph/i_have_fixed_an_error_in_the_german_translation/\", \"locked\": false, \"name\": \"t3_jbmph\", \"created\": 1312764979.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/jbmph/i_have_fixed_an_error_in_the_german_translation/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I have fixed an error in the german translation. Is it alright to issue a pull request?\", \"created_utc\": 1312736179.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m wondering what would happen in the case where my user has \\u003Cstrong\\u003Ea lot\\u003C/strong\\u003E of messages waiting in his inbox and I GET /messages/unread.json??\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWill I get all of the messages or just a portion of them?  If I only get a portion, how do I get the remaining messages?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnd if I only get a portion of the messages, when I GET /messages/inbox/ does that mark \\u003Cstrong\\u003Eall\\u003C/strong\\u003E unread messages read or just the portion of them?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm wondering what would happen in the case where my user has **a lot** of messages waiting in his inbox and I GET /messages/unread.json??\\n\\nWill I get all of the messages or just a portion of them?  If I only get a portion, how do I get the remaining messages?\\n\\nAnd if I only get a portion of the messages, when I GET /messages/inbox/ does that mark **all** unread messages read or just the portion of them?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"j61st\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"theycallmemorty\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/j61st/is_there_a_limit_on_what_messagesunreadjson_will/\", \"locked\": false, \"name\": \"t3_j61st\", \"created\": 1312262422.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/j61st/is_there_a_limit_on_what_messagesunreadjson_will/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a limit on what /messages/unread.json will return?\", \"created_utc\": 1312233622.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m trying to load all comments in a thread, and I\\u0026#39;m having a hard time figuring out how to fetch more than the standard reply. For instance, when I load this story: \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehttp://www.reddit.com/r/technology/comments/ib83z/us_navy_bought_59000_microchips_that_turned_out/.json\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI get children with the following format (the formatting is my own): \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E[\\u0026#39;kind\\u0026#39;] = \\u0026#39;more\\u0026#39; --- \\u0026lt;type \\u0026#39;unicode\\u0026#39;\\u0026gt;\\n[\\u0026#39;data\\u0026#39;][\\u0026#39;name\\u0026#39;] = \\u0026#39;t1_c22d3fb\\u0026#39; --- \\u0026lt;type \\u0026#39;unicode\\u0026#39;\\u0026gt;\\n[\\u0026#39;data\\u0026#39;][\\u0026#39;id\\u0026#39;] = \\u0026#39;c22d3fb\\u0026#39; --- \\u0026lt;type \\u0026#39;unicode\\u0026#39;\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EWhen I follow the instructions for loading additional comments, I call: \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehttp://reddit.com/comments/ib83z/c22d3fb.json\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eand pretty much the original page. What gives? \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm trying to load all comments in a thread, and I'm having a hard time figuring out how to fetch more than the standard reply. For instance, when I load this story: \\n\\n    http://www.reddit.com/r/technology/comments/ib83z/us_navy_bought_59000_microchips_that_turned_out/.json\\n\\nI get children with the following format (the formatting is my own): \\n\\n    ['kind'] = 'more' --- \\u003Ctype 'unicode'\\u003E\\n    ['data']['name'] = 't1_c22d3fb' --- \\u003Ctype 'unicode'\\u003E\\n    ['data']['id'] = 'c22d3fb' --- \\u003Ctype 'unicode'\\u003E\\n\\nWhen I follow the instructions for loading additional comments, I call: \\n\\n    http://reddit.com/comments/ib83z/c22d3fb.json\\n\\nand pretty much the original page. What gives? \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"icbh1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"jfasi\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 23, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/icbh1/how_do_i_use_the_more_entries_in_the_json_reply/\", \"locked\": false, \"name\": \"t3_icbh1\", \"created\": 1309386713.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/icbh1/how_do_i_use_the_more_entries_in_the_json_reply/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How do I use the 'more' entries in the json reply to load more comments? \", \"created_utc\": 1309357913.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"i635r\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"hyrulz\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/i635r/i_want_to_help_you_out_with_the_german/\", \"locked\": false, \"name\": \"t3_i635r\", \"created\": 1308767117.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/i635r/i_want_to_help_you_out_with_the_german/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I want to help you out with the german translation, but I don't know anything about github. Could somebody give me a brief introduction?\", \"created_utc\": 1308738317.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"techcrunch.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"fy53l\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"zbowling\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/fy53l/revealed_my_start_up_on_techcrunch_inspired_by/\", \"locked\": false, \"name\": \"t3_fy53l\", \"created\": 1299397899.0, \"url\": \"http://techcrunch.com/2011/03/05/view/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Revealed my start up on TechCrunch! Inspired by reddit but as an LBS. The original proof of concept was a reddit fork called Geodit.\", \"created_utc\": 1299369099.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m trying to figure out some statistics I\\u0026#39;m getting from an index I\\u0026#39;m building in solr. I am crawling \\u003Ca href=\\\"/r/worldnews\\\"\\u003E/r/worldnews\\u003C/a\\u003E.json every 15 minutes and there are 25 headlines in there. I should have 2400 (25\\u003Cem\\u003E4\\u003C/em\\u003E24) items in solr per day but I have a variety of numbers instead. Is it me, is it reddit? I don\\u0026#39;t know.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy lowest number is 303 on Feb 26th 2011.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: I just realized that my own hourly statistics contradicts the daily statistics so I\\u0026#39;m confused even more (hopefully no one saw the original and clearly stupid post :( ).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there a json equivalent of the reddit is down cute image? Perhaps an http error 500 code?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E--thanks\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm trying to figure out some statistics I'm getting from an index I'm building in solr. I am crawling /r/worldnews.json every 15 minutes and there are 25 headlines in there. I should have 2400 (25*4*24) items in solr per day but I have a variety of numbers instead. Is it me, is it reddit? I don't know.\\n\\nMy lowest number is 303 on Feb 26th 2011.\\n\\nEDIT: I just realized that my own hourly statistics contradicts the daily statistics so I'm confused even more (hopefully no one saw the original and clearly stupid post :( ).\\n\\nIs there a json equivalent of the reddit is down cute image? Perhaps an http error 500 code?\\n\\n--thanks\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"fvf8u\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"i_ate_god\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/fvf8u/reddit_downtime/\", \"locked\": false, \"name\": \"t3_fvf8u\", \"created\": 1299054202.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/fvf8u/reddit_downtime/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"reddit downtime?\", \"created_utc\": 1299025402.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi all,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo start with, I\\u0026#39;m very new to using the Reddit API so I apologize for my  naivet\\u00e9. I\\u0026#39;m currently writing an app which allows the user to log in. I\\u0026#39;m doing a POST to \\u003Ca href=\\\"http://www.reddit.com/api/login/\\\"\\u003Ehttp://www.reddit.com/api/login/\\u003C/a\\u003E\\u0026lt;username\\u0026gt; with the following as the request body:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eop=login-main\\u0026amp;user=\\u0026lt;username\\u0026gt;\\u0026amp;passwd=\\u0026lt;password\\u0026gt;\\u0026amp;id=%23login_login-main\\u0026amp;r=redditdev\\u0026amp;renderstyle=html\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI believe I copied that string out of the regular login requests that I was sniffing. Admittedly, I have little idea what most of those params mean (op, id, renderstyle).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIn any case, these logins are generally successful for around 10 times. After that, it seems that my account is basically locked out of reddit. In other words, I can\\u0026#39;t log in via my app or via reddit.com. The only way out of this seems to be to reset my password on reddit and log in again. Even then, after the inital reset of the password, I\\u0026#39;m still unabel to log in to reddit again.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny idea what I might be doing wrong? I\\u0026#39;ve looked around /redditdev and the API docs, but I haven\\u0026#39;t found anything. I did find a post from a while ago complaining about a similar problem when using the iPhone app, but there was no resolution to the problem in the thread.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks for your help!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT\\u003C/strong\\u003E: Just got a raging clue from this code:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eif login_throttle(username, wrong_password = form.has_errors(\\u0026quot;passwd\\u0026quot;,\\n                                                 errors.WRONG_PASSWORD)):\\n        VRatelimit.ratelimit(rate_ip = True, prefix = \\u0026#39;login_\\u0026#39;, seconds=1)\\n\\n        c.errors.add(errors.WRONG_PASSWORD, field = \\u0026quot;passwd\\u0026quot;)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m guessing I\\u0026#39;ve just tried to log in too many times from the same IP in the last couple hours. Does this explain it?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi all,\\n\\nTo start with, I'm very new to using the Reddit API so I apologize for my  naivet\\u00e9. I'm currently writing an app which allows the user to log in. I'm doing a POST to http://www.reddit.com/api/login/\\u003Cusername\\u003E with the following as the request body:\\n\\nop=login-main\\u0026user=\\u003Cusername\\u003E\\u0026passwd=\\u003Cpassword\\u003E\\u0026id=%23login_login-main\\u0026r=redditdev\\u0026renderstyle=html\\n\\nI believe I copied that string out of the regular login requests that I was sniffing. Admittedly, I have little idea what most of those params mean (op, id, renderstyle).\\n\\nIn any case, these logins are generally successful for around 10 times. After that, it seems that my account is basically locked out of reddit. In other words, I can't log in via my app or via reddit.com. The only way out of this seems to be to reset my password on reddit and log in again. Even then, after the inital reset of the password, I'm still unabel to log in to reddit again.\\n\\nAny idea what I might be doing wrong? I've looked around /redditdev and the API docs, but I haven't found anything. I did find a post from a while ago complaining about a similar problem when using the iPhone app, but there was no resolution to the problem in the thread.\\n\\nThanks for your help!\\n\\n**EDIT**: Just got a raging clue from this code:\\n\\n    if login_throttle(username, wrong_password = form.has_errors(\\\"passwd\\\",\\n                                                     errors.WRONG_PASSWORD)):\\n            VRatelimit.ratelimit(rate_ip = True, prefix = 'login_', seconds=1)\\n\\n            c.errors.add(errors.WRONG_PASSWORD, field = \\\"passwd\\\")\\n\\nI'm guessing I've just tried to log in too many times from the same IP in the last couple hours. Does this explain it?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ekrig\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"pjakubo86\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ekrig/logging_in_via_posts_to_apilogin_causes_account/\", \"locked\": false, \"name\": \"t3_ekrig\", \"created\": 1292235203.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ekrig/logging_in_via_posts_to_apilogin_causes_account/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Logging in via POSTs to /api/login causes account lockout?\", \"created_utc\": 1292206403.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EOr have a dedicated sub-reddit?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Or have a dedicated sub-reddit?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"dc3is\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bcodding\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/dc3is/any_universities_running_a_local_reddit_site/\", \"locked\": false, \"name\": \"t3_dc3is\", \"created\": 1284158768.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/dc3is/any_universities_running_a_local_reddit_site/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Any universities running a local reddit site?\", \"created_utc\": 1284129968.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"pastebin.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"cf1ar\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"meowmix4jo\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/cf1ar/search_reddit_and_sort_by_karma_proof_of_concept/\", \"locked\": false, \"name\": \"t3_cf1ar\", \"created\": 1276593118.0, \"url\": \"http://pastebin.com/919p9VEq\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Search reddit and sort by karma - proof of concept\", \"created_utc\": 1276564318.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAfter some recent confusions, the labeling in the #subreddit (moderator) messages should probably look more like:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EWhen a user sends a message to a mod inbox, it should say from [RANDOM_USER] to [#subreddit]\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EWhen a moderator replies it should say from [CAPTAIN_MODERATOR] via [#subreddit] to [RANDOM_USER], [#subreddit]\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EWhen a user responds to a moderator response: from [RANDOM_USER] to [CAPTAIN_MODERATOR], [#subreddit]\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EI think this will clarify that it\\u0026#39;s basically a group inbox where all messges are visible to all respondents.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"After some recent confusions, the labeling in the #subreddit (moderator) messages should probably look more like:\\n\\n\\n* When a user sends a message to a mod inbox, it should say from [RANDOM\\\\_USER] to [#subreddit]\\n\\n* When a moderator replies it should say from [CAPTAIN\\\\_MODERATOR] via [#subreddit] to [RANDOM\\\\_USER], [#subreddit]\\n\\n* When a user responds to a moderator response: from [RANDOM\\\\_USER] to [CAPTAIN\\\\_MODERATOR], [#subreddit]\\n\\n\\nI think this will clarify that it's basically a group inbox where all messges are visible to all respondents.\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"bkepo\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"dearsomething\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/bkepo/clearing_up_subreddit_message_confusion/\", \"locked\": false, \"name\": \"t3_bkepo\", \"created\": 1270019579.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/bkepo/clearing_up_subreddit_message_confusion/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Clearing up #subreddit message confusion\", \"created_utc\": 1269990779.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI found this link.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/r/programming/comments/6q0oq/ask_reddit_where_to_download_a_db_dump_of_reddit/\\\"\\u003Ehttp://www.reddit.com/r/programming/comments/6q0oq/ask_reddit_where_to_download_a_db_dump_of_reddit/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThey said that there would be a reddit db dump.\\nIs there the db dump of reddit really now?\\nActually, I\\u0026#39;d like to get DB dumps for only politics and WTF subreddit.\\nIs it possible?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I found this link.\\n\\nhttp://www.reddit.com/r/programming/comments/6q0oq/ask_reddit_where_to_download_a_db_dump_of_reddit/\\n\\nThey said that there would be a reddit db dump.\\nIs there the db dump of reddit really now?\\nActually, I'd like to get DB dumps for only politics and WTF subreddit.\\nIs it possible?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"arip5\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"asitdepends\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/arip5/reddit_db_dump/\", \"locked\": false, \"name\": \"t3_arip5\", \"created\": 1263949029.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/arip5/reddit_db_dump/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit DB dump\", \"created_utc\": 1263920229.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"alial\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ketralnis\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/alial/i_just_wrote_up_a_quick_tool_to_export_reddit/\", \"locked\": false, \"name\": \"t3_alial\", \"created\": 1262665818.0, \"url\": \"http://github.com/ketralnis/redditexporter\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I just wrote up a quick tool to export reddit listings to HTML\", \"created_utc\": 1262637018.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIf this worked the way I want, it would make reddit a lot more usable.  Say I open the comments page and start reading comments.  When I am done with a comment thread I collapse it.  After collapsing the first 20 threads, I encounter a link I would like to follow in the 21st thread.  I click the link and it turns out to be boring.  I click back in my browser, and all the comment threads are now re-expanded.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI guess I could \\u0026quot;open link in new tab/window\\u0026quot; but I\\u0026#39;m really a one window one tab browsing kind of guy and I never think of this at the time.  If there is a way to preserve the state of the comments page when I go back to it, that would be very cool.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"If this worked the way I want, it would make reddit a lot more usable.  Say I open the comments page and start reading comments.  When I am done with a comment thread I collapse it.  After collapsing the first 20 threads, I encounter a link I would like to follow in the 21st thread.  I click the link and it turns out to be boring.  I click back in my browser, and all the comment threads are now re-expanded.\\n\\nI guess I could \\\"open link in new tab/window\\\" but I'm really a one window one tab browsing kind of guy and I never think of this at the time.  If there is a way to preserve the state of the comments page when I go back to it, that would be very cool.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"al9qc\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Severian\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/al9qc/reddit_i_have_a_feature_request_i_want_to/\", \"locked\": false, \"name\": \"t3_al9qc\", \"created\": 1262615122.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/al9qc/reddit_i_have_a_feature_request_i_want_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit, I have a feature request.  I want to collapse threads, follow a link, and then click back.\", \"created_utc\": 1262586322.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIt\\u0026#39;s well known that Tinypic blocks incoming links from Reddit after a cap has been reached, why isn\\u0026#39;t it just blocked to divert submitters to an alternate image host?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"It's well known that Tinypic blocks incoming links from Reddit after a cap has been reached, why isn't it just blocked to divert submitters to an alternate image host?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"9ddj0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Trarcuri\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/9ddj0/ask_why_isnt_tinypic_blocked/\", \"locked\": false, \"name\": \"t3_9ddj0\", \"created\": 1251086730.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/9ddj0/ask_why_isnt_tinypic_blocked/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Ask: Why isn't Tinypic blocked?\", \"created_utc\": 1251057930.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m planning on starting a reddit implementation as a sort of forum for my comic. Every comic strip will have a reddit post, and people can mod it up and down and comment. But my current hosting doesn\\u0026#39;t meet the dependencies. Besides, every how-to I\\u0026#39;ve seen kind of assumes you\\u0026#39;re on a dedicated server.\\nDo you know of any hosting service that will make my reddit work out of the box? (I can\\u0026#39;t afford a dedicated server).\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm planning on starting a reddit implementation as a sort of forum for my comic. Every comic strip will have a reddit post, and people can mod it up and down and comment. But my current hosting doesn't meet the dependencies. Besides, every how-to I've seen kind of assumes you're on a dedicated server.\\nDo you know of any hosting service that will make my reddit work out of the box? (I can't afford a dedicated server).\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"8vjrj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"siovene\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/8vjrj/ask_do_you_know_any_hosting_plan_that_meets/\", \"locked\": false, \"name\": \"t3_8vjrj\", \"created\": 1245962753.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/8vjrj/ask_do_you_know_any_hosting_plan_that_meets/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Ask: do you know any hosting plan that meets reddit's dependencies?\", \"created_utc\": 1245933953.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"developer.mozilla.org\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"7mamu\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"freakball\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/7mamu/i_noticed_that_reddit_does_not_have_one_of_these/\", \"locked\": false, \"name\": \"t3_7mamu\", \"created\": 1230615012.0, \"url\": \"https://developer.mozilla.org/en/Creating_OpenSearch_plugins_for_Firefox\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I noticed that reddit does not have one of these...\", \"created_utc\": 1230586212.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"code.reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"78zhe\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ketralnis\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/78zhe/reddit_internal_api_documentation/\", \"locked\": false, \"name\": \"t3_78zhe\", \"created\": 1224825998.0, \"url\": \"http://code.reddit.com/docs/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"reddit internal API Documentation\", \"created_utc\": 1224797198.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"78w7b\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bennig\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/78w7b/introducing_the_greasemonkey_subreddit_for/\", \"locked\": false, \"name\": \"t3_78w7b\", \"created\": 1224799184.0, \"url\": \"http://www.reddit.com/r/GreaseMonkey\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Introducing: The greasemonkey subreddit, for reddit-related userscripts.\", \"created_utc\": 1224770384.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI want my bot to be able to respond to its keyword deep into comment threads on \\u003Ca href=\\\"/r/All\\\"\\u003E/r/All\\u003C/a\\u003E. How does the !remindme bot always seem to work no matter how deep into the comments I post?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I want my bot to be able to respond to its keyword deep into comment threads on /r/All. How does the !remindme bot always seem to work no matter how deep into the comments I post?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4mgz2d\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"Xander_The_Great\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4mgz2d/best_way_to_have_a_bot_look_for_its_keyword_deep/\", \"locked\": false, \"name\": \"t3_4mgz2d\", \"created\": 1465046556.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4mgz2d/best_way_to_have_a_bot_look_for_its_keyword_deep/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Best way to have a bot look for its keyword deep into comment sections.\", \"created_utc\": 1465017756.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"https://i.redditmedia.com/UKlR5a7NVi7sTp8AX-cVLFneF_NcZkJhcSO54B7z7Kk.jpg?s=c4bd4228dcf7989c419f1cdfee769aa8\\\"\\u003Ehttps://i.redditmedia.com/UKlR5a7NVi7sTp8AX-cVLFneF_NcZkJhcSO54B7z7Kk.jpg?s=c4bd4228dcf7989c419f1cdfee769aa8\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"/r/AdviceAnimals/comments/4lwaxz/angry_turn_signal_advice_mallard/\\\"\\u003E/r/AdviceAnimals/comments/4lwaxz/angry_turn_signal_advice_mallard/\\u003C/a\\u003E\\nFor JSON\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003E\\n{\\nkind: \\u0026quot;t3\\u0026quot;,\\ndata: {\\ndomain: \\u0026quot;livememe.com\\u0026quot;,\\nbanned_by: null,\\nmedia_embed: { },\\nsubreddit: \\u0026quot;AdviceAnimals\\u0026quot;,\\nselftext_html: null,\\nselftext: \\u0026quot;\\u0026quot;,\\nlikes: null,\\nsuggested_sort: null,\\nuser_reports: [ ],\\nsecure_media: null,\\nlink_flair_text: null,\\nid: \\u0026quot;4lwaxz\\u0026quot;,\\nfrom_kind: null,\\ngilded: 0,\\narchived: false,\\nclicked: false,\\nreport_reasons: null,\\nauthor: \\u0026quot;platypus34\\u0026quot;,\\nmedia: null,\\nscore: 1283,\\napproved_by: null,\\nover_18: false,\\nhidden: false,\\npreview: {\\nimages: [\\n{\\nsource: {\\nurl: \\u0026quot;https://i.redditmedia.com/UKlR5a7NVi7sTp8AX-cVLFneF_NcZkJhcSO54B7z7Kk.jpg?s=c4bd4228dcf7989c419f1cdfee769aa8\\u0026quot;,\\nwidth: 652,\\nheight: 750\\n},\\nresolutions: [\\n{\\nurl: \\u0026quot;https://i.redditmedia.com/UKlR5a7NVi7sTp8AX-cVLFneF_NcZkJhcSO54B7z7Kk.jpg?fit=crop\\u0026amp;amp;crop=faces%2Centropy\\u0026amp;amp;arh=2\\u0026amp;amp;w=108\\u0026amp;amp;s=451cb3bcb6bbe9074cf948f86f54fa43\\u0026quot;,\\nwidth: 108,\\nheight: 124\\n},\\n{\\nurl: \\u0026quot;https://i.redditmedia.com/UKlR5a7NVi7sTp8AX-cVLFneF_NcZkJhcSO54B7z7Kk.jpg?fit=crop\\u0026amp;amp;crop=faces%2Centropy\\u0026amp;amp;arh=2\\u0026amp;amp;w=216\\u0026amp;amp;s=a07ff77839eff52ba2487d2a429240a3\\u0026quot;,\\nwidth: 216,\\nheight: 248\\n},\\n{\\nurl: \\u0026quot;https://i.redditmedia.com/UKlR5a7NVi7sTp8AX-cVLFneF_NcZkJhcSO54B7z7Kk.jpg?fit=crop\\u0026amp;amp;crop=faces%2Centropy\\u0026amp;amp;arh=2\\u0026amp;amp;w=320\\u0026amp;amp;s=2d78d1dfd4b28dc88c375dd05757b139\\u0026quot;,\\nwidth: 320,\\nheight: 368\\n},\\n{\\nurl: \\u0026quot;https://i.redditmedia.com/UKlR5a7NVi7sTp8AX-cVLFneF_NcZkJhcSO54B7z7Kk.jpg?fit=crop\\u0026amp;amp;crop=faces%2Centropy\\u0026amp;amp;arh=2\\u0026amp;amp;w=640\\u0026amp;amp;s=e6be0cc0138277c641954ca7370bafc8\\u0026quot;,\\nwidth: 640,\\nheight: 736\\n}\\n],\\nvariants: { },\\nid: \\u0026quot;D9bjAvL1U23OR-JONM0lZXkUxCEO1l3z8br-ZYdGty8\\u0026quot;\\n}\\n]\\n},\\nnum_comments: 54,\\nthumbnail: \\u0026quot;http://b.thumbs.redditmedia.com/6ZyrXPGoJ3DtTYq8bQ-PMWwh_BnUJCNnfzNACvV2N3g.jpg\\u0026quot;,\\nsubreddit_id: \\u0026quot;t5_2s7tt\\u0026quot;,\\nhide_score: false,\\nedited: false,\\nlink_flair_css_class: null,\\nauthor_flair_css_class: null,\\ndowns: 0,\\nsecure_media_embed: { },\\nsaved: false,\\nremoval_reason: null,\\npost_hint: \\u0026quot;link\\u0026quot;,\\nstickied: false,\\nfrom: null,\\nis_self: false,\\nfrom_id: null,\\npermalink: \\u0026quot;/r/AdviceAnimals/comments/4lwaxz/angry_turn_signal_advice_mallard/\\u0026quot;,\\nlocked: false,\\nname: \\u0026quot;t3_4lwaxz\\u0026quot;,\\ncreated: 1464743506,\\nurl: \\u0026quot;http://www.livememe.com/1fqh0td\\u0026quot;,\\nauthor_flair_text: null,\\nquarantine: false,\\ntitle: \\u0026quot;Angry Turn Signal Advice Mallard\\u0026quot;,\\ncreated_utc: 1464714706,\\ndistinguished: null,\\nmod_reports: [ ],\\nvisited: false,\\nnum_reports: null,\\nups: 1283\\n}\\n}\\n\\u003C/code\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"https://i.redditmedia.com/UKlR5a7NVi7sTp8AX-cVLFneF_NcZkJhcSO54B7z7Kk.jpg?s=c4bd4228dcf7989c419f1cdfee769aa8\\n\\n/r/AdviceAnimals/comments/4lwaxz/angry_turn_signal_advice_mallard/\\nFor JSON\\n\\n```\\n{\\nkind: \\\"t3\\\",\\ndata: {\\ndomain: \\\"livememe.com\\\",\\nbanned_by: null,\\nmedia_embed: { },\\nsubreddit: \\\"AdviceAnimals\\\",\\nselftext_html: null,\\nselftext: \\\"\\\",\\nlikes: null,\\nsuggested_sort: null,\\nuser_reports: [ ],\\nsecure_media: null,\\nlink_flair_text: null,\\nid: \\\"4lwaxz\\\",\\nfrom_kind: null,\\ngilded: 0,\\narchived: false,\\nclicked: false,\\nreport_reasons: null,\\nauthor: \\\"platypus34\\\",\\nmedia: null,\\nscore: 1283,\\napproved_by: null,\\nover_18: false,\\nhidden: false,\\npreview: {\\nimages: [\\n{\\nsource: {\\nurl: \\\"https://i.redditmedia.com/UKlR5a7NVi7sTp8AX-cVLFneF_NcZkJhcSO54B7z7Kk.jpg?s=c4bd4228dcf7989c419f1cdfee769aa8\\\",\\nwidth: 652,\\nheight: 750\\n},\\nresolutions: [\\n{\\nurl: \\\"https://i.redditmedia.com/UKlR5a7NVi7sTp8AX-cVLFneF_NcZkJhcSO54B7z7Kk.jpg?fit=crop\\u0026amp;crop=faces%2Centropy\\u0026amp;arh=2\\u0026amp;w=108\\u0026amp;s=451cb3bcb6bbe9074cf948f86f54fa43\\\",\\nwidth: 108,\\nheight: 124\\n},\\n{\\nurl: \\\"https://i.redditmedia.com/UKlR5a7NVi7sTp8AX-cVLFneF_NcZkJhcSO54B7z7Kk.jpg?fit=crop\\u0026amp;crop=faces%2Centropy\\u0026amp;arh=2\\u0026amp;w=216\\u0026amp;s=a07ff77839eff52ba2487d2a429240a3\\\",\\nwidth: 216,\\nheight: 248\\n},\\n{\\nurl: \\\"https://i.redditmedia.com/UKlR5a7NVi7sTp8AX-cVLFneF_NcZkJhcSO54B7z7Kk.jpg?fit=crop\\u0026amp;crop=faces%2Centropy\\u0026amp;arh=2\\u0026amp;w=320\\u0026amp;s=2d78d1dfd4b28dc88c375dd05757b139\\\",\\nwidth: 320,\\nheight: 368\\n},\\n{\\nurl: \\\"https://i.redditmedia.com/UKlR5a7NVi7sTp8AX-cVLFneF_NcZkJhcSO54B7z7Kk.jpg?fit=crop\\u0026amp;crop=faces%2Centropy\\u0026amp;arh=2\\u0026amp;w=640\\u0026amp;s=e6be0cc0138277c641954ca7370bafc8\\\",\\nwidth: 640,\\nheight: 736\\n}\\n],\\nvariants: { },\\nid: \\\"D9bjAvL1U23OR-JONM0lZXkUxCEO1l3z8br-ZYdGty8\\\"\\n}\\n]\\n},\\nnum_comments: 54,\\nthumbnail: \\\"http://b.thumbs.redditmedia.com/6ZyrXPGoJ3DtTYq8bQ-PMWwh_BnUJCNnfzNACvV2N3g.jpg\\\",\\nsubreddit_id: \\\"t5_2s7tt\\\",\\nhide_score: false,\\nedited: false,\\nlink_flair_css_class: null,\\nauthor_flair_css_class: null,\\ndowns: 0,\\nsecure_media_embed: { },\\nsaved: false,\\nremoval_reason: null,\\npost_hint: \\\"link\\\",\\nstickied: false,\\nfrom: null,\\nis_self: false,\\nfrom_id: null,\\npermalink: \\\"/r/AdviceAnimals/comments/4lwaxz/angry_turn_signal_advice_mallard/\\\",\\nlocked: false,\\nname: \\\"t3_4lwaxz\\\",\\ncreated: 1464743506,\\nurl: \\\"http://www.livememe.com/1fqh0td\\\",\\nauthor_flair_text: null,\\nquarantine: false,\\ntitle: \\\"Angry Turn Signal Advice Mallard\\\",\\ncreated_utc: 1464714706,\\ndistinguished: null,\\nmod_reports: [ ],\\nvisited: false,\\nnum_reports: null,\\nups: 1283\\n}\\n}\\n```\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4lyme0\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"amleszk\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4lyme0/redditmedia_is_returnng_error_header_overflow/\", \"locked\": false, \"name\": \"t3_4lyme0\", \"created\": 1464771538.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4lyme0/redditmedia_is_returnng_error_header_overflow/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Redditmedia is returnng error 'Header overflow'\", \"created_utc\": 1464742738.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey there,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhen creating a new subreddit, there are several (unlisted) limitations on the subreddit name (special characters, min length, max length).  The error that comes back with an invalid name is \\u0026quot;that name isn\\u0026#39;t going to work\\u0026quot; - which makes it a bit difficult to figure out WHY the name is invalid.  So I\\u0026#39;m looking at adding more accurate responses to the error \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/lib/validator/validator.py#L692\\\"\\u003Ein the VSubredditName class\\u003C/a\\u003E.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EA set of more accurate error reasons exist for naming multi-reddits, \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/lib/validator/validator.py#L3076\\\"\\u003Efound here\\u003C/a\\u003E and listed below.  So it shouldn\\u0026#39;t be too hard to drop similar logic into where the subreddit name has come back as invalid (and the text already exists for translations).\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E        invalid_char = multi_name_chars_rx.search(name)\\n        if invalid_char:\\n            char = invalid_char.group()\\n            if char == \\u0026#39; \\u0026#39;:\\n                reason = _(\\u0026#39;no spaces allowed\\u0026#39;)\\n            else:\\n                reason = _(\\u0026quot;invalid character: \\u0026#39;%s\\u0026#39;\\u0026quot;) % char\\n        elif name[0] == \\u0026#39;_\\u0026#39;:\\n            reason = _(\\u0026quot;can\\u0026#39;t start with a \\u0026#39;_\\u0026#39;\\u0026quot;)\\n        elif len(name) \\u0026lt; 2:\\n            reason = _(\\u0026#39;that name is too short\\u0026#39;)\\n        elif len(name) \\u0026gt; 21:\\n            reason = _(\\u0026#39;that name is too long\\u0026#39;)\\n        else:\\n            reason = _(\\u0026quot;that name isn\\u0026#39;t going to work\\u0026quot;)\\n\\n        self.set_error(\\u0026#39;BAD_MULTI_NAME\\u0026#39;, {\\u0026#39;reason\\u0026#39;: reason}, code=400)\\n        return\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThe thing that I\\u0026#39;m not sure about is the error statement.  The current error is \\u0026#39;BAD_SR_NAME\\u0026#39; and is the generic \\u0026quot;that name isn\\u0026#39;t going to work\\u0026quot; message.  Looking at the multi-reddit error \\u0026#39;BAD_MULTI_NAME\\u0026#39;, it passes a reason as well and that is the error message.  I\\u0026#39;m not sure the workflow for the errors though.  Should I create a new error (and is that as simple as adding a new entry to the error_list in errors.py?) like \\u0026#39;BAD_SR_NAME_REASON\\u0026#39; and have the message be \\u0026quot;%(reason)s\\u0026quot; ?  Would it be better to edit the \\u0026#39;BAD_SR_NAME\\u0026#39; to use a reason string (might require several edits to wherever that error is used)?  I\\u0026#39;m assuming that using the existing \\u0026#39;BAD_MULTI_NAME\\u0026#39; error for a subreddit name would be a bad idea for code tracability.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny advice on whats the \\u003Cem\\u003Eright way\\u003C/em\\u003E to add a reason to this error?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey there,\\n\\nWhen creating a new subreddit, there are several (unlisted) limitations on the subreddit name (special characters, min length, max length).  The error that comes back with an invalid name is \\\"that name isn't going to work\\\" - which makes it a bit difficult to figure out WHY the name is invalid.  So I'm looking at adding more accurate responses to the error [in the VSubredditName class](https://github.com/reddit/reddit/blob/master/r2/r2/lib/validator/validator.py#L692).  \\n\\n\\nA set of more accurate error reasons exist for naming multi-reddits, [found here](https://github.com/reddit/reddit/blob/master/r2/r2/lib/validator/validator.py#L3076) and listed below.  So it shouldn't be too hard to drop similar logic into where the subreddit name has come back as invalid (and the text already exists for translations).\\n\\n            invalid_char = multi_name_chars_rx.search(name)\\n            if invalid_char:\\n                char = invalid_char.group()\\n                if char == ' ':\\n                    reason = _('no spaces allowed')\\n                else:\\n                    reason = _(\\\"invalid character: '%s'\\\") % char\\n            elif name[0] == '_':\\n                reason = _(\\\"can't start with a '_'\\\")\\n            elif len(name) \\u003C 2:\\n                reason = _('that name is too short')\\n            elif len(name) \\u003E 21:\\n                reason = _('that name is too long')\\n            else:\\n                reason = _(\\\"that name isn't going to work\\\")\\n\\n            self.set_error('BAD_MULTI_NAME', {'reason': reason}, code=400)\\n            return\\n\\n\\nThe thing that I'm not sure about is the error statement.  The current error is 'BAD_SR_NAME' and is the generic \\\"that name isn't going to work\\\" message.  Looking at the multi-reddit error 'BAD_MULTI_NAME', it passes a reason as well and that is the error message.  I'm not sure the workflow for the errors though.  Should I create a new error (and is that as simple as adding a new entry to the error_list in errors.py?) like 'BAD_SR_NAME_REASON' and have the message be \\\"%(reason)s\\\" ?  Would it be better to edit the 'BAD_SR_NAME' to use a reason string (might require several edits to wherever that error is used)?  I'm assuming that using the existing 'BAD_MULTI_NAME' error for a subreddit name would be a bad idea for code tracability.\\n\\nAny advice on whats the *right way* to add a reason to this error?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4kehr1\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"skullkid2424\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4kehr1/looking_to_add_better_error_responses_for_invalid/\", \"locked\": false, \"name\": \"t3_4kehr1\", \"created\": 1463880029.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4kehr1/looking_to_add_better_error_responses_for_invalid/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Looking to add better error responses for invalid subreddit names\", \"created_utc\": 1463851229.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello, I just install reddit on my ubuntu server but I get a 500 error when I try to confirm the email address of admin account.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EProbably I miss to set up something. Some help please ?\\nThank You in advance. ;)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello, I just install reddit on my ubuntu server but I get a 500 error when I try to confirm the email address of admin account.\\n\\nProbably I miss to set up something. Some help please ?\\nThank You in advance. ;)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4j1o1r\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"hhaevs\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 14, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4j1o1r/how_to_set_up_email_services/\", \"locked\": false, \"name\": \"t3_4j1o1r\", \"created\": 1463101213.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4j1o1r/how_to_set_up_email_services/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to set up email services ?\", \"created_utc\": 1463072413.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EFor example:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026gt;\\u0026gt;\\u0026gt; r = login()\\n\\u0026gt;\\u0026gt;\\u0026gt; me = r.get_me()\\n\\u0026gt;\\u0026gt;\\u0026gt; comments = me.comments(limit=500, fetch=True)\\n\\u0026gt;\\u0026gt;\\u0026gt; comments\\n\\u0026lt;generator object get_content at 0x03292C90\\u0026gt;\\n\\u0026gt;\\u0026gt;\\u0026gt; comments = list(comments)\\n\\u0026gt;\\u0026gt;\\u0026gt; len(comments)\\n500\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIt seems to ignore the fetch=True parameter, just returning a generator object instead of actually fetching the data. I can never get it to work, and I can only usually fetch content by listing it from a generator object. All of my fetches look like:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esubmissions = list(r.get_subreddit(\\u0026quot;xxx\\u0026quot;).get_new(limit=250))\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Ebecause omitting the list results in various errors because the generator object doesn\\u0026#39;t fetch the data when I try to access it. Looping through it works fine, but trying to access a single item never does:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026gt;\\u0026gt;\\u0026gt; sub = r.get_subreddit(\\u0026quot;asmr\\u0026quot;)\\n\\u0026gt;\\u0026gt;\\u0026gt; submissions = sub.get_new(limit=250, fetch=True)\\n\\u0026gt;\\u0026gt;\\u0026gt; submissions\\n\\u0026lt;generator object get_content at 0x03292C60\\u0026gt;\\n\\u0026gt;\\u0026gt;\\u0026gt; submissions[0].title\\nTraceback (most recent call last):\\n  File \\u0026quot;\\u0026lt;stdin\\u0026gt;\\u0026quot;, line 1, in \\u0026lt;module\\u0026gt;\\nTypeError: \\u0026#39;generator\\u0026#39; object is not subscriptable\\n\\u0026gt;\\u0026gt;\\u0026gt;\\n\\u0026gt;\\u0026gt;\\u0026gt; for s in submissions:\\n...     print(s.title) #works fine\\n...\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAm I doing something wrong?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"For example:\\n\\n    \\u003E\\u003E\\u003E r = login()\\n    \\u003E\\u003E\\u003E me = r.get_me()\\n    \\u003E\\u003E\\u003E comments = me.comments(limit=500, fetch=True)\\n    \\u003E\\u003E\\u003E comments\\n    \\u003Cgenerator object get_content at 0x03292C90\\u003E\\n    \\u003E\\u003E\\u003E comments = list(comments)\\n    \\u003E\\u003E\\u003E len(comments)\\n    500\\n\\nIt seems to ignore the fetch=True parameter, just returning a generator object instead of actually fetching the data. I can never get it to work, and I can only usually fetch content by listing it from a generator object. All of my fetches look like:\\n\\n    submissions = list(r.get_subreddit(\\\"xxx\\\").get_new(limit=250))\\n\\nbecause omitting the list results in various errors because the generator object doesn't fetch the data when I try to access it. Looping through it works fine, but trying to access a single item never does:\\n\\n    \\u003E\\u003E\\u003E sub = r.get_subreddit(\\\"asmr\\\")\\n    \\u003E\\u003E\\u003E submissions = sub.get_new(limit=250, fetch=True)\\n    \\u003E\\u003E\\u003E submissions\\n    \\u003Cgenerator object get_content at 0x03292C60\\u003E\\n    \\u003E\\u003E\\u003E submissions[0].title\\n    Traceback (most recent call last):\\n      File \\\"\\u003Cstdin\\u003E\\\", line 1, in \\u003Cmodule\\u003E\\n    TypeError: 'generator' object is not subscriptable\\n    \\u003E\\u003E\\u003E\\n    \\u003E\\u003E\\u003E for s in submissions:\\n    ...     print(s.title) #works fine\\n    ...\\n\\n\\n\\nAm I doing something wrong?\\n    \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4h8q7d\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"theonefoster\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1462103956.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4h8q7d/praw_could_someone_explain_how_the_fetchtrue/\", \"locked\": false, \"name\": \"t3_4h8q7d\", \"created\": 1462132023.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4h8q7d/praw_could_someone_explain_how_the_fetchtrue/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[PRAW] Could someone explain how the fetch=True parameter works? It doesn't seem to work for me.\", \"created_utc\": 1462103223.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHow would you implement this feature in a native client?\\nI can\\u0026#39;t find any JSON parameter that indicates comment is new since last visit\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://i.imgur.com/7lllNsB.png\\\"\\u003EExample\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"How would you implement this feature in a native client?\\nI can't find any JSON parameter that indicates comment is new since last visit\\n\\n[Example](http://i.imgur.com/7lllNsB.png)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4h4s5a\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"amleszk\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4h4s5a/highlight_comments_posted_since_previous_visit_api/\", \"locked\": false, \"name\": \"t3_4h4s5a\", \"created\": 1462057697.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4h4s5a/highlight_comments_posted_since_previous_visit_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Highlight comments posted since previous visit - API?\", \"created_utc\": 1462028897.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHere are my following settings below: \\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EAdd token to: \\u003Cstrong\\u003EHeader\\u003C/strong\\u003E | Url\\u003C/li\\u003E\\n\\u003Cli\\u003ECallback Url: \\u003Ca href=\\\"https://www.getpostman.com/oauth2/callback\\\"\\u003Ehttps://www.getpostman.com/oauth2/callback\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003EAuth Url: \\u003Ca href=\\\"https://www.reddit.com/api/v1/authorize?client_id=%5BmyClientID%5D\\u0026amp;response_type=code\\u0026amp;state=hello\\u0026amp;redirect_uri=https://www.getpostman.com/oauth2/callback\\u0026amp;duration=permanent\\u0026amp;scope=identity\\\"\\u003Ehttps://www.reddit.com/api/v1/authorize?client_id=[myClientID]\\u0026amp;response_type=code\\u0026amp;state=hello\\u0026amp;redirect_uri=https://www.getpostman.com/oauth2/callback\\u0026amp;duration=permanent\\u0026amp;scope=identity\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003EClient ID: [myClientId]\\u003C/li\\u003E\\n\\u003Cli\\u003EClient Secret: [myClientSecret]\\u003C/li\\u003E\\n\\u003Cli\\u003EScope: [n/a]\\u003C/li\\u003E\\n\\u003Cli\\u003EToken Name: [n/a]\\u003C/li\\u003E\\n\\u003Cli\\u003EGrant Type: \\u003Cstrong\\u003EClient Credentials\\u003C/strong\\u003E | Authorization Code\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003ENote: [n/a] i left blank, and for the options, I chose the bolded string above. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnyone have any clue what I\\u0026#39;m configuring wrong? I am getting 403 errors. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Here are my following settings below: \\n\\n* Add token to: **Header** | Url\\n* Callback Url: https://www.getpostman.com/oauth2/callback\\n* Auth Url: https://www.reddit.com/api/v1/authorize?client_id=[myClientID]\\u0026response_type=code\\u0026state=hello\\u0026redirect_uri=https://www.getpostman.com/oauth2/callback\\u0026duration=permanent\\u0026scope=identity\\n* Client ID: [myClientId]\\n* Client Secret: [myClientSecret]\\n* Scope: [n/a]\\n* Token Name: [n/a]\\n* Grant Type: **Client Credentials** | Authorization Code\\n\\nNote: [n/a] i left blank, and for the options, I chose the bolded string above. \\n\\nAnyone have any clue what I'm configuring wrong? I am getting 403 errors. \\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4fja20\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"Redditisshit11\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1461094058.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4fja20/using_postman_to_test_reddits_api_but_getting/\", \"locked\": false, \"name\": \"t3_4fja20\", \"created\": 1461122677.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4fja20/using_postman_to_test_reddits_api_but_getting/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Using Postman to test reddit's api, but getting errors for authentication\", \"created_utc\": 1461093877.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve tried messaging the account and didn\\u0026#39;t get a response, and couldn\\u0026#39;t find a human account linked to the bot.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EImgur\\u0026#39;s API used to have a function to obtain the text off a meme, but it appears that\\u0026#39;s no longer supported.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPytesseract works well for normal documents, but doesn\\u0026#39;t have good results with memes.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDoes anyone know how it works, or at least who is the human account behind the bot?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've tried messaging the account and didn't get a response, and couldn't find a human account linked to the bot.\\n\\nImgur's API used to have a function to obtain the text off a meme, but it appears that's no longer supported.\\n\\nPytesseract works well for normal documents, but doesn't have good results with memes.\\n\\nDoes anyone know how it works, or at least who is the human account behind the bot?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4chle4\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"TeroTheTerror\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4chle4/what_ocr_does_uimgurtranscriber_use/\", \"locked\": false, \"name\": \"t3_4chle4\", \"created\": 1459315342.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4chle4/what_ocr_does_uimgurtranscriber_use/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"What OCR does /u/imgurtranscriber use?\", \"created_utc\": 1459286542.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ETheir current markdown parser: \\u003Ca href=\\\"https://github.com/reddit/snudown\\\"\\u003Ehttps://github.com/reddit/snudown\\u003C/a\\u003E. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHoedown: \\u003Ca href=\\\"https://github.com/hoedown/hoedown\\\"\\u003Ehttps://github.com/hoedown/hoedown\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERedcarpet, developed by the person who developed Sundown and parts of SnuDown: \\u003Ca href=\\\"https://github.com/vmg/redcarpet\\\"\\u003Ehttps://github.com/vmg/redcarpet\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnd Sundown, the dead project they were all based off of: \\u003Ca href=\\\"https://github.com/vmg/sundown\\\"\\u003Ehttps://github.com/vmg/sundown\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWould Reddit benefit from switching to any of these in terms of performance? Is there any way to run tests on them to find out which one has best performance?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Their current markdown parser: https://github.com/reddit/snudown. \\n\\nHoedown: https://github.com/hoedown/hoedown\\n\\nRedcarpet, developed by the person who developed Sundown and parts of SnuDown: https://github.com/vmg/redcarpet\\n\\nAnd Sundown, the dead project they were all based off of: https://github.com/vmg/sundown\\n\\nWould Reddit benefit from switching to any of these in terms of performance? Is there any way to run tests on them to find out which one has best performance?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"45nud1\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"bored_reddit_user\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/45nud1/would_reddit_benefit_from_switching_from_snudown/\", \"locked\": false, \"name\": \"t3_45nud1\", \"created\": 1455441606.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/45nud1/would_reddit_benefit_from_switching_from_snudown/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Would Reddit benefit from switching from SnuDown to one of these markdown parsers?\", \"created_utc\": 1455412806.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m trying to authenticate a mobile app which I\\u0026#39;m writing.  I have successfully retrieved an authorization token, and receive the response:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eredditviewer://redirect?state=IftHM6TEpiTwdmAdv5ms\\u0026amp;code=JZH125IgzOR1iSrNMK8IVrXxp1g\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ESo far, so good.  Before I write the access token request I am trying to test it in Postman.  I form the following POST request:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EPOST /api/v1/access_token HTTP/1.1\\nHost: www.reddit.com\\nAuthorization: Basic [HIDDEN]\\nUser-Agent: Mozilla/5.0 (iPhone; CPU iPhone OS 9_1 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Mobile/13B137\\nCache-Control: no-cache\\nPostman-Token: 164b5690-40cf-36c7-6e24-0c257d01dfc5\\nContent-Type: application/x-www-form-urlencoded\\n\\ngrant_type=authorization_code\\u0026amp;code=JZH125IgzOR1iSrNMK8IVrXxp1g\\u0026amp;redirect_uri=google.com\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAnd I receive \\u003Ccode\\u003Einvalid_grant\\u003C/code\\u003E.  I\\u0026#39;ve checked all the obvious stuff: my auth is correct (otherwise I get a 401), it\\u0026#39;s a brand new code, it hasn\\u0026#39;t been used already - what might I be missing?  Is there any reason that testing using Postman could be causing me problems?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks in advance!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi\\n\\nI'm trying to authenticate a mobile app which I'm writing.  I have successfully retrieved an authorization token, and receive the response:\\n\\n    redditviewer://redirect?state=IftHM6TEpiTwdmAdv5ms\\u0026code=JZH125IgzOR1iSrNMK8IVrXxp1g\\n\\nSo far, so good.  Before I write the access token request I am trying to test it in Postman.  I form the following POST request:\\n\\n    POST /api/v1/access_token HTTP/1.1\\n    Host: www.reddit.com\\n    Authorization: Basic [HIDDEN]\\n    User-Agent: Mozilla/5.0 (iPhone; CPU iPhone OS 9_1 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Mobile/13B137\\n    Cache-Control: no-cache\\n    Postman-Token: 164b5690-40cf-36c7-6e24-0c257d01dfc5\\n    Content-Type: application/x-www-form-urlencoded\\n\\n    grant_type=authorization_code\\u0026code=JZH125IgzOR1iSrNMK8IVrXxp1g\\u0026redirect_uri=google.com\\n\\nAnd I receive `invalid_grant`.  I've checked all the obvious stuff: my auth is correct (otherwise I get a 401), it's a brand new code, it hasn't been used already - what might I be missing?  Is there any reason that testing using Postman could be causing me problems?\\n\\nThanks in advance!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"43dgek\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"PM_ME_YOUR_OVEN_MITT\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/43dgek/getting_invalid_grant_but_cant_figure_out_why/\", \"locked\": false, \"name\": \"t3_43dgek\", \"created\": 1454174718.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/43dgek/getting_invalid_grant_but_cant_figure_out_why/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Getting invalid_grant but can't figure out why\", \"created_utc\": 1454145918.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi,  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve recently decided to install reddit on my machine for dev purposes, so I grabbed a copy of Linux Mint 17.3 which I believe is based on Ubuntu 14.04.3 ..  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve installed the system, updated it, installed git, cloned the git repository, and whenever I try to do  \\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003Esudo ./install-reddit.sh  \\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003Eit throws out these errors:  \\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E./install-reddit.sh: 26: ./install-reddit.sh: source: not found\\u003Cbr/\\u003E\\n./install-reddit.sh: 32: ./install-reddit.sh: [[: not found\\u003Cbr/\\u003E\\n./install-reddit.sh: 37: ./install-reddit.sh: [[: not found\\u003Cbr/\\u003E\\n./install-reddit.sh: 51: ./install-reddit.sh: [[: not found\\u003Cbr/\\u003E\\n./install-reddit.sh: 66: ./install-reddit.sh: source: not found\\u003Cbr/\\u003E\\nERROR: Only Ubuntu 14.04 is supported.  \\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EI would be glad if you guys helped me through this :(\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi,  \\n  \\nI've recently decided to install reddit on my machine for dev purposes, so I grabbed a copy of Linux Mint 17.3 which I believe is based on Ubuntu 14.04.3 ..  \\n  \\nI've installed the system, updated it, installed git, cloned the git repository, and whenever I try to do  \\n\\u003Esudo ./install-reddit.sh  \\n  \\nit throws out these errors:  \\n  \\n\\u003E\\u003E ./install-reddit.sh: 26: ./install-reddit.sh: source: not found  \\n./install-reddit.sh: 32: ./install-reddit.sh: [[: not found  \\n./install-reddit.sh: 37: ./install-reddit.sh: [[: not found  \\n./install-reddit.sh: 51: ./install-reddit.sh: [[: not found  \\n./install-reddit.sh: 66: ./install-reddit.sh: source: not found  \\nERROR: Only Ubuntu 14.04 is supported.  \\n  \\nI would be glad if you guys helped me through this :(\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"42g48c\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"iEmerald\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/42g48c/the_redditinstallsh_script_fails_on_mint_173_xfce/\", \"locked\": false, \"name\": \"t3_42g48c\", \"created\": 1453676030.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/42g48c/the_redditinstallsh_script_fails_on_mint_173_xfce/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"The reddit-install.sh script fails on Mint 17.3 Xfce\", \"created_utc\": 1453647230.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"pokechu22.github.io\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"424c7w\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"Pokechu22\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/424c7w/my_experimental_search_query_editor_its_still_a/\", \"locked\": false, \"name\": \"t3_424c7w\", \"created\": 1453472808.0, \"url\": \"http://pokechu22.github.io/Reddit-search-tool/search.html\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"My experimental search query editor. It's still a work in progress, but I have phrase search and timestamps mostly set up (among other things).\", \"created_utc\": 1453444008.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}], \"after\": \"t3_424c7w\", \"before\": null}}"
        },
        "headers": {
          "CF-RAY": "2b59b9c0158d20c6-LAX",
          "Connection": "keep-alive",
          "Content-Type": "application/json; charset=UTF-8",
          "Date": "Sun, 19 Jun 2016 20:28:08 GMT",
          "Server": "cloudflare-nginx",
          "Strict-Transport-Security": "max-age=15552000; includeSubDomains; preload",
          "Transfer-Encoding": "chunked",
          "Vary": "accept-encoding",
          "X-Moose": "majestic",
          "cache-control": "private, s-maxage=0, max-age=0, must-revalidate",
          "expires": "-1",
          "x-content-type-options": "nosniff",
          "x-frame-options": "SAMEORIGIN",
          "x-ratelimit-remaining": "581.0",
          "x-ratelimit-reset": "113",
          "x-ratelimit-used": "19",
          "x-reddit-tracking": "https://pixel.redditmedia.com/pixel/of_destiny.png?v=vgWS31lI%2BW2rHhF5P%2BmdVUVvqSCIeVzgRTzGjexu5cCaJ%2FRMHJ7RYHoEVE9Omd6OMI4zphFkQqTly7OnlCGf1M3%2Byum3p%2BVX",
          "x-ua-compatible": "IE=edge",
          "x-xss-protection": "1; mode=block"
        },
        "status": {
          "code": 200,
          "message": "OK"
        },
        "url": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_2ak90p&raw_json=1"
      }
    },
    {
      "recorded_at": "2016-06-19T20:28:09",
      "request": {
        "body": {
          "encoding": "utf-8",
          "string": ""
        },
        "headers": {
          "Accept": "*/*",
          "Accept-Encoding": "identity",
          "Authorization": "bearer YE4ESXZ9lTjIP0ukcopwXM6361g",
          "Connection": "keep-alive",
          "Cookie": "loid=SdAZIdDQoUDzjNqfRk; loidcreated=2016-06-19T20%3A27%3A59.507Z; __cfduid=da3dbedd6bfa050f4f12fea14395b830f1466368079",
          "User-Agent": "prawtools/0.19 PRAW/4.0.0b4 prawcore/0.0.8"
        },
        "method": "GET",
        "uri": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_424c7w&raw_json=1"
      },
      "response": {
        "body": {
          "encoding": "UTF-8",
          "string": "{\"kind\": \"Listing\", \"data\": {\"modhash\": null, \"children\": [{\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi everyone. I\\u0026#39;m new to Reddit\\u0026#39;s API and PRAW. This problem came up in a more complex script I\\u0026#39;m writing for private subreddits, but I was able to simplify it down to these calls.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am using Python 3.4.4. I have tried PRAW 3.3.0 and the newest GitHub version.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe account I\\u0026#39;m using is a moderator of \\u0026lt;my_private_subreddit\\u0026gt;. It has granted all OAuth scopes to the script.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have authenticated manually with \\u003Ccode\\u003Er.refresh_access_information(\\u0026lt;token\\u0026gt;)\\u003C/code\\u003E (\\u003Ca href=\\\"https://www.reddit.com/r/GoldTesting/comments/3cm1p8/how_to_make_your_bot_use_oauth2/\\\"\\u003Eafter following this tutorial\\u003C/a\\u003E by \\u003Ca href=\\\"/u/GoldenSights\\\"\\u003E/u/GoldenSights\\u003C/a\\u003E) and with OAuth2Util but it doesn\\u0026#39;t change anything. So I don\\u0026#39;t think it\\u0026#39;s a problem with my authentication. All normal PRAW methods work as expected (\\u003Ccode\\u003Er.get_subreddit()\\u003C/code\\u003E, etc.). \\u003Ccode\\u003Erequest()\\u003C/code\\u003E and \\u003Ccode\\u003Erequest_json()\\u003C/code\\u003E seem like edge cases.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003Emybot.py\\u003C/code\\u003E is a setup script, using OAuth2Util for simplicity.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E# mybot.py\\n\\nimport praw\\nimport OAuth2Util\\n\\ndef login():\\n    r = praw.Reddit(\\u0026lt;my_user_agent\\u0026gt;, log_requests=2)\\n    o = OAuth2Util.OAuth2Util(r)\\n    # o.refresh(force=True) # Makes no difference to this problem\\n    return r\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003E1:\\u003C/strong\\u003E Here are some examples from the Python Interpreter (with \\u003Ccode\\u003Elog_requests=2\\u003C/code\\u003E) to show the problem:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026gt;\\u0026gt;\\u0026gt; import mybot\\n\\u0026gt;\\u0026gt;\\u0026gt; r=mybot.login() # Login works fine\\nsubstituting https://oauth.reddit.com for https://api.reddit.com in url\\nGET: https://oauth.reddit.com/api/v1/me.json\\nstatus: 200\\n\\u0026gt;\\u0026gt;\\u0026gt; r.request(\\u0026quot;https://www.reddit.com\\u0026quot;) # Works fine\\nGET: https://www.reddit.com\\nstatus: 200\\n\\u0026lt;Response [200]\\u0026gt;\\n\\u0026gt;\\u0026gt;\\u0026gt; r.request(\\u0026quot;https://www.reddit.com/r/AskReddit\\u0026quot;) # Public subreddit, works fine\\nGET: https://www.reddit.com/r/AskReddit\\nstatus: 200\\n\\u0026lt;Response [200]\\u0026gt;\\n\\u0026gt;\\u0026gt;\\u0026gt; r.request(\\u0026quot;https://www.reddit.com/r/\\u0026lt;my_private_subreddit\\u0026gt;\\u0026quot;) # Private subreddit moderated by account, does not work, returns 403\\nGET: https://www.reddit.com/r/\\u0026lt;my_private_subreddit\\u0026gt;\\nstatus: 403\\nTraceback (most recent call last):\\n  File \\u0026quot;\\u0026lt;stdin\\u0026gt;\\u0026quot;, line 1, in \\u0026lt;module\\u0026gt;\\n  File \\u0026quot;\\u0026lt;decorator-gen-7\\u0026gt;\\u0026quot;, line 2, in request\\n  File \\u0026quot;\\u0026lt;...\\u0026gt;/python3.4/site-packages/praw/decorators.py\\u0026quot;, line 116, in raise_api_exceptions\\n    return_value = function(*args, **kwargs)\\n  File \\u0026quot;\\u0026lt;...\\u0026gt;/python3.4/site-packages/praw/__init__.py\\u0026quot;, line 599, in request\\n    retry_on_error=retry_on_error, method=method)\\n  File \\u0026quot;\\u0026lt;...\\u0026gt;/python3.4/site-packages/praw/__init__.py\\u0026quot;, line 451, in _request\\n    _raise_response_exceptions(response)\\n  File \\u0026quot;\\u0026lt;...\\u0026gt;/python3.4/site-packages/praw/internal.py\\u0026quot;, line 208, in _raise_response_exceptions\\n    raise Forbidden(_raw=response)\\npraw.errors.Forbidden: HTTP error\\n\\u0026gt;\\u0026gt;\\u0026gt; r.request(\\u0026quot;https://www.reddit.com/r/\\u0026lt;my_private_subreddit\\u0026gt;\\u0026quot;) # Exact same request, this time it works fine\\nsubstituting https://oauth.reddit.com for https://www.reddit.com in url\\nGET: https://oauth.reddit.com/r/\\u0026lt;my_private_subreddit\\u0026gt;\\nstatus: 200\\n\\u0026lt;Response [200]\\u0026gt;\\n\\u0026gt;\\u0026gt;\\u0026gt; r.request(\\u0026quot;https://www.reddit.com/r/\\u0026lt;my_private_subreddit\\u0026gt;\\u0026quot;) # Exact same request, still works fine\\nsubstituting https://oauth.reddit.com for https://www.reddit.com in url\\nGET: https://oauth.reddit.com/r/\\u0026lt;my_private_subreddit\\u0026gt;\\nstatus: 200\\n\\u0026lt;Response [200]\\u0026gt;\\n\\u0026gt;\\u0026gt;\\u0026gt; r=mybot.login() # Authenticate again\\nsubstituting https://oauth.reddit.com for https://api.reddit.com in url\\nGET: https://oauth.reddit.com/api/v1/me.json\\nstatus: 200\\n\\u0026gt;\\u0026gt;\\u0026gt; r.request(\\u0026quot;https://www.reddit.com/r/\\u0026lt;my_private_subreddit\\u0026gt;\\u0026quot;) # Exact same request after authenticating again, returns 403\\nGET: https://www.reddit.com/r/\\u0026lt;my_private_subreddit\\u0026gt;\\nstatus: 403\\nTraceback (most recent call last):\\n  File \\u0026quot;\\u0026lt;stdin\\u0026gt;\\u0026quot;, line 1, in \\u0026lt;module\\u0026gt;\\n  File \\u0026quot;\\u0026lt;decorator-gen-7\\u0026gt;\\u0026quot;, line 2, in request\\n  File \\u0026quot;\\u0026lt;...\\u0026gt;/python3.4/site-packages/praw/decorators.py\\u0026quot;, line 116, in raise_api_exceptions\\n    return_value = function(*args, **kwargs)\\n  File \\u0026quot;\\u0026lt;...\\u0026gt;/python3.4/site-packages/praw/__init__.py\\u0026quot;, line 599, in request\\n    retry_on_error=retry_on_error, method=method)\\n  File \\u0026quot;\\u0026lt;...\\u0026gt;/python3.4/site-packages/praw/__init__.py\\u0026quot;, line 451, in _request\\n    _raise_response_exceptions(response)\\n  File \\u0026quot;\\u0026lt;...\\u0026gt;/python3.4/site-packages/praw/internal.py\\u0026quot;, line 208, in _raise_response_exceptions\\n    raise Forbidden(_raw=response)\\npraw.errors.Forbidden: HTTP error\\n\\u0026gt;\\u0026gt;\\u0026gt; r.request(\\u0026quot;https://www.reddit.com/r/\\u0026lt;my_private_subreddit\\u0026gt;\\u0026quot;) # Exact same request, working fine again\\nsubstituting https://oauth.reddit.com for https://www.reddit.com in url\\nGET: https://oauth.reddit.com/r/\\u0026lt;my_private_subreddit\\u0026gt;\\nstatus: 200\\n\\u0026lt;Response [200]\\u0026gt;\\n\\u0026gt;\\u0026gt;\\u0026gt; r.request(\\u0026quot;https://www.reddit.com/r/\\u0026lt;my_private_subreddit\\u0026gt;\\u0026quot;) # Exact same request, still working fine\\nsubstituting https://oauth.reddit.com for https://www.reddit.com in url\\nGET: https://oauth.reddit.com/r/\\u0026lt;my_private_subreddit\\u0026gt;\\nstatus: 200\\n\\u0026lt;Response [200]\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003E2:\\u003C/strong\\u003E Here is another Python Interpreter session showing the same problem (I think), without needing to use a private subreddit:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026gt;\\u0026gt;\\u0026gt; import mybot\\n\\u0026gt;\\u0026gt;\\u0026gt; r=mybot.login()\\nsubstituting https://oauth.reddit.com for https://api.reddit.com in url\\nGET: https://oauth.reddit.com/api/v1/me.json\\nstatus: 200\\n\\u0026gt;\\u0026gt;\\u0026gt; r.request(\\u0026quot;https://oauth.reddit.com\\u0026quot;)\\nGET: https://oauth.reddit.com\\nstatus: 403\\nTraceback (most recent call last):\\n  File \\u0026quot;\\u0026lt;stdin\\u0026gt;\\u0026quot;, line 1, in \\u0026lt;module\\u0026gt;\\n  File \\u0026quot;\\u0026lt;decorator-gen-7\\u0026gt;\\u0026quot;, line 2, in request\\n  File \\u0026quot;\\u0026lt;...\\u0026gt;/python3.4/site-packages/praw/decorators.py\\u0026quot;, line 116, in raise_api_exceptions\\n    return_value = function(*args, **kwargs)\\n  File \\u0026quot;\\u0026lt;...\\u0026gt;/python3.4/site-packages/praw/__init__.py\\u0026quot;, line 599, in request\\n    retry_on_error=retry_on_error, method=method)\\n  File \\u0026quot;\\u0026lt;...\\u0026gt;/python3.4/site-packages/praw/__init__.py\\u0026quot;, line 451, in _request\\n    _raise_response_exceptions(response)\\n  File \\u0026quot;\\u0026lt;...\\u0026gt;/python3.4/site-packages/praw/internal.py\\u0026quot;, line 208, in _raise_response_exceptions\\n    raise Forbidden(_raw=response)\\npraw.errors.Forbidden: HTTP error\\n\\u0026gt;\\u0026gt;\\u0026gt; r.request(\\u0026quot;https://oauth.reddit.com\\u0026quot;)\\nGET: https://oauth.reddit.com\\nstatus: 200\\n\\u0026lt;Response [200]\\u0026gt;\\n\\u0026gt;\\u0026gt;\\u0026gt; r.request(\\u0026quot;https://oauth.reddit.com\\u0026quot;)\\nGET: https://oauth.reddit.com\\nstatus: 200\\n\\u0026lt;Response [200]\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003E3:\\u003C/strong\\u003E The first 403 does not need to be on the exact same URL as the subsequent requests. Just the first request needing OAuth of the authenticated session:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026gt;\\u0026gt;\\u0026gt; import mybot\\n\\u0026gt;\\u0026gt;\\u0026gt; r=mybot.login()\\nsubstituting https://oauth.reddit.com for https://api.reddit.com in url\\nGET: https://oauth.reddit.com/api/v1/me.json\\nstatus: 200\\n\\u0026gt;\\u0026gt;\\u0026gt; r.request(\\u0026quot;https://oauth.reddit.com\\u0026quot;) # First request needing OAuth returns 403\\nGET: https://oauth.reddit.com\\nstatus: 403\\nTraceback (most recent call last):\\n  File \\u0026quot;\\u0026lt;stdin\\u0026gt;\\u0026quot;, line 1, in \\u0026lt;module\\u0026gt;\\n  File \\u0026quot;\\u0026lt;decorator-gen-7\\u0026gt;\\u0026quot;, line 2, in request\\n  File \\u0026quot;\\u0026lt;...\\u0026gt;/python3.4/site-packages/praw/decorators.py\\u0026quot;, line 116, in raise_api_exceptions\\n    return_value = function(*args, **kwargs)\\n  File \\u0026quot;\\u0026lt;...\\u0026gt;/python3.4/site-packages/praw/__init__.py\\u0026quot;, line 599, in request\\n    retry_on_error=retry_on_error, method=method)\\n  File \\u0026quot;\\u0026lt;...\\u0026gt;/python3.4/site-packages/praw/__init__.py\\u0026quot;, line 451, in _request\\n    _raise_response_exceptions(response)\\n  File \\u0026quot;\\u0026lt;...\\u0026gt;/python3.4/site-packages/praw/internal.py\\u0026quot;, line 208, in _raise_response_exceptions\\n    raise Forbidden(_raw=response)\\npraw.errors.Forbidden: HTTP error\\n\\u0026gt;\\u0026gt;\\u0026gt; r.request(\\u0026quot;https://www.reddit.com/r/\\u0026lt;my_private_subreddit\\u0026gt;\\u0026quot;) # Second request needing OAuth works fine, even though it\\u0026#39;s a different URL\\nsubstituting https://oauth.reddit.com for https://www.reddit.com in url\\nGET: https://oauth.reddit.com/r/\\u0026lt;my_private_subreddit\\u0026gt;\\nstatus: 200\\n\\u0026lt;Response [200]\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003ESubstitute \\u003Ccode\\u003Erequest_json()\\u003C/code\\u003E for \\u003Ccode\\u003Erequest()\\u003C/code\\u003E in these examples and it has the same problem.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EI could use a try/except like this:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Etry:\\n    r.request(\\u0026quot;https://www.reddit.com/r/\\u0026lt;my_private_subreddit\\u0026gt;\\u0026quot;) # Will always raise 403\\nexcept:\\n    r.request(\\u0026quot;https://www.reddit.com/r/\\u0026lt;my_private_subreddit\\u0026gt;\\u0026quot;) # Will work fine\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EBut I don\\u0026#39;t want to cause a lot of API errors, and I would rather fix the root of the problem.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFrom the above examples, it seems like the problem is:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe \\u003Cstrong\\u003Efirst\\u003C/strong\\u003E \\u003Ccode\\u003Erequest()\\u003C/code\\u003E or \\u003Ccode\\u003Erequest_json()\\u003C/code\\u003E call \\u003Cstrong\\u003Ethat needs to use OAuth\\u003C/strong\\u003E returns a 403 error, all other calls are fine.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/praw-dev/praw/issues/497\\\"\\u003EPRAW issue #497 on GitHub\\u003C/a\\u003E could possibly be related.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWould really appreciate any help with this. Thanks.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: Added example 3.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi everyone. I'm new to Reddit's API and PRAW. This problem came up in a more complex script I'm writing for private subreddits, but I was able to simplify it down to these calls.\\n\\nI am using Python 3.4.4. I have tried PRAW 3.3.0 and the newest GitHub version.\\n\\nThe account I'm using is a moderator of \\u003Cmy_private_subreddit\\u003E. It has granted all OAuth scopes to the script.\\n\\nI have authenticated manually with `r.refresh_access_information(\\u003Ctoken\\u003E)` ([after following this tutorial](https://www.reddit.com/r/GoldTesting/comments/3cm1p8/how_to_make_your_bot_use_oauth2/) by /u/GoldenSights) and with OAuth2Util but it doesn't change anything. So I don't think it's a problem with my authentication. All normal PRAW methods work as expected (`r.get_subreddit()`, etc.). `request()` and `request_json()` seem like edge cases.\\n\\n----\\n\\n`mybot.py` is a setup script, using OAuth2Util for simplicity.\\n\\n    # mybot.py\\n\\n    import praw\\n    import OAuth2Util\\n\\n    def login():\\n        r = praw.Reddit(\\u003Cmy_user_agent\\u003E, log_requests=2)\\n        o = OAuth2Util.OAuth2Util(r)\\n        # o.refresh(force=True) # Makes no difference to this problem\\n        return r\\n\\n----\\n\\n**1:** Here are some examples from the Python Interpreter (with `log_requests=2`) to show the problem:\\n\\n    \\u003E\\u003E\\u003E import mybot\\n    \\u003E\\u003E\\u003E r=mybot.login() # Login works fine\\n    substituting https://oauth.reddit.com for https://api.reddit.com in url\\n    GET: https://oauth.reddit.com/api/v1/me.json\\n    status: 200\\n    \\u003E\\u003E\\u003E r.request(\\\"https://www.reddit.com\\\") # Works fine\\n    GET: https://www.reddit.com\\n    status: 200\\n    \\u003CResponse [200]\\u003E\\n    \\u003E\\u003E\\u003E r.request(\\\"https://www.reddit.com/r/AskReddit\\\") # Public subreddit, works fine\\n    GET: https://www.reddit.com/r/AskReddit\\n    status: 200\\n    \\u003CResponse [200]\\u003E\\n    \\u003E\\u003E\\u003E r.request(\\\"https://www.reddit.com/r/\\u003Cmy_private_subreddit\\u003E\\\") # Private subreddit moderated by account, does not work, returns 403\\n    GET: https://www.reddit.com/r/\\u003Cmy_private_subreddit\\u003E\\n    status: 403\\n    Traceback (most recent call last):\\n      File \\\"\\u003Cstdin\\u003E\\\", line 1, in \\u003Cmodule\\u003E\\n      File \\\"\\u003Cdecorator-gen-7\\u003E\\\", line 2, in request\\n      File \\\"\\u003C...\\u003E/python3.4/site-packages/praw/decorators.py\\\", line 116, in raise_api_exceptions\\n        return_value = function(*args, **kwargs)\\n      File \\\"\\u003C...\\u003E/python3.4/site-packages/praw/__init__.py\\\", line 599, in request\\n        retry_on_error=retry_on_error, method=method)\\n      File \\\"\\u003C...\\u003E/python3.4/site-packages/praw/__init__.py\\\", line 451, in _request\\n        _raise_response_exceptions(response)\\n      File \\\"\\u003C...\\u003E/python3.4/site-packages/praw/internal.py\\\", line 208, in _raise_response_exceptions\\n        raise Forbidden(_raw=response)\\n    praw.errors.Forbidden: HTTP error\\n    \\u003E\\u003E\\u003E r.request(\\\"https://www.reddit.com/r/\\u003Cmy_private_subreddit\\u003E\\\") # Exact same request, this time it works fine\\n    substituting https://oauth.reddit.com for https://www.reddit.com in url\\n    GET: https://oauth.reddit.com/r/\\u003Cmy_private_subreddit\\u003E\\n    status: 200\\n    \\u003CResponse [200]\\u003E\\n    \\u003E\\u003E\\u003E r.request(\\\"https://www.reddit.com/r/\\u003Cmy_private_subreddit\\u003E\\\") # Exact same request, still works fine\\n    substituting https://oauth.reddit.com for https://www.reddit.com in url\\n    GET: https://oauth.reddit.com/r/\\u003Cmy_private_subreddit\\u003E\\n    status: 200\\n    \\u003CResponse [200]\\u003E\\n    \\u003E\\u003E\\u003E r=mybot.login() # Authenticate again\\n    substituting https://oauth.reddit.com for https://api.reddit.com in url\\n    GET: https://oauth.reddit.com/api/v1/me.json\\n    status: 200\\n    \\u003E\\u003E\\u003E r.request(\\\"https://www.reddit.com/r/\\u003Cmy_private_subreddit\\u003E\\\") # Exact same request after authenticating again, returns 403\\n    GET: https://www.reddit.com/r/\\u003Cmy_private_subreddit\\u003E\\n    status: 403\\n    Traceback (most recent call last):\\n      File \\\"\\u003Cstdin\\u003E\\\", line 1, in \\u003Cmodule\\u003E\\n      File \\\"\\u003Cdecorator-gen-7\\u003E\\\", line 2, in request\\n      File \\\"\\u003C...\\u003E/python3.4/site-packages/praw/decorators.py\\\", line 116, in raise_api_exceptions\\n        return_value = function(*args, **kwargs)\\n      File \\\"\\u003C...\\u003E/python3.4/site-packages/praw/__init__.py\\\", line 599, in request\\n        retry_on_error=retry_on_error, method=method)\\n      File \\\"\\u003C...\\u003E/python3.4/site-packages/praw/__init__.py\\\", line 451, in _request\\n        _raise_response_exceptions(response)\\n      File \\\"\\u003C...\\u003E/python3.4/site-packages/praw/internal.py\\\", line 208, in _raise_response_exceptions\\n        raise Forbidden(_raw=response)\\n    praw.errors.Forbidden: HTTP error\\n    \\u003E\\u003E\\u003E r.request(\\\"https://www.reddit.com/r/\\u003Cmy_private_subreddit\\u003E\\\") # Exact same request, working fine again\\n    substituting https://oauth.reddit.com for https://www.reddit.com in url\\n    GET: https://oauth.reddit.com/r/\\u003Cmy_private_subreddit\\u003E\\n    status: 200\\n    \\u003CResponse [200]\\u003E\\n    \\u003E\\u003E\\u003E r.request(\\\"https://www.reddit.com/r/\\u003Cmy_private_subreddit\\u003E\\\") # Exact same request, still working fine\\n    substituting https://oauth.reddit.com for https://www.reddit.com in url\\n    GET: https://oauth.reddit.com/r/\\u003Cmy_private_subreddit\\u003E\\n    status: 200\\n    \\u003CResponse [200]\\u003E\\n\\n\\n----\\n\\n**2:** Here is another Python Interpreter session showing the same problem (I think), without needing to use a private subreddit:\\n\\n    \\u003E\\u003E\\u003E import mybot\\n    \\u003E\\u003E\\u003E r=mybot.login()\\n    substituting https://oauth.reddit.com for https://api.reddit.com in url\\n    GET: https://oauth.reddit.com/api/v1/me.json\\n    status: 200\\n    \\u003E\\u003E\\u003E r.request(\\\"https://oauth.reddit.com\\\")\\n    GET: https://oauth.reddit.com\\n    status: 403\\n    Traceback (most recent call last):\\n      File \\\"\\u003Cstdin\\u003E\\\", line 1, in \\u003Cmodule\\u003E\\n      File \\\"\\u003Cdecorator-gen-7\\u003E\\\", line 2, in request\\n      File \\\"\\u003C...\\u003E/python3.4/site-packages/praw/decorators.py\\\", line 116, in raise_api_exceptions\\n        return_value = function(*args, **kwargs)\\n      File \\\"\\u003C...\\u003E/python3.4/site-packages/praw/__init__.py\\\", line 599, in request\\n        retry_on_error=retry_on_error, method=method)\\n      File \\\"\\u003C...\\u003E/python3.4/site-packages/praw/__init__.py\\\", line 451, in _request\\n        _raise_response_exceptions(response)\\n      File \\\"\\u003C...\\u003E/python3.4/site-packages/praw/internal.py\\\", line 208, in _raise_response_exceptions\\n        raise Forbidden(_raw=response)\\n    praw.errors.Forbidden: HTTP error\\n    \\u003E\\u003E\\u003E r.request(\\\"https://oauth.reddit.com\\\")\\n    GET: https://oauth.reddit.com\\n    status: 200\\n    \\u003CResponse [200]\\u003E\\n    \\u003E\\u003E\\u003E r.request(\\\"https://oauth.reddit.com\\\")\\n    GET: https://oauth.reddit.com\\n    status: 200\\n    \\u003CResponse [200]\\u003E\\n\\n----\\n\\n**3:** The first 403 does not need to be on the exact same URL as the subsequent requests. Just the first request needing OAuth of the authenticated session:\\n\\n    \\u003E\\u003E\\u003E import mybot\\n    \\u003E\\u003E\\u003E r=mybot.login()\\n    substituting https://oauth.reddit.com for https://api.reddit.com in url\\n    GET: https://oauth.reddit.com/api/v1/me.json\\n    status: 200\\n    \\u003E\\u003E\\u003E r.request(\\\"https://oauth.reddit.com\\\") # First request needing OAuth returns 403\\n    GET: https://oauth.reddit.com\\n    status: 403\\n    Traceback (most recent call last):\\n      File \\\"\\u003Cstdin\\u003E\\\", line 1, in \\u003Cmodule\\u003E\\n      File \\\"\\u003Cdecorator-gen-7\\u003E\\\", line 2, in request\\n      File \\\"\\u003C...\\u003E/python3.4/site-packages/praw/decorators.py\\\", line 116, in raise_api_exceptions\\n        return_value = function(*args, **kwargs)\\n      File \\\"\\u003C...\\u003E/python3.4/site-packages/praw/__init__.py\\\", line 599, in request\\n        retry_on_error=retry_on_error, method=method)\\n      File \\\"\\u003C...\\u003E/python3.4/site-packages/praw/__init__.py\\\", line 451, in _request\\n        _raise_response_exceptions(response)\\n      File \\\"\\u003C...\\u003E/python3.4/site-packages/praw/internal.py\\\", line 208, in _raise_response_exceptions\\n        raise Forbidden(_raw=response)\\n    praw.errors.Forbidden: HTTP error\\n    \\u003E\\u003E\\u003E r.request(\\\"https://www.reddit.com/r/\\u003Cmy_private_subreddit\\u003E\\\") # Second request needing OAuth works fine, even though it's a different URL\\n    substituting https://oauth.reddit.com for https://www.reddit.com in url\\n    GET: https://oauth.reddit.com/r/\\u003Cmy_private_subreddit\\u003E\\n    status: 200\\n    \\u003CResponse [200]\\u003E\\n\\n----\\n\\nSubstitute `request_json()` for `request()` in these examples and it has the same problem.\\n\\n----\\n\\nI could use a try/except like this:\\n\\n    try:\\n        r.request(\\\"https://www.reddit.com/r/\\u003Cmy_private_subreddit\\u003E\\\") # Will always raise 403\\n    except:\\n        r.request(\\\"https://www.reddit.com/r/\\u003Cmy_private_subreddit\\u003E\\\") # Will work fine\\n\\nBut I don't want to cause a lot of API errors, and I would rather fix the root of the problem.\\n\\nFrom the above examples, it seems like the problem is:\\n\\nThe **first** `request()` or `request_json()` call **that needs to use OAuth** returns a 403 error, all other calls are fine.\\n\\n[PRAW issue #497 on GitHub](https://github.com/praw-dev/praw/issues/497) could possibly be related.\\n\\nWould really appreciate any help with this. Thanks.\\n\\nEdit: Added example 3.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3zfnsf\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"Developx\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1451941871.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/\", \"locked\": false, \"name\": \"t3_3zfnsf\", \"created\": 1451956898.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"403 Forbidden on first PRAW request() or request_json() call that needs to use OAuth. All subsequent calls work fine.\", \"created_utc\": 1451928098.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been browsing the source code at \\u003Ca href=\\\"https://github.com/reddit/reddit\\\"\\u003Ehttps://github.com/reddit/reddit\\u003C/a\\u003E and I\\u0026#39;m not seeing how reddit does a/b testing. (I\\u0026#39;ve searched for an \\u0026quot;experiments\\u0026quot; class and browsed a few different directories but still no luck) Can someone point me to the ab/\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been browsing the source code at https://github.com/reddit/reddit and I'm not seeing how reddit does a/b testing. (I've searched for an \\\"experiments\\\" class and browsed a few different directories but still no luck) Can someone point me to the ab/\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3z8lt2\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"karmelajeremy\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3z8lt2/how_does_reddit_run_ab_tests/\", \"locked\": false, \"name\": \"t3_3z8lt2\", \"created\": 1451827206.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3z8lt2/how_does_reddit_run_ab_tests/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How does reddit run a/b tests?\", \"created_utc\": 1451798406.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m trying to get the thumbnails for posts on \\u003Ca href=\\\"/r/NBA\\\"\\u003E/r/NBA\\u003C/a\\u003E, my understanding is that the \\u0026quot;thumbnail\\u0026quot; value is always either \\u0026quot;self\\u0026quot; or the image url.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhen i parse the JSON with Android i always get the \\u0026quot;thumbnail\\u0026quot; value as an empty string \\u0026quot;\\u0026quot;.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhen i open the url (\\u003Ca href=\\\"http://www.reddit.com/r/nba.json\\\"\\u003Ehttp://www.reddit.com/r/nba.json\\u003C/a\\u003E) on Firefox, IE and some online tools like \\u003Ca href=\\\"https://jsonformatter.curiousconcept.com/\\\"\\u003Ehttps://jsonformatter.curiousconcept.com/\\u003C/a\\u003E, \\u0026quot;thumbnail\\u0026quot; is always \\u0026quot;\\u0026quot;, but when i use Chrome i get the real value (either \\u0026quot;self\\u0026quot; or the url).  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat\\u0026#39;s going on here?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm trying to get the thumbnails for posts on /r/NBA, my understanding is that the \\\"thumbnail\\\" value is always either \\\"self\\\" or the image url.  \\n\\nWhen i parse the JSON with Android i always get the \\\"thumbnail\\\" value as an empty string \\\"\\\".  \\n\\nWhen i open the url (http://www.reddit.com/r/nba.json) on Firefox, IE and some online tools like https://jsonformatter.curiousconcept.com/, \\\"thumbnail\\\" is always \\\"\\\", but when i use Chrome i get the real value (either \\\"self\\\" or the url).  \\n\\nWhat's going on here?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3y8htd\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"jorgegil96\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3y8htd/thumbnail_value_in_json_string_appears_as_empty/\", \"locked\": false, \"name\": \"t3_3y8htd\", \"created\": 1451120347.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3y8htd/thumbnail_value_in_json_string_appears_as_empty/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"\\\"thumbnail\\\" value in JSON string appears as empty\", \"created_utc\": 1451091547.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThere is this hack that allows you to download all submissions from a given subreddit [by doing multiple search requests using cloudsearch syntax with timestamp:XXXXXXXXXX..YYYYYYYYYY query.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI used this hack/feature multiple times, so after my fifth time using it I made \\u003Ca href=\\\"https://github.com/praw-dev/praw/pull/554\\\"\\u003Ea pull request to PRAW\\u003C/a\\u003E. It took a few weeks and 57 comments to merge it, but now it is available in the master branch. Which means that if you\\u0026#39;d like to try it out, you can simply do:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E# pip install git+https://github.com/praw-dev/praw.git\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eand then do:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Efrom praw.helpers import submissions_between\\nr = praw.Reddit(\\u0026quot;\\u0026lt;your user_agent\\u0026gt;\\u0026quot;)\\nfor s in submissions_between(r, \\u0026#39;redditdev\\u0026#39;):\\n    print s  # or your own great code \\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThe helper additionally takes lowest_timestamp and highest_timestamp for filtering using, well, timestamps. Also, there is extra_cloudsearch_fields which gives you a way to filter by parameters like \\u0026quot;self\\u0026quot;, \\u0026quot;author\\u0026quot;, \\u0026quot;title\\u0026quot;, etc: \\u003Ca href=\\\"https://www.reddit.com/wiki/search\\\"\\u003Efull list of (mostly working) fields\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI figured I could make this announcement because why not and because many people use PRAW and maybe they\\u0026#39;ll find this helper useful(also because I like bragging about my work). \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPS: If any reddit developers are reading this, please go and fix several bugs I found while developing this helper: \\u003Ca href=\\\"https://www.reddit.com/r/bugs/comments/3x9hgd/nsfw1_includes_nonnsfw_results_at_least_when/\\\"\\u003Ebug1\\u003C/a\\u003E, \\u003Ca href=\\\"https://www.reddit.com/r/bugs/comments/3vkzts/cloudsearch_with_timestamp_for_rmod_returns_503/\\\"\\u003Ebug2\\u003C/a\\u003E, \\u003Ca href=\\\"https://www.reddit.com/r/bugs/comments/3uxc09/api_sorting_by_new_is_broken/\\\"\\u003Ebug3\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E(at least you could\\u0026#39;ve changed the flairs to \\u0026quot;confirmed\\u0026quot;)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"There is this hack that allows you to download all submissions from a given subreddit [by doing multiple search requests using cloudsearch syntax with timestamp:XXXXXXXXXX..YYYYYYYYYY query.\\n\\nI used this hack/feature multiple times, so after my fifth time using it I made [a pull request to PRAW](https://github.com/praw-dev/praw/pull/554). It took a few weeks and 57 comments to merge it, but now it is available in the master branch. Which means that if you'd like to try it out, you can simply do:\\n\\n    # pip install git+https://github.com/praw-dev/praw.git\\n\\nand then do:\\n\\n    from praw.helpers import submissions_between\\n    r = praw.Reddit(\\\"\\u003Cyour user_agent\\u003E\\\")\\n    for s in submissions_between(r, 'redditdev'):\\n        print s  # or your own great code \\n\\nThe helper additionally takes lowest_timestamp and highest_timestamp for filtering using, well, timestamps. Also, there is extra_cloudsearch_fields which gives you a way to filter by parameters like \\\"self\\\", \\\"author\\\", \\\"title\\\", etc: [full list of (mostly working) fields](https://www.reddit.com/wiki/search)\\n\\nI figured I could make this announcement because why not and because many people use PRAW and maybe they'll find this helper useful(also because I like bragging about my work). \\n\\nPS: If any reddit developers are reading this, please go and fix several bugs I found while developing this helper: [bug1](https://www.reddit.com/r/bugs/comments/3x9hgd/nsfw1_includes_nonnsfw_results_at_least_when/), [bug2](https://www.reddit.com/r/bugs/comments/3vkzts/cloudsearch_with_timestamp_for_rmod_returns_503/), [bug3](https://www.reddit.com/r/bugs/comments/3uxc09/api_sorting_by_new_is_broken/)\\n\\n(at least you could've changed the flairs to \\\"confirmed\\\")\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3xem6j\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"godlikesme\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3xem6j/psa_ive_just_added_a_praw_helper_that_allows_you/\", \"locked\": false, \"name\": \"t3_3xem6j\", \"created\": 1450510452.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3xem6j/psa_ive_just_added_a_praw_helper_that_allows_you/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"PSA: I've just added a PRAW helper that allows you to easily download all submissions from a subreddit(or all submissions between two timestamps)\", \"created_utc\": 1450481652.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/r/AskScienceFiction/search?q=dune\\u0026amp;restrict_sr=on\\u0026amp;sort=top\\u0026amp;t=all\\\"\\u003Edune on /r/AskScienceFiction\\u003C/a\\u003E (20+ results)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/r/AskScienceFiction/search?q=flair%3Adune\\u0026amp;sort=top\\u0026amp;restrict_sr=on\\u0026amp;t=all\\\"\\u003Edune flair on /r/AskScienceFiction\\u003C/a\\u003E (0 results)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/r/AskScienceFiction/search?q=tag%3Adune\\u0026amp;sort=top\\u0026amp;restrict_sr=on\\u0026amp;t=all\\\"\\u003Edune tag on /r/AskScienceFiction\\u003C/a\\u003E (0 results)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ewhat gives? Why doesn\\u0026#39;t tag/flair search work every time?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"\\n[dune on /r/AskScienceFiction](https://www.reddit.com/r/AskScienceFiction/search?q=dune\\u0026restrict_sr=on\\u0026sort=top\\u0026t=all) (20+ results)\\n\\n[dune flair on /r/AskScienceFiction](https://www.reddit.com/r/AskScienceFiction/search?q=flair%3Adune\\u0026sort=top\\u0026restrict_sr=on\\u0026t=all) (0 results)\\n\\n[dune tag on /r/AskScienceFiction](https://www.reddit.com/r/AskScienceFiction/search?q=tag%3Adune\\u0026sort=top\\u0026restrict_sr=on\\u0026t=all) (0 results)\\n\\nwhat gives? Why doesn't tag/flair search work every time?\\n\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3wknim\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"SamSlate\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3wknim/why_doesnt_search_by_flair_always_work/\", \"locked\": false, \"name\": \"t3_3wknim\", \"created\": 1449989394.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3wknim/why_doesnt_search_by_flair_always_work/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why doesn't \\\"search by flair\\\" always work?\", \"created_utc\": 1449960594.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi all, so I\\u0026#39;ve just gotten into Python so I thought what better way to learn than to write a few reddit bots?! (I\\u0026#39;m sure there\\u0026#39;s a whole list of things)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBut, I had a question as it pertains to throttling post (\\u0026quot;You are doing that too much. try again in X mins.\\u0026quot;).  Right now I am just putting my whole post call (and clean up thereafter) in a big try and excepting out the throttle error, sleeping for 60 on an exception and doing it all over again.  Is this the best way to go about this?  Seems a little heavy handed to just keep trying until it gets through.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks for any help :)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi all, so I've just gotten into Python so I thought what better way to learn than to write a few reddit bots?! (I'm sure there's a whole list of things)\\n\\nBut, I had a question as it pertains to throttling post (\\\"You are doing that too much. try again in X mins.\\\").  Right now I am just putting my whole post call (and clean up thereafter) in a big try and excepting out the throttle error, sleeping for 60 on an exception and doing it all over again.  Is this the best way to go about this?  Seems a little heavy handed to just keep trying until it gets through.\\n\\nThanks for any help :)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3wc0jn\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"spookyyz\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3wc0jn/praw_more_eloquent_way_of_handling_throttling/\", \"locked\": false, \"name\": \"t3_3wc0jn\", \"created\": 1449839092.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3wc0jn/praw_more_eloquent_way_of_handling_throttling/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[PRAW] More eloquent way of handling throttling...\", \"created_utc\": 1449810292.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI paused execution to \\u003Ca href=\\\"https://imgur.com/8anwxE2\\\"\\u003Echeck the attributes of a submission object\\u003C/a\\u003E, but nowhere was the full URL provided. If you go to the \\u003Ca href=\\\"https://www.reddit.com/r/food/comments/3udre2/honey_roasted/\\\"\\u003Esubmission in question\\u003C/a\\u003E, you\\u0026#39;ll see that the link has a \\u0026quot;.jpg\\u0026quot; on the end, whereas in my object watch window it just has the imgur link. There are no other attributes containing the full link.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.reddit.com/r/food/comments/3udre2/honey_roasted/\\\"\\u003EHere\\u0026#39;s the API link to that submission\\u003C/a\\u003E. You can see that there\\u0026#39;s no extension under \\u0026quot;url\\u0026quot;, and a page search for \\u0026quot;jpg\\u0026quot; returns no results other than thumbnail links.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHow can I get the full link in praw? I need to get the full link, then check whether or not it ends in \\u0026quot;.jpg\\u0026quot;, \\u0026quot;.png\\u0026quot; etc to see whether the submission is a direct image link.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003Eedit: \\u003Ca href=\\\"https://api.reddit.com/r/WTF/comments/3uchq3/who_needs_a_boat_when_you_got_a_car/\\\"\\u003Ethis submission seems to have the extension..\\u003C/a\\u003E | \\u003Ca href=\\\"https://api.reddit.com/r/WTF/comments/3ua3gy/a_rare_condition_called_congenital_arthrogryposis/\\\"\\u003Eas does this one\\u003C/a\\u003E. What\\u0026#39;s going on then?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit2: for future people with the same problem, RES was the issue as per \\u003Ca href=\\\"https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/cxe19nc\\\"\\u003Ethis comment\\u003C/a\\u003E. Disabling RES displays the correct link in the browser.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I paused execution to [check the attributes of a submission object](https://imgur.com/8anwxE2), but nowhere was the full URL provided. If you go to the [submission in question](https://www.reddit.com/r/food/comments/3udre2/honey_roasted/), you'll see that the link has a \\\".jpg\\\" on the end, whereas in my object watch window it just has the imgur link. There are no other attributes containing the full link.\\n\\n[Here's the API link to that submission](https://api.reddit.com/r/food/comments/3udre2/honey_roasted/). You can see that there's no extension under \\\"url\\\", and a page search for \\\"jpg\\\" returns no results other than thumbnail links.\\n\\n\\nHow can I get the full link in praw? I need to get the full link, then check whether or not it ends in \\\".jpg\\\", \\\".png\\\" etc to see whether the submission is a direct image link.\\n\\n---\\n\\nedit: [this submission seems to have the extension..](https://api.reddit.com/r/WTF/comments/3uchq3/who_needs_a_boat_when_you_got_a_car/) | [as does this one](https://api.reddit.com/r/WTF/comments/3ua3gy/a_rare_condition_called_congenital_arthrogryposis/). What's going on then?\\n\\nEdit2: for future people with the same problem, RES was the issue as per [this comment](https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/cxe19nc). Disabling RES displays the correct link in the browser.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3udusd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"theonefoster\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1448570734.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/\", \"locked\": false, \"name\": \"t3_3udusd\", \"created\": 1448596985.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How do I get the FULL submission URL from a praw submission object? submission.url seems to chop the file extension off imgur links\", \"created_utc\": 1448568185.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI went delving into the API documentation only to find \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/JSON#message-implements-created\\\"\\u003Ethis\\u003C/a\\u003E:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003Eboolean |  new | unread? not sure\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://i.imgur.com/uOEMf5t.png\\\"\\u003Ehttp://i.imgur.com/uOEMf5t.png\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESorry to be meme about it, but I need to know. Thanks in advance!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I went delving into the API documentation only to find [this](https://github.com/reddit/reddit/wiki/JSON#message-implements-created):\\n\\n\\u003Eboolean |\\tnew | unread? not sure\\n\\nhttp://i.imgur.com/uOEMf5t.png\\n\\nSorry to be meme about it, but I need to know. Thanks in advance!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3u0ijb\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"iwanttowatchyoupoop\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3u0ijb/des_the_new_attribute_on_a_reddit_messages_json/\", \"locked\": false, \"name\": \"t3_3u0ijb\", \"created\": 1448353878.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3u0ijb/des_the_new_attribute_on_a_reddit_messages_json/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Des the \\\"new\\\" attribute on a reddit message's JSON mean unread?\", \"created_utc\": 1448325078.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ETLDR; Check the demo! - \\u003Ca href=\\\"https://kekday.herokuapp.com\\\"\\u003Ehttps://kekday.herokuapp.com\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EReddit gives all the user info in a handy JSON at this URL: \\u003Ccode\\u003Ehttps://www.reddit.com/user/\\u0026lt;username here\\u0026gt;/about.json\\u003C/code\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eexample: \\u003Ca href=\\\"https://www.reddit.com/user/spez/about.json\\\"\\u003Ehttps://www.reddit.com/user/spez/about.json\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe \\u003Ccode\\u003Ecreated_utc\\u003C/code\\u003E field in \\u003Ccode\\u003Edata\\u003C/code\\u003E is the date of user\\u0026#39;s registration aka Cake Day in \\u003Ca href=\\\"https://en.wikipedia.org/wiki/Unix_time\\\"\\u003Eunix epoch\\u003C/a\\u003E format (in UTC) and we can easily convert that to readable format: \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026gt;\\u0026gt;\\u0026gt; import time\\n\\u0026gt;\\u0026gt;\\u0026gt; time.strftime(\\u0026quot;%D\\u0026quot;, time.gmtime(1118030400))\\n\\u0026#39;06/06/05\\u0026#39;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EUsing \\u003Ca href=\\\"http://python-requests.org\\\"\\u003EPython Requests\\u003C/a\\u003E, we can turn this into a handy function:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport time\\nimport requests\\n\\ndef get_my_cake_day(username):\\n    url = \\u0026quot;https://www.reddit.com/user/{}/about.json\\u0026quot;.format(username)\\n    r = requests.get(url)\\n    created_at = r.json()[\\u0026#39;data\\u0026#39;][\\u0026#39;created_utc\\u0026#39;]\\n    return time.strftime(\\u0026quot;%D\\u0026quot;, time.gmtime(created_at))\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThough above function will work, but soon it will start throwing HTTP 429 error i.e Too Many Requests. Thing is, Reddit doesn\\u0026#39;t really like when someone tries to fetch the data like this. The requests are made directly on Reddit servers without using the API. Now if you have want to find cake day of hundreds of users, you cannot use this method.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESolution? Use \\u003Ca href=\\\"https://www.reddit.com/dev/api\\\"\\u003EReddit\\u0026#39;s API\\u003C/a\\u003E. In Python, we will use \\u003Ca href=\\\"https://github.com/praw-dev/praw\\\"\\u003Epraw\\u003C/a\\u003E and \\u003Ca href=\\\"https://github.com/avinassh/prawoauth2\\\"\\u003Eprawoauth2\\u003C/a\\u003E. praw is a Python wrapper for Reddit\\u0026#39;s API and prawoauth2 helps dealing with \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/OAuth2\\\"\\u003EOAuth2\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELet\\u0026#39;s start by installing praw:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Epip install praw\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENow we can convert the \\u003Ccode\\u003Eget_my_cake_day\\u003C/code\\u003E to praw version and get the user details like this:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport time\\nimport praw\\n\\nreddit_client = praw.Reddit(user_agent=\\u0026#39;my amazing cake day bot\\u0026#39;)\\n\\ndef get_my_cake_day(username):\\n    redditor = reddit_client.get_redditor(username)\\n    return time.strftime(\\u0026quot;%D\\u0026quot;, time.gmtime(redditor.created_utc))\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAbove code pretty much self explanatory. What if the user doesn\\u0026#39;t exist or shadowbanned? In such cases, praw throws an exception: \\u003Ccode\\u003Epraw.errors.NotFound\\u003C/code\\u003E. Lets modify \\u003Ccode\\u003Eget_my_cake_day\\u003C/code\\u003E to catch this:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Edef get_my_cake_day(username):\\n    try:\\n        redditor = reddit_client.get_redditor(username)\\n        return time.strftime(\\u0026quot;%D\\u0026quot;, time.gmtime(redditor.created_utc))\\n    except praw.errors.NotFound:\\n        return \\u0026#39;User does not exist or shadowbanned\\u0026#39;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThis is better compared to earlier version and we will stop getting rate limit errors often. Also, praw will handle such cases and makes requests again to fetch the data. But what if we want to increase the limit?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe above requests are not authenticated, meaning Reddit does not recognise your app. However, if we register this app in Reddit and let Reddit know, then requests limits will increase. So to authenticate our app over Oauth2, we will use prawoauth2. Lets install it first:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Epip install prawoauth2\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EFollow the simple steps \\u003Ca href=\\\"https://prawoauth2.readthedocs.org/usage_guide.html\\\"\\u003Ehere\\u003C/a\\u003E to register your app on Reddit. Once done, you will get \\u003Ccode\\u003Eapp_token\\u003C/code\\u003E and \\u003Ccode\\u003Eapp_secret\\u003C/code\\u003E. Then you need to get \\u003Ccode\\u003Eaccess_token\\u003C/code\\u003E and \\u003Ccode\\u003Erefresh_token\\u003C/code\\u003E. You could use this handy \\u003Ca href=\\\"https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/onetime.py\\\"\\u003E\\u003Ccode\\u003Eonetime.py\\u003C/code\\u003E\\u003C/a\\u003E script. For detailed instructions check the documentation of \\u003Ca href=\\\"https://prawoauth2.readthedocs.org\\\"\\u003Eprawoauth2\\u003C/a\\u003E. You should never make \\u003Ccode\\u003Eapp_token\\u003C/code\\u003E, \\u003Ccode\\u003Eapp_secret\\u003C/code\\u003E, \\u003Ccode\\u003Eaccess_token\\u003C/code\\u003E and \\u003Ccode\\u003Erefresh_token\\u003C/code\\u003E public and never commit them to version control. Keep them always secret.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere is the complete script using prawoauth2:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport time\\nimport praw\\n\\nfrom secret import (app_key, app_secret, access_token, refresh_token,\\n                    user_agent, scopes)\\n\\nreddit_client = praw.Reddit(user_agent=\\u0026#39;my amazing cakeday bot\\u0026#39;)\\noauth_helper = PrawOAuth2Mini(reddit_client, app_key=app_key,\\n                              app_secret=app_secret,\\n                              access_token=access_token,\\n                              refresh_token=refresh_token, scopes=scopes)\\n\\n\\ndef get_my_cake_day(username):\\n    try:\\n        redditor = reddit_client.get_redditor(username)\\n        return time.strftime(\\u0026quot;%D\\u0026quot;, time.gmtime(redditor.created_utc))\\n    except praw.errors.NotFound:\\n        return \\u0026#39;User does not exists or shadowbanned\\u0026#39;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAgain, pretty much self explanatory. If your tokens are correct and once \\u003Ccode\\u003EPrawOAuth2Mini\\u003C/code\\u003E is initialized properly, there will be no issues with the app and you will have twice as many requests as compared to unauthenticated version.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWant to see above app in action? Check this - \\u003Ca href=\\\"https://kekday.herokuapp.com\\\"\\u003Ekekday\\u003C/a\\u003E. The \\u003Ca href=\\\"https://github.com/avinassh/kekday\\\"\\u003Eapp is open source\\u003C/a\\u003E and released under MIT License.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"TLDR; Check the demo! - https://kekday.herokuapp.com\\n\\n---\\n\\nReddit gives all the user info in a handy JSON at this URL: `https://www.reddit.com/user/\\u003Cusername here\\u003E/about.json`\\n\\nexample: [https://www.reddit.com/user/spez/about.json](https://www.reddit.com/user/spez/about.json)\\n\\nThe `created_utc` field in `data` is the date of user's registration aka Cake Day in [unix epoch](https://en.wikipedia.org/wiki/Unix_time) format (in UTC) and we can easily convert that to readable format: \\n\\n    \\u003E\\u003E\\u003E import time\\n    \\u003E\\u003E\\u003E time.strftime(\\\"%D\\\", time.gmtime(1118030400))\\n    '06/06/05'\\n\\nUsing [Python Requests](http://python-requests.org), we can turn this into a handy function:\\n\\n    import time\\n    import requests\\n\\n    def get_my_cake_day(username):\\n        url = \\\"https://www.reddit.com/user/{}/about.json\\\".format(username)\\n        r = requests.get(url)\\n        created_at = r.json()['data']['created_utc']\\n        return time.strftime(\\\"%D\\\", time.gmtime(created_at))\\n\\n\\nThough above function will work, but soon it will start throwing HTTP 429 error i.e Too Many Requests. Thing is, Reddit doesn't really like when someone tries to fetch the data like this. The requests are made directly on Reddit servers without using the API. Now if you have want to find cake day of hundreds of users, you cannot use this method.\\n\\nSolution? Use [Reddit's API](https://www.reddit.com/dev/api). In Python, we will use [praw](https://github.com/praw-dev/praw) and [prawoauth2](https://github.com/avinassh/prawoauth2). praw is a Python wrapper for Reddit's API and prawoauth2 helps dealing with [OAuth2](https://github.com/reddit/reddit/wiki/OAuth2).\\n\\nLet's start by installing praw:\\n\\n    pip install praw\\n\\nNow we can convert the `get_my_cake_day` to praw version and get the user details like this:\\n\\n    import time\\n    import praw\\n\\n    reddit_client = praw.Reddit(user_agent='my amazing cake day bot')\\n\\n    def get_my_cake_day(username):\\n        redditor = reddit_client.get_redditor(username)\\n        return time.strftime(\\\"%D\\\", time.gmtime(redditor.created_utc))\\n\\n\\nAbove code pretty much self explanatory. What if the user doesn't exist or shadowbanned? In such cases, praw throws an exception: `praw.errors.NotFound`. Lets modify `get_my_cake_day` to catch this:\\n\\n\\n    def get_my_cake_day(username):\\n        try:\\n            redditor = reddit_client.get_redditor(username)\\n            return time.strftime(\\\"%D\\\", time.gmtime(redditor.created_utc))\\n        except praw.errors.NotFound:\\n            return 'User does not exist or shadowbanned'\\n\\n\\nThis is better compared to earlier version and we will stop getting rate limit errors often. Also, praw will handle such cases and makes requests again to fetch the data. But what if we want to increase the limit?\\n\\nThe above requests are not authenticated, meaning Reddit does not recognise your app. However, if we register this app in Reddit and let Reddit know, then requests limits will increase. So to authenticate our app over Oauth2, we will use prawoauth2. Lets install it first:\\n\\n    pip install prawoauth2\\n\\nFollow the simple steps [here](https://prawoauth2.readthedocs.org/usage_guide.html) to register your app on Reddit. Once done, you will get `app_token` and `app_secret`. Then you need to get `access_token` and `refresh_token`. You could use this handy [`onetime.py`](https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/onetime.py) script. For detailed instructions check the documentation of [prawoauth2](https://prawoauth2.readthedocs.org). You should never make `app_token`, `app_secret`, `access_token` and `refresh_token` public and never commit them to version control. Keep them always secret.\\n\\nHere is the complete script using prawoauth2:\\n\\n    import time\\n    import praw\\n\\n    from secret import (app_key, app_secret, access_token, refresh_token,\\n                        user_agent, scopes)\\n\\n    reddit_client = praw.Reddit(user_agent='my amazing cakeday bot')\\n    oauth_helper = PrawOAuth2Mini(reddit_client, app_key=app_key,\\n                                  app_secret=app_secret,\\n                                  access_token=access_token,\\n                                  refresh_token=refresh_token, scopes=scopes)\\n\\n\\n    def get_my_cake_day(username):\\n        try:\\n            redditor = reddit_client.get_redditor(username)\\n            return time.strftime(\\\"%D\\\", time.gmtime(redditor.created_utc))\\n        except praw.errors.NotFound:\\n            return 'User does not exists or shadowbanned'\\n\\n\\nAgain, pretty much self explanatory. If your tokens are correct and once `PrawOAuth2Mini` is initialized properly, there will be no issues with the app and you will have twice as many requests as compared to unauthenticated version.\\n\\nWant to see above app in action? Check this - [kekday](https://kekday.herokuapp.com). The [app is open source](https://github.com/avinassh/kekday) and released under MIT License.\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3tlt8o\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"avinassh\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3tlt8o/tutorial_find_a_redditors_cake_day_using_praw_and/\", \"locked\": false, \"name\": \"t3_3tlt8o\", \"created\": 1448078003.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3tlt8o/tutorial_find_a_redditors_cake_day_using_praw_and/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Tutorial] Find a Redditor's cake day using praw and prawoauth2 over OAuth\", \"created_utc\": 1448049203.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAccording to the API, you can query for \\u0026quot;related\\u0026quot; posts given a submission_id.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/dev/api#GET_related_%7Barticle%7D\\\"\\u003Ehttps://www.reddit.com/dev/api#GET_related_{article}\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBut as far as I could tell, you can\\u0026#39;t do this in PRAW... or am I incorrect?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"According to the API, you can query for \\\"related\\\" posts given a submission_id.\\n\\nhttps://www.reddit.com/dev/api#GET_related_{article}\\n\\nBut as far as I could tell, you can't do this in PRAW... or am I incorrect?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3t4cax\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"alexleavitt\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3t4cax/can_you_query_praw_for_related_posts/\", \"locked\": false, \"name\": \"t3_3t4cax\", \"created\": 1447766222.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3t4cax/can_you_query_praw_for_related_posts/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can you query PRAW for 'related' posts?\", \"created_utc\": 1447737422.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been working on an uni project about building documentation for reddit open source and right now i need to represent the reddit 4+1 architectural view model.\\nI need to do the UML representation of the following diagrams:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EPackage diagram\\u003C/li\\u003E\\n\\u003Cli\\u003EComponent diagram\\u003C/li\\u003E\\n\\u003Cli\\u003EDeployment diagram\\u003C/li\\u003E\\n\\u003Cli\\u003EActivity diagram\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EWould appreciate any help or advices in obtaining any information needed to build these.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI study Informatics and Computation Engineering in Porto, Portugal (FEUP).\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been working on an uni project about building documentation for reddit open source and right now i need to represent the reddit 4+1 architectural view model.\\nI need to do the UML representation of the following diagrams:\\n\\n* Package diagram\\n* Component diagram\\n* Deployment diagram\\n* Activity diagram\\n\\nWould appreciate any help or advices in obtaining any information needed to build these.\\n\\nI study Informatics and Computation Engineering in Porto, Portugal (FEUP).\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3qjdz2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"DavidGB\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1446024222.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3qjdz2/reddit_41_architectural_view_model/\", \"locked\": false, \"name\": \"t3_3qjdz2\", \"created\": 1446052092.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3qjdz2/reddit_41_architectural_view_model/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit 4+1 architectural view model\", \"created_utc\": 1446023292.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI just tried the following with PRAW:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Efor s in r.get_subreddit(\\u0026quot;all\\u0026quot;).get_top_from_day(limit=None):\\nwords = s.title.split()\\nif getDates(words): print s.score, s.title\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eand I get way less results if I change it to \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Elimit=1000\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EDid anyone else notice anything similar?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I just tried the following with PRAW:\\n\\n    for s in r.get_subreddit(\\\"all\\\").get_top_from_day(limit=None):\\n    words = s.title.split()\\n    if getDates(words): print s.score, s.title\\n\\nand I get way less results if I change it to \\n\\n    limit=1000\\n\\nDid anyone else notice anything similar?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3p1nu1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Naurgul\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3p1nu1/was_the_1000_limit_removed/\", \"locked\": false, \"name\": \"t3_3p1nu1\", \"created\": 1445062395.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3p1nu1/was_the_1000_limit_removed/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Was the 1000 limit removed?\", \"created_utc\": 1445033595.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve done searches on github as generic as \\u0026#39;cap\\u0026#39;, but I\\u0026#39;ve been unable to find anything unrelated to what appears to be related to advertising.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've done searches on github as generic as 'cap', but I've been unable to find anything unrelated to what appears to be related to advertising.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3nbb2l\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"deadowl\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3nbb2l/where_in_the_reddit_source_code_is_softcapping/\", \"locked\": false, \"name\": \"t3_3nbb2l\", \"created\": 1443877871.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3nbb2l/where_in_the_reddit_source_code_is_softcapping/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Where in the reddit source code is soft-capping?\", \"created_utc\": 1443849071.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello guys, this is my first time posting here and i\\u0026#39;d like the help of everyone who could help me with reddit documentation. I was given an assignment recently to choose an open source project, and build documentation for it, since most of them have none. I chose reddit and i\\u0026#39;ve encountered a problem. I was asked what type of a project model reddit was and i don\\u0026#39;t know the answer. If anyone that has made any type of contribution to the site or even knows anything about the matter, please let me know.\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EWaterfall model - this model is very straightforward, it\\u0026#39;s based on the principle that before moving to a next phase, a previous one must be completed.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EIncremental development and delivery - this model develops the system in increments and evaluates each\\nincrement before proceeding to the development of the\\nnext increment \\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EReuse oriented software engineering - this model is based on systematic reuse where systems are assembled\\nfrom existing components or COTS (Commercial-off-theshelf)\\nsystems\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003ESoftware prototyping - this model consists on a prototype which is an initial version of a system used to demonstrate concepts and try out design options\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EThis is basically it, if anyone can help me by sending me a pm or just commenting on the post, i\\u0026#39;d appreciate it.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eedit: i\\u0026#39;m a student in the Masters of Informatics and Computation(MIEIC) on the university of engineering(FEUP) from Porto, Portugal.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello guys, this is my first time posting here and i'd like the help of everyone who could help me with reddit documentation. I was given an assignment recently to choose an open source project, and build documentation for it, since most of them have none. I chose reddit and i've encountered a problem. I was asked what type of a project model reddit was and i don't know the answer. If anyone that has made any type of contribution to the site or even knows anything about the matter, please let me know.\\n\\n* Waterfall model - this model is very straightforward, it's based on the principle that before moving to a next phase, a previous one must be completed.\\n\\n\\n* Incremental development and delivery - this model develops the system in increments and evaluates each\\nincrement before proceeding to the development of the\\nnext increment \\n\\n\\n* Reuse oriented software engineering - this model is based on systematic reuse where systems are assembled\\nfrom existing components or COTS (Commercial-off-theshelf)\\nsystems\\n\\n\\n* Software prototyping - this model consists on a prototype which is an initial version of a system used to demonstrate concepts and try out design options\\n\\nThis is basically it, if anyone can help me by sending me a pm or just commenting on the post, i'd appreciate it.\\n\\nedit: i'm a student in the Masters of Informatics and Computation(MIEIC) on the university of engineering(FEUP) from Porto, Portugal.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3mztkk\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"tomislaaaav\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3mztkk/making_reddit_documentation/\", \"locked\": false, \"name\": \"t3_3mztkk\", \"created\": 1443664634.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3mztkk/making_reddit_documentation/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Making reddit documentation\", \"created_utc\": 1443635834.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI want to build the world\\u0026#39;s simplest application. When someone on my website pastes in a permalink to a comment, I want my site to retrieve that comment and return it. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe person doing the pasting does not even need to log in. \\u003Cstrong\\u003EI just want to retrieve a comment.\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYet, it seems like I\\u0026#39;m being forced to jump through 1000 hoops of fire to get everything to work. Why is Reddit\\u0026#39;s API so complicated to use compared to Twitter, AWS, or any other API?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBased off what I read, it seemed like I needed an API wrapper, so I went and downloaded \\u003Ca href=\\\"https://github.com/jcleblanc/reddit-php-sdk\\\"\\u003Ehttps://github.com/jcleblanc/reddit-php-sdk\\u003C/a\\u003E. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnd so, the problems began. \\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EOh, my project uses composer. This API wrapper doesn\\u0026#39;t. I tried for hours to try and get class autoloading set up. It didn\\u0026#39;t work. I then had to fork it, reorganize the project, add PSR-0 support, and then add it to packagist. That took up all my time last night.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003ECool, now I\\u0026#39;m using an semi-abandoned API wrapper to try and interact with a poorly documented API. \\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003ENow I can\\u0026#39;t get OAuth2 to work correctly. Why do I need to even use OAuth2 for this?! \\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003ESpend some more hours wasting my time looking at other API wrappers, PRAW, Guzzle, OAuth-2 repositories.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve spend the last 2 days of my time doing this, with no results to show for it. The simplest task ever, and there\\u0026#39;s roadblocks everywhere.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESorry for the rant, but I feel like I\\u0026#39;m doing something fundamentally wrong here. Is it really this difficult to retrieve a comment from Reddit? I feel like I\\u0026#39;m not even using the correct architecture here, should I just be using a bot? How does that work and how does it differ from an \\u0026quot;app\\u0026quot;?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I want to build the world's simplest application. When someone on my website pastes in a permalink to a comment, I want my site to retrieve that comment and return it. \\n\\nThe person doing the pasting does not even need to log in. **I just want to retrieve a comment.**\\n\\nYet, it seems like I'm being forced to jump through 1000 hoops of fire to get everything to work. Why is Reddit's API so complicated to use compared to Twitter, AWS, or any other API?\\n\\nBased off what I read, it seemed like I needed an API wrapper, so I went and downloaded https://github.com/jcleblanc/reddit-php-sdk. \\n\\nAnd so, the problems began. \\n\\n1. Oh, my project uses composer. This API wrapper doesn't. I tried for hours to try and get class autoloading set up. It didn't work. I then had to fork it, reorganize the project, add PSR-0 support, and then add it to packagist. That took up all my time last night.\\n\\n2. Cool, now I'm using an semi-abandoned API wrapper to try and interact with a poorly documented API. \\n\\n3. Now I can't get OAuth2 to work correctly. Why do I need to even use OAuth2 for this?! \\n\\n4. Spend some more hours wasting my time looking at other API wrappers, PRAW, Guzzle, OAuth-2 repositories.\\n\\nI've spend the last 2 days of my time doing this, with no results to show for it. The simplest task ever, and there's roadblocks everywhere.\\n\\nSorry for the rant, but I feel like I'm doing something fundamentally wrong here. Is it really this difficult to retrieve a comment from Reddit? I feel like I'm not even using the correct architecture here, should I just be using a bot? How does that work and how does it differ from an \\\"app\\\"?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ligwq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"EchoLogic\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 21, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/\", \"locked\": false, \"name\": \"t3_3ligwq\", \"created\": 1442659476.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How does a reddit bot differ from an application? Are they the same thing? When do I need OAuth2?\", \"created_utc\": 1442630676.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWe\\u0026#39;re making a bot that downloads images from 4chan\\u0026#39;s /w/ and posts them to \\u003Ca href=\\\"/r/slashw\\\"\\u003E/r/slashw\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe code runs daily and uploads the images, and then at the end of the week, it consolidates those from the same thread into albums and posts to reddit.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EExcept, imgur tells us that we\\u0026#39;ve crossed it\\u0026#39;s api limit even though we limit the bot to 1000 uploads a day.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EImgur hasn\\u0026#39;t been responding to emails or whitelist requests, so can you guys please help us?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe code can be found on github.com/Shazambom/wallpaperBot\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"We're making a bot that downloads images from 4chan's /w/ and posts them to /r/slashw\\n\\nThe code runs daily and uploads the images, and then at the end of the week, it consolidates those from the same thread into albums and posts to reddit.\\n\\nExcept, imgur tells us that we've crossed it's api limit even though we limit the bot to 1000 uploads a day.\\n\\nImgur hasn't been responding to emails or whitelist requests, so can you guys please help us?\\n\\nThe code can be found on github.com/Shazambom/wallpaperBot\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3kfc9p\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Shazambom\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3kfc9p/imgur_help_uploading_1000_images_a_day_but_imgur/\", \"locked\": false, \"name\": \"t3_3kfc9p\", \"created\": 1441934008.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3kfc9p/imgur_help_uploading_1000_images_a_day_but_imgur/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Imgur help: uploading 1000 images a day, but imgur kicks us for too many calls\", \"created_utc\": 1441905208.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi guys,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m trying to make a Universal app using the RedditSharp wrapper, but I don\\u0026#39;t think it\\u0026#39;s compatible with it. Is there a way to fix this or something?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi guys,\\n\\nI'm trying to make a Universal app using the RedditSharp wrapper, but I don't think it's compatible with it. Is there a way to fix this or something?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3h7nzn\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Trollzore\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3h7nzn/redditsharp_for_windows_10_universal/\", \"locked\": false, \"name\": \"t3_3h7nzn\", \"created\": 1439771618.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3h7nzn/redditsharp_for_windows_10_universal/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"RedditSharp for Windows 10 Universal?\", \"created_utc\": 1439742818.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI spun up a 12.04 LTS VM in azure and ran the install script.  Install script ran into no problems.  I opened the public port 80 and mapped it to the internal port 80.  I can from within the VM use wget to pull down \\u0026#39;index.html\\u0026#39; from: 127.0.0.1, localhost, \\u0026lt;serverhostname\\u0026gt;, and the internal IP address.  I can also view the instance through IE11 running on a VM in the same group by accessing the internal IP.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat I can\\u0026#39;t get to work is accessing the website from my local machine, ie: a machine that is not in Azure.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy best guess is that it has to do with Nginx, HAProxy, my /etc/hosts and/or my iptables.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny help would be greatly appreciated.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: So for whatever reason, Google Chrome will not pull down the bytes from the server, but IE and Edge will.  I am going to DL a few other browsers and test them out, but this looks like it isn\\u0026#39;t a config problem but a Chrome problem.  Once I get everything working I will update again.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I spun up a 12.04 LTS VM in azure and ran the install script.  Install script ran into no problems.  I opened the public port 80 and mapped it to the internal port 80.  I can from within the VM use wget to pull down 'index.html' from: 127.0.0.1, localhost, \\u003Cserverhostname\\u003E, and the internal IP address.  I can also view the instance through IE11 running on a VM in the same group by accessing the internal IP.\\n\\nWhat I can't get to work is accessing the website from my local machine, ie: a machine that is not in Azure.\\n\\nMy best guess is that it has to do with Nginx, HAProxy, my /etc/hosts and/or my iptables.\\n\\nAny help would be greatly appreciated.\\n\\nEDIT: So for whatever reason, Google Chrome will not pull down the bytes from the server, but IE and Edge will.  I am going to DL a few other browsers and test them out, but this looks like it isn't a config problem but a Chrome problem.  Once I get everything working I will update again.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3em4av\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"dlp211\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1438019273.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3em4av/reddit_clone_on_azure/\", \"locked\": false, \"name\": \"t3_3em4av\", \"created\": 1437900482.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3em4av/reddit_clone_on_azure/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Clone on Azure\", \"created_utc\": 1437871682.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ejjag\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"rtheunissen\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ejjag/rockets_pushes_new_reddit_content_to_subscribers/\", \"locked\": false, \"name\": \"t3_3ejjag\", \"created\": 1437841178.0, \"url\": \"https://github.com/rtheunissen/rockets\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Rockets: pushes new reddit content to subscribers using WebSockets\", \"created_utc\": 1437812378.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve arrived at a situation where I\\u0026#39;ve got links in my permacache (cassandra) that don\\u0026#39;t really exist, which blows up a sub listing with a \\u0026#39;link not found\\u0026#39; error. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there a safe way to clean up the permacache? Will it re-populate if I just clear out the SubredditQueryCache table?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've arrived at a situation where I've got links in my permacache (cassandra) that don't really exist, which blows up a sub listing with a 'link not found' error. \\n\\nIs there a safe way to clean up the permacache? Will it re-populate if I just clear out the SubredditQueryCache table?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3e4m7b\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"grandpaslab\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3e4m7b/reddit_clone_clean_up_permacache/\", \"locked\": false, \"name\": \"t3_3e4m7b\", \"created\": 1437546803.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3e4m7b/reddit_clone_clean_up_permacache/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[reddit clone] Clean up permacache?\", \"created_utc\": 1437518003.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m trying to get comments from a specific article, \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://api.reddit.com/r/\\\"\\u003Ehttps://api.reddit.com/r/\\u003C/a\\u003E\\u0026lt;subreddit\\u0026gt;/comments/\\u0026lt;articleid\\u0026gt;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eand if I open it up via incognito on a browser, I can get information back, but when I request via file_get_contents using php, I get forbidden. Does this call require oAUTH?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm trying to get comments from a specific article, \\n\\nhttps://api.reddit.com/r/\\u003Csubreddit\\u003E/comments/\\u003Carticleid\\u003E\\n\\nand if I open it up via incognito on a browser, I can get information back, but when I request via file_get_contents using php, I get forbidden. Does this call require oAUTH?\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3dn1wn\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"cbartholomew\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3dn1wn/making_a_standard_get_request_using_file_get/\", \"locked\": false, \"name\": \"t3_3dn1wn\", \"created\": 1437179594.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3dn1wn/making_a_standard_get_request_using_file_get/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Making a standard GET request using file_get_contents to collect comments from thread, does this still require OAUTH?\", \"created_utc\": 1437150794.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Cem\\u003EI will preface this with the tacit acknowledgement that I may (and very likely am) authenticating incorrectly, or making any one of a thousand other mistakes\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/OAuth2#token-retrieval-implicit-grant-flow\\\"\\u003EOAuth2\\u003C/a\\u003E allows applicaitons to make \\u0026quot;API requests to reddit\\u0026#39;s servers on behalf of that user\\u0026quot;, but litterally any request to oauth.reddit.com I\\u0026#39;ve made bounce back with \\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003ENo \\u0026#39;Access-Control-Allow-Origin\\u0026#39; header is present on the requested resource. Origin \\u0026#39;mywebsite.com\\u0026#39; is therefore not allowed access. The response had HTTP status code 403.\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/r/redditdev/comments/2owrnn/oauth2_implicit_grants_cors_apponly_oauth2/\\\"\\u003Ethis post\\u003C/a\\u003E makes it sound as though javascript applications will be able to login users and,as stated above \\u0026quot;act on their behalf\\u0026quot;. and, I assumed, at the very least access the user\\u0026#39;s front page. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI can authenticate and even receive an access_token via ajax request, but any additional request to oauth.reddit.com or reddit.com comeback 403: cross-domain access denied. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo what\\u0026#39;s going on?? Why can\\u0026#39;t I access a user\\u0026#39;s front page \\u003Cem\\u003Eor any information\\u003C/em\\u003E via javascript and implicit authentication? What am I doing wrong and what exactly is the point of implicit flow if I get nothing but an access token with nothing to access. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://pastebin.com/raw.php?i=dXgQ8hxL\\\"\\u003Epastebin in case I\\u0026#39;m missing something perfectly obvious\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"*I will preface this with the tacit acknowledgement that I may (and very likely am) authenticating incorrectly, or making any one of a thousand other mistakes*\\n\\n[OAuth2](https://github.com/reddit/reddit/wiki/OAuth2#token-retrieval-implicit-grant-flow) allows applicaitons to make \\\"API requests to reddit's servers on behalf of that user\\\", but litterally any request to oauth.reddit.com I've made bounce back with \\n\\n\\u003ENo 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'mywebsite.com' is therefore not allowed access. The response had HTTP status code 403.\\n\\n[this post](https://www.reddit.com/r/redditdev/comments/2owrnn/oauth2_implicit_grants_cors_apponly_oauth2/) makes it sound as though javascript applications will be able to login users and,as stated above \\\"act on their behalf\\\". and, I assumed, at the very least access the user's front page. \\n\\nI can authenticate and even receive an access_token via ajax request, but any additional request to oauth.reddit.com or reddit.com comeback 403: cross-domain access denied. \\n\\nSo what's going on?? Why can't I access a user's front page *or any information* via javascript and implicit authentication? What am I doing wrong and what exactly is the point of implicit flow if I get nothing but an access token with nothing to access. \\n\\n[pastebin in case I'm missing something perfectly obvious](http://pastebin.com/raw.php?i=dXgQ8hxL)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3cxl19\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"SamSlate\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 20, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3cxl19/what_is_the_point_of_oauth2_implicit_flow_for_js/\", \"locked\": false, \"name\": \"t3_3cxl19\", \"created\": 1436666453.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3cxl19/what_is_the_point_of_oauth2_implicit_flow_for_js/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"What is the point of OAuth2 Implicit Flow for js if cross-domain access is denied??\", \"created_utc\": 1436637653.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhat are the limits for \\u003Ccode\\u003E/subreddit/about/log\\u003C/code\\u003E?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s not the typical 1000 items. I\\u0026#39;ve heard rumors that it\\u0026#39;s 2months, 3months, even 4months. Or a combination of # of items over a time range. I can\\u0026#39;t find it written in the API documentation or the associated code.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECan someone point me to the answer?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"What are the limits for `/subreddit/about/log`?\\n\\nIt's not the typical 1000 items. I've heard rumors that it's 2months, 3months, even 4months. Or a combination of # of items over a time range. I can't find it written in the API documentation or the associated code.\\n\\nCan someone point me to the answer?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3b9oww\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"amici_ursi\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3b9oww/what_are_the_limits_for_subredditaboutlog/\", \"locked\": false, \"name\": \"t3_3b9oww\", \"created\": 1435403600.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3b9oww/what_are_the_limits_for_subredditaboutlog/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"What are the limits for /subreddit/about/log?\", \"created_utc\": 1435374800.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi there.  My situation is quite simple: I run a single server that facilitates lots of OAuth-authenticated network requests to various API endpoints.  On a per-client basis I\\u0026#39;m \\u003Cem\\u003Ewell\\u003C/em\\u003E within the API guidelines, with an average fetch rate of 1 req./40 sec.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EUnfortunately, my main request server just got IP banned (* requests -\\u0026gt; err 429) after nearly two months of steady operation.  The only configuration setting I recently changed was the connect and read timeouts (5s -\\u0026gt; 15s), which I\\u0026#39;m guessing triggered an automatic ban.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve moved everything to (yet another) node, but this marks the \\u003Cem\\u003Ethird\\u003C/em\\u003E time I\\u0026#39;ve had to grab new IPs after reddit\\u0026#39;s servers went rouge.  Is there anything I can do to stop this from happening again?  Perhaps a whitelist of some sorts (not sure who to contact for this)?  I could create a request cluster and RR load balance on my end, but that really defeats the purpose I think.  Any help would be greatly appreciated, thanks.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi there.  My situation is quite simple: I run a single server that facilitates lots of OAuth-authenticated network requests to various API endpoints.  On a per-client basis I'm *well* within the API guidelines, with an average fetch rate of 1 req./40 sec.\\n\\nUnfortunately, my main request server just got IP banned (* requests -\\u003E err 429) after nearly two months of steady operation.  The only configuration setting I recently changed was the connect and read timeouts (5s -\\u003E 15s), which I'm guessing triggered an automatic ban.\\n\\nI've moved everything to (yet another) node, but this marks the *third* time I've had to grab new IPs after reddit's servers went rouge.  Is there anything I can do to stop this from happening again?  Perhaps a whitelist of some sorts (not sure who to contact for this)?  I could create a request cluster and RR load balance on my end, but that really defeats the purpose I think.  Any help would be greatly appreciated, thanks.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3aofz6\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"RedBannedHammer\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3aofz6/oauthauthenticated_servers_getting_ip_banned/\", \"locked\": false, \"name\": \"t3_3aofz6\", \"created\": 1434977334.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3aofz6/oauthauthenticated_servers_getting_ip_banned/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OAuth-authenticated servers getting IP banned\", \"created_utc\": 1434948534.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have been trying to use the Reddit API through the reddit-php-sdk library.  I have everything set up, and I am getting a response from reddit, but can\\u0026#39;t seem to get it to send the refresh_token.  According to the docs, all I need to do is include the post field duration=permanent, but I still can\\u0026#39;t seem to get the proper response.  The scope is included in the config file, and it is clearly being sent because the response is returning all scopes, which is what I ask for in the query.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Erequire(\\u0026quot;lib/reddit-php-sdk-master/reddit.php\\u0026quot;);\\n$reddit = new reddit();\\n\\n//GET ACCESS AND REFRESH TOKEN\\n$url =\\u0026#39;https://ssl.reddit.com/api/v1/access_token\\u0026#39;;\\n$clientId = \\u0026#39;XXX\\u0026#39;; //XXX is filled in on my live version\\n$clientSecret = \\u0026#39;XXX\\u0026#39;; //XXX is filled in on my live version\\n\\n//SET POST VARIABLES\\n$fields = array (\\n    \\u0026#39;grant_type\\u0026#39; =\\u0026gt; \\u0026#39;client_credentials\\u0026#39;,\\n    \\u0026#39;duration\\u0026#39; =\\u0026gt; \\u0026#39;permanent\\u0026#39;,\\n    \\u0026#39;response_type\\u0026#39; =\\u0026gt; \\u0026#39;code\\u0026#39;,\\n    \\u0026#39;state\\u0026#39; =\\u0026gt; \\u0026#39;abcdefghijklmnopqrstuvwxyz\\u0026#39;\\n);\\n\\n//URL-IFY DATA FOR POST\\n$field_string = http_build_query($fields);\\n\\necho $field_string;\\n\\n//OPEN CONNECTION\\n$ch = curl_init($url);\\n\\n//SET CURL DATA\\ncurl_setopt( $ch, CURLOPT_HTTPHEADER, array(\\u0026#39;Authorization: Basic \\u0026#39; . base64_encode($clientId . \\u0026#39;:\\u0026#39; . $clientSecret) ) );\\ncurl_setopt($ch,CURLOPT_RETURNTRANSFER,1);\\ncurl_setopt($ch,CURLOPT_POST, 1);\\ncurl_setopt($ch,CURLOPT_POSTFIELDS, $field_string);\\n\\n//EXECUTE POST\\necho curl_exec($ch);\\n\\n//CLOSE CONNECTION\\ncurl_close($ch);\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch2\\u003E\\u003C/h2\\u003E\\n\\n\\u003Cp\\u003EHere is the part of the url string with the POST values:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Egrant_type=client_credentials\\u0026amp;duration=permanent\\u0026amp;response_type=code\\u0026amp;state=abcdefghijklmnopqrstuvwxyz\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EHere is the response I am getting:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E{\\u0026quot;access_token\\u0026quot;: \\u0026quot;-zQjcXuqld_bz6OltZQmdYA3Slr0\\u0026quot;, \\u0026quot;token_type\\u0026quot;: \\u0026quot;bearer\\u0026quot;, \\u0026quot;expires_in\\u0026quot;: 3600, \\u0026quot;scope\\u0026quot;: \\u0026quot;*\\u0026quot;}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI think there should be an extra line that contains the refresh_token as well.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny help is greatly appreciated!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have been trying to use the Reddit API through the reddit-php-sdk library.  I have everything set up, and I am getting a response from reddit, but can't seem to get it to send the refresh_token.  According to the docs, all I need to do is include the post field duration=permanent, but I still can't seem to get the proper response.  The scope is included in the config file, and it is clearly being sent because the response is returning all scopes, which is what I ask for in the query.\\n\\n    require(\\\"lib/reddit-php-sdk-master/reddit.php\\\");\\n    $reddit = new reddit();\\n    \\n    //GET ACCESS AND REFRESH TOKEN\\n    $url ='https://ssl.reddit.com/api/v1/access_token';\\n    $clientId = 'XXX'; //XXX is filled in on my live version\\n    $clientSecret = 'XXX'; //XXX is filled in on my live version\\n    \\n    //SET POST VARIABLES\\n    $fields = array (\\n    \\t'grant_type' =\\u003E 'client_credentials',\\n    \\t'duration' =\\u003E 'permanent',\\n    \\t'response_type' =\\u003E 'code',\\n    \\t'state' =\\u003E 'abcdefghijklmnopqrstuvwxyz'\\n    );\\n    \\n    //URL-IFY DATA FOR POST\\n    $field_string = http_build_query($fields);\\n    \\n    echo $field_string;\\n    \\n    //OPEN CONNECTION\\n    $ch = curl_init($url);\\n    \\n    //SET CURL DATA\\n    curl_setopt( $ch, CURLOPT_HTTPHEADER, array('Authorization: Basic ' . base64_encode($clientId . ':' . $clientSecret) ) );\\n    curl_setopt($ch,CURLOPT_RETURNTRANSFER,1);\\n    curl_setopt($ch,CURLOPT_POST, 1);\\n    curl_setopt($ch,CURLOPT_POSTFIELDS, $field_string);\\n    \\n    //EXECUTE POST\\n    echo curl_exec($ch);\\n    \\n    //CLOSE CONNECTION\\n    curl_close($ch);\\n\\n-\\nHere is the part of the url string with the POST values:\\n\\n    grant_type=client_credentials\\u0026duration=permanent\\u0026response_type=code\\u0026state=abcdefghijklmnopqrstuvwxyz\\n\\nHere is the response I am getting:\\n\\n    {\\\"access_token\\\": \\\"-zQjcXuqld_bz6OltZQmdYA3Slr0\\\", \\\"token_type\\\": \\\"bearer\\\", \\\"expires_in\\\": 3600, \\\"scope\\\": \\\"*\\\"}\\n\\nI think there should be an extra line that contains the refresh_token as well.\\n\\n\\n\\nAny help is greatly appreciated!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3a7o82\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"radleybobins\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3a7o82/getting_refresh_token_using_phpcurl/\", \"locked\": false, \"name\": \"t3_3a7o82\", \"created\": 1434605954.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3a7o82/getting_refresh_token_using_phpcurl/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Getting refresh_token using PHP/CURL\", \"created_utc\": 1434577154.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI switched my app to OAuth as requested by reddit, but I lost several things in the process.  Such as giving the users the ability to create a new account in the app.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERight now, the oauth page allows the user to create an account but it doesn\\u0026#39;t validate the application once the account is created, nor does it create a token.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhich means the process to create a new account via the app is to click login -\\u0026gt; navigate to the new account section in the oauth page -\\u0026gt; create the account -\\u0026gt; exit out of the login window -\\u0026gt; reopen the login window -\\u0026gt; log in with the credentials of the new account.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I switched my app to OAuth as requested by reddit, but I lost several things in the process.  Such as giving the users the ability to create a new account in the app.  \\r\\n\\r\\nRight now, the oauth page allows the user to create an account but it doesn't validate the application once the account is created, nor does it create a token.\\r\\n\\r\\nWhich means the process to create a new account via the app is to click login -\\u003E navigate to the new account section in the oauth page -\\u003E create the account -\\u003E exit out of the login window -\\u003E reopen the login window -\\u003E log in with the credentials of the new account.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"397zra\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"JavaLSU\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/397zra/request_update_the_oauth_login_page_to_properly/\", \"locked\": false, \"name\": \"t3_397zra\", \"created\": 1433917802.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/397zra/request_update_the_oauth_login_page_to_properly/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Request] Update the OAuth login page to properly handle new accounts\", \"created_utc\": 1433889002.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI learned that all posts have a base 36 id.. however i only came upon this by accident while experimenting and then from google search. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs this documented, why not?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EImportantly what other things are there which are not obvious and or not documented about reddit api.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I learned that all posts have a base 36 id.. however i only came upon this by accident while experimenting and then from google search. \\n\\nIs this documented, why not?\\n\\nImportantly what other things are there which are not obvious and or not documented about reddit api.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"37co3s\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"techsin101\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/37co3s/where_is_redditcombase36id_documented/\", \"locked\": false, \"name\": \"t3_37co3s\", \"created\": 1432690431.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/37co3s/where_is_redditcombase36id_documented/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Where is reddit.com/{base36id} documented?\", \"created_utc\": 1432661631.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EYesterday, I tried replying to a comment after 9 hours\\u0026#39; sleep, and it said the standard \\u0026quot;You\\u0026#39;re doing that too much\\u0026quot; thing, even though it had been at least 10 hours since I last wrote a comment.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThen today, yet again, after writing 2 consecutive comments, I was \\u0026quot;doing that too much\\u0026quot; again. Why is the comment quota so high?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI suppose the real question is, is this intentional?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Yesterday, I tried replying to a comment after 9 hours' sleep, and it said the standard \\\"You're doing that too much\\\" thing, even though it had been at least 10 hours since I last wrote a comment.\\n\\nThen today, yet again, after writing 2 consecutive comments, I was \\\"doing that too much\\\" again. Why is the comment quota so high?\\n\\nI suppose the real question is, is this intentional?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"33sque\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Ketchup901\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/33sque/why_is_the_comment_quota_so_high/\", \"locked\": false, \"name\": \"t3_33sque\", \"created\": 1429973404.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/33sque/why_is_the_comment_quota_so_high/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why is the comment quota so high?\", \"created_utc\": 1429944604.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhile updates are important, i would much rather go through my reddit feeds on weekends. Except mediocre solutions like IFTTT and Zapier, is it A) possible to create a custom weekly email digest for specific subreddits, that emails me posts over a certain karma threshold every sunday morning at 7 am? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EB) Get notified about a pre-defined search term over a specific karma threshold in a specific subreddit?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EExample A) Email me posts over 100 karma in \\u003Ca href=\\\"/r/imaginaryTechnology\\\"\\u003E/r/imaginaryTechnology\\u003C/a\\u003E to my inbox every Sunday morning at 7 am.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EExample B) Notify me when: A new jailbreak post in \\u003Ca href=\\\"/r/jailbreak\\\"\\u003E/r/jailbreak\\u003C/a\\u003E has the title \\u0026quot;JAILBREAK RELEASED\\u0026quot; with over 200 karma.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESort of like this: \\u003Ca href=\\\"http://pipes.yahoo.com/pipes/pipe.info?_id=4a3af06dcba612424a858ea79dd263db\\\"\\u003Ehttp://pipes.yahoo.com/pipes/pipe.info?_id=4a3af06dcba612424a858ea79dd263db\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOr this: \\u003Ca href=\\\"http://redditdelivery.com\\\"\\u003Ehttp://redditdelivery.com\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMore: \\u003Ca href=\\\"http://stackoverflow.com/questions/3459817/is-it-possible-to-get-a-rss-feed-of-a-reddit-with-links-to-posts-with-x-upvotes\\\"\\u003Ehttp://stackoverflow.com/questions/3459817/is-it-possible-to-get-a-rss-feed-of-a-reddit-with-links-to-posts-with-x-upvotes\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAdditional resources: \\u003Ca href=\\\"https://www.reddit.com/wiki/rss#wiki_can_i_use_reddit_as_a_newsreader.3F\\\"\\u003Ehttps://www.reddit.com/wiki/rss#wiki_can_i_use_reddit_as_a_newsreader.3F\\u003C/a\\u003E / \\u003Ca href=\\\"https://www.reddit.com/r/pathogendavid/comments/tv8m9/pathogendavids_guide_to_rss_and_reddit/\\\"\\u003Ehttps://www.reddit.com/r/pathogendavid/comments/tv8m9/pathogendavids_guide_to_rss_and_reddit/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt should be possible by extracting information from the .json data (i\\u0026#39;m not a dev, pls help). Any help/solutions would be appreciated. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECheers.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"While updates are important, i would much rather go through my reddit feeds on weekends. Except mediocre solutions like IFTTT and Zapier, is it A) possible to create a custom weekly email digest for specific subreddits, that emails me posts over a certain karma threshold every sunday morning at 7 am? \\n\\nB) Get notified about a pre-defined search term over a specific karma threshold in a specific subreddit?\\n\\nExample A) Email me posts over 100 karma in /r/imaginaryTechnology to my inbox every Sunday morning at 7 am.\\n\\nExample B) Notify me when: A new jailbreak post in /r/jailbreak has the title \\\"JAILBREAK RELEASED\\\" with over 200 karma.\\n\\nSort of like this: http://pipes.yahoo.com/pipes/pipe.info?_id=4a3af06dcba612424a858ea79dd263db\\n\\nOr this: http://redditdelivery.com\\n\\nMore: http://stackoverflow.com/questions/3459817/is-it-possible-to-get-a-rss-feed-of-a-reddit-with-links-to-posts-with-x-upvotes\\n\\nAdditional resources: https://www.reddit.com/wiki/rss#wiki_can_i_use_reddit_as_a_newsreader.3F / https://www.reddit.com/r/pathogendavid/comments/tv8m9/pathogendavids_guide_to_rss_and_reddit/\\n\\nIt should be possible by extracting information from the .json data (i'm not a dev, pls help). Any help/solutions would be appreciated. \\n\\n\\nCheers.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"32bx51\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"KO__\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1428848524.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/32bx51/is_it_possible_create_a_custom_reddit_weekly/\", \"locked\": false, \"name\": \"t3_32bx51\", \"created\": 1428874771.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/32bx51/is_it_possible_create_a_custom_reddit_weekly/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is it possible: Create a custom reddit weekly (email) digest from submissions over a certain upvote threshold and mail it to my inbox?\", \"created_utc\": 1428845971.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m writing an Android app to display the buttons time, and it was working perfectly, until it stopped working and came back with\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EW/System.err\\ufe55 java.net.UnknownHostException: Unable to resolve host \\u0026quot;wss.redditmedia.com\\u0026quot;: No address associated with hostname\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;d taken a link like this off another button app I found on github\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ewss://wss.redditmedia.com/thebutton?h=be515588cdb495371480532f1da1b772916dbc6f\\u0026amp;e=1428363306\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt worked, and then stopped working.  I checked the guys github again, and he had a new link in there.  When I replaced the link, it started working again?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cem\\u003EUPDATE\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe socket connection stopped functioning again after about 24h.  Replacing it with a new link worked.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBad Link: (\\u0026quot;wss://wss.redditmedia.com/thebutton?h=be515588cdb495371480532f1da1b772916dbc6f\\u0026amp;e=1428363306\\u0026quot;);\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EGood Link: (\\u0026quot;wss://wss.redditmedia.com/thebutton?h=91bd1f9cbc4913dfd9e145029095293c9a482b57\\u0026amp;e=1428409783\\u0026quot;);\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EToggling between them results in the bad link not working, and then the good link working.  What is going on?  I can\\u0026#39;t manage uploading a new app once a day for however long this goes on.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm writing an Android app to display the buttons time, and it was working perfectly, until it stopped working and came back with\\n\\nW/System.err\\ufe55 java.net.UnknownHostException: Unable to resolve host \\\"wss.redditmedia.com\\\": No address associated with hostname\\n\\nI'd taken a link like this off another button app I found on github\\n\\nwss://wss.redditmedia.com/thebutton?h=be515588cdb495371480532f1da1b772916dbc6f\\u0026e=1428363306\\n\\nIt worked, and then stopped working.  I checked the guys github again, and he had a new link in there.  When I replaced the link, it started working again?\\n\\n*UPDATE*\\n\\nThe socket connection stopped functioning again after about 24h.  Replacing it with a new link worked.\\n\\nBad Link: (\\\"wss://wss.redditmedia.com/thebutton?h=be515588cdb495371480532f1da1b772916dbc6f\\u0026e=1428363306\\\");\\n\\nGood Link: (\\\"wss://wss.redditmedia.com/thebutton?h=91bd1f9cbc4913dfd9e145029095293c9a482b57\\u0026e=1428409783\\\");\\n\\nToggling between them results in the bad link not working, and then the good link working.  What is going on?  I can't manage uploading a new app once a day for however long this goes on.\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"31l2c7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"NebuLights\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1428371411.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/31l2c7/why_does_my_websocket_connect_to_the_button_stop/\", \"locked\": false, \"name\": \"t3_31l2c7\", \"created\": 1428315713.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/31l2c7/why_does_my_websocket_connect_to_the_button_stop/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why does my websocket connect to The Button stop working after some time and require a new one?\", \"created_utc\": 1428286913.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ETrying to get usernames from all the instances in the mod-log where the action is \\u003Ccode\\u003Ebanuser\\u003C/code\\u003E or \\u003Ccode\\u003Eunbanuser\\u003C/code\\u003E.  However it doesn\\u0026#39;t give you the username, just the id.  Tried using:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Er.get_info(thing_id=\\u0026#39;t2_......\\u0026#39;)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EBut that only seems to work for posts and comments.  Is there any way to either get the username from the id or see the username from the result of \\u003Ccode\\u003Er.get_mod_log(sub)\\u003C/code\\u003E?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Trying to get usernames from all the instances in the mod-log where the action is `banuser` or `unbanuser`.  However it doesn't give you the username, just the id.  Tried using:\\n\\n    r.get_info(thing_id='t2_......')\\n\\nBut that only seems to work for posts and comments.  Is there any way to either get the username from the id or see the username from the result of `r.get_mod_log(sub)`?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"318l67\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"TeroTheTerror\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/318l67/getting_a_user_from_their_t2_id/\", \"locked\": false, \"name\": \"t3_318l67\", \"created\": 1428038809.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/318l67/getting_a_user_from_their_t2_id/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Getting a user from their t2_ id\", \"created_utc\": 1428010009.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2yxl4k\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"thorarakis\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2yxl4k/tweak_to_the_subreddit_object_when_using_the/\", \"locked\": false, \"name\": \"t3_2yxl4k\", \"created\": 1426298416.0, \"url\": \"https://github.com/reddit/reddit/commit/f68dbdec0282387e926ffb27c5e77a913d2ed688\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Tweak to the subreddit object when using the multi api\", \"created_utc\": 1426269616.0, \"distinguished\": \"admin\", \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI wrote shim to allow my reddit clone to user Solr for search, rather than cloudsearch. It\\u0026#39;s not particularly elegant or even DRY, but my main goals were: 1) get it done quick and 2) don\\u0026#39;t mess with cloudsearch.py. Check it out, let me know if you have any suggestions for improvement.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/pull/1275\\\"\\u003Ehttps://github.com/reddit/reddit/pull/1275\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I wrote shim to allow my reddit clone to user Solr for search, rather than cloudsearch. It's not particularly elegant or even DRY, but my main goals were: 1) get it done quick and 2) don't mess with cloudsearch.py. Check it out, let me know if you have any suggestions for improvement.\\n\\nhttps://github.com/reddit/reddit/pull/1275\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2xq93z\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"grandpaslab\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2xq93z/reddit_clone_solr_search_shim/\", \"locked\": false, \"name\": \"t3_2xq93z\", \"created\": 1425370169.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2xq93z/reddit_clone_solr_search_shim/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[reddit clone] Solr search shim\", \"created_utc\": 1425341369.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAt the end of the published tag there\\u0026#39;s either a plus or a minus followed by 4 digits. Lately I\\u0026#39;ve been getting a lot of messages that are about a week late with -0800 at the end. I assume the 8 is days. Why is this happening? It\\u0026#39;s becoming more frequent as time goes on. Here are some example publication dates all from today;\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003ESat, 21 Feb 2015 07:00:57 -0800\\n\\nThu, 19 Feb 2015 14:25:27 -0800\\n\\nSat, 21 Feb 2015 11:28:35 -0800\\n\\nSat, 14 Feb 2015 13:34:18 -0800\\n\\nSat, 21 Feb 2015 11:29:39 -0800\\n\\nWed, 18 Feb 2015 08:34:21 -0800\\n\\nSun, 15 Feb 2015 04:41:18 -0800\\n\\nSun, 01 Mar 2015 20:58:05 +0000\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"At the end of the published tag there's either a plus or a minus followed by 4 digits. Lately I've been getting a lot of messages that are about a week late with -0800 at the end. I assume the 8 is days. Why is this happening? It's becoming more frequent as time goes on. Here are some example publication dates all from today;\\n\\n\\tSat, 21 Feb 2015 07:00:57 -0800\\n\\n\\tThu, 19 Feb 2015 14:25:27 -0800\\n\\n\\tSat, 21 Feb 2015 11:28:35 -0800\\n\\n\\tSat, 14 Feb 2015 13:34:18 -0800\\n\\n\\tSat, 21 Feb 2015 11:29:39 -0800\\n\\n\\tWed, 18 Feb 2015 08:34:21 -0800\\n\\n\\tSun, 15 Feb 2015 04:41:18 -0800\\n\\n\\tSun, 01 Mar 2015 20:58:05 +0000\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2xm82q\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"wantonballbag\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2xm82q/reddit_rss_publication_date_and_rss_feed_lagging/\", \"locked\": false, \"name\": \"t3_2xm82q\", \"created\": 1425285655.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2xm82q/reddit_rss_publication_date_and_rss_feed_lagging/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit RSS publication date and RSS feed lagging a week behind?\", \"created_utc\": 1425256855.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi, I\\u0026#39;m not sure if the problem is from my end, but refreshing the access token doesn\\u0026#39;t seem to work.  I used the general token retrieval method (for non-installed apps) last night and got a valid access token + refresh token that worked with the API.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EUsing the access_token 24hrs later doesn\\u0026#39;t work (as expected), and using the refresh token on /api/v1/access_token responds with a new access token upon each request.  However, these access_tokens don\\u0026#39;t work with the API (returns a 401 error).  Re-authorizing the app (going back to step 1 of the token retrieval method) appears to reset everything and make it work.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAm I using the refresh token incorrectly or something?  Should I be refreshing access tokens before hitting a 401 (say, every 30m)?  Thanks!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit (Mar 1): It looks like something is definitely broken on reddit\\u0026#39;s side.  Last night, refreshing the initial access_token before the expiration produced an identical access token.  Refreshing the initial access token now produces a different token each time, none of which work (only the initial access_token works).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit 2 (Mar 1): Interesting. Refreshing the access token on the re-authorized account 7hrs later produced a total of two valid access tokens (both work with the API currently).  However, any further access tokens produced by using the refresh token do not work with the API (401).  Might be an issue with generating overlapping access token entries?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi, I'm not sure if the problem is from my end, but refreshing the access token doesn't seem to work.  I used the general token retrieval method (for non-installed apps) last night and got a valid access token + refresh token that worked with the API.\\n\\nUsing the access_token 24hrs later doesn't work (as expected), and using the refresh token on /api/v1/access_token responds with a new access token upon each request.  However, these access_tokens don't work with the API (returns a 401 error).  Re-authorizing the app (going back to step 1 of the token retrieval method) appears to reset everything and make it work.\\n\\nAm I using the refresh token incorrectly or something?  Should I be refreshing access tokens before hitting a 401 (say, every 30m)?  Thanks!\\n\\nEdit (Mar 1): It looks like something is definitely broken on reddit's side.  Last night, refreshing the initial access_token before the expiration produced an identical access token.  Refreshing the initial access token now produces a different token each time, none of which work (only the initial access_token works).\\n\\nEdit 2 (Mar 1): Interesting. Refreshing the access token on the re-authorized account 7hrs later produced a total of two valid access tokens (both work with the API currently).  However, any further access tokens produced by using the refresh token do not work with the API (401).  Might be an issue with generating overlapping access token entries?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2xjshf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"RedBanHammer\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 18, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1425305937.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2xjshf/oauth2_refresh_token_issues/\", \"locked\": false, \"name\": \"t3_2xjshf\", \"created\": 1425231832.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2xjshf/oauth2_refresh_token_issues/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OAuth2 refresh_token issues\", \"created_utc\": 1425203032.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://198.167.140.144/r/news/comments/1/welcome_to_darkkit_front_page_of_the_dark_net/\\\"\\u003Ehttp://198.167.140.144/r/news/comments/1/welcome_to_darkkit_front_page_of_the_dark_net/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDoes anyone know what the problem is? Or could give me some advice?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks, willing to pay to get it fully setup and working.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"http://198.167.140.144/r/news/comments/1/welcome_to_darkkit_front_page_of_the_dark_net/\\n\\nDoes anyone know what the problem is? Or could give me some advice?\\n\\n\\nThanks, willing to pay to get it fully setup and working.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2vk53o\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2vk53o/you_broke_reddit/\", \"locked\": false, \"name\": \"t3_2vk53o\", \"created\": 1423707065.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2vk53o/you_broke_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"\\\"You broke reddit\\\"\", \"created_utc\": 1423678265.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EMy script is meant to scrape a specific subreddit\\u0026#39;s link posts. It calls \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/r/subreddit/new.json?after=%5BsomeID%5D\\\"\\u003Ehttp://www.reddit.com/r/subreddit/new.json?after=[someID]\\u003C/a\\u003E,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eidentifies the last id on the page, \\u003Cstrong\\u003ESleeps 4 seconds\\u003C/strong\\u003E, then calls\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/r/programming/new.json?after=%5BlastIDOnPreviousPage%5D\\\"\\u003Ehttp://www.reddit.com/r/programming/new.json?after=[lastIDOnPreviousPage]\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAfter about 20 or so iterations this returns some json with a bunch of null fields. The same url returns the regular listing I want when typed into the browser.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there a limitation to the API I\\u0026#39;m not aware of? Am I running into some other issue?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"My script is meant to scrape a specific subreddit's link posts. It calls \\n\\nhttp://www.reddit.com/r/subreddit/new.json?after=[someID],\\n\\nidentifies the last id on the page, **Sleeps 4 seconds**, then calls\\n\\nhttp://www.reddit.com/r/programming/new.json?after=[lastIDOnPreviousPage]\\n\\nAfter about 20 or so iterations this returns some json with a bunch of null fields. The same url returns the regular listing I want when typed into the browser.\\n\\nIs there a limitation to the API I'm not aware of? Am I running into some other issue?\\n\\nThanks.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2vic2i\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ZigguratOfUr\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2vic2i/reddit_api_blocks_my_requests_for_pages_that_are/\", \"locked\": false, \"name\": \"t3_2vic2i\", \"created\": 1423662683.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2vic2i/reddit_api_blocks_my_requests_for_pages_that_are/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit API blocks my requests for pages that are too old. The json shows up in the browser. What gives?\", \"created_utc\": 1423633883.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey \\u003Ca href=\\\"/r/RedditDev\\\"\\u003Er/RedditDev\\u003C/a\\u003E community,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have been working on a project as part of an Interaction Design Graduate Thesis project utilizing Reddit AMA. The project, called AskUsAnything, is meant to be an annotated and analytic tool for reading AMA threads. I am interested in bringing a new way of viewing and understanding, even comparing and contrasting AMAs, with the intention of making the wealth of individual information and knowledge in each thread viewable in a new and unique way.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe prototype is online at \\u003Ca href=\\\"http://askusanything.cc\\\"\\u003Ehttp://askusanything.cc\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI wanted to bring the prototype version to the subreddit here so that some people who have spent a lot of time working on the platform and with the API could see the project. The project is emphatically \\u0026quot;beta\\u0026quot; at this stage, and I am fast at work on the next version. I have built this on Python/Flask, using the Reddit API and PRAW, utilizing NLTK for language analysis, BokehJS for visualization, and deployed on a DigitalOcean Ubuntu droplet.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you have any feedback on how you might use something like this (as potentially a Reddit/AMA user), how I might do something differently, or if this work reminds of you anything you have seen someone doing with Reddit data, I would love to hear about it.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECheers!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey r/RedditDev community,\\n\\nI have been working on a project as part of an Interaction Design Graduate Thesis project utilizing Reddit AMA. The project, called AskUsAnything, is meant to be an annotated and analytic tool for reading AMA threads. I am interested in bringing a new way of viewing and understanding, even comparing and contrasting AMAs, with the intention of making the wealth of individual information and knowledge in each thread viewable in a new and unique way.\\n\\nThe prototype is online at [http://askusanything.cc](http://askusanything.cc)\\n\\nI wanted to bring the prototype version to the subreddit here so that some people who have spent a lot of time working on the platform and with the API could see the project. The project is emphatically \\\"beta\\\" at this stage, and I am fast at work on the next version. I have built this on Python/Flask, using the Reddit API and PRAW, utilizing NLTK for language analysis, BokehJS for visualization, and deployed on a DigitalOcean Ubuntu droplet.\\n\\nIf you have any feedback on how you might use something like this (as potentially a Reddit/AMA user), how I might do something differently, or if this work reminds of you anything you have seen someone doing with Reddit data, I would love to hear about it.\\n\\nCheers!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2v5s0c\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"chrisarr\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2v5s0c/building_a_tool_for_reddit_ama_take_a_peek/\", \"locked\": false, \"name\": \"t3_2v5s0c\", \"created\": 1423396138.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2v5s0c/building_a_tool_for_reddit_ama_take_a_peek/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Building a tool for Reddit AMA \\u2013 Take a peek!\", \"created_utc\": 1423367338.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2v1cz1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"go1dfish\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2v1cz1/do_reports_always_have_to_be_of_bad_things_can_i/\", \"locked\": false, \"name\": \"t3_2v1cz1\", \"created\": 1423293665.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2v1cz1/do_reports_always_have_to_be_of_bad_things_can_i/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Do reports always have to be of bad things? Can I have a bot take advantage of custom reporting reasons to crowdsource things like tagging without biasing reddit filters against a post or user?\", \"created_utc\": 1423264865.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi, is there a way to download a whole subreddit? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m experimenting with making a search engine(it is \\u003Ca href=\\\"https://github.com/eleweek/SearchingReddit\\\"\\u003Eopensource\\u003C/a\\u003E). The subreddit I\\u0026#39;m interested in is \\u003Ca href=\\\"/r/learnprogramming\\\"\\u003E/r/learnprogramming\\u003C/a\\u003E \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi, is there a way to download a whole subreddit? \\n\\nI'm experimenting with making a search engine(it is [opensource](https://github.com/eleweek/SearchingReddit)). The subreddit I'm interested in is /r/learnprogramming \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2uzidx\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"godlikesme\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 23, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2uzidx/downloading_a_whole_subreddit/\", \"locked\": false, \"name\": \"t3_2uzidx\", \"created\": 1423261647.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2uzidx/downloading_a_whole_subreddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Downloading a whole subreddit?\", \"created_utc\": 1423232847.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Cem\\u003EI apologise if this has been asked already\\u003C/em\\u003E\\nI have to setup a forum style website for internal only use (like an intranet but nothing is accesible from outside our network) at my company where people can collaborate and share ideas. It will not have the reddit logo as per the licensing rules but is there a commercial license fee to pay if i want to use this at work? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWe\\u0026#39;ve looked at a lot of other platforms but I wanted to setup something like reddit, maybe without the bots or advanced stuff, just a basic discussion board with the upvoting feature.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI found this: \\u003Ca href=\\\"https://github.com/reddit\\\"\\u003Ehttps://github.com/reddit\\u003C/a\\u003E is that the source code for reddit? Does that mean i can slap that on a box, and plug in the company logo and have an internal reddit-like site?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECould anyone please help me out where i should start (any tutorial?) or if the github content is something entirely different from what im thinking. Im a developer but not much of an infrastructure guy, so i was hoping to find a tutorial to help me through.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"*I apologise if this has been asked already*\\nI have to setup a forum style website for internal only use (like an intranet but nothing is accesible from outside our network) at my company where people can collaborate and share ideas. It will not have the reddit logo as per the licensing rules but is there a commercial license fee to pay if i want to use this at work? \\n\\nWe've looked at a lot of other platforms but I wanted to setup something like reddit, maybe without the bots or advanced stuff, just a basic discussion board with the upvoting feature.\\n\\nI found this: https://github.com/reddit is that the source code for reddit? Does that mean i can slap that on a box, and plug in the company logo and have an internal reddit-like site?\\n\\nCould anyone please help me out where i should start (any tutorial?) or if the github content is something entirely different from what im thinking. Im a developer but not much of an infrastructure guy, so i was hoping to find a tutorial to help me through.\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2uwpw3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"manual_mode\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2uwpw3/noob_here_need_some_help_setting_up_an_internal/\", \"locked\": false, \"name\": \"t3_2uwpw3\", \"created\": 1423197090.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2uwpw3/noob_here_need_some_help_setting_up_an_internal/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Noob here, need some help setting up an internal reddit-like site and whether I am looking at the right thing with reddit's code on github\", \"created_utc\": 1423168290.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo\\u2026 today, instead of working, I messed around with coding stuff. I\\u2019m not very good at it. I\\u2019m sure my boss would be pleased...\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnyway, the end result was a simple, very simple, reddit bot. All it does is play \\u201cRock, Paper, Scissors.\\u201d \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo play, type \\u201c\\u003Ca href=\\\"/u/friendlygame\\\"\\u003E/u/friendlygame\\u003C/a\\u003E rps \\u0026lt;rock, paper, or scissors\\u0026gt;.\\u201d\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor example, you\\u2019d type:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E\\u003Ca href=\\\"/u/friendlygame\\\"\\u003E/u/friendlygame\\u003C/a\\u003E rps rock\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003Eto play rock.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnyway, I decided to post here for feedback and/or advice. I\\u0026#39;m only around 20% sure of what I\\u0026#39;m doing. Yes, I know I should be working instead of screwing around, the boss doesn\\u0026#39;t care so long as customers are happy and I don\\u0026#39;t burn down the place.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So\\u2026 today, instead of working, I messed around with coding stuff. I\\u2019m not very good at it. I\\u2019m sure my boss would be pleased...\\n\\nAnyway, the end result was a simple, very simple, reddit bot. All it does is play \\u201cRock, Paper, Scissors.\\u201d \\n\\nTo play, type \\u201c/u/friendlygame rps \\u003Crock, paper, or scissors\\u003E.\\u201d\\n\\nFor example, you\\u2019d type:\\n\\n\\u003E/u/friendlygame rps rock\\n\\nto play rock.\\n\\nAnyway, I decided to post here for feedback and/or advice. I'm only around 20% sure of what I'm doing. Yes, I know I should be working instead of screwing around, the boss doesn't care so long as customers are happy and I don't burn down the place.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2rwyy4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"friendlygame\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2rwyy4/hi_would_you_like_to_play_a_game_or_give_advice/\", \"locked\": false, \"name\": \"t3_2rwyy4\", \"created\": 1420878535.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2rwyy4/hi_would_you_like_to_play_a_game_or_give_advice/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Hi! Would you like to play a game? Or... give advice to a noobish dev regarding the development of reddit bots?\", \"created_utc\": 1420849735.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EToday \\u003Ca href=\\\"http://www.redditblog.com/2015/01/create-your-own-reddit-alien-avatar.html\\\"\\u003ESnoovatars\\u003C/a\\u003E were announced, which are pretty cool. Is there an API for this/are there any plans for one to exist?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Today [Snoovatars](http://www.redditblog.com/2015/01/create-your-own-reddit-alien-avatar.html) were announced, which are pretty cool. Is there an API for this/are there any plans for one to exist?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2rnm26\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"iamthatis\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2rnm26/api_for_the_new_snoovatars/\", \"locked\": false, \"name\": \"t3_2rnm26\", \"created\": 1420686331.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2rnm26/api_for_the_new_snoovatars/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API for the new Snoovatars?\", \"created_utc\": 1420657531.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey guys,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m trying to give gold via the Reddit API. I want it to utilize the creddits already in the account. I saw the \\u003Ccode\\u003Eapi/v1/gold/give\\u003C/code\\u003E method in the API docs but couldn\\u0026#39;t figure out how to use it.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI can get the current user\\u0026#39;s \\u003Ccode\\u003Emodhash\\u003C/code\\u003E by logging in with PRAW. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI tried this:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport requests\\nimport praw\\n\\nreddit = praw.Reddit(user_agent = \\u0026quot;Gold Testing Bot\\u0026quot;)\\nreddit.login(my_user, my_pass)\\n\\nmodhash = reddit.modhash\\n\\nclient = requests.session()\\nclient.headers = {\\u0026quot;user-agent\\u0026quot;: \\u0026quot;Gold Testing Bot\\u0026quot;}\\nresponse = client.post(\\u0026quot;https://www.reddit.com/api/v1/gold/give\\u0026quot;, {\\u0026quot;months\\u0026quot;: 1, \\u0026quot;username\\u0026quot;: some_user})\\nprint response.text\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI am receiving a bunch of HTML, which appears to be the default Reddit home page.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there any way to do this? I\\u0026#39;m not sure how to utilize the modhash, so I think that\\u0026#39;s part of the issue.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey guys,\\n\\nI'm trying to give gold via the Reddit API. I want it to utilize the creddits already in the account. I saw the `api/v1/gold/give` method in the API docs but couldn't figure out how to use it.\\n\\nI can get the current user's `modhash` by logging in with PRAW. \\n\\nI tried this:\\n    \\n    import requests\\n    import praw\\n    \\n    reddit = praw.Reddit(user_agent = \\\"Gold Testing Bot\\\")\\n    reddit.login(my_user, my_pass)\\n    \\n    modhash = reddit.modhash\\n    \\n    client = requests.session()\\n    client.headers = {\\\"user-agent\\\": \\\"Gold Testing Bot\\\"}\\n    response = client.post(\\\"https://www.reddit.com/api/v1/gold/give\\\", {\\\"months\\\": 1, \\\"username\\\": some_user})\\n    print response.text\\n\\nI am receiving a bunch of HTML, which appears to be the default Reddit home page.\\n\\nIs there any way to do this? I'm not sure how to utilize the modhash, so I think that's part of the issue.\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2hmh42\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"F3AR3DLEGEND\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2hmh42/how_to_give_gold_with_the_api/\", \"locked\": false, \"name\": \"t3_2hmh42\", \"created\": 1411863164.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2hmh42/how_to_give_gold_with_the_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to Give Gold with the API\", \"created_utc\": 1411834364.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIs it available to anyone? If its javascript isn\\u0026#39;t that dangerous?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Is it available to anyone? If its javascript isn't that dangerous?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2folpo\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"seekoon\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2folpo/newb_question_how_do_bots_such_as_autowikibot/\", \"locked\": false, \"name\": \"t3_2folpo\", \"created\": 1410081208.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2folpo/newb_question_how_do_bots_such_as_autowikibot/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Newb question: How do bots such as autowikibot have the 'hover over comment to view' feature?\", \"created_utc\": 1410052408.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAs you may know, reddit supports multiple subdomains and HTTP/S for access. Which one is best to use for API calls? For instance, these are the ones that I know of so far:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Ca href=\\\"http://www.reddit.com/\\\"\\u003Ehttp://www.reddit.com/\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ca href=\\\"https://www.reddit.com/\\\"\\u003Ehttps://www.reddit.com/\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ca href=\\\"https://pay.reddit.com/\\\"\\u003Ehttps://pay.reddit.com/\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Ca href=\\\"https://ssl.reddit.com/\\\"\\u003Ehttps://ssl.reddit.com/\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve forgotten the specifics, but some API calls don\\u0026#39;t work with the above base URL\\u0026#39;s. From what I remember, I\\u0026#39;ve been least successful with the last option. However, I would like to avoid using the first option since its insecure.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"As you may know, reddit supports multiple subdomains and HTTP/S for access. Which one is best to use for API calls? For instance, these are the ones that I know of so far:\\n\\n* http://www.reddit.com/\\n* https://www.reddit.com/\\n* https://pay.reddit.com/\\n* https://ssl.reddit.com/\\n\\nI've forgotten the specifics, but some API calls don't work with the above base URL's. From what I remember, I've been least successful with the last option. However, I would like to avoid using the first option since its insecure.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2dew9d\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"rotorcowboy\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2dew9d/which_base_url_should_i_use_for_api_calls/\", \"locked\": false, \"name\": \"t3_2dew9d\", \"created\": 1407938692.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2dew9d/which_base_url_should_i_use_for_api_calls/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Which base URL should I use for API calls?\", \"created_utc\": 1407909892.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Cdel\\u003EI\\u0026#39;m the developer of ReddHub for Windows8, and I recently had a user report this bug - \\u003Ca href=\\\"https://github.com/ferasm/reddhub/issues/31\\\"\\u003Ehttps://github.com/ferasm/reddhub/issues/31\\u003C/a\\u003E\\u003C/del\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cdel\\u003EAfter looking into it, it appears to maybe be a bug in the reddit API - This only happens when a user is viewing their messages, and if you look at \\u003Ca href=\\\"http://www.reddit.com/message/inbox.json\\\"\\u003Ehttp://www.reddit.com/message/inbox.json\\u003C/a\\u003E , you will see likes is set to null, rather than the correct number.\\u003C/del\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis may be a known bug, but I wanted to be sure to report it in case.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESorry, what I really should of said is - the messages list above will give comments (kind T1) but doesnt give the score - it\\u0026#39;d be very useful to have the score, so users can more easily view the data.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs this something that can be added?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"~~I'm the developer of ReddHub for Windows8, and I recently had a user report this bug - https://github.com/ferasm/reddhub/issues/31~~\\n\\n~~After looking into it, it appears to maybe be a bug in the reddit API - This only happens when a user is viewing their messages, and if you look at http://www.reddit.com/message/inbox.json , you will see likes is set to null, rather than the correct number.~~\\n\\nThis may be a known bug, but I wanted to be sure to report it in case.\\n\\nThanks\\n\\nSorry, what I really should of said is - the messages list above will give comments (kind T1) but doesnt give the score - it'd be very useful to have the score, so users can more easily view the data.\\n\\nIs this something that can be added?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"27asaa\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"csmaster2005\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1401900333.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/27asaa/reddit_message_api_bug_likes_set_to_null_instead/\", \"locked\": false, \"name\": \"t3_27asaa\", \"created\": 1401925883.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/27asaa/reddit_message_api_bug_likes_set_to_null_instead/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit message API bug - likes set to null instead of correct #\", \"created_utc\": 1401897083.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"youtube.com\", \"banned_by\": null, \"media_embed\": {\"content\": \"\\u003Ciframe class=\\\"embedly-embed\\\" src=\\\"//cdn.embedly.com/widgets/media.html?url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DPFmB7dTdQyk\\u0026src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FPFmB7dTdQyk%3Ffeature%3Doembed\\u0026type=text%2Fhtml\\u0026key=2aa3c4d5f3de4f5b9120b660ad850dc9\\u0026schema=youtube\\\" width=\\\"600\\\" height=\\\"338\\\" scrolling=\\\"no\\\" frameborder=\\\"0\\\" allowfullscreen\\u003E\\u003C/iframe\\u003E\", \"width\": 600, \"scrolling\": false, \"height\": 338}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": {\"oembed\": {\"provider_url\": \"http://www.youtube.com/\", \"description\": \"Well, that's a mouthful. Watch the video to see what's it about.\", \"title\": \"CodeLive S02E01 - Reddit API, JSON, GSON and Declarative Programming\", \"url\": \"http://www.youtube.com/watch?v=PFmB7dTdQyk\", \"author_name\": \"FizzlNet\", \"height\": 338, \"width\": 600, \"html\": \"\\u003Ciframe class=\\\"embedly-embed\\\" src=\\\"https://cdn.embedly.com/widgets/media.html?url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DPFmB7dTdQyk\\u0026src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FPFmB7dTdQyk%3Ffeature%3Doembed\\u0026type=text%2Fhtml\\u0026key=2aa3c4d5f3de4f5b9120b660ad850dc9\\u0026schema=youtube\\\" width=\\\"600\\\" height=\\\"338\\\" scrolling=\\\"no\\\" frameborder=\\\"0\\\" allowfullscreen\\u003E\\u003C/iframe\\u003E\", \"author_url\": \"http://www.youtube.com/user/FizzlNet\", \"version\": \"1.0\", \"provider_name\": \"YouTube\", \"type\": \"video\"}, \"type\": \"youtube.com\"}, \"link_flair_text\": null, \"id\": \"26sbc3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"fizzl\", \"media\": {\"oembed\": {\"provider_url\": \"http://www.youtube.com/\", \"description\": \"Well, that's a mouthful. Watch the video to see what's it about.\", \"title\": \"CodeLive S02E01 - Reddit API, JSON, GSON and Declarative Programming\", \"url\": \"http://www.youtube.com/watch?v=PFmB7dTdQyk\", \"author_name\": \"FizzlNet\", \"height\": 338, \"width\": 600, \"html\": \"\\u003Ciframe class=\\\"embedly-embed\\\" src=\\\"//cdn.embedly.com/widgets/media.html?url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DPFmB7dTdQyk\\u0026src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FPFmB7dTdQyk%3Ffeature%3Doembed\\u0026type=text%2Fhtml\\u0026key=2aa3c4d5f3de4f5b9120b660ad850dc9\\u0026schema=youtube\\\" width=\\\"600\\\" height=\\\"338\\\" scrolling=\\\"no\\\" frameborder=\\\"0\\\" allowfullscreen\\u003E\\u003C/iframe\\u003E\", \"author_url\": \"http://www.youtube.com/user/FizzlNet\", \"version\": \"1.0\", \"provider_name\": \"YouTube\", \"type\": \"video\"}, \"type\": \"youtube.com\"}, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {\"content\": \"\\u003Ciframe class=\\\"embedly-embed\\\" src=\\\"https://cdn.embedly.com/widgets/media.html?url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DPFmB7dTdQyk\\u0026src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FPFmB7dTdQyk%3Ffeature%3Doembed\\u0026type=text%2Fhtml\\u0026key=2aa3c4d5f3de4f5b9120b660ad850dc9\\u0026schema=youtube\\\" width=\\\"600\\\" height=\\\"338\\\" scrolling=\\\"no\\\" frameborder=\\\"0\\\" allowfullscreen\\u003E\\u003C/iframe\\u003E\", \"width\": 600, \"scrolling\": false, \"height\": 338}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/26sbc3/reddit_api_json_gson_and_declarative_programming/\", \"locked\": false, \"name\": \"t3_26sbc3\", \"created\": 1401400688.0, \"url\": \"https://www.youtube.com/watch?v=PFmB7dTdQyk\\u0026feature=youtu.be\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit API, JSON, GSON and Declarative Programming\", \"created_utc\": 1401371888.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIn the spirit of not doing the same work twice, I\\u0026#39;m going to show you how to do something that helps you avoid doing the same work twice. It is surprisingly easy to set up an SQL database and interface it with your bot.\\u003C/p\\u003E\\n\\n\\u003Ch3\\u003EThe problem: Not doing the same work twice.\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EIf you followed the PRAW documentation, you may have used \\u003Ca href=\\\"https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html#not-doing-the-same-work-twice\\\"\\u003Ethis technique\\u003C/a\\u003E to prevent your bot doing something twice like leaving a comment, sending a message, or really anything. We don\\u0026#39;t want our bot spamming up reddit just because it doesn\\u0026#39;t know what it already did. So, we make use of a temporary local variable like already_done and keep track of what we have processed in it via:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ealready_done.append(submission.id)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch3\\u003EThe other problem: Losing your records of work done\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EIn my apps, I use a two-factor sanity check to avoid doing the same work: the above list method and a thorough search. The problem with the list method (already_done.append(id)) is that if the bot crashes, you lose the record of what work you have done. What I will often do is comb through my bot\\u0026#39;s history to make sure I didn\\u0026#39;t already reply to a comment or process a command, often using something like this: \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Edef replied(comment):\\n    replies = comment.replies\\n    replied = False\\n    for reply in replies:\\n        if reply.author.name == username:\\n            return True\\n    return False\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThis makes a TON of reddit calls, which results in a TON of delay. Just to see if we already did something! \\u003C/p\\u003E\\n\\n\\u003Ch3\\u003EWhy databases?\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EThe issue with my two-factor system is that reddit requests take time and are not very efficient compared to a local look-up. On the other hand, I can have a local database for my 2nd factor which isn\\u0026#39;t limited in calls by the reddit API and makes use of super efficient search algorithms.\\u003C/p\\u003E\\n\\n\\u003Ch3\\u003EInstalling prerequisites\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003ESo we need a few things for our python app to use a database. I\\u0026#39;m going to be using MySQL because fuck it, why not. So you need these packages installed:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esudo apt-get install python-mysqldb\\nsudo apt-get install mysql-server\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EYou will need to pick a username for mysql, I use \\u0026quot;root\\u0026quot;. I don\\u0026#39;t like to write raw SQL queries, so I use a wrapper called \\u0026quot;peewee\\u0026quot; for friendly commands. \\u003Ca href=\\\"https://www.google.com/url?sa=t\\u0026amp;rct=j\\u0026amp;q=\\u0026amp;esrc=s\\u0026amp;source=web\\u0026amp;cd=1\\u0026amp;cad=rja\\u0026amp;uact=8\\u0026amp;ved=0CCkQFjAA\\u0026amp;url=http%3A%2F%2Fpeewee.readthedocs.org%2Fen%2Flatest%2Fpeewee%2Fcookbook.html\\u0026amp;ei=Lr9YU9pKhreSBbHjgIgP\\u0026amp;usg=AFQjCNF8hSPe9PFvF-PeIt67QobCKT4Eow\\u0026amp;sig2=IkJgo1KANGZjd2fZYz9SZw\\u0026amp;bvm=bv.65397613,d.dGI\\\"\\u003EPeewee documentation\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Epip install peewee\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Ch3\\u003EBasic set up\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003EYou will need to create an SQL database, which is very easy. Enter MySQL with:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Emysql -u root -p\\nmysql\\u0026gt; create database redditbot \\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIn our example, we want to store submission IDs we have already processed in case our bot crashes and we lose our \\u0026quot;already_done\\u0026quot; list. In peewee, we need to connect to our database and create a \\u0026quot;model\\u0026quot; which represents our submission.id (or whatever we want to store):\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Efrom peewee import *\\ndb = MySQLDatabase(\\u0026#39;redditbot\\u0026#39;, host=\\u0026#39;localhost\\u0026#39;, user=\\u0026#39;root\\u0026#39;, passwd=\\u0026#39;SQLPASS\\u0026#39;)\\nclass Submissions(Model):\\n    subid = TextField()\\n\\n    class Meta:\\n        database = db\\n\\ndb.connect()\\nSubmissions.create_table(True) #the True flag won\\u0026#39;t alarm if table exists\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ECool, so now we are connected to our database, we have established what a \\u0026quot;submission\\u0026quot; model is, and we created a table of submissions. Now, we can add submissions to our database, search for existing submissions, or even flush the database of all entries. I\\u0026#39;ll show you those functions here:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Edef is_added(submission_id):   #is ID already in database?\\n    try:\\n        submissions = Submissions.get(Submissions.subid == submission_id)\\n            return True \\n    except:\\n        return False\\n\\ndef add_entry(submission_id): #add new entry\\n    if not is_added(submission_id):\\n        print \\u0026quot;Adding %s\\u0026quot; % submission_id\\n        Submissions(subid= submission_id).save()\\n\\ndef flush_db():    #remove all entries\\n    subs = Submissions.select()\\n    for sub in subs:\\n        sub.delete_instance()\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ERather than typing out ANY queries when we want to add things, I have wrapped all of the peewee code in custom python methods. Now, we can simply use my function to add a submission to our database:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esubmissions = reddit.get_new(limit =10)\\nfor submission in submissions:\\n    add_entry(submission.id)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIf the entry is already in the table, it won\\u0026#39;t be added again due to the is_added() method. This means at the \\u0026quot;reddit bot\\u0026quot; layer, we aren\\u0026#39;t doing any checks, which makes for much cleaner code. \\u003C/p\\u003E\\n\\n\\u003Ch3\\u003EIntegrating our 2-factor check\\u003C/h3\\u003E\\n\\n\\u003Cp\\u003ENow we don\\u0026#39;t want to rely on the DB when we can use already_done \\u003Cem\\u003Emost\\u003C/em\\u003E of the time, so let\\u0026#39;s put both together:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E already_done = []\\n submissions = reddit.get_new(limit =10)\\n for submission in submissions:\\n    if submission.id not in already_done and not is_added(submission.id):\\n        #Do whatever to your submission on this line\\n        already_done.append(submission.id)\\n        add_entry(submission.id)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EBoom. So we will check already_done for our ID and if it isn\\u0026#39;t there (because it\\u0026#39;s new OR we crashed), it will check the database. If it isn\\u0026#39;t in the database, it will add it. This way, we don\\u0026#39;t have to do any reddit calls to check if we already did this work. This makes for a MASSIVE performance increase. If you want, you can create one function to handle already_done and your database, like so:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Edef add_done(submission.id):\\n    already_done.append(submission.id)\\n    add_entry(submission.id)        \\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAnd a method to check if something is already done:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Edef is_done(submission.id):\\n    if not submission.id in already_done and not is_added(submission.id):\\n        return True\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ESo our final code could be:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ealready_done = []\\nsubmissions = reddit.get_new(limit =10)\\nfor submission in submissions:\\n    if not is_done(submission.id):\\n        #Do whatever to your submission on this line\\n        add_done(submission.id)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EI hope that all makes sense! Let me know if you have any questions. I\\u0026#39;m not super experienced with databases, but this simple implementation works for most of my purposes.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"In the spirit of not doing the same work twice, I'm going to show you how to do something that helps you avoid doing the same work twice. It is surprisingly easy to set up an SQL database and interface it with your bot.\\n\\n###The problem: Not doing the same work twice.\\n\\nIf you followed the PRAW documentation, you may have used [this technique](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html#not-doing-the-same-work-twice) to prevent your bot doing something twice like leaving a comment, sending a message, or really anything. We don't want our bot spamming up reddit just because it doesn't know what it already did. So, we make use of a temporary local variable like already_done and keep track of what we have processed in it via:\\n\\n    already_done.append(submission.id)\\n\\n###The other problem: Losing your records of work done\\n\\nIn my apps, I use a two-factor sanity check to avoid doing the same work: the above list method and a thorough search. The problem with the list method (already_done.append(id)) is that if the bot crashes, you lose the record of what work you have done. What I will often do is comb through my bot's history to make sure I didn't already reply to a comment or process a command, often using something like this: \\n\\n    def replied(comment):\\n        replies = comment.replies\\n        replied = False\\n        for reply in replies:\\n            if reply.author.name == username:\\n                return True\\n        return False\\n\\nThis makes a TON of reddit calls, which results in a TON of delay. Just to see if we already did something! \\n\\n###Why databases?\\n\\nThe issue with my two-factor system is that reddit requests take time and are not very efficient compared to a local look-up. On the other hand, I can have a local database for my 2nd factor which isn't limited in calls by the reddit API and makes use of super efficient search algorithms.\\n\\n###Installing prerequisites\\n\\nSo we need a few things for our python app to use a database. I'm going to be using MySQL because fuck it, why not. So you need these packages installed:\\n\\n    sudo apt-get install python-mysqldb\\n    sudo apt-get install mysql-server\\n\\nYou will need to pick a username for mysql, I use \\\"root\\\". I don't like to write raw SQL queries, so I use a wrapper called \\\"peewee\\\" for friendly commands. [Peewee documentation](https://www.google.com/url?sa=t\\u0026rct=j\\u0026q=\\u0026esrc=s\\u0026source=web\\u0026cd=1\\u0026cad=rja\\u0026uact=8\\u0026ved=0CCkQFjAA\\u0026url=http%3A%2F%2Fpeewee.readthedocs.org%2Fen%2Flatest%2Fpeewee%2Fcookbook.html\\u0026ei=Lr9YU9pKhreSBbHjgIgP\\u0026usg=AFQjCNF8hSPe9PFvF-PeIt67QobCKT4Eow\\u0026sig2=IkJgo1KANGZjd2fZYz9SZw\\u0026bvm=bv.65397613,d.dGI)\\n\\n    pip install peewee\\n\\n###Basic set up\\n\\nYou will need to create an SQL database, which is very easy. Enter MySQL with:\\n\\n    mysql -u root -p\\n    mysql\\u003E create database redditbot \\n\\nIn our example, we want to store submission IDs we have already processed in case our bot crashes and we lose our \\\"already_done\\\" list. In peewee, we need to connect to our database and create a \\\"model\\\" which represents our submission.id (or whatever we want to store):\\n\\n    from peewee import *\\n    db = MySQLDatabase('redditbot', host='localhost', user='root', passwd='SQLPASS')\\n    class Submissions(Model):\\n        subid = TextField()\\n\\n        class Meta:\\n            database = db\\n\\n    db.connect()\\n    Submissions.create_table(True) #the True flag won't alarm if table exists\\n\\nCool, so now we are connected to our database, we have established what a \\\"submission\\\" model is, and we created a table of submissions. Now, we can add submissions to our database, search for existing submissions, or even flush the database of all entries. I'll show you those functions here:\\n\\n    def is_added(submission_id):   #is ID already in database?\\n        try:\\n            submissions = Submissions.get(Submissions.subid == submission_id)\\n                return True\\t\\n        except:\\n            return False\\n\\n    def add_entry(submission_id): #add new entry\\n        if not is_added(submission_id):\\n            print \\\"Adding %s\\\" % submission_id\\n            Submissions(subid= submission_id).save()\\n\\n    def flush_db():    #remove all entries\\n        subs = Submissions.select()\\n        for sub in subs:\\n            sub.delete_instance()\\n\\nRather than typing out ANY queries when we want to add things, I have wrapped all of the peewee code in custom python methods. Now, we can simply use my function to add a submission to our database:\\n\\n    submissions = reddit.get_new(limit =10)\\n    for submission in submissions:\\n        add_entry(submission.id)\\n\\nIf the entry is already in the table, it won't be added again due to the is_added() method. This means at the \\\"reddit bot\\\" layer, we aren't doing any checks, which makes for much cleaner code. \\n\\n###Integrating our 2-factor check\\n\\nNow we don't want to rely on the DB when we can use already_done *most* of the time, so let's put both together:\\n\\n     already_done = []\\n     submissions = reddit.get_new(limit =10)\\n     for submission in submissions:\\n        if submission.id not in already_done and not is_added(submission.id):\\n            #Do whatever to your submission on this line\\n            already_done.append(submission.id)\\n            add_entry(submission.id)\\n\\n\\nBoom. So we will check already_done for our ID and if it isn't there (because it's new OR we crashed), it will check the database. If it isn't in the database, it will add it. This way, we don't have to do any reddit calls to check if we already did this work. This makes for a MASSIVE performance increase. If you want, you can create one function to handle already_done and your database, like so:\\n\\n    def add_done(submission.id):\\n        already_done.append(submission.id)\\n        add_entry(submission.id)        \\n\\nAnd a method to check if something is already done:\\n\\n    def is_done(submission.id):\\n        if not submission.id in already_done and not is_added(submission.id):\\n            return True\\n\\nSo our final code could be:\\n\\n    already_done = []\\n    submissions = reddit.get_new(limit =10)\\n    for submission in submissions:\\n        if not is_done(submission.id):\\n            #Do whatever to your submission on this line\\n            add_done(submission.id)\\n\\n\\n---\\n\\nI hope that all makes sense! Let me know if you have any questions. I'm not super experienced with databases, but this simple implementation works for most of my purposes.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"23ucco\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Phteven_j\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1398339787.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/23ucco/guide_using_databases_with_your_reddit_bot_in/\", \"locked\": false, \"name\": \"t3_23ucco\", \"created\": 1398353698.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/23ucco/guide_using_databases_with_your_reddit_bot_in/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Guide] Using databases with your reddit bot in python\", \"created_utc\": 1398324898.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"bitbucket.org\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"23n0h9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Doctor_McKay\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/23n0h9/rawjs_a_wip_api_wrapper_for_nodejs/\", \"locked\": false, \"name\": \"t3_23n0h9\", \"created\": 1398156618.0, \"url\": \"https://bitbucket.org/Doctor_McKay/raw.js\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"raw.js - a WIP API wrapper for Node.js\", \"created_utc\": 1398127818.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo after implementing OAuth I have a few issues/missing features. I need a response on this one.\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E/message/unread doesn\\u0026#39;t work, but message/inbox and message/sent works just fine. I get a 403 error.\\u003C/li\\u003E\\n\\u003Cli\\u003E/api/store_links doesn\\u0026#39;t work. This is another HUGE dealbreaker for me and my gold users\\u003C/li\\u003E\\n\\u003Cli\\u003E/api/friend and api/unfriend - I know we talked about this. But I need this for adding/removing friends, moderators, contributors, banned users, etc..\\u003C/li\\u003E\\n\\u003Cli\\u003ENot so huge, but another thing I noticed. You have to make an unauthenticated request to \\u003Ca href=\\\"/r/srname/stylesheet\\\"\\u003E/r/srname/stylesheet\\u003C/a\\u003E or the request is denied.\\u003C/li\\u003E\\n\\u003Cli\\u003EThis is pretty important - a mobile oauth login page. Can we please get that? It sucks trying to login when it is a desktop page.\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003ECan I get at least a response on api/store_links and message/unread? I know we talked about point 3 above, I hope that is getting developed but I know it is on the table now.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So after implementing OAuth I have a few issues/missing features. I need a response on this one.\\n\\n1. /message/unread doesn't work, but message/inbox and message/sent works just fine. I get a 403 error.\\n2. /api/store_links doesn't work. This is another HUGE dealbreaker for me and my gold users\\n3. /api/friend and api/unfriend - I know we talked about this. But I need this for adding/removing friends, moderators, contributors, banned users, etc..\\n4. Not so huge, but another thing I noticed. You have to make an unauthenticated request to /r/srname/stylesheet or the request is denied.\\n5. This is pretty important - a mobile oauth login page. Can we please get that? It sucks trying to login when it is a desktop page.\\n\\nCan I get at least a response on api/store_links and message/unread? I know we talked about point 3 above, I hope that is getting developed but I know it is on the table now.\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"22mf8s\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"calebkeith\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1397072827.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/22mf8s/oauth_api_missing_necessary_featuresissues_with/\", \"locked\": false, \"name\": \"t3_22mf8s\", \"created\": 1397093665.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/22mf8s/oauth_api_missing_necessary_featuresissues_with/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OAuth API: Missing necessary features/issues with API call\", \"created_utc\": 1397064865.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been exploring reddit\\u0026#39;s hotness algorithm lately and noticed that the frontpage doesn\\u0026#39;t necessarily rank default subreddit submissions by hotness. e.g., the hotness rankings of the frontpage posts right now are:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E5812.1369619\\n5812.169139\\n5812.2089064\\n5812.0283168\\n5811.9627681\\n5811.9825994\\n5811.9551621\\n5811.9063022\\n5811.8554327\\n5811.8588905\\n5811.8499365\\n5811.6755575\\n5811.6340618\\n5811.6168447\\n5811.6003195\\n5811.5935791\\n5811.2944976\\n5811.2182234\\n5811.2321869\\n5811.175046\\n5811.1757153\\n5811.0423248\\n5810.9233763\\n5811.8441782\\n5811.8022487\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Ewhich aren\\u0026#39;t entirely ordered by hotness. Is it just because the upvote and downvote data I\\u0026#39;m getting from the API is fuzzed or out of date, or is there another variable on top of hotness that determines what makes the frontpage?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been exploring reddit's hotness algorithm lately and noticed that the frontpage doesn't necessarily rank default subreddit submissions by hotness. e.g., the hotness rankings of the frontpage posts right now are:\\n\\n    5812.1369619\\n    5812.169139\\n    5812.2089064\\n    5812.0283168\\n    5811.9627681\\n    5811.9825994\\n    5811.9551621\\n    5811.9063022\\n    5811.8554327\\n    5811.8588905\\n    5811.8499365\\n    5811.6755575\\n    5811.6340618\\n    5811.6168447\\n    5811.6003195\\n    5811.5935791\\n    5811.2944976\\n    5811.2182234\\n    5811.2321869\\n    5811.175046\\n    5811.1757153\\n    5811.0423248\\n    5810.9233763\\n    5811.8441782\\n    5811.8022487\\n\\nwhich aren't entirely ordered by hotness. Is it just because the upvote and downvote data I'm getting from the API is fuzzed or out of date, or is there another variable on top of hotness that determines what makes the frontpage?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"210ocy\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"rhiever\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/210ocy/how_is_the_frontpage_ordering_determined/\", \"locked\": false, \"name\": \"t3_210ocy\", \"created\": 1395457687.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/210ocy/how_is_the_frontpage_ordering_determined/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How is the frontpage ordering determined?\", \"created_utc\": 1395428887.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThere\\u0026#39;s gotta be an API call for this, but I\\u0026#39;ll be damned if I can find it.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"There's gotta be an API call for this, but I'll be damned if I can find it.\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"20ri3c\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"JaedenStormes\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/20ri3c/checked_the_docs_and_cant_find_it_how_do_you_see/\", \"locked\": false, \"name\": \"t3_20ri3c\", \"created\": 1395214619.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/20ri3c/checked_the_docs_and_cant_find_it_how_do_you_see/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Checked the docs and can't find it -- how do you see if a user is banned from a subreddit?\", \"created_utc\": 1395185819.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhen was this API call added, and how does it work behind the scenes? Example:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003Eimport praw\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Er = praw.Reddit(user_agent=\\u0026quot;bot by /u/{0}\\u0026quot;.format(\\u0026quot;rhiever\\u0026quot;))\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Er.get_subreddit_recommendations([\\u0026quot;redditdev\\u0026quot;])\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E[Subreddit(display_name=\\u0026#39;badkarma\\u0026#39;), Subreddit(display_name=\\u0026#39;ModerationLog\\u0026#39;), Subreddit(display_name=\\u0026#39;help\\u0026#39;), Subreddit(display_name=\\u0026#39;ideasfortheadmins\\u0026#39;), Subreddit(display_name=\\u0026#39;goldbenefits\\u0026#39;), Subreddit(display_name=\\u0026#39;modhelp\\u0026#39;), Subreddit(display_name=\\u0026#39;redditrequest\\u0026#39;), Subreddit(display_name=\\u0026#39;webgl\\u0026#39;)]\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EFrom the subreddits I frequent, the recommendations seem pretty nice. Is reddit finally implementing a recommendation system?! :-D\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"When was this API call added, and how does it work behind the scenes? Example:\\n\\n\\u003Eimport praw\\n\\n\\u003Er = praw.Reddit(user_agent=\\\"bot by /u/{0}\\\".format(\\\"rhiever\\\"))\\n\\n\\u003Er.get_subreddit_recommendations([\\\"redditdev\\\"])\\n\\n\\u003E\\u003E[Subreddit(display_name='badkarma'), Subreddit(display_name='ModerationLog'), Subreddit(display_name='help'), Subreddit(display_name='ideasfortheadmins'), Subreddit(display_name='goldbenefits'), Subreddit(display_name='modhelp'), Subreddit(display_name='redditrequest'), Subreddit(display_name='webgl')]\\n\\nFrom the subreddits I frequent, the recommendations seem pretty nice. Is reddit finally implementing a recommendation system?! :-D\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1tay79\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"rhiever\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1387514764.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1tay79/noticed_new_api_call_get_subreddit_recommendations/\", \"locked\": false, \"name\": \"t3_1tay79\", \"created\": 1387543292.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1tay79/noticed_new_api_call_get_subreddit_recommendations/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Noticed new API call: get_subreddit_recommendations()\", \"created_utc\": 1387514492.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have a repository to store changes to the source code so I\\u0026#39;ve got the application side covered, but how do I back up the databases? Is it possible to do backups with a single-server site running? Can the postgres and cassandra databases be \\u0026quot;out of sync\\u0026quot; with each other? What do I actually need to back up?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf anyone can answer those questions or give me some general advice on how to do backups, I\\u0026#39;d really appreciate it!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have a repository to store changes to the source code so I've got the application side covered, but how do I back up the databases? Is it possible to do backups with a single-server site running? Can the postgres and cassandra databases be \\\"out of sync\\\" with each other? What do I actually need to back up?\\n\\nIf anyone can answer those questions or give me some general advice on how to do backups, I'd really appreciate it!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1s14x9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Corgita\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1s14x9/how_do_i_make_backups/\", \"locked\": false, \"name\": \"t3_1s14x9\", \"created\": 1386141147.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1s14x9/how_do_i_make_backups/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How do I make backups?\", \"created_utc\": 1386112347.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ETake RedReader for example, an Android app, it provides much higher resolution thumbnails than Reddit\\u0026#39;s default max of 70x70. How are they getting the higher resolution images?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EExample from RedReader: \\u003Ca href=\\\"http://i.imgur.com/BQAtUXs.png\\\"\\u003Ehttp://i.imgur.com/BQAtUXs.png\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E(Clearly much higher than 70x70.)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAre they crawling the page and finding the image? If so, how do the subreddits and thumbnails load so fast if they\\u0026#39;re grabbing all the full resolution images and shrinking them down?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Take RedReader for example, an Android app, it provides much higher resolution thumbnails than Reddit's default max of 70x70. How are they getting the higher resolution images?\\n\\nExample from RedReader: http://i.imgur.com/BQAtUXs.png\\n\\n(Clearly much higher than 70x70.)\\n\\nAre they crawling the page and finding the image? If so, how do the subreddits and thumbnails load so fast if they're grabbing all the full resolution images and shrinking them down?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1rxzcd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"curtainlikeobstacles\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1rxzcd/how_are_some_clients_able_to_get_high_quality/\", \"locked\": false, \"name\": \"t3_1rxzcd\", \"created\": 1386051176.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1rxzcd/how_are_some_clients_able_to_get_high_quality/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How are some clients able to get high quality thumbnails (much higher than what seemingly reddit provides)?\", \"created_utc\": 1386022376.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1q94gh\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"JW989\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1q94gh/show_reddit_alienfeed_a_reddit_terminal_client/\", \"locked\": false, \"name\": \"t3_1q94gh\", \"created\": 1384045818.0, \"url\": \"https://github.com/jawerty/AlienFeed\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Show Reddit: AlienFeed - a Reddit terminal client (x-post r/programming)\", \"created_utc\": 1384017018.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have searched through reddit and I know a lot of people have tried to make this work but I can\\u0026#39;t find a version that I could use yet.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"/r/rss_feed\\\"\\u003E/r/rss_feed\\u003C/a\\u003E[1] seems to have succeeded in doing this and so I know this is possible. Anyone care to point me the right way?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI don\\u0026#39;t know enough to build one myself so I hope you guys can help out.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis is for my own subreddit and not to spam any subreddit. If it is a bot, that works too. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: I have managed to get some help with it. Thanks everyone!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have searched through reddit and I know a lot of people have tried to make this work but I can't find a version that I could use yet.\\n\\n/r/rss_feed[1] seems to have succeeded in doing this and so I know this is possible. Anyone care to point me the right way?\\n\\nI don't know enough to build one myself so I hope you guys can help out.\\n\\nThis is for my own subreddit and not to spam any subreddit. If it is a bot, that works too. \\n\\nEdit: I have managed to get some help with it. Thanks everyone!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1pevsq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"lastresort09\", \"media\": null, \"score\": 9, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1382999384.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1pevsq/request_is_it_possible_to_make_a_bot_for_reddit/\", \"locked\": false, \"name\": \"t3_1pevsq\", \"created\": 1383022051.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1pevsq/request_is_it_possible_to_make_a_bot_for_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Request] Is it possible to make a bot for Reddit that posts from an RSS feed?\", \"created_utc\": 1382993251.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 9}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis is a bot that I built today in order to help moderate \\u003Ca href=\\\"/r/AnimeSuggest\\\"\\u003E/r/AnimeSuggest\\u003C/a\\u003E. It checks new posts and pm the author if they do not tag their post with flair within 3 minutes. Here is my code:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport praw\\nimport time\\nimport re\\nimport sys\\ndef main():\\n    #log in\\n    username=open(\\u0026quot;Username.txt\\u0026quot;,\\u0026quot;r\\u0026quot;).read().rstrip()\\n    password=open(\\u0026quot;Password.txt\\u0026quot;,\\u0026quot;r\\u0026quot;).read().rstrip()\\n    user_agent=(\\u0026quot;Auto flair moderator for /r/AnimeSuggest\\u0026quot;)\\n    r=praw.Reddit(user_agent=user_agent)\\n    r.login(username=username,password=password)\\n    JBHUTT09=\\u0026quot;JBHUTT09\\u0026quot;\\n    #initialize array to hold checked posts\\n    already_done=[]\\n    #specify subreddit\\n    subreddit=r.get_subreddit(\\u0026#39;animesuggesttesting\\u0026#39;)\\n    #start endless loop\\n    while True:\\n            for submission in subreddit.get_new_by_date(limit=5):\\n            if submission.id not in already_done:\\n                time.sleep(180)\\n                if (submission.link_flair_text is None):\\n                    author=submission.author\\n                    subLine=\\u0026#39;You have not tagged your post.\\u0026#39;\\n                    msg=\\u0026quot;[Your recent post](%s) in /r/AnimeSuggest does not have any flair. Please add flair to your post. \\\\n\\\\n If you are unsure of how to add flair, refer to [this post](http://redd.it/1ml8km).\\\\n\\\\n*This is a brand new bot I wrote to help mod. It\\u0026#39;s also the first bot I\\u0026#39;ve written. If shit is broken and everything is going to hell, please pm me: /u/JBHUTT09.\\u0026quot; % submission.short_link\\n                    r.send_message(author,subLine,msg)\\n                    JBSubLine=\\u0026quot;Message sent to /u/%s\\u0026quot;%author\\n                    JBmsg=\\u0026quot;Message sent concerning [this post.](%s)\\u0026quot;%submission.short_link\\n                    r.send_message(JBHUTT09,JBSubLine,JBmsg)\\n                    print \\u0026quot;Sent message to: %s\\u0026quot; % author\\n                already_done.append(submission.id)\\n                print \\u0026quot;Posts checked: %d\\u0026quot; % len(already_done)\\nmain()\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENow, this code was running absolutely fine this afternoon. No problems at all. I had to shut down my computer so I had to stop the bot. That\\u0026#39;s when the trouble started. Ever since I reran the bot, it\\u0026#39;s been crashing when sending PMs. But it only crashes sometimes and I think it\\u0026#39;s based on the time delay. I had set it to a 20 second delay for testing purposes when trying to figure out the problem, because I wasn\\u0026#39;t going to wait 3 minutes. It suddenly worked. So I thought it was a weird one time thing. But when I put the delay back to 3 minutes, it started crashing again. Does anyone have any idea why this happens?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEdit:\\u003C/strong\\u003E Crash message:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003ETraceback (most recent call last):\\n  File \\u0026quot;C:\\\\Python27\\\\botTest.py\\u0026quot;, line 34, in \\u0026lt;module\\u0026gt; main()\\n  File \\u0026quot;C:\\\\Python27\\\\botTest.py\\u0026quot;, line 27, in main r.send_message(author,subLine,msg)\\n  File \\u0026quot;C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\decorators.py\\u0026quot;, line 303, in wrapped return function(cls, *args, **kwargs)\\n  File \\u0026quot;C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\decorators.py\\u0026quot;, line 205, in wrapped return function(obj, *args, **kwargs)\\n  File \\u0026quot;C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\__init__.py\\u0026quot;, line 1909, in send_message retry_on_error=False)\\n  File \\u0026quot;C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\decorators.py\\u0026quot;, line 141, in wrapped return_value = function(reddit_session, *args, **kwargs)\\n  File \\u0026quot;C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\__init__.py\\u0026quot;, line 479, in request_json retry_on_error=retry_on_error)\\n  File \\u0026quot;C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\__init__.py\\u0026quot;, line 348, in _request response = handle_redirect()\\n  File \\u0026quot;C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\__init__.py\\u0026quot;, line 321, in handle_redirect timeout=timeout, **kwargs)\\n  File \\u0026quot;C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\handlers.py\\u0026quot;, line 135, in wrapped result = function(cls, **kwargs)\\n  File \\u0026quot;C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\handlers.py\\u0026quot;, line 54, in wrapped return function(cls, **kwargs)\\n  File \\u0026quot;C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\handlers.py\\u0026quot;, line 90, in request allow_redirects=False)\\n  File \\u0026quot;C:\\\\Python27\\\\lib\\\\site-packages\\\\requests-2.0.0-py2.7.egg\\\\requests\\\\sessions.py\\u0026quot;, line 460, in send r = adapter.send(request, **kwargs)\\n  File \\u0026quot;C:\\\\Python27\\\\lib\\\\site-packages\\\\requests-2.0.0-py2.7.egg\\\\requests\\\\adapters.py\\u0026quot;, line 354, in send raise ConnectionError(e)\\nConnectionError: HTTPConnectionPool(host=\\u0026#39;www.reddit.com\\u0026#39;, port=80): Max retries exceeded with url: /api/compose/.json (Caused by \\u0026lt;class \\u0026#39;socket.error\\u0026#39;\\u0026gt;: [Errno 10054] An existing connection was forcibly closed by the remote host)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This is a bot that I built today in order to help moderate /r/AnimeSuggest. It checks new posts and pm the author if they do not tag their post with flair within 3 minutes. Here is my code:\\n\\n    import praw\\n    import time\\n    import re\\n    import sys\\n    def main():\\n\\t    #log in\\n\\t    username=open(\\\"Username.txt\\\",\\\"r\\\").read().rstrip()\\n\\t    password=open(\\\"Password.txt\\\",\\\"r\\\").read().rstrip()\\n\\t    user_agent=(\\\"Auto flair moderator for /r/AnimeSuggest\\\")\\n\\t    r=praw.Reddit(user_agent=user_agent)\\n\\t    r.login(username=username,password=password)\\n\\t    JBHUTT09=\\\"JBHUTT09\\\"\\n\\t    #initialize array to hold checked posts\\n\\t    already_done=[]\\n\\t    #specify subreddit\\n\\t    subreddit=r.get_subreddit('animesuggesttesting')\\n\\t    #start endless loop\\n\\t    while True:\\n\\t        \\tfor submission in subreddit.get_new_by_date(limit=5):\\n\\t\\t\\t    if submission.id not in already_done:\\n\\t\\t\\t\\t    time.sleep(180)\\n\\t\\t\\t\\t    if (submission.link_flair_text is None):\\n\\t\\t\\t\\t\\t    author=submission.author\\n\\t\\t\\t\\t\\t    subLine='You have not tagged your post.'\\n\\t\\t\\t\\t\\t    msg=\\\"[Your recent post](%s) in /r/AnimeSuggest does not have any flair. Please add flair to your post. \\\\n\\\\n If you are unsure of how to add flair, refer to [this post](http://redd.it/1ml8km).\\\\n\\\\n*This is a brand new bot I wrote to help mod. It's also the first bot I've written. If shit is broken and everything is going to hell, please pm me: /u/JBHUTT09.\\\" % submission.short_link\\n\\t\\t\\t\\t\\t    r.send_message(author,subLine,msg)\\n\\t\\t\\t\\t\\t    JBSubLine=\\\"Message sent to /u/%s\\\"%author\\n\\t\\t\\t\\t\\t    JBmsg=\\\"Message sent concerning [this post.](%s)\\\"%submission.short_link\\n\\t\\t\\t\\t\\t    r.send_message(JBHUTT09,JBSubLine,JBmsg)\\n\\t\\t\\t\\t\\t    print \\\"Sent message to: %s\\\" % author\\n\\t\\t\\t\\t    already_done.append(submission.id)\\n\\t\\t\\t\\t    print \\\"Posts checked: %d\\\" % len(already_done)\\n    main()\\n\\nNow, this code was running absolutely fine this afternoon. No problems at all. I had to shut down my computer so I had to stop the bot. That's when the trouble started. Ever since I reran the bot, it's been crashing when sending PMs. But it only crashes sometimes and I think it's based on the time delay. I had set it to a 20 second delay for testing purposes when trying to figure out the problem, because I wasn't going to wait 3 minutes. It suddenly worked. So I thought it was a weird one time thing. But when I put the delay back to 3 minutes, it started crashing again. Does anyone have any idea why this happens?\\n\\n**Edit:** Crash message:\\n\\n    Traceback (most recent call last):\\n      File \\\"C:\\\\Python27\\\\botTest.py\\\", line 34, in \\u003Cmodule\\u003E main()\\n      File \\\"C:\\\\Python27\\\\botTest.py\\\", line 27, in main r.send_message(author,subLine,msg)\\n      File \\\"C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\decorators.py\\\", line 303, in wrapped return function(cls, *args, **kwargs)\\n      File \\\"C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\decorators.py\\\", line 205, in wrapped return function(obj, *args, **kwargs)\\n      File \\\"C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\__init__.py\\\", line 1909, in send_message retry_on_error=False)\\n      File \\\"C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\decorators.py\\\", line 141, in wrapped return_value = function(reddit_session, *args, **kwargs)\\n      File \\\"C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\__init__.py\\\", line 479, in request_json retry_on_error=retry_on_error)\\n      File \\\"C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\__init__.py\\\", line 348, in _request response = handle_redirect()\\n      File \\\"C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\__init__.py\\\", line 321, in handle_redirect timeout=timeout, **kwargs)\\n      File \\\"C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\handlers.py\\\", line 135, in wrapped result = function(cls, **kwargs)\\n      File \\\"C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\handlers.py\\\", line 54, in wrapped return function(cls, **kwargs)\\n      File \\\"C:\\\\Python27\\\\lib\\\\site-packages\\\\praw-2.1.9-py2.7.egg\\\\praw\\\\handlers.py\\\", line 90, in request allow_redirects=False)\\n      File \\\"C:\\\\Python27\\\\lib\\\\site-packages\\\\requests-2.0.0-py2.7.egg\\\\requests\\\\sessions.py\\\", line 460, in send r = adapter.send(request, **kwargs)\\n      File \\\"C:\\\\Python27\\\\lib\\\\site-packages\\\\requests-2.0.0-py2.7.egg\\\\requests\\\\adapters.py\\\", line 354, in send raise ConnectionError(e)\\n    ConnectionError: HTTPConnectionPool(host='www.reddit.com', port=80): Max retries exceeded with url: /api/compose/.json (Caused by \\u003Cclass 'socket.error'\\u003E: [Errno 10054] An existing connection was forcibly closed by the remote host)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1obxel\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"JBHUTT09\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 17, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1381670665.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/\", \"locked\": false, \"name\": \"t3_1obxel\", \"created\": 1381660811.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Having issues with a time delay in a bot\", \"created_utc\": 1381632011.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo basically I thought of making a bot that could go through a Reddit post/ or any other website and scrap up all the links?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI went to this website:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/dev/api\\\"\\u003Ehttp://www.reddit.com/dev/api\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eand this one as well:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/API-Wrappers\\\"\\u003Ehttps://github.com/reddit/reddit/wiki/API-Wrappers\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ebut I am not sure exactly what to do with all of this.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDoes anyone have any suggestions?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So basically I thought of making a bot that could go through a Reddit post/ or any other website and scrap up all the links?\\n\\nI went to this website:\\n\\nhttp://www.reddit.com/dev/api\\n\\nand this one as well:\\n\\nhttps://github.com/reddit/reddit/wiki/API-Wrappers\\n\\n\\nbut I am not sure exactly what to do with all of this.\\n\\nDoes anyone have any suggestions?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1nh7z8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Isatis_tinctoria\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 18, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1nh7z8/i_have_an_idea_for_a_bot_on_reddit_but_i_dont/\", \"locked\": false, \"name\": \"t3_1nh7z8\", \"created\": 1380617255.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1nh7z8/i_have_an_idea_for_a_bot_on_reddit_but_i_dont/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I have an idea for a bot on reddit, but I don't have a background in programming. Any tips?\", \"created_utc\": 1380588455.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI need to scrape certain bot\\u0026#39;s comments to create a statistics page (I don\\u0026#39;t control the bot). I need to be able to parse every single comment of the bot. I know there\\u0026#39;s a \\u0026#39;limit\\u0026#39; variable that can\\u0026#39;t go higher than 1,000. Through Reddit UI, I can continuously press \\u0026quot;Next\\u0026quot; to get all user\\u0026#39;s comments. Is there an equivalent in PRAW? Thanks.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I need to scrape certain bot's comments to create a statistics page (I don't control the bot). I need to be able to parse every single comment of the bot. I know there's a 'limit' variable that can't go higher than 1,000. Through Reddit UI, I can continuously press \\\"Next\\\" to get all user's comments. Is there an equivalent in PRAW? Thanks.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1n7164\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"im14\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1n7164/can_i_get_all_comments_of_a_user_by_calling/\", \"locked\": false, \"name\": \"t3_1n7164\", \"created\": 1380250981.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1n7164/can_i_get_all_comments_of_a_user_by_calling/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can I get all comments of a user by calling something like user.get_comments().next() repeatedly?\", \"created_utc\": 1380222181.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/JSON\\\"\\u003EJSON API documentation\\u003C/a\\u003E uses the following definition for a \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/JSON#listing\\\"\\u003Elisting\\u003C/a\\u003E:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Etype            name\\n------         ------\\nString          before\\nString          after\\nString          modhash\\nList\\u0026lt;thing\\u0026gt;     data (basically, an array of Things, which we\\u0026#39;ll get to later)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIn actuality, the type structure is like so (taken from \\u003Ca href=\\\"http://www.reddit.com/r/mvc3.json):\\\"\\u003Ehttp://www.reddit.com/r/mvc3.json):\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E{\\n    \\u0026quot;kind\\u0026quot;: \\u0026quot;Listing\\u0026quot;,\\n    \\u0026quot;data\\u0026quot;: \\n    {\\n        \\u0026quot;modhash\\u0026quot;: \\u0026quot;bne23ar90u91672c8728ae7009bb1b413088cfab5365b8a52b\\u0026quot;,\\n        \\u0026quot;children\\u0026quot;: [],\\n        \\u0026quot;after\\u0026quot;: \\u0026quot;t3_1lm5ze\\u0026quot;,\\n        \\u0026quot;before\\u0026quot;: null\\n    }\\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENote that the \\u003Ccode\\u003Ebefore\\u003C/code\\u003E, \\u003Ccode\\u003Eafter\\u003C/code\\u003E and \\u003Ccode\\u003Emodhash\\u003C/code\\u003E are actually in the \\u003Ccode\\u003Edata\\u003C/code\\u003E section of the listing.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMoving to the \\u003Ccode\\u003Echildren\\u003C/code\\u003E (which really should be \\u003Ccode\\u003Edata\\u003C/code\\u003E), each one should be a \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/JSON#thing-reddit-base-class\\\"\\u003E\\u003Ccode\\u003EThing\\u003C/code\\u003E\\u003C/a\\u003E, defined as:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Etype           name\\n------         ------\\nString         id\\nString         name\\nString         kind\\nObject         data\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EHowever, looking at one of the \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/JSON#link-implements-votable--created\\\"\\u003Elinks\\u003C/a\\u003E in the above representation, it is:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E{\\n    \\u0026quot;kind\\u0026quot;: \\u0026quot;t3\\u0026quot;,\\n    \\u0026quot;data\\u0026quot;: {\\n        \\u0026quot;domain\\u0026quot;: \\u0026quot;youtube.com\\u0026quot;,\\n        \\u0026quot;banned_by\\u0026quot;: null,\\n        \\u0026quot;media_embed\\u0026quot;: {},\\n        \\u0026quot;subreddit\\u0026quot;: \\u0026quot;MvC3\\u0026quot;,\\n        \\u0026quot;selftext_html\\u0026quot;: null,\\n        \\u0026quot;selftext\\u0026quot;: \\u0026quot;\\u0026quot;,\\n        \\u0026quot;likes\\u0026quot;: null,\\n        \\u0026quot;saved\\u0026quot;: false,\\n        \\u0026quot;id\\u0026quot;: \\u0026quot;1lqcg9\\u0026quot;,\\n        \\u0026quot;clicked\\u0026quot;: false,\\n        \\u0026quot;stickied\\u0026quot;: false,\\n        \\u0026quot;author\\u0026quot;: \\u0026quot;xtankid\\u0026quot;,\\n        \\u0026quot;media\\u0026quot;: {},\\n        \\u0026quot;score\\u0026quot;: 42,\\n        \\u0026quot;approved_by\\u0026quot;: null,\\n        \\u0026quot;over_18\\u0026quot;: false,\\n        \\u0026quot;hidden\\u0026quot;: false,\\n        \\u0026quot;thumbnail\\u0026quot;: \\u0026quot;http://d.thumbs.redditmedia.com/MmmBLdIPh0-6gGac.jpg\\u0026quot;,\\n        \\u0026quot;subreddit_id\\u0026quot;: \\u0026quot;t5_2s01r\\u0026quot;,\\n        \\u0026quot;edited\\u0026quot;: false,\\n        \\u0026quot;link_flair_css_class\\u0026quot;: null,\\n        \\u0026quot;author_flair_css_class\\u0026quot;: \\u0026quot;frankWest\\u0026quot;,\\n        \\u0026quot;downs\\u0026quot;: 6,\\n        \\u0026quot;is_self\\u0026quot;: false,\\n        \\u0026quot;permalink\\u0026quot;: \\u0026quot;/r/MvC3/comments/1lqcg9/umvc3_andrew_h_vs_vhs_cless_the_runback_18_summer/\\u0026quot;,\\n        \\u0026quot;name\\u0026quot;: \\u0026quot;t3_1lqcg9\\u0026quot;,\\n        \\u0026quot;created\\u0026quot;: 1378349602,\\n        \\u0026quot;url\\u0026quot;: \\u0026quot;http://www.youtube.com/watch?v=WO-_5I3w_70\\u0026quot;,\\n        \\u0026quot;author_flair_text\\u0026quot;: \\u0026quot;XBL - SRxDayz\\u0026quot;,\\n        \\u0026quot;title\\u0026quot;: \\u0026quot;UMvC3 Andrew H vs VHS Cless - The Runback 1.8 Summer Edition\\u0026quot;,\\n        \\u0026quot;created_utc\\u0026quot;: 1378320802,\\n        \\u0026quot;link_flair_text\\u0026quot;: null,\\n        \\u0026quot;ups\\u0026quot;: 48,\\n        \\u0026quot;num_comments\\u0026quot;: 26,\\n        \\u0026quot;num_reports\\u0026quot;: null,\\n        \\u0026quot;distinguished\\u0026quot;: null\\n    }\\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENote that \\u003Ccode\\u003Ekind\\u003C/code\\u003E and \\u003Ccode\\u003Edata\\u003C/code\\u003E are on the top level, while \\u003Ccode\\u003Eid\\u003C/code\\u003E and \\u003Ccode\\u003Ename\\u003C/code\\u003E are embedded in the data.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo, I\\u0026#39;m assuming the documentation is wrong, given that \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/JSON/06b64509f552d6612b83f46813d05a015f625305\\\"\\u003Eit was last updated 16 days ago\\u003C/a\\u003E (as of this writing).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOr, is there something I\\u0026#39;m missing?  If the \\u003Ccode\\u003EThing\\u003C/code\\u003E is the Reddit base class (with the exception of \\u003Ccode\\u003EListing\\u003C/code\\u003E) then I\\u0026#39;d expect all of those attributes to be on the top level.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERather, it seems that there is a container for \\u003Cem\\u003Eeverything\\u003C/em\\u003E which consists of \\u003Ccode\\u003Ekind\\u003C/code\\u003E and \\u003Ccode\\u003Edata\\u003C/code\\u003E, and everything else that should be on \\u003Ccode\\u003EThing\\u003C/code\\u003E or \\u003Ccode\\u003EListing\\u003C/code\\u003E is pushed down into the \\u003Ccode\\u003Edata\\u003C/code\\u003E section.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENote, I\\u0026#39;m asking because I\\u0026#39;d rather code against contract than implementation, but it seems that the two in this case are wildly out of line.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks in advance for any insight.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The [JSON API documentation](https://github.com/reddit/reddit/wiki/JSON) uses the following definition for a [listing](https://github.com/reddit/reddit/wiki/JSON#listing):\\n\\n    type            name\\n    ------         ------\\n    String          before\\n    String          after\\n    String          modhash\\n    List\\u003Cthing\\u003E     data (basically, an array of Things, which we'll get to later)\\n\\nIn actuality, the type structure is like so (taken from http://www.reddit.com/r/mvc3.json):\\n\\n    {\\n        \\\"kind\\\": \\\"Listing\\\",\\n        \\\"data\\\": \\n        {\\n            \\\"modhash\\\": \\\"bne23ar90u91672c8728ae7009bb1b413088cfab5365b8a52b\\\",\\n            \\\"children\\\": [],\\n            \\\"after\\\": \\\"t3_1lm5ze\\\",\\n            \\\"before\\\": null\\n        }\\n    }\\n\\nNote that the `before`, `after` and `modhash` are actually in the `data` section of the listing.\\n\\nMoving to the `children` (which really should be `data`), each one should be a [`Thing`](https://github.com/reddit/reddit/wiki/JSON#thing-reddit-base-class), defined as:\\n\\n    type           name\\n    ------         ------\\n    String         id\\n    String         name\\n    String         kind\\n    Object         data\\n\\nHowever, looking at one of the [links](https://github.com/reddit/reddit/wiki/JSON#link-implements-votable--created) in the above representation, it is:\\n\\n\\t{\\n\\t\\t\\\"kind\\\": \\\"t3\\\",\\n\\t\\t\\\"data\\\": {\\n\\t\\t\\t\\\"domain\\\": \\\"youtube.com\\\",\\n\\t\\t\\t\\\"banned_by\\\": null,\\n\\t\\t\\t\\\"media_embed\\\": {},\\n\\t\\t\\t\\\"subreddit\\\": \\\"MvC3\\\",\\n\\t\\t\\t\\\"selftext_html\\\": null,\\n\\t\\t\\t\\\"selftext\\\": \\\"\\\",\\n\\t\\t\\t\\\"likes\\\": null,\\n\\t\\t\\t\\\"saved\\\": false,\\n\\t\\t\\t\\\"id\\\": \\\"1lqcg9\\\",\\n\\t\\t\\t\\\"clicked\\\": false,\\n\\t\\t\\t\\\"stickied\\\": false,\\n\\t\\t\\t\\\"author\\\": \\\"xtankid\\\",\\n\\t\\t\\t\\\"media\\\": {},\\n\\t\\t\\t\\\"score\\\": 42,\\n\\t\\t\\t\\\"approved_by\\\": null,\\n\\t\\t\\t\\\"over_18\\\": false,\\n\\t\\t\\t\\\"hidden\\\": false,\\n\\t\\t\\t\\\"thumbnail\\\": \\\"http://d.thumbs.redditmedia.com/MmmBLdIPh0-6gGac.jpg\\\",\\n\\t\\t\\t\\\"subreddit_id\\\": \\\"t5_2s01r\\\",\\n\\t\\t\\t\\\"edited\\\": false,\\n\\t\\t\\t\\\"link_flair_css_class\\\": null,\\n\\t\\t\\t\\\"author_flair_css_class\\\": \\\"frankWest\\\",\\n\\t\\t\\t\\\"downs\\\": 6,\\n\\t\\t\\t\\\"is_self\\\": false,\\n\\t\\t\\t\\\"permalink\\\": \\\"/r/MvC3/comments/1lqcg9/umvc3_andrew_h_vs_vhs_cless_the_runback_18_summer/\\\",\\n\\t\\t\\t\\\"name\\\": \\\"t3_1lqcg9\\\",\\n\\t\\t\\t\\\"created\\\": 1378349602,\\n\\t\\t\\t\\\"url\\\": \\\"http://www.youtube.com/watch?v=WO-_5I3w_70\\\",\\n\\t\\t\\t\\\"author_flair_text\\\": \\\"XBL - SRxDayz\\\",\\n\\t\\t\\t\\\"title\\\": \\\"UMvC3 Andrew H vs VHS Cless - The Runback 1.8 Summer Edition\\\",\\n\\t\\t\\t\\\"created_utc\\\": 1378320802,\\n\\t\\t\\t\\\"link_flair_text\\\": null,\\n\\t\\t\\t\\\"ups\\\": 48,\\n\\t\\t\\t\\\"num_comments\\\": 26,\\n\\t\\t\\t\\\"num_reports\\\": null,\\n\\t\\t\\t\\\"distinguished\\\": null\\n\\t\\t}\\n\\t}\\n\\nNote that `kind` and `data` are on the top level, while `id` and `name` are embedded in the data.\\n\\nSo, I'm assuming the documentation is wrong, given that [it was last updated 16 days ago](https://github.com/reddit/reddit/wiki/JSON/06b64509f552d6612b83f46813d05a015f625305) (as of this writing).\\n\\nOr, is there something I'm missing?  If the `Thing` is the Reddit base class (with the exception of `Listing`) then I'd expect all of those attributes to be on the top level.\\n\\nRather, it seems that there is a container for *everything* which consists of `kind` and `data`, and everything else that should be on `Thing` or `Listing` is pushed down into the `data` section.\\n\\nNote, I'm asking because I'd rather code against contract than implementation, but it seems that the two in this case are wildly out of line.\\n\\nThanks in advance for any insight.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1luufu\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"OneFrameLink\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1luufu/inconsistencies_in_json_api_documentation_and/\", \"locked\": false, \"name\": \"t3_1luufu\", \"created\": 1378508735.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1luufu/inconsistencies_in_json_api_documentation_and/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Inconsistencies in JSON API documentation and actual results.\", \"created_utc\": 1378479935.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI know it all comes down to Mako templates but there is fat level of indirection above them that makes my head spin every time I need to find relation between controller and templates that are involved in rendering response.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I know it all comes down to Mako templates but there is fat level of indirection above them that makes my head spin every time I need to find relation between controller and templates that are involved in rendering response.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ladep\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"sianrus\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ladep/is_there_any_docswiki_on_how_reddits_rendering/\", \"locked\": false, \"name\": \"t3_1ladep\", \"created\": 1377755125.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1ladep/is_there_any_docswiki_on_how_reddits_rendering/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there any docs/wiki on how reddit's rendering subsystem works?\", \"created_utc\": 1377726325.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHas anyone compiled the metareddit source / figured out how to get it to run? The source is up at \\u003Ca href=\\\"https://github.com/modemuser/metareddit\\\"\\u003Ehttps://github.com/modemuser/metareddit\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s written in python, which I\\u0026#39;m familiar with, but using frameworks that I haven\\u0026#39;t done much in (sqlalchemy, werkzeug). \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve gotten in far in trying to get it to run, but in the end cant get it all the way through, I get stumped at trying to actually init my DB - it seems like I cant get the correct version of alchemy that was initially used, despite having tried them all.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDoes anyone have any suggestions or info on how to run this?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ethanks\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Has anyone compiled the metareddit source / figured out how to get it to run? The source is up at https://github.com/modemuser/metareddit\\n\\nIt's written in python, which I'm familiar with, but using frameworks that I haven't done much in (sqlalchemy, werkzeug). \\n\\nI've gotten in far in trying to get it to run, but in the end cant get it all the way through, I get stumped at trying to actually init my DB - it seems like I cant get the correct version of alchemy that was initially used, despite having tried them all.\\n\\nDoes anyone have any suggestions or info on how to run this?\\n\\nthanks\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1kj9zg\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"hitech-hillbilly\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1kj9zg/compiling_and_using_metareddit_source/\", \"locked\": false, \"name\": \"t3_1kj9zg\", \"created\": 1376744353.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1kj9zg/compiling_and_using_metareddit_source/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Compiling and using metareddit source?\", \"created_utc\": 1376715553.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI wrote a simple bot for reddit which needs to run every ten minutes or so to adequately perform its job. As I didnt want to leave my computer turned on forever in order to let the script I decided to try hosting it on google app engine.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENow I\\u0026#39;m a newb to both app engine and praw, so bear with me here.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhen I uploaded my script to the app engine it threw me an error saying that it could not find/import any packages named \\u0026quot;praw.\\u0026quot; That\\u0026#39;s fine. To fix this, and all the later dependency errors that came up, I just ran the following commands in a python shell\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport dependencypackage\\nprint(dependencypackage)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI then copied all the files/folders that the shell spit out into my app\\u0026#39;s src directory. By doing this a few times I managed to trade one error for another, but now I\\u0026#39;m running into the following error:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E    Traceback (most recent call last):\\n  File \\u0026quot;/base/data/home/apps/s~reddit-spelunky-bot/1.369458891331523467/SpelunkyDailyBot.py\\u0026quot;, line 2, in \\u0026lt;module\\u0026gt;\\n    import praw\\n  File \\u0026quot;/base/data/home/apps/s~reddit-spelunky-bot/1.369458891331523467/praw/__init__.py\\u0026quot;, line 34, in \\u0026lt;module\\u0026gt;\\n    from praw import decorators, errors\\nImportError: cannot import name decorators\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m stumped by this one, because when I check the praw folder in my app\\u0026#39;s src dir there exist both a decorators.py and a decorators.pyc. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo the question here is twofold-\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E1) What am I doing wrong here? Is the decorators module not supported by App Engine or something? Can I even use praw in a GAE script? I\\u0026#39;d really love to get this bot on the cloud, so this question is the main one\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E2) Is there any easier way to do this? I feel like the process I\\u0026#39;ve followed to get all the dependencies in the src is one of the least efficient possible. Is there some sort of tool I can use, some sort of magic button I can use in my IDE (pycharm or pydev for eclipse) which will take care of uploading all these dependencies for me?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnswers and explanations are welcome (duh)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELooking forward to your answers!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: I made some changes to how the packages were imported, and now I\\u0026#39;m getting the following error\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E    Traceback (most recent call last):\\n  File \\u0026quot;/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/SpelunkyDailyBot.py\\u0026quot;, line 2, in \\u0026lt;module\\u0026gt;\\n    import praw\\n  File \\u0026quot;/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/praw/__init__.py\\u0026quot;, line 43, in \\u0026lt;module\\u0026gt;\\n    from update_checker import update_check\\n  File \\u0026quot;/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/update_checker.py\\u0026quot;, line 87, in \\u0026lt;module\\u0026gt;\\n    class UpdateChecker(object):\\n  File \\u0026quot;/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/update_checker.py\\u0026quot;, line 93, in UpdateChecker\\n    @cache_results\\n  File \\u0026quot;/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/update_checker.py\\u0026quot;, line 43, in cache_results\\n    filename = os.path.join(gettempdir(), \\u0026#39;update_checker_cache.pkl\\u0026#39;)\\n  File \\u0026quot;/base/data/home/runtimes/python27/python27_dist/lib/python2.7/tempfile.py\\u0026quot;, line 45, in PlaceHolder\\n    raise NotImplementedError(\\u0026quot;Only tempfile.TemporaryFile is available for use\\u0026quot;)\\nNotImplementedError: Only tempfile.TemporaryFile is available for use\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I wrote a simple bot for reddit which needs to run every ten minutes or so to adequately perform its job. As I didnt want to leave my computer turned on forever in order to let the script I decided to try hosting it on google app engine.\\n\\nNow I'm a newb to both app engine and praw, so bear with me here.\\n\\nWhen I uploaded my script to the app engine it threw me an error saying that it could not find/import any packages named \\\"praw.\\\" That's fine. To fix this, and all the later dependency errors that came up, I just ran the following commands in a python shell\\n\\n    import dependencypackage\\n    print(dependencypackage)\\n    \\nI then copied all the files/folders that the shell spit out into my app's src directory. By doing this a few times I managed to trade one error for another, but now I'm running into the following error:\\n\\n        Traceback (most recent call last):\\n      File \\\"/base/data/home/apps/s~reddit-spelunky-bot/1.369458891331523467/SpelunkyDailyBot.py\\\", line 2, in \\u003Cmodule\\u003E\\n        import praw\\n      File \\\"/base/data/home/apps/s~reddit-spelunky-bot/1.369458891331523467/praw/__init__.py\\\", line 34, in \\u003Cmodule\\u003E\\n        from praw import decorators, errors\\n    ImportError: cannot import name decorators\\n\\nI'm stumped by this one, because when I check the praw folder in my app's src dir there exist both a decorators.py and a decorators.pyc. \\n\\nSo the question here is twofold-\\n\\n1) What am I doing wrong here? Is the decorators module not supported by App Engine or something? Can I even use praw in a GAE script? I'd really love to get this bot on the cloud, so this question is the main one\\n\\n2) Is there any easier way to do this? I feel like the process I've followed to get all the dependencies in the src is one of the least efficient possible. Is there some sort of tool I can use, some sort of magic button I can use in my IDE (pycharm or pydev for eclipse) which will take care of uploading all these dependencies for me?\\n\\nAnswers and explanations are welcome (duh)\\n\\nLooking forward to your answers!\\n\\nEDIT: I made some changes to how the packages were imported, and now I'm getting the following error\\n\\n        Traceback (most recent call last):\\n      File \\\"/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/SpelunkyDailyBot.py\\\", line 2, in \\u003Cmodule\\u003E\\n        import praw\\n      File \\\"/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/praw/__init__.py\\\", line 43, in \\u003Cmodule\\u003E\\n        from update_checker import update_check\\n      File \\\"/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/update_checker.py\\\", line 87, in \\u003Cmodule\\u003E\\n        class UpdateChecker(object):\\n      File \\\"/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/update_checker.py\\\", line 93, in UpdateChecker\\n        @cache_results\\n      File \\\"/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/update_checker.py\\\", line 43, in cache_results\\n        filename = os.path.join(gettempdir(), 'update_checker_cache.pkl')\\n      File \\\"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/tempfile.py\\\", line 45, in PlaceHolder\\n        raise NotImplementedError(\\\"Only tempfile.TemporaryFile is available for use\\\")\\n    NotImplementedError: Only tempfile.TemporaryFile is available for use\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1k8ocm\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"pipsqueaker117\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1376352070.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/\", \"locked\": false, \"name\": \"t3_1k8ocm\", \"created\": 1376378343.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there any way to easily host a reddit bot using praw on Google App Engine?\", \"created_utc\": 1376349543.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EDidn\\u0026#39;t find anything about it in the reddit API or PRAW documentation.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs this something the admins object to?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Didn't find anything about it in the reddit API or PRAW documentation.\\n\\nIs this something the admins object to?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1jmzm7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"IAmAnAnonymousCoward\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/\", \"locked\": false, \"name\": \"t3_1jmzm7\", \"created\": 1375584369.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[PRAW] Can a bot give gold?\", \"created_utc\": 1375555569.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m trying to log in via POST to \\u003Ccode\\u003Ehttps://ssl.reddit.com/api/login/\\u003C/code\\u003E and I\\u0026#39;m getting an SSL handshake error on ruby:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003EOpenSSL::SSL::SSLError: SSL_connect returned=1 errno=0 state=SSLv3 read server hello A: sslv3 alert handshake failure\\u003C/code\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis has worked 2 days ago, but not anymore. Has your certificate changed?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm trying to log in via POST to `https://ssl.reddit.com/api/login/` and I'm getting an SSL handshake error on ruby:\\n\\n`OpenSSL::SSL::SSLError: SSL_connect returned=1 errno=0 state=SSLv3 read server hello A: sslv3 alert handshake failure`\\n\\nThis has worked 2 days ago, but not anymore. Has your certificate changed?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ivr0u\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"SkaveRat\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ivr0u/ssl_problems_on_login/\", \"locked\": false, \"name\": \"t3_1ivr0u\", \"created\": 1374613553.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1ivr0u/ssl_problems_on_login/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"SSL problems on login\", \"created_utc\": 1374584753.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe bot I\\u0026quot;m working on posts reply\\u0026#39;s in a subreddit but it has to wait ~8 mins or it will get the \\u0026quot;you are doing that too much.\\u0026quot; error. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there a way to do some error checking for this or to get around it?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The bot I\\\"m working on posts reply's in a subreddit but it has to wait ~8 mins or it will get the \\\"you are doing that too much.\\\" error. \\n\\nIs there a way to do some error checking for this or to get around it?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1i7fma\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"IPostDumbShit\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1i7fma/been_messing_around_with_praw_but_ive_been/\", \"locked\": false, \"name\": \"t3_1i7fma\", \"created\": 1373725406.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1i7fma/been_messing_around_with_praw_but_ive_been/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Been messing around with praw but I've been running into issues with the RateLimitExceeded.\", \"created_utc\": 1373696606.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m using jQuery\\u0026#39;s getJSON request to get submissions from a subreddit. The problem is the reddit API limits me to no more than 100 results per request. Is there any way I can get the 100 submissions after the previous hundred, etc?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere\\u0026#39;s an example of what I\\u0026#39;m doing:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E$.getJSON(\\u0026quot;http://www.reddit.com/r/subreddit.json?jsonp=?\\u0026amp;limit=100\\u0026quot;, function(data) {\\n    // do something with my data, but I want more!\\n}    \\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eedit: grammar\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm using jQuery's getJSON request to get submissions from a subreddit. The problem is the reddit API limits me to no more than 100 results per request. Is there any way I can get the 100 submissions after the previous hundred, etc?\\n\\nHere's an example of what I'm doing:\\n\\n    $.getJSON(\\\"http://www.reddit.com/r/subreddit.json?jsonp=?\\u0026limit=100\\\", function(data) {\\n        // do something with my data, but I want more!\\n    }    \\n\\nedit: grammar\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1hfptr\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ChadBroChill_17\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1hfptr/use_getjson_requests_to_retrieve_lots_of/\", \"locked\": false, \"name\": \"t3_1hfptr\", \"created\": 1372727398.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1hfptr/use_getjson_requests_to_retrieve_lots_of/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Use getJSON requests to retrieve lots of submissions\", \"created_utc\": 1372698598.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAlternatively if something similar exists and is available for modification that\\u0026#39;d be neat, too.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m not a dev. I don\\u0026#39;t understand 80% of what I\\u0026#39;m even reading here and I\\u0026#39;m sure I could not do anything on my own \\u003Cem\\u003Eright now\\u003C/em\\u003E. Which is why I want to learn how to!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERather than copypasta, I\\u0026#39;ll just link to where my search began: \\u003Ca href=\\\"http://www.reddit.com/r/modhelp/comments/1h0lvd/bot_related_need_to_be_pointed_in_the_right/\\\"\\u003Ehere\\u003C/a\\u003E since I had no idea where to even begin.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEssentially I want to create a bot that will scan a single subreddit for certain key words / parameters and, if appropriate, create a reply to answer certain questions. It\\u0026#39;s something I\\u0026#39;ll have to regularly update, too, until it\\u0026#39;s hopefully no longer necessary.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThink of it this way: If you\\u0026#39;ve ever been a sub of a small subreddit that doesn\\u0026#39;t see much in terms of new content, old stuff tends to repeat itself very, very often. Rather than turn away people who might not even know how to search through old posts, I want to provide them with some information before they figure the subreddit is completely dead. I just don\\u0026#39;t want to have to do it manually each time, y\\u0026#39;know?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Alternatively if something similar exists and is available for modification that'd be neat, too.\\n\\nI'm not a dev. I don't understand 80% of what I'm even reading here and I'm sure I could not do anything on my own *right now*. Which is why I want to learn how to!\\n\\nRather than copypasta, I'll just link to where my search began: [here](http://www.reddit.com/r/modhelp/comments/1h0lvd/bot_related_need_to_be_pointed_in_the_right/) since I had no idea where to even begin.\\n\\nEssentially I want to create a bot that will scan a single subreddit for certain key words / parameters and, if appropriate, create a reply to answer certain questions. It's something I'll have to regularly update, too, until it's hopefully no longer necessary.\\n\\nThink of it this way: If you've ever been a sub of a small subreddit that doesn't see much in terms of new content, old stuff tends to repeat itself very, very often. Rather than turn away people who might not even know how to search through old posts, I want to provide them with some information before they figure the subreddit is completely dead. I just don't want to have to do it manually each time, y'know?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1h0scw\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"CubedWorld\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 133, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1h0scw/could_use_some_help_with_creating_a_faqesque_bot/\", \"locked\": false, \"name\": \"t3_1h0scw\", \"created\": 1372167491.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1h0scw/could_use_some_help_with_creating_a_faqesque_bot/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Could use some help with creating a FAQ-esque bot\", \"created_utc\": 1372138691.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m running a script which sucks down new and top posts to \\u003Ca href=\\\"/r/pics\\\"\\u003E/r/pics\\u003C/a\\u003E. It works fine for new but has recently stopped working for pics. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECompare, for example, these results via the interpreter, after connecting via PRAW\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Enew_submissions_generator = r.get_subreddit(\\u0026#39;pics\\u0026#39;).get_new(limit=100)\\nfor submission in new_submissions_generator:\\n    name = submission.author.name\\nname\\nu\\u0026#39;anoteduser\\u0026#39;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EWorks as expected (assigns and returns username). \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESimilarly: \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehot_submissions_generator = r.get_subreddit(\\u0026#39;pics\\u0026#39;).get_hot(limit=25)\\nfor submission in hot_submissions_generator:\\n    nameh = submission.author.name\\nnameh\\nu\\u0026#39;preggit\\u0026#39;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EWorks as expected (assigns and returns username). \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBut: \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Etop_submissions_generator = r.get_subreddit(\\u0026#39;pics\\u0026#39;).get_top(limit=25)\\nfor submission in top_submissions_generator:\\n    namet = submission.author.name\\nTraceback (most recent call last):\\nFile \\u0026quot;\\u0026lt;stdin\\u0026gt;\\u0026quot;, line 2, in \\u0026lt;module\\u0026gt;\\nAttributeError: \\u0026#39;NoneType\\u0026#39; object has no attribute \\u0026#39;name\\u0026#39;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EBreaks with an error that there is no attribute name for the object. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat is going on? I am 99% sure this wasn\\u0026#39;t happening a few days ago...\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm running a script which sucks down new and top posts to /r/pics. It works fine for new but has recently stopped working for pics. \\n\\nCompare, for example, these results via the interpreter, after connecting via PRAW\\n\\n    new_submissions_generator = r.get_subreddit('pics').get_new(limit=100)\\n    for submission in new_submissions_generator:\\n        name = submission.author.name\\n    name\\n    u'anoteduser'\\n\\nWorks as expected (assigns and returns username). \\n\\nSimilarly: \\n\\n    hot_submissions_generator = r.get_subreddit('pics').get_hot(limit=25)\\n    for submission in hot_submissions_generator:\\n        nameh = submission.author.name\\n    nameh\\n    u'preggit'\\n\\nWorks as expected (assigns and returns username). \\n\\nBut: \\n\\n    top_submissions_generator = r.get_subreddit('pics').get_top(limit=25)\\n    for submission in top_submissions_generator:\\n        namet = submission.author.name\\n    Traceback (most recent call last):\\n    File \\\"\\u003Cstdin\\u003E\\\", line 2, in \\u003Cmodule\\u003E\\n    AttributeError: 'NoneType' object has no attribute 'name'\\n\\nBreaks with an error that there is no attribute name for the object. \\n\\nWhat is going on? I am 99% sure this wasn't happening a few days ago...\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1gu6y5\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"peteyMIT\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1371874164.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1gu6y5/did_something_just_break_with_praws_get_top_method/\", \"locked\": false, \"name\": \"t3_1gu6y5\", \"created\": 1371901539.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1gu6y5/did_something_just_break_with_praws_get_top_method/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Did something just break with PRAW's get_top method?\", \"created_utc\": 1371872739.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/yd2c3/praw_question_resolving_a_comment_object_from/\\\"\\u003EThis ancient thread is related.\\u003C/a\\u003E That guy knew he had a comment, so he just did \\u003Ccode\\u003Esubmission = rh.get_submission(url = \\u0026lt;whatever\\u0026gt;)\\u003C/code\\u003E, and then looked at \\u003Ccode\\u003Esubmission._comments[0]\\u003C/code\\u003E, and that was his comment.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAssume I don\\u0026#39;t know whether my thing is a submission or a comment.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf I don\\u0026#39;t want to parse the URL, presumably I can do this:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esubmission = rh.get_submission(url = my_url)\\nif submission.permalink == my_url:\\n    object = submission\\nelif submission._comments[0].permalink == my_url:\\n    object = submission._comments[0]\\nelse\\n    \\u0026lt;burst into flames\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ESo, is this the proper way to do this? I\\u0026#39;m concerned because I don\\u0026#39;t have total control over the URL, and the URL might not be normalized.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eedit:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m concerned this would crash if the URL is not normalized and there are no comments, but that would presumably be an easy fix. It may also make sense to test the comment first and see if its ID is present in the URL, then test the submission ID against the URL:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esubmission = rh.get_submission(url = my_url)\\nif len(submission._comments) \\u0026gt; 0 and my_url.find(submission._comments[0].id) \\u0026gt;= 0:\\n    object = submission._comments[0]\\nelif my_url.find(submission.id) \\u0026gt;= 0:\\n    object = submission\\nelse\\n    \\u0026lt;burst into flames\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EBut this doesn\\u0026#39;t sound like the right way to do something that I would think should be easy to do: \\u0026quot;Here\\u0026#39;s a URL -- get the object.\\u0026quot;\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"[This ancient thread is related.](http://www.reddit.com/r/redditdev/comments/yd2c3/praw_question_resolving_a_comment_object_from/) That guy knew he had a comment, so he just did `submission = rh.get_submission(url = \\u003Cwhatever\\u003E)`, and then looked at `submission._comments[0]`, and that was his comment.\\n\\nAssume I don't know whether my thing is a submission or a comment.\\n\\nIf I don't want to parse the URL, presumably I can do this:\\n\\n    submission = rh.get_submission(url = my_url)\\n    if submission.permalink == my_url:\\n        object = submission\\n    elif submission._comments[0].permalink == my_url:\\n        object = submission._comments[0]\\n    else\\n        \\u003Cburst into flames\\u003E\\n\\nSo, is this the proper way to do this? I'm concerned because I don't have total control over the URL, and the URL might not be normalized.\\n\\nedit:\\n\\nI'm concerned this would crash if the URL is not normalized and there are no comments, but that would presumably be an easy fix. It may also make sense to test the comment first and see if its ID is present in the URL, then test the submission ID against the URL:\\n\\n    submission = rh.get_submission(url = my_url)\\n    if len(submission._comments) \\u003E 0 and my_url.find(submission._comments[0].id) \\u003E= 0:\\n        object = submission._comments[0]\\n    elif my_url.find(submission.id) \\u003E= 0:\\n        object = submission\\n    else\\n        \\u003Cburst into flames\\u003E\\n\\nBut this doesn't sound like the right way to do something that I would think should be easy to do: \\\"Here's a URL -- get the object.\\\"\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1g98a4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"brucemo\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1371146692.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/\", \"locked\": false, \"name\": \"t3_1g98a4\", \"created\": 1371138738.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"PRAW: Looking for the proper way to get a submission or comment object from a URL\", \"created_utc\": 1371109938.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Cstrong\\u003EThe background:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve been writing a minecraft mod that lets a user set for themselves on the subreddit. So far, I can log in and set the flair for a user with /api/flair \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy problem is that I want to append the new class, not replace the whole thing. For this, I \\u003Cem\\u003Ethink\\u003C/em\\u003E I need to use /api/flairlist to get their current flair, and then set the new flair based on the results.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EMy problem:\\u003C/strong\\u003E \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI can\\u0026#39;t seem to get /api/flairlist to work. I get a 500 error every time. The \\u003Ca href=\\\"http://www.reddit.com/dev/api\\\"\\u003Eapi documentation page\\u003C/a\\u003E is absolutely terrible at the moment, it doesn\\u0026#39;t cover the \\u0026quot;r=SUBREDDIT\\u0026quot; parameter anywhere, and many parameters have no description.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EWhat I need:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe exact params I need to pass to get the flair of a user\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EWhat I\\u0026#39;ve tried:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny and every combination of the following (I think):\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehttp://www.reddit.com/api/flairlist/ (Both POST and GET)\\nr=minez\\n\\u0026amp;limit=1\\n\\u0026amp;name=t2_5va4\\n\\u0026amp;count=1\\n\\u0026amp;uh=jo0m6mnanm17133909ca4fd1f1ddd29c27...\\n\\u0026amp;before=t2_5aw8r\\n\\u0026amp;after=t2_5aw8r\\n\\u0026amp;limit=1\\n\\u0026amp;count=1\\n\\u0026amp;name=amoliski\\n\\u0026amp;prayersToTheRedditGod=1000\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"**The background:**\\n\\nI've been writing a minecraft mod that lets a user set for themselves on the subreddit. So far, I can log in and set the flair for a user with /api/flair \\n\\nMy problem is that I want to append the new class, not replace the whole thing. For this, I *think* I need to use /api/flairlist to get their current flair, and then set the new flair based on the results.\\n\\n**My problem:** \\n\\nI can't seem to get /api/flairlist to work. I get a 500 error every time. The [api documentation page](http://www.reddit.com/dev/api) is absolutely terrible at the moment, it doesn't cover the \\\"r=SUBREDDIT\\\" parameter anywhere, and many parameters have no description.\\n\\n**What I need:**\\n\\nThe exact params I need to pass to get the flair of a user\\n\\n**What I've tried:**\\n\\nAny and every combination of the following (I think):\\n\\n    http://www.reddit.com/api/flairlist/ (Both POST and GET)\\n    r=minez\\n    \\u0026limit=1\\n    \\u0026name=t2_5va4\\n    \\u0026count=1\\n    \\u0026uh=jo0m6mnanm17133909ca4fd1f1ddd29c27...\\n    \\u0026before=t2_5aw8r\\n    \\u0026after=t2_5aw8r\\n    \\u0026limit=1\\n    \\u0026count=1\\n    \\u0026name=amoliski\\n    \\u0026prayersToTheRedditGod=1000\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1g3pd5\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"amoliski\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1g3pd5/issues_with_the_apis_flairlist/\", \"locked\": false, \"name\": \"t3_1g3pd5\", \"created\": 1370956843.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1g3pd5/issues_with_the_apis_flairlist/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Issues with the api's flairlist\", \"created_utc\": 1370928043.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been looking through the API and I can\\u0026#39;t seem to find a function for getting all my flair templates, even though this function must exist. I found get_flairs, but I get an error when I try to call it.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI also have some other small questions.\\u003Cbr/\\u003E\\nIs there a way to set multiple flair templates with one command, or do I have to set them one by one, with each taking 2 seconds?\\u003Cbr/\\u003E\\nI need to edit the flair text of every user on my reddit but get_flair_list only returns the first 100 users. How do I access the rest? \\nDoes set_flair_csv take 2 second to update each users flair or is it done all at once?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been looking through the API and I can't seem to find a function for getting all my flair templates, even though this function must exist. I found get_flairs, but I get an error when I try to call it.  \\n\\nI also have some other small questions.  \\nIs there a way to set multiple flair templates with one command, or do I have to set them one by one, with each taking 2 seconds?  \\nI need to edit the flair text of every user on my reddit but get_flair_list only returns the first 100 users. How do I access the rest? \\nDoes set_flair_csv take 2 second to update each users flair or is it done all at once?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1fj88q\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Imosa1\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1fj88q/how_do_i_get_a_list_of_my_user_flair_templates/\", \"locked\": false, \"name\": \"t3_1fj88q\", \"created\": 1370230051.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1fj88q/how_do_i_get_a_list_of_my_user_flair_templates/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How do I get a list of my user flair templates? Also other small questions.\", \"created_utc\": 1370201251.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EFor example, how does it find out which comments I have up/down voted when loading the comments of a post?  Does it fetch all my votes and matches against all loaded comments?  Is this efficient?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks :)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"For example, how does it find out which comments I have up/down voted when loading the comments of a post?  Does it fetch all my votes and matches against all loaded comments?  Is this efficient?\\n\\nThanks :)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1fglw4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bpq\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1fglw4/how_does_reddit_handle_votes_so_efficiently_and/\", \"locked\": false, \"name\": \"t3_1fglw4\", \"created\": 1370117807.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1fglw4/how_does_reddit_handle_votes_so_efficiently_and/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How does Reddit handle votes so efficiently and quickly?\", \"created_utc\": 1370089007.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am just wondering, if I go to \\u003Ca href=\\\"http://www.reddit.com/r/this+that\\\"\\u003Ehttp://www.reddit.com/r/this+that\\u003C/a\\u003E.\\nAre the submissions just randomly picked from this and that frontpage, is it 1/2, is it a more complex algorythm?\\nThanks.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am just wondering, if I go to http://www.reddit.com/r/this+that.\\nAre the submissions just randomly picked from this and that frontpage, is it 1/2, is it a more complex algorythm?\\nThanks.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1f4qho\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Sympastache\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1f4qho/what_happens_when_chaining_subreddits/\", \"locked\": false, \"name\": \"t3_1f4qho\", \"created\": 1369679374.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1f4qho/what_happens_when_chaining_subreddits/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"What happens when chaining subreddits?\", \"created_utc\": 1369650574.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m working on some API wrappers for a bot but when I access the fancy calls a lot I keep triggering the rate-limiter. How do you guys handle that? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso I want to move to proper development with unit-tests but I imagine running a whole suite at once will trigger this as well, or doesn\\u0026#39;t it? What\\u0026#39;s you experience in this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm working on some API wrappers for a bot but when I access the fancy calls a lot I keep triggering the rate-limiter. How do you guys handle that? \\n\\nAlso I want to move to proper development with unit-tests but I imagine running a whole suite at once will trigger this as well, or doesn't it? What's you experience in this?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1ebi2b\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"brtt3000\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1ebi2b/how_do_you_develop_and_unittest_your_wrappers_and/\", \"locked\": false, \"name\": \"t3_1ebi2b\", \"created\": 1368574742.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1ebi2b/how_do_you_develop_and_unittest_your_wrappers_and/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How do you develop and unit-test your wrappers and bots without tripping the rate-limit?\", \"created_utc\": 1368545942.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI don\\u0026#39;t know what your policy is on exposing gold features through the API, but if you have nothing against it, it\\u0026#39;d be great if comment data had the \\u0026quot;saved\\u0026quot;: bool property on them when logged in with an account with Reddit Gold.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI want users to be able to save/unsave comments through my app, but I have no way to tell which are saved and which aren\\u0026#39;t. I have a hack when I\\u0026#39;m currently viewing /saved/ to assume things are saved, but that\\u0026#39;s far from an ideal solution.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I don't know what your policy is on exposing gold features through the API, but if you have nothing against it, it'd be great if comment data had the \\\"saved\\\": bool property on them when logged in with an account with Reddit Gold.\\n\\nI want users to be able to save/unsave comments through my app, but I have no way to tell which are saved and which aren't. I have a hack when I'm currently viewing /saved/ to assume things are saved, but that's far from an ideal solution.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1cg2ek\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ross456\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1cg2ek/api_request_saved_property_on_comments/\", \"locked\": false, \"name\": \"t3_1cg2ek\", \"created\": 1366120317.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1cg2ek/api_request_saved_property_on_comments/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API request: \\\"saved\\\" property on comments\", \"created_utc\": 1366091517.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIt looks like when I pull a user\\u0026#39;s messages, for comment replies I can\\u0026#39;t get the thread title (without performing an additional request). \\u003Ca href=\\\"http://pastebin.com/BDr3FdNA\\\"\\u003EHere\\u0026#39;s\\u003C/a\\u003E an example of the data I receive.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe website has the title I want, but there\\u0026#39;s no way to get it from the API? Is this something that could possibly be added? I\\u0026#39;d offer to do it myself, but I\\u0026#39;ll need some babysitting...\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"It looks like when I pull a user's messages, for comment replies I can't get the thread title (without performing an additional request). [Here's](http://pastebin.com/BDr3FdNA) an example of the data I receive.\\n\\nThe website has the title I want, but there's no way to get it from the API? Is this something that could possibly be added? I'd offer to do it myself, but I'll need some babysitting...\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1c1nha\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ross456\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1c1nha/api_cant_get_thread_title_of_comment_reply_from/\", \"locked\": false, \"name\": \"t3_1c1nha\", \"created\": 1365599889.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1c1nha/api_cant_get_thread_title_of_comment_reply_from/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API: can't get thread title of comment reply from /message/inbox?\", \"created_utc\": 1365571089.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe script is watching and fetching submissions on a subreddit\\u0026#39;s \\u0026quot;new\\u0026quot; feed every 30 minutes and I\\u0026#39;m stuck thinking of a way to only fetch new posts.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m using PRAW and thinking of storing the reddit id of the first post (ex. dtg4j), everytime I fetch, in a flat file. (since the first post is always the newest)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBut my program fetches 50 posts everytime, what if only 1 of that is new, then the other 49 posts I fetched are useless, right? So I\\u0026#39;m not sure yet with that.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnd even if there\\u0026#39;s only a small chance, when I fetch 50 posts, what if there were 60 new submissions. That means I will miss the other 10.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease help me. Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The script is watching and fetching submissions on a subreddit's \\\"new\\\" feed every 30 minutes and I'm stuck thinking of a way to only fetch new posts.\\n\\nI'm using PRAW and thinking of storing the reddit id of the first post (ex. dtg4j), everytime I fetch, in a flat file. (since the first post is always the newest)\\n\\nBut my program fetches 50 posts everytime, what if only 1 of that is new, then the other 49 posts I fetched are useless, right? So I'm not sure yet with that.\\n\\nAnd even if there's only a small chance, when I fetch 50 posts, what if there were 60 new submissions. That means I will miss the other 10.\\n\\nPlease help me. Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1bv06t\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"yowmamasita\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1bv06t/stuck_at_processing_each_submission_on_a/\", \"locked\": false, \"name\": \"t3_1bv06t\", \"created\": 1365383995.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1bv06t/stuck_at_processing_each_submission_on_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Stuck at processing each submission on a subreddit without repeating\", \"created_utc\": 1365355195.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI noticed that in recent installs of reddit script, the code behaves a bit differently. Now, upon installing, I see multiple instances of paster serve --reload run.ini command, 3-4 of them. Before, it was only one, I usually ran the daemon one.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo, is this maybe causing this issues, I noticed that sometimes karma is not working as it should be, maybe those multiple instances are messing up with the consumers, or this is way it should be, due to some updates to pylons or something like that?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey,\\n\\nI noticed that in recent installs of reddit script, the code behaves a bit differently. Now, upon installing, I see multiple instances of paster serve --reload run.ini command, 3-4 of them. Before, it was only one, I usually ran the daemon one.\\n\\nSo, is this maybe causing this issues, I noticed that sometimes karma is not working as it should be, maybe those multiple instances are messing up with the consumers, or this is way it should be, due to some updates to pylons or something like that?\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1aza9y\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"elAhmo\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1aza9y/multiple_instances_of_paster/\", \"locked\": false, \"name\": \"t3_1aza9y\", \"created\": 1364257138.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1aza9y/multiple_instances_of_paster/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Multiple instances of paster\", \"created_utc\": 1364228338.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have installed a Reddit clone and everything is working except the TOP and CONTROVERSIAL tabs in the Front Page and Sub-Reddits. The time periods \\u0026quot;This hour\\u0026quot;, \\u0026quot;This today\\u0026quot;, \\u0026quot;This week\\u0026quot;, \\u0026quot;This month\\u0026quot; are showing no posts. The only time period showing posts seems to be \\u0026quot;All time.\\u0026quot;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat\\u0026#39;s strange is that TOP and CONTROVERSIAL in the All Page seems to be working fine showing all the posts in each time period.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI and others have tried installing this on different servers and this same issue appears. So is this a known bug with the clone or does it require some type of activation for these tabs to be fully functional?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThank You!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello,\\n\\nI have installed a Reddit clone and everything is working except the TOP and CONTROVERSIAL tabs in the Front Page and Sub-Reddits. The time periods \\\"This hour\\\", \\\"This today\\\", \\\"This week\\\", \\\"This month\\\" are showing no posts. The only time period showing posts seems to be \\\"All time.\\\"\\n\\nWhat's strange is that TOP and CONTROVERSIAL in the All Page seems to be working fine showing all the posts in each time period.\\n\\nI and others have tried installing this on different servers and this same issue appears. So is this a known bug with the clone or does it require some type of activation for these tabs to be fully functional?\\n\\nThank You!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1awuok\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"liveonethere\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1364139304.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1awuok/top_and_controversial_broken/\", \"locked\": false, \"name\": \"t3_1awuok\", \"created\": 1364160889.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1awuok/top_and_controversial_broken/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Top and Controversial broken\", \"created_utc\": 1364132089.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI want to enable my users to embed images and videos in their comments. I\\u0026#39;m not trying to create a subreddit, but install the reddit software on my own domain and modify it to suit my needs. How difficult would it be to modify the source code to enable this? Are there any reddit-powered sites where this is possible?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I want to enable my users to embed images and videos in their comments. I'm not trying to create a subreddit, but install the reddit software on my own domain and modify it to suit my needs. How difficult would it be to modify the source code to enable this? Are there any reddit-powered sites where this is possible?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1auyii\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"uir\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1auyii/is_it_possible_to_mod_reddit_to_allow_embedding/\", \"locked\": false, \"name\": \"t3_1auyii\", \"created\": 1364078316.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1auyii/is_it_possible_to_mod_reddit_to_allow_embedding/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is it possible to mod reddit to allow embedding images in comments?\", \"created_utc\": 1364049516.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been trying to complete the setup of the AutoModerator bot on a machine running reddit. So far, the script runs periodically and is able to access the system and to make the corresponding checks. The problem is I cannot get the right configuration for the database mainly because I\\u0026#39;ve found no info on the subject.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThere are three tables that are responsible for determining the AutoModerator\\u0026#39;s behavior: conditions, subreddits and subreddits_conditions. However, the valid values for each field on those tables don\\u0026#39;t appear on the setup page and they are not particularly obvious.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDo you have some info on the subject? I\\u0026#39;d really appreciate it.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks in advance for your time.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been trying to complete the setup of the AutoModerator bot on a machine running reddit. So far, the script runs periodically and is able to access the system and to make the corresponding checks. The problem is I cannot get the right configuration for the database mainly because I've found no info on the subject.\\n\\nThere are three tables that are responsible for determining the AutoModerator's behavior: conditions, subreddits and subreddits_conditions. However, the valid values for each field on those tables don't appear on the setup page and they are not particularly obvious.\\n\\nDo you have some info on the subject? I'd really appreciate it.\\n\\nThanks in advance for your time.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1asoa5\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"csosa\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1asoa5/configuring_automoderator_conditions_is_there/\", \"locked\": false, \"name\": \"t3_1asoa5\", \"created\": 1363985861.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1asoa5/configuring_automoderator_conditions_is_there/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Configuring AutoModerator conditions. Is there documentation on the subject?\", \"created_utc\": 1363957061.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EA few days ago, I added a YouTube bot, \\u003Ca href=\\\"https://github.com/AndrewNeo/groompbot\\\"\\u003EGroompbot\\u003C/a\\u003E, to the \\u003Ca href=\\\"/r/nerdcubed\\\"\\u003E/r/nerdcubed\\u003C/a\\u003E subreddit, I made it run every minute via cron job, so it would always be the first to post, and it worked great. Then one day it stops posting, I take a look to find that it throws an error 429 (Too many requests) every time, I contacted the maker of it a few days ago, but he hasn\\u0026#39;t replied yet. Here\\u0026#39;s the error:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EERROR:root:Error logging into Reddit.\\nTraceback (most recent call last):\\n  File \\u0026quot;groompbot.py\\u0026quot;, line 27, in getReddit\\n    r.login(settings[\\u0026quot;reddit_username\\u0026quot;], settings[\\u0026quot;reddit_password\\u0026quot;])\\n  File \\u0026quot;/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/__init__.py\\u0026quot;, line 857, in login\\n    self.request_json(self.config[\\u0026#39;login\\u0026#39;], data=data)\\n  File \\u0026quot;/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/decorators.py\\u0026quot;, line 223, in error_checked_function\\n    return_value = function(cls, *args, **kwargs)\\n  File \\u0026quot;/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/__init__.py\\u0026quot;, line 396, in request_json\\n    response = self._request(url, params, data)\\n  File \\u0026quot;/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/__init__.py\\u0026quot;, line 283, in _request\\n    timeout=timeout)\\n  File \\u0026quot;/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/decorators.py\\u0026quot;, line 64, in __call__\\n    result = self.function(reddit_session, url, *args, **kwargs)\\n  File \\u0026quot;/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/decorators.py\\u0026quot;, line 167, in __call__\\n    return self.function(*args, **kwargs)\\n  File \\u0026quot;/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/helpers.py\\u0026quot;, line 161, in _request\\n    response.raise_for_status()\\n  File \\u0026quot;/usr/local/lib/python2.6/dist-packages/requests/models.py\\u0026quot;, line 638, in raise_for_status\\n    raise http_error\\nHTTPError: 429 Client Error: Too Many Requests\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAny ideas, Reddit?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"A few days ago, I added a YouTube bot, [Groompbot](https://github.com/AndrewNeo/groompbot), to the /r/nerdcubed subreddit, I made it run every minute via cron job, so it would always be the first to post, and it worked great. Then one day it stops posting, I take a look to find that it throws an error 429 (Too many requests) every time, I contacted the maker of it a few days ago, but he hasn't replied yet. Here's the error:\\n\\n    ERROR:root:Error logging into Reddit.\\n    Traceback (most recent call last):\\n      File \\\"groompbot.py\\\", line 27, in getReddit\\n        r.login(settings[\\\"reddit_username\\\"], settings[\\\"reddit_password\\\"])\\n      File \\\"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/__init__.py\\\", line 857, in login\\n        self.request_json(self.config['login'], data=data)\\n      File \\\"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/decorators.py\\\", line 223, in error_checked_function\\n        return_value = function(cls, *args, **kwargs)\\n      File \\\"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/__init__.py\\\", line 396, in request_json\\n        response = self._request(url, params, data)\\n      File \\\"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/__init__.py\\\", line 283, in _request\\n        timeout=timeout)\\n      File \\\"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/decorators.py\\\", line 64, in __call__\\n        result = self.function(reddit_session, url, *args, **kwargs)\\n      File \\\"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/decorators.py\\\", line 167, in __call__\\n        return self.function(*args, **kwargs)\\n      File \\\"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/helpers.py\\\", line 161, in _request\\n        response.raise_for_status()\\n      File \\\"/usr/local/lib/python2.6/dist-packages/requests/models.py\\\", line 638, in raise_for_status\\n        raise http_error\\n    HTTPError: 429 Client Error: Too Many Requests\\n    \\nAny ideas, Reddit?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"19rubs\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"SN4T14\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 14, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/19rubs/issues_with_youtube_bot/\", \"locked\": false, \"name\": \"t3_19rubs\", \"created\": 1362604333.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Issues with YouTube bot\", \"created_utc\": 1362575533.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"praw.readthedocs.org\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"19i0r2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bboe\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/19i0r2/new_location_for_praw_documentation/\", \"locked\": false, \"name\": \"t3_19i0r2\", \"created\": 1362218290.0, \"url\": \"https://praw.readthedocs.org/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New location for PRAW documentation\", \"created_utc\": 1362189490.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Ewhile running reddit clone in my machine, im getting these errors on the terminal\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eerror connecting to amqp reddit @ localhost:5672 (IOError(\\u0026#39;Socket closed\\u0026#39;,))\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eas ive checked on the open ports using NMAP, apparently, RabbitMQ is not listening on port 5672, or on any other port. Its not listening at all... Ive checked rabbitmqctl for status, and it says rabbitmq-server is running.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHave anyone encountered a problem as such before? Do I really need RabbitMQ to be running?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"while running reddit clone in my machine, im getting these errors on the terminal\\n\\nerror connecting to amqp reddit @ localhost:5672 (IOError('Socket closed',))\\n\\nas ive checked on the open ports using NMAP, apparently, RabbitMQ is not listening on port 5672, or on any other port. Its not listening at all... Ive checked rabbitmqctl for status, and it says rabbitmq-server is running.\\n\\nHave anyone encountered a problem as such before? Do I really need RabbitMQ to be running?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"18xtpb\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/18xtpb/amqp_server_not_listening_on_port_5672/\", \"locked\": false, \"name\": \"t3_18xtpb\", \"created\": 1361454538.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/18xtpb/amqp_server_not_listening_on_port_5672/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"AMQP Server Not listening on port 5672\", \"created_utc\": 1361425738.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Eit seems everyone knows how to add certain functions to there programs yet i cant find a standard place to view examples.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Efor instance \\u0026quot;submission.selftext.lower()\\u0026quot;\\nhow or where can i find information on this function?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Efrom what i can tell is that this code snipbit returns a links text. but what could i change?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eaka \\u003C/p\\u003E\\n\\n\\u003Cp\\u003Esubmission.titletext()?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Esubmission.linkurl()?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eor something of the sort. where would i be able to find information on different functions?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ei guess im having a hard time understanding praw because i dont know what i can do with it. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ethanks guys, sorry for newb question.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"it seems everyone knows how to add certain functions to there programs yet i cant find a standard place to view examples.\\n\\nfor instance \\\"submission.selftext.lower()\\\"\\nhow or where can i find information on this function?\\n\\nfrom what i can tell is that this code snipbit returns a links text. but what could i change?\\n\\naka \\n\\nsubmission.titletext()?\\n\\nsubmission.linkurl()?\\n\\n\\nor something of the sort. where would i be able to find information on different functions?\\n\\ni guess im having a hard time understanding praw because i dont know what i can do with it. \\n\\nthanks guys, sorry for newb question.\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"18qq0d\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"JRDubstepcom\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/\", \"locked\": false, \"name\": \"t3_18qq0d\", \"created\": 1361205144.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"is there a praw library of some sort?\", \"created_utc\": 1361176344.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI recently ported moderatorbot to PRAW 2, and had a couple of issues.  bboe was helpful as always, and I thought I\\u0026#39;d reproduce here what I did to get my code working, in case others can learn from this.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EIssue 1 - my_moderation\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIn PRAW 1.x the way to get a list of subreddits that the logged in user moderates, you use:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EFor subreddit in r.user.my_moderation():\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIn PRAW 2.x the correct way is now:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EFor subreddit in r.get_my_moderator():\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E(and don\\u0026#39;t forget the () after r.get_my_moderator as I initially did)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EIssue 2 - _request\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIn PRAW 1.x, I used the following to make HTTP requests\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eresponse = r._request(url, params)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIn PRAW 2.x params has been changed to data, and the following needs to be used.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eresponse = r._request(url, data=data)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIn this particular case I was trying to make an HTTP request to remove a comment.  bboe recommended that I use \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Er.request_json(url, data=data)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E\\u003Cem\\u003Eyou should use r.request_json as you can\\u0026#39;t count on r._request to always be around.\\u003C/em\\u003E\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EIn this case that will work (even though I am not actually trying to get json data).  In other cases I believe that I still need to use _request.  I believe that some pages do not return a json properly (or the json does not have all the data I need), but the result of _request can be parsed using beatifulsoup.  I don\\u0026#39;t have any code currently in production that uses beautifulsoup on _request, so I can\\u0026#39;t remember the exact situations I used it for.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo, I\\u0026#39;d like to ask bboe to consider putting some sort of _request functionality into supported \\u0026quot;user space\\u0026quot;\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I recently ported moderatorbot to PRAW 2, and had a couple of issues.  bboe was helpful as always, and I thought I'd reproduce here what I did to get my code working, in case others can learn from this.\\n\\n**Issue 1 - my_moderation**\\n\\nIn PRAW 1.x the way to get a list of subreddits that the logged in user moderates, you use:\\n\\n    For subreddit in r.user.my_moderation():\\n\\nIn PRAW 2.x the correct way is now:\\n\\n    For subreddit in r.get_my_moderator():\\n\\n(and don't forget the () after r.get_my_moderator as I initially did)\\n\\n**Issue 2 - _request**\\n\\nIn PRAW 1.x, I used the following to make HTTP requests\\n\\n    response = r._request(url, params)\\n\\nIn PRAW 2.x params has been changed to data, and the following needs to be used.\\n\\n    response = r._request(url, data=data)\\n\\nIn this particular case I was trying to make an HTTP request to remove a comment.  bboe recommended that I use \\n\\n    r.request_json(url, data=data)\\n\\n\\u003E *you should use r.request_json as you can't count on r._request to always be around.*\\n\\nIn this case that will work (even though I am not actually trying to get json data).  In other cases I believe that I still need to use _request.  I believe that some pages do not return a json properly (or the json does not have all the data I need), but the result of _request can be parsed using beatifulsoup.  I don't have any code currently in production that uses beautifulsoup on _request, so I can't remember the exact situations I used it for.\\n\\nSo, I'd like to ask bboe to consider putting some sort of _request functionality into supported \\\"user space\\\"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"18eeep\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Bornhuetter\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/18eeep/resolved_issues_porting_to_praw_2/\", \"locked\": false, \"name\": \"t3_18eeep\", \"created\": 1360732248.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/18eeep/resolved_issues_porting_to_praw_2/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Resolved issues porting to PRAW 2\", \"created_utc\": 1360703448.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI read the \\u0026#39;writing a bot\\u0026#39; and \\u0026#39;parsing comments\\u0026#39; and what i took away for it was that there is an upper limit of 500 comments unless you have a gold account. Is this correct? Any comments below the 500 are unreadable unless you actually go on the site into the comments and keep pressing the more comment button that only shows about 20  more comments at a time?\\u003Cbr/\\u003E\\nIf this isn\\u0026#39;t correct could someone please give me a simple example of pulling in ALL comments in a thread?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I read the 'writing a bot' and 'parsing comments' and what i took away for it was that there is an upper limit of 500 comments unless you have a gold account. Is this correct? Any comments below the 500 are unreadable unless you actually go on the site into the comments and keep pressing the more comment button that only shows about 20  more comments at a time?   \\nIf this isn't correct could someone please give me a simple example of pulling in ALL comments in a thread?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"16s64a\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"lamerx\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/16s64a/praw_more_comments/\", \"locked\": false, \"name\": \"t3_16s64a\", \"created\": 1358494234.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/16s64a/praw_more_comments/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Praw + 'MORE' Comments\", \"created_utc\": 1358465434.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://pastebin.com/m2sy3E02\\\"\\u003Ehttp://pastebin.com/m2sy3E02\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDon\\u0026#39;t remember this being an array and the \\u0026quot;{}\\u0026quot; string at the beginning of it seems weird. Is this intentional or is the Reddit API bugged?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"http://pastebin.com/m2sy3E02\\n\\nDon't remember this being an array and the \\\"{}\\\" string at the beginning of it seems weird. Is this intentional or is the Reddit API bugged?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"16a0p8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ebianco\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/16a0p8/reddit_comments_list_api_acting_funny/\", \"locked\": false, \"name\": \"t3_16a0p8\", \"created\": 1357801981.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/16a0p8/reddit_comments_list_api_acting_funny/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Comments List API acting funny?\", \"created_utc\": 1357773181.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"15iwjj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"one_more_minute\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/15iwjj/robbit_reddit_api_and_bot_framework_for_clojure/\", \"locked\": false, \"name\": \"t3_15iwjj\", \"created\": 1356655985.0, \"url\": \"https://github.com/one-more-minute/robbit\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"robbit: reddit api and bot framework for clojure\", \"created_utc\": 1356627185.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have been trying to figure this out for hours but im getting nowhere and I cant seem to find any examples with google searches. Can someone give me an example or code snippit of pulling the inbox and iterating though it. Also; what would be the command to write to the side bar in the admin area of a subreddit? \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have been trying to figure this out for hours but im getting nowhere and I cant seem to find any examples with google searches. Can someone give me an example or code snippit of pulling the inbox and iterating though it. Also; what would be the command to write to the side bar in the admin area of a subreddit? \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"15cc7k\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"lamerx\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/15cc7k/a_couple_of_praw_questions/\", \"locked\": false, \"name\": \"t3_15cc7k\", \"created\": 1356328112.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/15cc7k/a_couple_of_praw_questions/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"A Couple of Praw Questions\", \"created_utc\": 1356299312.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIt looks like the \\u0026quot;full name\\u0026quot; for a submitted link \\u0026quot;thing\\u0026quot; is its \\u0026quot;kind\\u0026quot; and \\u0026quot;id\\u0026quot; joined with an underscore.  It also looks like all links are kind: \\u0026quot;t3\\u0026quot;.  Is this true?  If so, can I always assume that the full name for a link is \\u0026quot;t3_1a2b3\\u0026quot;, assuming the ID is \\u0026quot;1a2b3\\u0026quot;?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe reason I\\u0026#39;m asking is because I want to derive the full name for a submitted link so that I can comment later, and the response I get back from a submitted link with the API is a bunch of Jquery arrays that contain mostly useless info.  The one useful thing it does contain is a url to the comments for the newly created link.  I can scrape the ID out of the comments URL, and if I can just attach \\u0026quot;t3_\\u0026quot; to the id then I\\u0026#39;m all set.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBy the way, I\\u0026#39;m totally willing to hear a better way to get the \\u0026quot;full name\\u0026quot; of a submitted link, but I don\\u0026#39;t see it as part of the returned JSON at the moment.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"It looks like the \\\"full name\\\" for a submitted link \\\"thing\\\" is its \\\"kind\\\" and \\\"id\\\" joined with an underscore.  It also looks like all links are kind: \\\"t3\\\".  Is this true?  If so, can I always assume that the full name for a link is \\\"t3_1a2b3\\\", assuming the ID is \\\"1a2b3\\\"?\\n\\nThe reason I'm asking is because I want to derive the full name for a submitted link so that I can comment later, and the response I get back from a submitted link with the API is a bunch of Jquery arrays that contain mostly useless info.  The one useful thing it does contain is a url to the comments for the newly created link.  I can scrape the ID out of the comments URL, and if I can just attach \\\"t3_\\\" to the id then I'm all set.\\n\\nBy the way, I'm totally willing to hear a better way to get the \\\"full name\\\" of a submitted link, but I don't see it as part of the returned JSON at the moment.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1568t4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"leavebot3000\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1568t4/where_can_i_find_documentation_on_the_full_name/\", \"locked\": false, \"name\": \"t3_1568t4\", \"created\": 1356049898.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1568t4/where_can_i_find_documentation_on_the_full_name/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Where can I find documentation on the \\\"full name\\\" for a thing, and how it gets created?\", \"created_utc\": 1356021098.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey all. I was thinking, when you perform an oauth authorization you have to define the scopes that you wish to use for that session. Would creating something like an \\u0026quot;extended\\u0026quot; scope solve the permanent token issue?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECurrently the tokens are only valid for an hour, but what if you could specify you wanted an \\u0026quot;extended\\u0026quot; scope, which would then make the lifetime of the returned token 6-12 months?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am not that experienced with scope, but it seems as though this could be an easy solution to the temporary token issue.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks for any feedback!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey all. I was thinking, when you perform an oauth authorization you have to define the scopes that you wish to use for that session. Would creating something like an \\\"extended\\\" scope solve the permanent token issue?\\n\\nCurrently the tokens are only valid for an hour, but what if you could specify you wanted an \\\"extended\\\" scope, which would then make the lifetime of the returned token 6-12 months?\\n\\nI am not that experienced with scope, but it seems as though this could be an easy solution to the temporary token issue.\\n\\nThanks for any feedback!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"154ysn\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bobbonew\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/154ysn/possible_permanent_oauth_tokens_solution/\", \"locked\": false, \"name\": \"t3_154ysn\", \"created\": 1355990258.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/154ysn/possible_permanent_oauth_tokens_solution/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Possible permanent OAuth tokens solution\", \"created_utc\": 1355961458.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EFor example here is the URL that gets generated when i try to order my comments by \\u0026quot;top\\u0026quot;, and \\u0026quot;all time\\u0026quot;.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/user/Stebon24/comments/?sort=top\\u0026amp;t=all\\\"\\u003Ehttp://www.reddit.com/user/Stebon24/comments/?sort=top\\u0026amp;t=all\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe comments seem to remain in the order they where created however.  The same goes for any of the other \\u0026quot;?sort\\u0026quot; or \\u0026quot;?t\\u0026quot; queries when applied to user comments. Am i missing something here, or is this feature not working?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"For example here is the URL that gets generated when i try to order my comments by \\\"top\\\", and \\\"all time\\\".\\n\\nhttp://www.reddit.com/user/Stebon24/comments/?sort=top\\u0026t=all\\n\\nThe comments seem to remain in the order they where created however.  The same goes for any of the other \\\"?sort\\\" or \\\"?t\\\" queries when applied to user comments. Am i missing something here, or is this feature not working?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"14kmg6\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stebon24\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/14kmg6/is_the_sort_query_broken_when_applied_to_user/\", \"locked\": false, \"name\": \"t3_14kmg6\", \"created\": 1355125523.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/14kmg6/is_the_sort_query_broken_when_applied_to_user/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is the '?sort=' query broken when applied to user comments? \", \"created_utc\": 1355096723.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"blog.webfaction.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"13vnhe\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bboe\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/13vnhe/setting_up_the_reddit_app_without_root_access_on/\", \"locked\": false, \"name\": \"t3_13vnhe\", \"created\": 1354062322.0, \"url\": \"http://blog.webfaction.com/2012/11/setting-up-the-reddit-app-without-root-access/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Setting up the reddit app without root access on WebFaction\", \"created_utc\": 1354033522.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m thinking of setting up a small website / service for a subreddit, to help organizing contests hosted there. From this site I would like to know if the user is currently logged into reddit, and if he is get his reddit name.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there a way to do this automatically (without having to deal with the user\\u0026#39;s password)?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI imagine that this can\\u0026#39;t be completely seamless for privacy reasons - you wouldn\\u0026#39;t want any website to be able to know your reddit name, right? - but something like the stackoverflow loggin via Google Oauth or facebook authorization thing would be great. Is this possible?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm thinking of setting up a small website / service for a subreddit, to help organizing contests hosted there. From this site I would like to know if the user is currently logged into reddit, and if he is get his reddit name.\\n\\n\\nIs there a way to do this automatically (without having to deal with the user's password)?\\n\\n\\nI imagine that this can't be completely seamless for privacy reasons - you wouldn't want any website to be able to know your reddit name, right? - but something like the stackoverflow loggin via Google Oauth or facebook authorization thing would be great. Is this possible?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"13l8tg\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"froginthesun\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/13l8tg/is_there_a_way_to_know_if_a_user_is_logged_in/\", \"locked\": false, \"name\": \"t3_13l8tg\", \"created\": 1353565501.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/13l8tg/is_there_a_way_to_know_if_a_user_is_logged_in/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a way to know if a user is logged in reddit and get his reddit name?\", \"created_utc\": 1353536701.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAt the moment the modlog is available via the api and has its own oauth scope (although it seems to be documented incorrectly), but it\\u0026#39;s pretty poor compared to the rest of the site. The only useful information contained in the response is the action and sometimes the details, depending on the action.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E{\\n    \\u0026quot;kind\\u0026quot;:\\u0026quot;modaction\\u0026quot;,\\n    \\u0026quot;data\\u0026quot;:\\n    {\\n        \\u0026quot;description\\u0026quot;:null,\\n        \\u0026quot;mod_id36\\u0026quot;:\\u0026quot;4fer6\\u0026quot;,\\n        \\u0026quot;details\\u0026quot;:\\u0026quot;confirmed ham\\u0026quot;,\\n        \\u0026quot;action\\u0026quot;:\\u0026quot;approvelink\\u0026quot;,\\n        \\u0026quot;sr_id36\\u0026quot;:\\u0026quot;2sz7j\\u0026quot;,\\n        \\u0026quot;target_fullname\\u0026quot;:\\u0026quot;t3_12jm86\\u0026quot;\\n    }\\n},\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIs there any intention to expand on this, adding timestamps, actual usernames, perhaps even information about the submission, etc. any time soon?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/pull/371\\\"\\u003ERelevant pull request\\u003C/a\\u003E \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"At the moment the modlog is available via the api and has its own oauth scope (although it seems to be documented incorrectly), but it's pretty poor compared to the rest of the site. The only useful information contained in the response is the action and sometimes the details, depending on the action.\\n\\n    {\\n        \\\"kind\\\":\\\"modaction\\\",\\n        \\\"data\\\":\\n        {\\n            \\\"description\\\":null,\\n            \\\"mod_id36\\\":\\\"4fer6\\\",\\n            \\\"details\\\":\\\"confirmed ham\\\",\\n            \\\"action\\\":\\\"approvelink\\\",\\n            \\\"sr_id36\\\":\\\"2sz7j\\\",\\n            \\\"target_fullname\\\":\\\"t3_12jm86\\\"\\n        }\\n    },\\n\\nIs there any intention to expand on this, adding timestamps, actual usernames, perhaps even information about the submission, etc. any time soon?\\n\\n\\n\\n[Relevant pull request](https://github.com/reddit/reddit/pull/371) \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"12xz7x\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Buttscicles\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/12xz7x/useful_moderation_log_api_access/\", \"locked\": false, \"name\": \"t3_12xz7x\", \"created\": 1352539017.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/12xz7x/useful_moderation_log_api_access/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Useful moderation log API access\", \"created_utc\": 1352510217.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have a bunch of subreddits that I added to the automatic_reddits setting. Everything worked as expected except that the top bar only seemed to list one of them, even after running\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esudo start reddit-job-update_reddits\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EEventually I noticed that the one that worked was listed in lower case only in the automatic_reddits setting in run.update and the others had at least one capital letter. I switched them to lower case, then did:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esudo make ini\\nsudo start reddit-job-update_reddits\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAnd then they all appeared. Not just a coincidence; adding a capital letter back to one of the names made it disappear again.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESomething is case sensitive that should not be, but this workaround is good enough for me.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have a bunch of subreddits that I added to the automatic_reddits setting. Everything worked as expected except that the top bar only seemed to list one of them, even after running\\n\\n    sudo start reddit-job-update_reddits\\n\\nEventually I noticed that the one that worked was listed in lower case only in the automatic_reddits setting in run.update and the others had at least one capital letter. I switched them to lower case, then did:\\n\\n    sudo make ini\\n    sudo start reddit-job-update_reddits\\n\\nAnd then they all appeared. Not just a coincidence; adding a capital letter back to one of the names made it disappear again.\\n\\nSomething is case sensitive that should not be, but this workaround is good enough for me.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"11yttx\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"DontTreadOnMe\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/11yttx/top_bar_how_i_made_it_work/\", \"locked\": false, \"name\": \"t3_11yttx\", \"created\": 1351054762.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/11yttx/top_bar_how_i_made_it_work/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Top bar: how I made it work\", \"created_utc\": 1351025962.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}], \"after\": \"t3_11yttx\", \"before\": null}}"
        },
        "headers": {
          "CF-RAY": "2b59b9c8369020c6-LAX",
          "Connection": "keep-alive",
          "Content-Type": "application/json; charset=UTF-8",
          "Date": "Sun, 19 Jun 2016 20:28:09 GMT",
          "Server": "cloudflare-nginx",
          "Strict-Transport-Security": "max-age=15552000; includeSubDomains; preload",
          "Transfer-Encoding": "chunked",
          "Vary": "accept-encoding",
          "X-Moose": "majestic",
          "cache-control": "private, s-maxage=0, max-age=0, must-revalidate",
          "expires": "-1",
          "x-content-type-options": "nosniff",
          "x-frame-options": "SAMEORIGIN",
          "x-ratelimit-remaining": "580.0",
          "x-ratelimit-reset": "112",
          "x-ratelimit-used": "20",
          "x-reddit-tracking": "https://pixel.redditmedia.com/pixel/of_destiny.png?v=Khz8raLcJ8TIXMYYwRE0iBavIxh%2FbtX0MJdRKLAeDhaPGui5MfMp5CRrQS7cvsFEKWRubYW7O3185kKrqSuHY9DTtFSpoyNV",
          "x-ua-compatible": "IE=edge",
          "x-xss-protection": "1; mode=block"
        },
        "status": {
          "code": 200,
          "message": "OK"
        },
        "url": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_424c7w&raw_json=1"
      }
    },
    {
      "recorded_at": "2016-06-19T20:28:10",
      "request": {
        "body": {
          "encoding": "utf-8",
          "string": ""
        },
        "headers": {
          "Accept": "*/*",
          "Accept-Encoding": "identity",
          "Authorization": "bearer YE4ESXZ9lTjIP0ukcopwXM6361g",
          "Connection": "keep-alive",
          "Cookie": "loid=SdAZIdDQoUDzjNqfRk; loidcreated=2016-06-19T20%3A27%3A59.507Z; __cfduid=da3dbedd6bfa050f4f12fea14395b830f1466368079",
          "User-Agent": "prawtools/0.19 PRAW/4.0.0b4 prawcore/0.0.8"
        },
        "method": "GET",
        "uri": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_11yttx&raw_json=1"
      },
      "response": {
        "body": {
          "encoding": "UTF-8",
          "string": "{\"kind\": \"Listing\", \"data\": {\"modhash\": null, \"children\": [{\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am working on a \\u003Ca href=\\\"http://www.reddit.com/r/test/comments/11iby2/bitcointip_tip_redditors_with_bitcoin/\\\"\\u003Ebitcoin tipping bot\\u003C/a\\u003E and have a couple questions about optimizing it.  Once the bot is working again, I will pay responders if they are helpful.  The more helpful, the larger the tip.\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EWhat is the best way to get comments from a large group of users?  My bot scans its users\\u0026#39; comments for a command.  Right now it goes through the list of users and gets the contents of \\u0026quot;\\u003Ca href=\\\"http://www.reddit.com/user/$username/comments.json\\\"\\u003Ehttp://www.reddit.com/user/$username/comments.json\\u003C/a\\u003E\\u0026quot; for each user.  With reddit\\u0026#39;s limit of 2 requests/sec, 5,000 users might take 3 hours to go through even if none of them use the command.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there a better way?\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EWhen someone does a tip like \\u0026quot;+bitcointip $1\\u0026quot;, the bot doesn\\u0026#39;t know who the tip is to, so it has to look at the \\u0026quot;parent_id\\u0026quot; of that comment, then get all the comments in the entire post, and then search the array of comments for the \\u0026quot;author\\u0026quot; of the comment with the same \\u0026quot;name\\u0026quot; as the original comment\\u0026#39;s \\u0026quot;parent_id\\u0026quot;.  This is fine for posts with a small number of comments, but in posts with a large number of comments \\u0026quot;\\u003Ca href=\\\"http://www.reddit.com/comments/$link_id.json\\\"\\u003Ehttp://www.reddit.com/comments/$link_id.json\\u003C/a\\u003E\\u0026quot; does not return all of the comments.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there a better way to get the \\u0026quot;author\\u0026quot; of a known \\u0026quot;parent_id\\u0026quot;?\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am working on a [bitcoin tipping bot](http://www.reddit.com/r/test/comments/11iby2/bitcointip_tip_redditors_with_bitcoin/) and have a couple questions about optimizing it.  Once the bot is working again, I will pay responders if they are helpful.  The more helpful, the larger the tip.\\n\\n1. What is the best way to get comments from a large group of users?  My bot scans its users' comments for a command.  Right now it goes through the list of users and gets the contents of \\\"http://www.reddit.com/user/$username/comments.json\\\" for each user.  With reddit's limit of 2 requests/sec, 5,000 users might take 3 hours to go through even if none of them use the command.  \\n\\n Is there a better way?\\n\\n\\n\\n2.  When someone does a tip like \\\"+bitcointip $1\\\", the bot doesn't know who the tip is to, so it has to look at the \\\"parent_id\\\" of that comment, then get all the comments in the entire post, and then search the array of comments for the \\\"author\\\" of the comment with the same \\\"name\\\" as the original comment's \\\"parent_id\\\".  This is fine for posts with a small number of comments, but in posts with a large number of comments \\\"http://www.reddit.com/comments/$link_id.json\\\" does not return all of the comments.\\n\\n Is there a better way to get the \\\"author\\\" of a known \\\"parent_id\\\"?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"11t7l4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"NerdfighterSean\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/11t7l4/optimizing_comments_retrieval_will_tip_helpful/\", \"locked\": false, \"name\": \"t3_11t7l4\", \"created\": 1350792040.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/11t7l4/optimizing_comments_retrieval_will_tip_helpful/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Optimizing comments retrieval (will tip helpful responses)\", \"created_utc\": 1350763240.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAre there any pre-existing tools/scripts for scraping an entire subreddit? By which I mean, returning all of the textual information (not retrieving images) for each post, along with the nested comments?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Are there any pre-existing tools/scripts for scraping an entire subreddit? By which I mean, returning all of the textual information (not retrieving images) for each post, along with the nested comments?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"11sej5\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"beslayed\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/11sej5/scraping_entire_text_of_a_subreddit/\", \"locked\": false, \"name\": \"t3_11sej5\", \"created\": 1350742628.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/11sej5/scraping_entire_text_of_a_subreddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Scraping entire text of a subreddit?\", \"created_utc\": 1350713828.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m working on a radio/ music player for reddit.  I\\u0026#39;m hoping to increase activity in my subreddit by playing the song while showing the original reddit post (and associated comments) in an \\u0026lt;iframe\\u0026gt;.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe display works fine, as you\\u0026#39;d expect, but all comments and votes submitted from within the iframe are ignored.  (Voting appears to work since the UI is all client-side but comments don\\u0026#39;t get past the \\u0026quot;saving\\u0026quot; dialog.)  I can only assume the AJAX calls aren\\u0026#39;t being made, or reddit is ignoring them since they\\u0026#39;re coming from a different domain.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there a way around this or is this a limitation set by reddit to prevent just this sort of thing?  Any tips or hints are much appreciated.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm working on a radio/ music player for reddit.  I'm hoping to increase activity in my subreddit by playing the song while showing the original reddit post (and associated comments) in an \\u003Ciframe\\u003E.\\n\\nThe display works fine, as you'd expect, but all comments and votes submitted from within the iframe are ignored.  (Voting appears to work since the UI is all client-side but comments don't get past the \\\"saving\\\" dialog.)  I can only assume the AJAX calls aren't being made, or reddit is ignoring them since they're coming from a different domain.\\n\\nIs there a way around this or is this a limitation set by reddit to prevent just this sort of thing?  Any tips or hints are much appreciated.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"10ms07\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"listentous\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/10ms07/is_it_possible_to_vote_comment_on_a_reddit_post/\", \"locked\": false, \"name\": \"t3_10ms07\", \"created\": 1348887887.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/10ms07/is_it_possible_to_vote_comment_on_a_reddit_post/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is it possible to vote/ comment on a reddit post in an \\u003Ciframe\\u003E?\", \"created_utc\": 1348859087.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EOne thing I\\u0026#39;ve noticed is that when I do an API call to get comments, using, the comments api (/comments/id/.json) get returned as a \\u0026#39;tree\\u0026#39;. Meaning replies for a given comment are in the \\u0026#39;replies\\u0026#39; object.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf I do a request for /api/morechildren, however, the result is a \\u0026#39;flat\\u0026#39; array where all the \\u0026#39;more children\\u0026#39; are in the first level of the array, and I must look at the parent property to figure out where it goes.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe above leads to a bit of messier code than is needed. Am I overlooking something, and there is a way to get the morecomments similar to how I get them in the initial request (meaning I get back a comment with replies in the replies property, etc)? Or is the above expected?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"One thing I've noticed is that when I do an API call to get comments, using, the comments api (/comments/id/.json) get returned as a 'tree'. Meaning replies for a given comment are in the 'replies' object.\\n\\nIf I do a request for /api/morechildren, however, the result is a 'flat' array where all the 'more children' are in the first level of the array, and I must look at the parent property to figure out where it goes.\\n\\nThe above leads to a bit of messier code than is needed. Am I overlooking something, and there is a way to get the morecomments similar to how I get them in the initial request (meaning I get back a comment with replies in the replies property, etc)? Or is the above expected?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"10llwa\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"appbuilderman\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/10llwa/why_the_discrepancy_between_comments_and_more/\", \"locked\": false, \"name\": \"t3_10llwa\", \"created\": 1348829292.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/10llwa/why_the_discrepancy_between_comments_and_more/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why the discrepancy between comments and more comments type?\", \"created_utc\": 1348800492.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EOur initial OAuth 2 examples used \\u003Ca href=\\\"https://oauth.reddit.com\\\"\\u003Ehttps://oauth.reddit.com\\u003C/a\\u003E for everything. We originally intended this domain to support the authorize and access_token API calls, in addition to all OAuth 2 calls (i.e., calls with bearer tokens). However, this doesn\\u0026#39;t make for a good user interface and complicates our haproxy configs.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFrom this point on, use \\u003Ca href=\\\"https://oauth.reddit.com\\\"\\u003Ehttps://oauth.reddit.com\\u003C/a\\u003E only for API calls with bearer tokens. Any other request will be rejected with a 403 error. Use \\u003Ca href=\\\"https://ssl.reddit.com\\\"\\u003Ehttps://ssl.reddit.com\\u003C/a\\u003E for the authorization URL to send users to, and for the access_token API call you make to obtain the bearer token.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Our initial OAuth 2 examples used https://oauth.reddit.com for everything. We originally intended this domain to support the authorize and access_token API calls, in addition to all OAuth 2 calls (i.e., calls with bearer tokens). However, this doesn't make for a good user interface and complicates our haproxy configs.\\n\\nFrom this point on, use https://oauth.reddit.com only for API calls with bearer tokens. Any other request will be rejected with a 403 error. Use https://ssl.reddit.com for the authorization URL to send users to, and for the access_token API call you make to obtain the bearer token.\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"107sl5\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"intortus\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/107sl5/api_notice_use_sslredditcom_for_apiv1authorize/\", \"locked\": false, \"name\": \"t3_107sl5\", \"created\": 1348207984.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/107sl5/api_notice_use_sslredditcom_for_apiv1authorize/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[api notice] Use ssl.reddit.com for /api/v1/authorize and /api/v1/access_token, oauth.reddit.com for requests with bearer tokens only\", \"created_utc\": 1348179184.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/dev/api#POST_api_site_admin\\\"\\u003EHere is the API I\\u0026#39;m using.\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnd here is my code (Ruby)\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E# Update the subreddit description\\nuri = URI(\\u0026#39;http://www.reddit.com/api/site_admin\\u0026#39;)\\nreq = Net::HTTP::Post.new(uri.path)\\nreq[\\u0026#39;cookie\\u0026#39;] = \\u0026quot;reddit_session= \\u0026quot; + session\\nreq.set_form_data(\\u0026#39;api_type\\u0026#39; =\\u0026gt; \\u0026#39;json\\u0026#39;,            \\n            \\u0026#39;allow_top\\u0026#39; =\\u0026gt; config.get_value(\\u0026#39;allow_top\\u0026#39;),\\n            \\u0026#39;css_on_cname\\u0026#39; =\\u0026gt; config.get_value(\\u0026#39;css_on_cname\\u0026#39;),\\n            \\u0026#39;description\\u0026#39; =\\u0026gt; description_string,                  \\n            \\u0026#39;header_title\\u0026#39; =\\u0026gt; config.get_value(\\u0026#39;header_title\\u0026#39;),             \\n            \\u0026#39;lang\\u0026#39; =\\u0026gt; config.get_value(\\u0026#39;language\\u0026#39;),\\n            \\u0026#39;link_type\\u0026#39; =\\u0026gt; config.get_value(\\u0026#39;link_type\\u0026#39;),          \\n            \\u0026#39;name\\u0026#39; =\\u0026gt; config.get_value(\\u0026#39;subbreddit_name\\u0026#39;),\\n            \\u0026#39;over_18\\u0026#39; =\\u0026gt; config.get_value(\\u0026#39;over_18\\u0026#39;),\\n            \\u0026#39;show_cname_sidebar\\u0026#39; =\\u0026gt; config.get_value(\\u0026#39;show_cname_sidebar\\u0026#39;),\\n            \\u0026#39;show_media\\u0026#39; =\\u0026gt; config.get_value(\\u0026#39;show_media\\u0026#39;),\\n            \\u0026#39;sr\\u0026#39; =\\u0026gt; config.get_value(\\u0026#39;subreddit_thing\\u0026#39;),\\n            \\u0026#39;title\\u0026#39; =\\u0026gt; config.get_value(\\u0026#39;page_title\\u0026#39;),\\n            \\u0026#39;type\\u0026#39; =\\u0026gt; config.get_value(\\u0026#39;subreddit_privacy\\u0026#39;),\\n            \\u0026#39;domain\\u0026#39; =\\u0026gt; config.get_value(\\u0026#39;domain\\u0026#39;),\\n            \\u0026#39;wikimode\\u0026#39; =\\u0026gt; config.get_value(\\u0026#39;wikimode\\u0026#39;),\\n            \\u0026#39;uh\\u0026#39; =\\u0026gt; modhash)\\n\\nres = Net::HTTP.start(uri.hostname, uri.port) do |http|\\n    http.request(req)\\nend\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIt was working perfectly fine before until the wiki features were added. I updated the wikimode field, and now I\\u0026#39;m getting the following error:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E[[\\u0026quot;CONFLICT\\u0026quot;, \\u0026quot;conflict error while saving\\u0026quot;, \\u0026quot;description\\u0026quot;]]\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve browsed through the Reddit source for a while, couldn\\u0026#39;t find anything that explained what a conflict error was, or how to resolve it. My thoughts are that maybe there are some banned characters, or I\\u0026#39;m not using valid markup, but I haven\\u0026#39;t figured it out yet.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe description string is usually something along these lines:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E[Please read our posting guidelines.](http://www.reddit.com/r/CalgarySocialClub/comments/t3tj6/posting_guidelines/)\\n\\nSun|Mon|Tue|Wed|Thu|Fri|Sat\\n:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:\\n~~[26](http://goo.gl/sXbff)[^1](http://goo.gl/B7H4p)~~|~~[27](http://goo.gl/jf4DS)~~|~~[28](http://goo.gl/mt2Dc)[^1](http://goo.gl/5Wh8j)~~|~~29~~|~~30~~|~~31~~|~~1~~\\n~~[2](http://goo.gl/Sjws8)~~|~~3~~|~~[4](http://goo.gl/qv1on)[^1](http://goo.gl/BptSm)[^2](http://goo.gl/MgMT8)~~|~~5~~|~~6~~|~~7~~|~~[8](http://goo.gl/Fcdhk)~~\\n~~[9](http://goo.gl/DS1aW)~~|~~10~~|~~[11](http://goo.gl/983sn)~~|~~12~~|~~13~~|~~14~~|~~15~~\\n~~[16](http://goo.gl/0tp2p)~~|**17**|18|19|20|21|22\\n23|24|25|26|27|28|29\\n30|1|2|3|4|5|6\\n\\n## Upcoming Events\\n\\n* [Oct 13 - 2012 Calgary Zombie Walk @ Olympic Plaza/City Hall @  1:00 PM](http://goo.gl/Plqxh)\\n\\n[Twitter](http://twitter.com/YYCSocialClub)\\n[Google Plus](https://plus.google.com/117956267917340211251)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EFrom that, the markdown looks valid and renders properly, so I don\\u0026#39;t know where to go from here.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"[Here is the API I'm using.](http://www.reddit.com/dev/api#POST_api_site_admin)\\n\\nAnd here is my code (Ruby)\\n\\n    # Update the subreddit description\\n    uri = URI('http://www.reddit.com/api/site_admin')\\n    req = Net::HTTP::Post.new(uri.path)\\n    req['cookie'] = \\\"reddit_session= \\\" + session\\n    req.set_form_data('api_type' =\\u003E 'json',            \\n                'allow_top' =\\u003E config.get_value('allow_top'),\\n                'css_on_cname' =\\u003E config.get_value('css_on_cname'),\\n                'description' =\\u003E description_string,                  \\n                'header_title' =\\u003E config.get_value('header_title'),             \\n                'lang' =\\u003E config.get_value('language'),\\n                'link_type' =\\u003E config.get_value('link_type'),          \\n                'name' =\\u003E config.get_value('subbreddit_name'),\\n                'over_18' =\\u003E config.get_value('over_18'),\\n                'show_cname_sidebar' =\\u003E config.get_value('show_cname_sidebar'),\\n                'show_media' =\\u003E config.get_value('show_media'),\\n                'sr' =\\u003E config.get_value('subreddit_thing'),\\n                'title' =\\u003E config.get_value('page_title'),\\n                'type' =\\u003E config.get_value('subreddit_privacy'),\\n                'domain' =\\u003E config.get_value('domain'),\\n                'wikimode' =\\u003E config.get_value('wikimode'),\\n                'uh' =\\u003E modhash)\\n\\n    res = Net::HTTP.start(uri.hostname, uri.port) do |http|\\n        http.request(req)\\n    end\\n\\n\\nIt was working perfectly fine before until the wiki features were added. I updated the wikimode field, and now I'm getting the following error:\\n\\n    [[\\\"CONFLICT\\\", \\\"conflict error while saving\\\", \\\"description\\\"]]\\n\\nI've browsed through the Reddit source for a while, couldn't find anything that explained what a conflict error was, or how to resolve it. My thoughts are that maybe there are some banned characters, or I'm not using valid markup, but I haven't figured it out yet.\\n\\nThe description string is usually something along these lines:\\n\\n\\n    [Please read our posting guidelines.](http://www.reddit.com/r/CalgarySocialClub/comments/t3tj6/posting_guidelines/)\\n\\n    Sun|Mon|Tue|Wed|Thu|Fri|Sat\\n    :-----------:|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:\\n    ~~[26](http://goo.gl/sXbff)[^1](http://goo.gl/B7H4p)~~|~~[27](http://goo.gl/jf4DS)~~|~~[28](http://goo.gl/mt2Dc)[^1](http://goo.gl/5Wh8j)~~|~~29~~|~~30~~|~~31~~|~~1~~\\n    ~~[2](http://goo.gl/Sjws8)~~|~~3~~|~~[4](http://goo.gl/qv1on)[^1](http://goo.gl/BptSm)[^2](http://goo.gl/MgMT8)~~|~~5~~|~~6~~|~~7~~|~~[8](http://goo.gl/Fcdhk)~~\\n    ~~[9](http://goo.gl/DS1aW)~~|~~10~~|~~[11](http://goo.gl/983sn)~~|~~12~~|~~13~~|~~14~~|~~15~~\\n    ~~[16](http://goo.gl/0tp2p)~~|**17**|18|19|20|21|22\\n    23|24|25|26|27|28|29\\n    30|1|2|3|4|5|6\\n\\n    ## Upcoming Events\\n\\n    * [Oct 13 - 2012 Calgary Zombie Walk @ Olympic Plaza/City Hall @  1:00 PM](http://goo.gl/Plqxh)\\n\\n    [Twitter](http://twitter.com/YYCSocialClub)\\n    [Google Plus](https://plus.google.com/117956267917340211251)\\n\\n\\nFrom that, the markdown looks valid and renders properly, so I don't know where to go from here.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"100zp1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/100zp1/since_the_update_on_sept_11_my_bot_to_update_the/\", \"locked\": false, \"name\": \"t3_100zp1\", \"created\": 1347926485.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/100zp1/since_the_update_on_sept_11_my_bot_to_update_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Since the update on Sept 11, my bot to update the sidebar is getting a 'conflict error' on the description field when using /api/site_admin \", \"created_utc\": 1347897685.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIs there a large list of all 8000+ subreddits? We need to take a random weighted sample of subreddits, and it would help if we had all the subreddits and their numbers of subscribers.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI checked online, and the best I could find was a \\u0026quot;Wall of Subreddits\\u0026quot; which only had around 3000.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Is there a large list of all 8000+ subreddits? We need to take a random weighted sample of subreddits, and it would help if we had all the subreddits and their numbers of subscribers.\\n\\nI checked online, and the best I could find was a \\\"Wall of Subreddits\\\" which only had around 3000.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"zyp17\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MrFanzyPanz\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/zyp17/listosubreddits/\", \"locked\": false, \"name\": \"t3_zyp17\", \"created\": 1347808670.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/zyp17/listosubreddits/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"List-O-Subreddits\", \"created_utc\": 1347779870.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EOn Firefox, the submit button has a border-radius on all left corners. The style only exists using the -moz- prefix, so it\\u0026#39;s only visible on that browser engine.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m guessing browser-specific code wasn\\u0026#39;t intended, and I have 2 possible fixes ready to submit as pull requests, but I need to know if that style needs to be removed or if it should be applied to all browsers.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"On Firefox, the submit button has a border-radius on all left corners. The style only exists using the -moz- prefix, so it's only visible on that browser engine.\\n\\nI'm guessing browser-specific code wasn't intended, and I have 2 possible fixes ready to submit as pull requests, but I need to know if that style needs to be removed or if it should be applied to all browsers.\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"zo8rt\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"rolmos\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/zo8rt/a_style_is_only_being_applied_to_firefox_users/\", \"locked\": false, \"name\": \"t3_zo8rt\", \"created\": 1347340931.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/zo8rt/a_style_is_only_being_applied_to_firefox_users/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"A style is only being applied to Firefox users, and to fix it I need to know if it's intended to be displayed for everyone or removed.\", \"created_utc\": 1347312131.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://stuartfeldt.com/r/vis\\\"\\u003Ehttp://stuartfeldt.com/r/vis\\u003C/a\\u003E has been a weekend project for me...  Kind of a rip-off of what digg did ~5 years ago with their labs, and is more of a POC than intended for full scale use.  I would love to see something like \\u0026#39;Reddit Labs\\u0026#39;, where we try and develop crazy new ways of consuming reddit with \\u003Ca href=\\\"http://d3js.org/\\\"\\u003ED3\\u003C/a\\u003E,  \\u003Ca href=\\\"https://developers.google.com/chart/\\\"\\u003EGoogle Charts\\u003C/a\\u003E, etc...\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"[http://stuartfeldt.com/r/vis](http://stuartfeldt.com/r/vis) has been a weekend project for me...  Kind of a rip-off of what digg did ~5 years ago with their labs, and is more of a POC than intended for full scale use.  I would love to see something like 'Reddit Labs', where we try and develop crazy new ways of consuming reddit with [D3](http://d3js.org/),  [Google Charts](https://developers.google.com/chart/), etc...\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"z8pdp\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ugart\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/z8pdp/a_slightly_different_presentation_of_front_pages/\", \"locked\": false, \"name\": \"t3_z8pdp\", \"created\": 1346641914.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/z8pdp/a_slightly_different_presentation_of_front_pages/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"A slightly different presentation of front pages\", \"created_utc\": 1346613114.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIs it possible to teach reddit about some sites\\u0026#39; URI schemes, so it knows which parts of the submitted link are not important and looks for duplicates using only the important parts?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe motivating example would be youtube. Links to youtube often contain extra stuff in the query string that changes nothing but causes reddit to miss a repost. If reddit knows that only the \\u003Ccode\\u003Ev=.{11}\\u003C/code\\u003E part and the \\u003Ccode\\u003E#t=\\\\d+s\\u003C/code\\u003E part are important, de-duplication (and automatic links to same video on different submissions) would be much more useful.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Is it possible to teach reddit about some sites' URI schemes, so it knows which parts of the submitted link are not important and looks for duplicates using only the important parts?\\n\\nThe motivating example would be youtube. Links to youtube often contain extra stuff in the query string that changes nothing but causes reddit to miss a repost. If reddit knows that only the `v=.{11}` part and the `#t=\\\\d+s` part are important, de-duplication (and automatic links to same video on different submissions) would be much more useful.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"w1nef\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"wolf550e\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/w1nef/links_deduplication/\", \"locked\": false, \"name\": \"t3_w1nef\", \"created\": 1341461302.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/w1nef/links_deduplication/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"links de-duplication\", \"created_utc\": 1341432502.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am looking to make my reddit mobile friendly while still having a cool header while browsing on a desktop. I tried to do this with CSS media query, but the CSS editor didn\\u0026#39;t recognize it. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDid I make a typo or is this not supported in reddit? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere is my code: \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E@media screen \\nand (max-width: 480px) { \\ndiv#header-img.default-header {\\ntext-indent: -9999px;\\nbackground-image: url(sprite-reddit.rQ7y8qN-wzQ.png);\\nbackground-position: -0px -258px;\\nbackground-repeat: no-repeat;\\nheight: 40px;\\nwidth: 120px;\\ndisplay: inline-block;\\nvertical-align: bottom;\\nmargin-bottom: 3px;\\n}\\n}\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E@media screen \\nand (max-width: 1152px) { \\u003C/p\\u003E\\n\\n\\u003Ch1\\u003Eheader-img.default-header{\\u003C/h1\\u003E\\n\\n\\u003Cp\\u003Ebackground-image:URL(\\u0026#39;xxxxxxxxxxxxxxxxxxxxx\\u0026#39;);\\ndisplay:block;\\nmargin-left:auto;\\nmargin-right:auto;\\n}\\n}\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E@media screen \\nand (min-width: 1280px) { \\u003C/p\\u003E\\n\\n\\u003Ch1\\u003Eheader-img.default-header{\\u003C/h1\\u003E\\n\\n\\u003Cp\\u003Ebackground-image:URL(\\u0026#39;xxxxxxxxxxxxxxxxxxxxx\\u0026#39;);\\ndisplay:block;\\nmargin-left:auto;\\nmargin-right:auto;\\n}\\n}\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am looking to make my reddit mobile friendly while still having a cool header while browsing on a desktop. I tried to do this with CSS media query, but the CSS editor didn't recognize it. \\n\\nDid I make a typo or is this not supported in reddit? \\n\\nHere is my code: \\n\\n@media screen \\nand (max-width: 480px) { \\ndiv#header-img.default-header {\\ntext-indent: -9999px;\\nbackground-image: url(sprite-reddit.rQ7y8qN-wzQ.png);\\nbackground-position: -0px -258px;\\nbackground-repeat: no-repeat;\\nheight: 40px;\\nwidth: 120px;\\ndisplay: inline-block;\\nvertical-align: bottom;\\nmargin-bottom: 3px;\\n}\\n}\\n\\n@media screen \\nand (max-width: 1152px) { \\n#header-img.default-header{\\nbackground-image:URL('xxxxxxxxxxxxxxxxxxxxx');\\ndisplay:block;\\nmargin-left:auto;\\nmargin-right:auto;\\n}\\n}\\n\\n@media screen \\nand (min-width: 1280px) { \\n#header-img.default-header{\\nbackground-image:URL('xxxxxxxxxxxxxxxxxxxxx');\\ndisplay:block;\\nmargin-left:auto;\\nmargin-right:auto;\\n}\\n}\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"voh2x\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"LauriKot\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/voh2x/css_media_queries/\", \"locked\": false, \"name\": \"t3_voh2x\", \"created\": 1340823057.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/voh2x/css_media_queries/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"CSS media queries \", \"created_utc\": 1340794257.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI was wondering if there was a way to call subreddit galleries on Imgur ( \\u003Ca href=\\\"http://imgur.com/r/spaceporn\\\"\\u003Ehttp://imgur.com/r/spaceporn\\u003C/a\\u003E ), and how it would be called. I can\\u0026#39;t seem to find it.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I was wondering if there was a way to call subreddit galleries on Imgur ( http://imgur.com/r/spaceporn ), and how it would be called. I can't seem to find it.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ti0wj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Thadav3\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ti0wj/imgur_api_call_for_subreddit_galleries/\", \"locked\": false, \"name\": \"t3_ti0wj\", \"created\": 1336764629.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ti0wj/imgur_api_call_for_subreddit_galleries/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Imgur API call for subreddit galleries?\", \"created_utc\": 1336735829.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIs there a way to find posts from a specific date?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI couldn\\u0026#39;t find this in the API.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cem\\u003EEDIT:\\u003C/em\\u003E I was thinking about finding a post of the specific day and then continue by searching most of the rest of the posts through \\u003Ca href=\\\"http://www.reddit.com/new/?sort=new\\u0026amp;after=id_of_specific_post\\\"\\u003Ehttp://www.reddit.com/new/?sort=new\\u0026amp;after=id_of_specific_post\\u003C/a\\u003E, and updating the \\u0026quot;id_of_specific_post\\u0026quot; by the last post found.\\nExample:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/new/?sort=new\\u0026amp;after=t3_t9ks2\\\"\\u003Ehttp://www.reddit.com/new/?sort=new\\u0026amp;after=t3_t9ks2\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/new/?sort=new\\u0026amp;after=t3_t9j9a\\\"\\u003Ehttp://www.reddit.com/new/?sort=new\\u0026amp;after=t3_t9j9a\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/new/?sort=new\\u0026amp;after=t3_t9hxl\\\"\\u003Ehttp://www.reddit.com/new/?sort=new\\u0026amp;after=t3_t9hxl\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eetc...\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBut this looks a bit stupid if there is an easier way.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Is there a way to find posts from a specific date?\\n\\nI couldn't find this in the API.\\n\\n\\n*EDIT:* I was thinking about finding a post of the specific day and then continue by searching most of the rest of the posts through http://www.reddit.com/new/?sort=new\\u0026after=id_of_specific_post, and updating the \\\"id_of_specific_post\\\" by the last post found.\\nExample:\\n\\n[http://www.reddit.com/new/?sort=new\\u0026after=t3_t9ks2](http://www.reddit.com/new/?sort=new\\u0026after=t3_t9ks2)\\n\\n[http://www.reddit.com/new/?sort=new\\u0026after=t3_t9j9a](http://www.reddit.com/new/?sort=new\\u0026after=t3_t9j9a)\\n\\n[http://www.reddit.com/new/?sort=new\\u0026after=t3_t9hxl](http://www.reddit.com/new/?sort=new\\u0026after=t3_t9hxl)\\n\\netc...\\n\\n\\nBut this looks a bit stupid if there is an easier way.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"tbmpn\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Xochipilli\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/tbmpn/is_there_a_way_to_find_posts_from_a_specific_date/\", \"locked\": false, \"name\": \"t3_tbmpn\", \"created\": 1336448460.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/tbmpn/is_there_a_way_to_find_posts_from_a_specific_date/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a way to find posts from a specific date?\", \"created_utc\": 1336419660.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have my own reddit installation up and running with an admin configured (the \\u0026quot;turn admin on/off\\u0026quot; shows up).  Unfortunately I can\\u0026#39;t figure out how to use it to delete users, reset passwords, or delete subreddits.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs the only way to \\u0026quot;delete\\u0026quot; a subreddit to \\u0026quot;ban\\u0026quot; it?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhen I\\u0026#39;m on a user\\u0026#39;s page (ie /user/username) I see an \\u0026quot;admin\\u0026quot; dropdown tab, but it isn\\u0026#39;t populated with anything.  Did one of my modifications break this, or is there just nothing there?  Is there a way to reset user passwords or delete users?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"\\nI have my own reddit installation up and running with an admin configured (the \\\"turn admin on/off\\\" shows up).  Unfortunately I can't figure out how to use it to delete users, reset passwords, or delete subreddits.\\n\\nIs the only way to \\\"delete\\\" a subreddit to \\\"ban\\\" it?\\n\\nWhen I'm on a user's page (ie /user/username) I see an \\\"admin\\\" dropdown tab, but it isn't populated with anything.  Did one of my modifications break this, or is there just nothing there?  Is there a way to reset user passwords or delete users?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"t8dt2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"RedditSrc4Research\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/t8dt2/admin_delete_users_reset_passwords_and_delete/\", \"locked\": false, \"name\": \"t3_t8dt2\", \"created\": 1336259022.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/t8dt2/admin_delete_users_reset_passwords_and_delete/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Admin delete users, reset passwords, and delete subreddits?\", \"created_utc\": 1336230222.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EDoes anyone know of any libraries that can pull in the thumbnail for a url? Similar to how reddit does it?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Does anyone know of any libraries that can pull in the thumbnail for a url? Similar to how reddit does it?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"sz0qb\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bostonvaulter\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/sz0qb/any_other_libraries_to_pull_in_thumbnail_for_a_url/\", \"locked\": false, \"name\": \"t3_sz0qb\", \"created\": 1335778340.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/sz0qb/any_other_libraries_to_pull_in_thumbnail_for_a_url/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Any other libraries to pull in thumbnail for a url?\", \"created_utc\": 1335749540.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m trying to code a script that download data from reddit. It works quite well, however I \\u003Cem\\u003Ealways\\u003C/em\\u003E get a \\u0026quot;500 error\\u0026quot; when I try to get data from user \\u0026quot;AirRaven\\u0026quot;, and I really don\\u0026#39;t know why.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis error is also reproducible on the website: Go to \\u003Ca href=\\\"http://www.reddit.com/user/AirRaven/liked/?count=400\\u0026amp;limit=100\\u0026amp;after=t3_kz9zj\\\"\\u003Ehttp://www.reddit.com/user/AirRaven/liked/?count=400\\u0026amp;limit=100\\u0026amp;after=t3_kz9zj\\u003C/a\\u003E , scroll down then click next. I always get a \\u0026quot;You broke reddit\\u0026quot; page this way.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI really don\\u0026#39;t know why. Any body else has the same issue? Does any one know a fix? \\n(Note that that I handle errors, etc. so that does not crash my script or anything, it\\u0026#39;s just annoying)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi!\\n\\nI'm trying to code a script that download data from reddit. It works quite well, however I *always* get a \\\"500 error\\\" when I try to get data from user \\\"AirRaven\\\", and I really don't know why.\\n\\nThis error is also reproducible on the website: Go to http://www.reddit.com/user/AirRaven/liked/?count=400\\u0026limit=100\\u0026after=t3_kz9zj , scroll down then click next. I always get a \\\"You broke reddit\\\" page this way.\\n\\nI really don't know why. Any body else has the same issue? Does any one know a fix? \\n(Note that that I handle errors, etc. so that does not crash my script or anything, it's just annoying)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"onjvh\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"quentinms\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/onjvh/i_always_get_a_500_error_when_trying_to_get_data/\", \"locked\": false, \"name\": \"t3_onjvh\", \"created\": 1327024899.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/onjvh/i_always_get_a_500_error_when_trying_to_get_data/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I always get a 500 error when trying to get data from a certain user.\", \"created_utc\": 1326996099.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ECode is \\u003Ca href=\\\"http://pastebin.com/PYa2vnFj\\\"\\u003Ehere\\u003C/a\\u003E. I\\u0026#39;m following the API to a T, but when I make the second call to post_something curl gets a 403. Any help would be much appreciated, as this project is done once I get rid of the damn 403. Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Code is [here](http://pastebin.com/PYa2vnFj). I'm following the API to a T, but when I make the second call to post_something curl gets a 403. Any help would be much appreciated, as this project is done once I get rid of the damn 403. Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"o523l\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Killobyte\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/o523l/anyone_want_to_help_me_figure_out_why_my_php/\", \"locked\": false, \"name\": \"t3_o523l\", \"created\": 1325856576.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/o523l/anyone_want_to_help_me_figure_out_why_my_php/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Anyone want to help me figure out why my php script to change user flair isn't working?\", \"created_utc\": 1325827776.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m using api/morechildren to load more comments, but it will not give me the same comment attributes as the comments I already have.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EExample:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhen loading the comments page with GET using urls similar to \\u003Ca href=\\\"http://www.reddit.com/r/pics/comments/linkid/some_link_title/.json\\\"\\u003Ehttp://www.reddit.com/r/pics/comments/linkid/some_link_title/.json\\u003C/a\\u003E\\nI get comments with the below attributes.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026quot;data\\u0026quot;: {\\n    \\u0026quot;author\\u0026quot;: \\u0026quot;authorname\\u0026quot;, \\n    \\u0026quot;author_flair_css_class\\u0026quot;: null, \\n    \\u0026quot;author_flair_text\\u0026quot;: null, \\n    \\u0026quot;body\\u0026quot;: \\u0026quot;Comment text here\\u0026quot;, \\n    \\u0026quot;body_html\\u0026quot;: \\u0026quot;html version of comment text here\\u0026quot;, \\n    \\u0026quot;created\\u0026quot;: 1325824590.0, \\n    \\u0026quot;created_utc\\u0026quot;: 1325799390.0, \\n    \\u0026quot;downs\\u0026quot;: 0, \\n    \\u0026quot;id\\u0026quot;: \\u0026quot;idstring\\u0026quot;, \\n    \\u0026quot;levenshtein\\u0026quot;: null, \\n    \\u0026quot;likes\\u0026quot;: null, \\n    \\u0026quot;link_id\\u0026quot;: \\u0026quot;t3_linkid\\u0026quot;, \\n    \\u0026quot;name\\u0026quot;: \\u0026quot;t1_idstring\\u0026quot;, \\n    \\u0026quot;parent_id\\u0026quot;: \\u0026quot;t1_parentid\\u0026quot;, \\n    \\u0026quot;replies\\u0026quot;: \\u0026quot;\\u0026quot;, \\n    \\u0026quot;subreddit\\u0026quot;: \\u0026quot;pics\\u0026quot;, \\n    \\u0026quot;subreddit_id\\u0026quot;: \\u0026quot;t5_2qh0u\\u0026quot;, \\n    \\u0026quot;ups\\u0026quot;: 52\\n}, \\n\\u0026quot;kind\\u0026quot;: \\u0026quot;t1\\u0026quot;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EBut when I load more comments using by POST to \\u003Cem\\u003Eapi/morechildren\\u003C/em\\u003E with the parameters \\u003Cem\\u003Echildren, link_id, r and api_type\\u003C/em\\u003E with \\u003Cem\\u003Eapi_type\\u003C/em\\u003E set to json I only get about half of the comment attributes\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026quot;data\\u0026quot;: {\\n    \\u0026quot;content\\u0026quot;: \\u0026quot;alot lot lot of strange content text\\u0026quot;, \\n    \\u0026quot;contentHTML\\u0026quot;: \\u0026quot;html formatted contentText\\u0026quot;, \\n    \\u0026quot;contentText\\u0026quot;: \\u0026quot;the actual comment\\u0026quot;, \\n    \\u0026quot;id\\u0026quot;: \\u0026quot;t1_idstring\\u0026quot;, \\n    \\u0026quot;link\\u0026quot;: \\u0026quot;t3_linkid\\u0026quot;, \\n    \\u0026quot;parent\\u0026quot;: \\u0026quot;t1_parentid\\u0026quot;, \\n    \\u0026quot;replies\\u0026quot;: \\u0026quot;\\u0026quot;\\n}, \\n\\u0026quot;kind\\u0026quot;: \\u0026quot;t1\\u0026quot;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003ETL;DR\\u003C/strong\\u003E How can I get all the comment data (ups, downs, likes, created...) when loading more comments via json?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm using api/morechildren to load more comments, but it will not give me the same comment attributes as the comments I already have.\\n\\nExample:\\n\\nWhen loading the comments page with GET using urls similar to http://www.reddit.com/r/pics/comments/linkid/some_link_title/.json\\nI get comments with the below attributes.\\n\\n    \\\"data\\\": {\\n        \\\"author\\\": \\\"authorname\\\", \\n        \\\"author_flair_css_class\\\": null, \\n        \\\"author_flair_text\\\": null, \\n        \\\"body\\\": \\\"Comment text here\\\", \\n        \\\"body_html\\\": \\\"html version of comment text here\\\", \\n        \\\"created\\\": 1325824590.0, \\n        \\\"created_utc\\\": 1325799390.0, \\n        \\\"downs\\\": 0, \\n        \\\"id\\\": \\\"idstring\\\", \\n        \\\"levenshtein\\\": null, \\n        \\\"likes\\\": null, \\n        \\\"link_id\\\": \\\"t3_linkid\\\", \\n        \\\"name\\\": \\\"t1_idstring\\\", \\n        \\\"parent_id\\\": \\\"t1_parentid\\\", \\n        \\\"replies\\\": \\\"\\\", \\n        \\\"subreddit\\\": \\\"pics\\\", \\n        \\\"subreddit_id\\\": \\\"t5_2qh0u\\\", \\n        \\\"ups\\\": 52\\n    }, \\n    \\\"kind\\\": \\\"t1\\\"\\n\\nBut when I load more comments using by POST to *api/morechildren* with the parameters *children, link_id, r and api_type* with *api_type* set to json I only get about half of the comment attributes\\n\\n    \\\"data\\\": {\\n        \\\"content\\\": \\\"alot lot lot of strange content text\\\", \\n        \\\"contentHTML\\\": \\\"html formatted contentText\\\", \\n        \\\"contentText\\\": \\\"the actual comment\\\", \\n        \\\"id\\\": \\\"t1_idstring\\\", \\n        \\\"link\\\": \\\"t3_linkid\\\", \\n        \\\"parent\\\": \\\"t1_parentid\\\", \\n        \\\"replies\\\": \\\"\\\"\\n    }, \\n    \\\"kind\\\": \\\"t1\\\"\\n\\n**TL;DR** How can I get all the comment data (ups, downs, likes, created...) when loading more comments via json?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"o4jq0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Lindby\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/o4jq0/is_it_possible_to_get_a_complete_comment_when/\", \"locked\": false, \"name\": \"t3_o4jq0\", \"created\": 1325831430.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/o4jq0/is_it_possible_to_get_a_complete_comment_when/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is it possible to get a complete comment when loading more comments via json?\", \"created_utc\": 1325802630.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m building a websocket stream of new posts at the moment (\\u003Ca href=\\\"http://jsfiddle.net/BFzeM/\\\"\\u003Ehttp://jsfiddle.net/BFzeM/\\u003C/a\\u003E) for a few ideas I have.\\nTo do this I am grabbing new posts from \\u003Ca href=\\\"/r/all/new/\\\"\\u003E/r/all/new/\\u003C/a\\u003E.json?sort=new\\u0026amp;before=t3_? every 7 seconds on the server side, but every now and then it will hit a good 2 mins of not finding any new posts then suddenly they all start coming through and has to catch up.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs it reddits caching that causes this? Or is there something I can adjust that will get the latest data?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm building a websocket stream of new posts at the moment (http://jsfiddle.net/BFzeM/) for a few ideas I have.\\nTo do this I am grabbing new posts from /r/all/new/.json?sort=new\\u0026before=t3_? every 7 seconds on the server side, but every now and then it will hit a good 2 mins of not finding any new posts then suddenly they all start coming through and has to catch up.\\n\\nIs it reddits caching that causes this? Or is there something I can adjust that will get the latest data?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"o3xiu\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"prawnsalad\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/o3xiu/im_noticing_long_periods_where_the_api_returns/\", \"locked\": false, \"name\": \"t3_o3xiu\", \"created\": 1325799802.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/o3xiu/im_noticing_long_periods_where_the_api_returns/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I'm noticing long periods where the API returns the same results every time. Is this due to caching?\", \"created_utc\": 1325771002.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIs there anything resembling a working script for installing Reddit on any linux distribution? All the guides are out of date and the script located at;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://raw.github.com/gist/922144/install-reddit.sh\\\"\\u003Ehttps://raw.github.com/gist/922144/install-reddit.sh\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eis as well.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Is there anything resembling a working script for installing Reddit on any linux distribution? All the guides are out of date and the script located at;\\n\\nhttps://raw.github.com/gist/922144/install-reddit.sh\\n\\nis as well.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"nxyne\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"wantonballbag\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/nxyne/reddit_installation_script/\", \"locked\": false, \"name\": \"t3_nxyne\", \"created\": 1325399211.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/nxyne/reddit_installation_script/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit installation script\", \"created_utc\": 1325370411.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am following the directions on github. I just cloned the git repository and I am now trying to install the python module dependencies.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ewhen I enter:\\n make pyx\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI get an error:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETraceback (most recent call last):\\n  File \\u0026quot;\\u0026lt;string\\u0026gt;\\u0026quot;, line 1, in \\u0026lt;module\\u0026gt;\\nImportError: No module named i18n\\nTraceback (most recent call last):\\n  File \\u0026quot;r2/lib/js.py\\u0026quot;, line 8, in \\u0026lt;module\\u0026gt;\\n    from r2.lib.translation import iter_langs\\nImportError: No module named r2.lib.translation\\nTraceback (most recent call last):\\n  File \\u0026quot;r2/lib/js.py\\u0026quot;, line 8, in \\u0026lt;module\\u0026gt;\\n    from r2.lib.translation import iter_langs\\nImportError: No module named r2.lib.translation\\npython setup.py build_ext --inplace\\nTraceback (most recent call last):\\n  File \\u0026quot;setup.py\\u0026quot;, line 30, in \\u0026lt;module\\u0026gt;\\n    from Cython.Distutils import build_ext\\nImportError: No module named Cython.Distutils\\nmake: *** [build/pyx-buildstamp] Error 1\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have tried uninstalling and reinstalling cython but it hasn\\u0026#39;t helped.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat do I do? Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am following the directions on github. I just cloned the git repository and I am now trying to install the python module dependencies.\\n\\nwhen I enter:\\n make pyx\\n\\nI get an error:\\n\\nTraceback (most recent call last):\\n  File \\\"\\u003Cstring\\u003E\\\", line 1, in \\u003Cmodule\\u003E\\nImportError: No module named i18n\\nTraceback (most recent call last):\\n  File \\\"r2/lib/js.py\\\", line 8, in \\u003Cmodule\\u003E\\n    from r2.lib.translation import iter_langs\\nImportError: No module named r2.lib.translation\\nTraceback (most recent call last):\\n  File \\\"r2/lib/js.py\\\", line 8, in \\u003Cmodule\\u003E\\n    from r2.lib.translation import iter_langs\\nImportError: No module named r2.lib.translation\\npython setup.py build_ext --inplace\\nTraceback (most recent call last):\\n  File \\\"setup.py\\\", line 30, in \\u003Cmodule\\u003E\\n    from Cython.Distutils import build_ext\\nImportError: No module named Cython.Distutils\\nmake: *** [build/pyx-buildstamp] Error 1\\n\\nI have tried uninstalling and reinstalling cython but it hasn't helped.\\n\\nWhat do I do? Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"nvwp3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"franchdressing\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/nvwp3/help_setting_up/\", \"locked\": false, \"name\": \"t3_nvwp3\", \"created\": 1325251786.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/nvwp3/help_setting_up/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Help setting up\", \"created_utc\": 1325222986.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EBefore the URLs changed recently the thumbnail url was based on the post\\u0026#39;s thing ID (data-fullname), but now it looks like it\\u0026#39;s just a random string. Is there any way I can query the thumbnail URL even if the logged in user\\u0026#39;s preferences would normally hide it?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Before the URLs changed recently the thumbnail url was based on the post's thing ID (data-fullname), but now it looks like it's just a random string. Is there any way I can query the thumbnail URL even if the logged in user's preferences would normally hide it?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"m1z5r\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ArbitraryEntity\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/m1z5r/is_there_any_way_to_get_the_thumbnail_url_for_a/\", \"locked\": false, \"name\": \"t3_m1z5r\", \"created\": 1320573140.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/m1z5r/is_there_any_way_to_get_the_thumbnail_url_for_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there any way to get the thumbnail URL for a normally hidden thumbnail?\", \"created_utc\": 1320544340.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI know the API has a \\u0026quot;site_admin\\u0026quot; method that lets you to edit subreddits, but I\\u0026#39;m not sure how the appropriate POST request should be formed. Anyone have any ideas or could you at least point me in the right direction?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENone of the Python reddit API wrappers seem to have this API method implemented :/\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I know the API has a \\\"site_admin\\\" method that lets you to edit subreddits, but I'm not sure how the appropriate POST request should be formed. Anyone have any ideas or could you at least point me in the right direction?\\n\\nNone of the Python reddit API wrappers seem to have this API method implemented :/\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"l8ydv\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"dfritter4\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/l8ydv/editing_a_subreddits_description_in_the_sidebar/\", \"locked\": false, \"name\": \"t3_l8ydv\", \"created\": 1318408734.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/l8ydv/editing_a_subreddits_description_in_the_sidebar/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Editing a subreddit's description (in the sidebar) via the API\", \"created_utc\": 1318379934.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello - \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have the reddit codebase running for a project of mine - and have also split up the separate components into different machines/roles, for future growth.  I am also using Amazon EC2, the same as reddit.  I know that it\\u0026#39;s easy to handle low traffic, but things get tough when it starts ramping up.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo can you give some insight into how your servers are split up, and how many there are?  Quick question, how many app servers (uwsgi+nginx) are you using and what Amazon machine size?  And then, how many memcached machines and what size?  And a similar question for cassandra?  How many machines do you have for rabbitmq processing - one for each queue?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks - this will just give me some insight as to what I should be aiming towards.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello - \\n\\nI have the reddit codebase running for a project of mine - and have also split up the separate components into different machines/roles, for future growth.  I am also using Amazon EC2, the same as reddit.  I know that it's easy to handle low traffic, but things get tough when it starts ramping up.\\n\\nSo can you give some insight into how your servers are split up, and how many there are?  Quick question, how many app servers (uwsgi+nginx) are you using and what Amazon machine size?  And then, how many memcached machines and what size?  And a similar question for cassandra?  How many machines do you have for rabbitmq processing - one for each queue?\\n\\nThanks - this will just give me some insight as to what I should be aiming towards.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"l0pq6\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"reddit_dev_questions\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/l0pq6/questions_on_the_current_reddit_server_capacity/\", \"locked\": false, \"name\": \"t3_l0pq6\", \"created\": 1317778828.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/l0pq6/questions_on_the_current_reddit_server_capacity/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Questions on the current reddit server capacity\", \"created_utc\": 1317750028.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo I really like the reddit software, and after a couple of tweaks to finally get everything from search to embed.ly, I got it to work. But I have one question.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHow would one get the traffic functionality to work? Is it just proprietary code that we can\\u0026#39;t have and we have to make a replacement?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So I really like the reddit software, and after a couple of tweaks to finally get everything from search to embed.ly, I got it to work. But I have one question.\\n\\nHow would one get the traffic functionality to work? Is it just proprietary code that we can't have and we have to make a replacement?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"kz5vd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"onekopaka\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/kz5vd/getting_traffic_working/\", \"locked\": false, \"name\": \"t3_kz5vd\", \"created\": 1317656074.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/kz5vd/getting_traffic_working/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Getting Traffic working?\", \"created_utc\": 1317627274.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E(Background: developing on webOS)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHow do I log out of reddit? A call to /api/login sets a cookie from the server via the Set-Cookie header, and that\\u0026#39;s great. However, as my app is in a different domain, I can\\u0026#39;t seemingly tamper with the cookies for another site. So, I want to get reddit to issue a \\u0026quot;Set-Cookie\\u0026quot; command to wipe my reddit_session cookie for me. Is there a simple API call that can do this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"(Background: developing on webOS)\\n\\nHow do I log out of reddit? A call to /api/login sets a cookie from the server via the Set-Cookie header, and that's great. However, as my app is in a different domain, I can't seemingly tamper with the cookies for another site. So, I want to get reddit to issue a \\\"Set-Cookie\\\" command to wipe my reddit_session cookie for me. Is there a simple API call that can do this?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"kc5bl\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"sargant\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/kc5bl/how_to_logout_of_reddit_using_the_api/\", \"locked\": false, \"name\": \"t3_kc5bl\", \"created\": 1315787886.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/kc5bl/how_to_logout_of_reddit_using_the_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to logout of reddit using the API?\", \"created_utc\": 1315759086.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis call will load more comments at the bottom of the page:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E$(\\u0026#39;.morecomments a:last\\u0026#39;).click();\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EWhich you can run from the address bar like so:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ejavascript:(function(){$(\\u0026#39;.morecomments a:last\\u0026#39;).click();})();\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EBut using jQuery you can also set up an event watcher to do this as you scroll:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ejavascript:(function(){$(document).ready(function () { $(window).scroll(function () { if ($(window).scrollTop() == $(document).height() - $(window).height()) { $(\\u0026#39;.morecomments a:last\\u0026#39;).click(); } }); });})();\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI also turned this into a \\u003Ca href=\\\"http://dl.dropbox.com/u/16366/reddit_more_comments.user.js\\\"\\u003Euser javascript\\u003C/a\\u003E (tested and working on Opera) - more comments loaded automatically! :)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This call will load more comments at the bottom of the page:\\r\\n\\r\\n    $('.morecomments a:last').click();\\r\\n\\r\\nWhich you can run from the address bar like so:\\r\\n\\r\\n    javascript:(function(){$('.morecomments a:last').click();})();\\r\\n\\r\\nBut using jQuery you can also set up an event watcher to do this as you scroll:\\r\\n\\r\\n    javascript:(function(){$(document).ready(function () { $(window).scroll(function () { if ($(window).scrollTop() == $(document).height() - $(window).height()) { $('.morecomments a:last').click(); } }); });})();\\r\\n\\r\\nI also turned this into a [user javascript](http://dl.dropbox.com/u/16366/reddit_more_comments.user.js) (tested and working on Opera) - more comments loaded automatically! :)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"gv4ok\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"DaveChild\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/gv4ok/javascript_to_load_more_comments_when_you_get_to/\", \"locked\": false, \"name\": \"t3_gv4ok\", \"created\": 1303396854.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/gv4ok/javascript_to_load_more_comments_when_you_get_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"JavaScript to load more comments when you get to \\r\\nthe bottom of the page, like Twitter.\", \"created_utc\": 1303368054.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI can use api/read_message to mark messages as read. However, even if I mark every unread message in the user\\u0026#39;s inbox as read, their envelope is still orangered on reddit.com. If they click the orangered envelope, it brings them to a page that says \\u0026quot;there doesn\\u0026#39;t seem to be anything here\\u0026quot;.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnything I can do besides sending a request to the unread page?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I can use api/read_message to mark messages as read. However, even if I mark every unread message in the user's inbox as read, their envelope is still orangered on reddit.com. If they click the orangered envelope, it brings them to a page that says \\\"there doesn't seem to be anything here\\\".\\n\\nAnything I can do besides sending a request to the unread page?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"gp6su\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"meinhyperspeed\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/gp6su/apiread_message_doesnt_affect_orangered_envelope/\", \"locked\": false, \"name\": \"t3_gp6su\", \"created\": 1302742976.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/gp6su/apiread_message_doesnt_affect_orangered_envelope/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"api/read_message doesn't affect orangered envelope on reddit.com?\", \"created_utc\": 1302714176.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ELong story short, We\\u0026#39;ve got everything up and running, however trying to run the above script we find that we don\\u0026#39;t have either this production.ini file or this plugin: --plugin=r2\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI should mention that the subreddits aren\\u0026#39;t displaying in the top bar of reddit. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere\\u0026#39;s the script:\\u003C/p\\u003E\\n\\n\\u003Ch1\\u003E!/bin/bash\\u003C/h1\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ecd ~/reddit/r2\\n/usr/local/bin/saferun /tmp/updatereddits.pid nice /usr/local/bin/paster --plugin=r2 run production.ini r2/lib/sr_pops.py -c \\u0026quot;run()\\u0026quot;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Long story short, We've got everything up and running, however trying to run the above script we find that we don't have either this production.ini file or this plugin: --plugin=r2\\n\\nI should mention that the subreddits aren't displaying in the top bar of reddit. \\n\\nHere's the script:\\n\\n#!/bin/bash\\n\\t\\n\\tcd ~/reddit/r2\\n\\t/usr/local/bin/saferun /tmp/updatereddits.pid nice /usr/local/bin/paster --plugin=r2 run production.ini r2/lib/sr_pops.py -c \\\"run()\\\"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"gdvi0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"slavishmuffin\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/gdvi0/im_trying_to_get_the_subreddits_to_display_on_an/\", \"locked\": false, \"name\": \"t3_gdvi0\", \"created\": 1301429570.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/gdvi0/im_trying_to_get_the_subreddits_to_display_on_an/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I'm trying to get the subreddits to display on an installation of reddit at work. However, nothing happens. Can someone explain to me the update_reddits.sh script to me?\", \"created_utc\": 1301400770.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m getting sick of the reddit toolbar and the increasing number of websites that refuse to work while framed.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI would really like a version of the firefox socialite extension for chrome.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs far as I can see, \\u003Ca href=\\\"http://marvin.cat-v.org/\\\"\\u003EMarvin\\u003C/a\\u003E has been abandoned for over a year, and \\u003Ca href=\\\"http://www.reddit.com/r/Marvin/comments/c2y8a/any_news/c0pv8wb\\\"\\u003Echromakode\\u0026#39;s somthing\\u003C/a\\u003E hasn\\u0026#39;t been touched in 9 months.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs anyone working on anything else, or should I just attempt to make my own version?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: And annoyingly, this got suck in the spam filter, at a time when all the admins are asleep.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm getting sick of the reddit toolbar and the increasing number of websites that refuse to work while framed.\\n\\nI would really like a version of the firefox socialite extension for chrome.\\n\\nAs far as I can see, [Marvin](http://marvin.cat-v.org/) has been abandoned for over a year, and [chromakode's somthing](http://www.reddit.com/r/Marvin/comments/c2y8a/any_news/c0pv8wb) hasn't been touched in 9 months.\\n\\nIs anyone working on anything else, or should I just attempt to make my own version?\\n\\nEdit: And annoyingly, this got suck in the spam filter, at a time when all the admins are asleep.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"fgnef\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"phire\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/fgnef/socialite_extension_for_chrome/\", \"locked\": false, \"name\": \"t3_fgnef\", \"created\": 1297081948.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/fgnef/socialite_extension_for_chrome/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Socialite extension for chrome?\", \"created_utc\": 1297053148.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Cstrong\\u003ETL;DR\\u003C/strong\\u003E \\u2013 Setup new Reddit dev VM. Summary of steps: Used VMWare to create Ubuntu 10.04.1 VM, installed g++, and \\u003Cem\\u003EBeautifulSoup-3.0.8.1\\u003C/em\\u003E. Modified and ran \\u003Ca href=\\\"https://gist.github.com/738525\\\"\\u003Einstall-reddit.sh\\u003C/a\\u003E script. Updated \\u003Cem\\u003Erun.ini\\u003C/em\\u003E. \\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EThis is a follow up to my \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/f36q5/updating_the_vm/\\\"\\u003Epost submitted this past weekend\\u003C/a\\u003E on trying to setup and update the published Reddit development VM. Basically, that was a flop as the published VM is out of date and updating it was simply getting too painful. Instead, I focused on setting up a new VM image for myself based on the latest Ubuntu ISO (10.04.1). Note: this VM was setup using VMWare with the same default user (\\u201creddit\\u201d) and password (\\u201cpassword\\u201d) as the publicly distributed VM.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat follows is (relatively) detailed but streamlined description of the steps taken to get the environment up to a nominal running state. There were many false steps and unnecessary side-tracks taken to get this to work. I\\u0026#39;m fairly sure that I\\u0026#39;ve captured the minimum steps necessary, but of course, let me know if anything is missing or if you would like more information about some of the missteps/issues.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E1) Logged in as \\u201creddit\\u201d initially then set password for \\u201croot\\u201d (I\\u0026#39;m lazy and hate having to always type sudo in front of everything and then having to type the password repeatedly). Remaining steps were performed as \\u201croot\\u201d.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E2) Installed SSH server and then logged in as \\u201croot\\u201d from putty instead (prefer accessing VM using putty rather than the VMware console).\\n    apt-get install openssh-server\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E3) Installed g++\\n    apt-get install g++\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E4) Downloaded BeautifulSoup  3.0.8.1 from \\u003Ca href=\\\"http://www.crummy.com/software/BeautifulSoup/download/\\\"\\u003Ehttp://www.crummy.com/software/BeautifulSoup/download/\\u003C/a\\u003E. Then install was performed using the following commands:\\n    cd /home/reddit\\n    wget \\u003Ca href=\\\"http://www.crummy.com/software/BeautifulSoup/download/3.x/BeautifulSoup-3.0.8.1.tar.gz\\\"\\u003Ehttp://www.crummy.com/software/BeautifulSoup/download/3.x/BeautifulSoup-3.0.8.1.tar.gz\\u003C/a\\u003E\\n    tar -xzf BeautifulSoup.tar.gz\\n    cd BeautifulSoup-3.0.8.1/\\n    python setup.py install\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E5) Edited the \\u003Cem\\u003E/etc/hosts\\u003C/em\\u003E file to add the following line:\\n    127.0.0.1       reddit.local reddit pay.localhost cslowe.local\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E6) Downloaded \\u003Ca href=\\\"https://gist.github.com/738525\\\"\\u003Einstall-reddit.sh\\u003C/a\\u003E to \\u003Cem\\u003E/home/reddit\\u003C/em\\u003E and modified it to retrieve the 0.6.9 version of cassandra\\n    cd /home/reddit\\n    mkdir tmp\\n    cd tmp\\n    wget \\u003Ca href=\\\"https://gist.github.com/gists/738525/download\\\"\\u003Ehttps://gist.github.com/gists/738525/download\\u003C/a\\u003E\\n    tar -xzf download\\n    cd gist738525-26f2a656ecf1417376210e2d172a918e53491d97\\n    sed -e \\u0026quot;s/unstable/06x/g\\u0026quot; install-reddit.sh \\u0026gt; /home/reddit/install-reddit.sh\\n    cd /home/reddit\\n    rm -rfd tmp\\n    chmod 755 install-reddit.sh\\n    ./install-reddit.sh\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E7) Go get some coffee, go to lunch, do some real work, surf Reddit, etc.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E8) Edited the \\u003Cem\\u003Erun.ini\\u003C/em\\u003E file to change the \\u003Cem\\u003Euse_query_cache\\u003C/em\\u003E setting to \\u003Cem\\u003ETrue\\u003C/em\\u003E (Reddit app fails to start otherwise) and other settings to get it start and run nominally.\\n    cd /home/reddit/reddit/r2\\n    rm run.ini\\n    sed -e \\u0026quot;s/use_query_cache = False/use_query_cache = True/g\\u0026quot; -e \\u0026quot;s/domain = localhost/domain = reddit.local/g\\u0026quot; -e \\u0026quot;s/media_domain = localhost/media_domain = reddit.local/g\\u0026quot; -e \\u0026quot;s/*:hc/*:hc:hc/g\\u0026quot; example.ini \\u0026gt; development.ini\\n    vi development.ini\\n(Add the following line after after [DEFAULT])\\n    INDEXTANK_API_URL =\\n(save file and exit vi)\\n    ln -s development.ini run.ini\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E9) Reset ownership of all files to \\u201creddit\\u201d under the \\u003Cem\\u003E/home/reddit\\u003C/em\\u003E home folder\\n    cd /home/reddit\\n    chown -R reddit:reddit *\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E10) Secured the root user (removed password) and rebooted the VM.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAt this point the VM is running and is externally accessible. There are no sub-reddits defined, so I\\u0026#39;m still working on how to post content, but I was able to register and log in, so that much is working. I\\u0026#39;m currently researching how to load some test data to get going. If anyone has any insight on this it would be greatly appreciated.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESpecial thanks to \\u003Ca href=\\\"http://www.reddit.com/user/spladug\\\"\\u003Espladug\\u003C/a\\u003E for his assistance and pointing out the \\u003Ca href=\\\"https://gist.github.com/738525\\\"\\u003Einstall-reddit.sh\\u003C/a\\u003E script.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ELastly, I can\\u0026#39;t say I appreciate the Reddit servers being started by way of svscan in a development environment. Since I had many issues getting it to work, having it automatically started (and restarted) became a pain since the only way I found of killing it was to reboot or switch to single-user mode. I\\u0026#39;ve therefor removed the \\u003Cem\\u003Ereddit-app01\\u003C/em\\u003E and \\u003Cem\\u003Ereddit-app02\\u003C/em\\u003E links from the \\u003Cem\\u003E/etc/service\\u003C/em\\u003E folder. Perhaps once things are more stable these links will be restored.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: formatting\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT:\\u003C/strong\\u003E Not sure how I missed this originally, but I noticed yesterday that the Cassandra service wasn\\u0026#39;t starting automatically. I found it necessary to modify the \\u0026quot;\\u003Cem\\u003E/home/reddit/reddit/srv/cassandra/run\\u003C/em\\u003E\\u0026quot; script to get it to work. In my environment the executable for Cassandra 0.6.9 was found in the \\u0026quot;\\u003Cem\\u003E/usr/sbin/\\u003C/em\\u003E\\u0026quot; where the \\u003Cem\\u003Erun\\u003C/em\\u003E script was trying to locate it in the \\u0026quot;\\u003Cem\\u003E/usr/local/cassandra/bin\\u003C/em\\u003E\\u0026quot; folder which doesn\\u0026#39;t exist. A simple change to the script results in Cassandra starting up normally (commented out the \\u0026quot;cd\\u0026quot; line and removed the \\u0026quot;\\u003Cem\\u003Ebin/\\u003C/em\\u003E\\u0026quot; prefix from \\u003Cem\\u003Eexec\\u003C/em\\u003E line).\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"**TL;DR** \\u2013 Setup new Reddit dev VM. Summary of steps: Used VMWare to create Ubuntu 10.04.1 VM, installed g++, and *BeautifulSoup-3.0.8.1*. Modified and ran [install-reddit.sh](https://gist.github.com/738525) script. Updated *run.ini*. \\n\\n-----------------------------------------------------------\\n\\nThis is a follow up to my [post submitted this past weekend](http://www.reddit.com/r/redditdev/comments/f36q5/updating_the_vm/) on trying to setup and update the published Reddit development VM. Basically, that was a flop as the published VM is out of date and updating it was simply getting too painful. Instead, I focused on setting up a new VM image for myself based on the latest Ubuntu ISO (10.04.1). Note: this VM was setup using VMWare with the same default user (\\u201creddit\\u201d) and password (\\u201cpassword\\u201d) as the publicly distributed VM.\\n\\nWhat follows is (relatively) detailed but streamlined description of the steps taken to get the environment up to a nominal running state. There were many false steps and unnecessary side-tracks taken to get this to work. I'm fairly sure that I've captured the minimum steps necessary, but of course, let me know if anything is missing or if you would like more information about some of the missteps/issues.\\n\\n1) Logged in as \\u201creddit\\u201d initially then set password for \\u201croot\\u201d (I'm lazy and hate having to always type sudo in front of everything and then having to type the password repeatedly). Remaining steps were performed as \\u201croot\\u201d.\\n\\n2) Installed SSH server and then logged in as \\u201croot\\u201d from putty instead (prefer accessing VM using putty rather than the VMware console).\\n\\tapt-get install openssh-server\\n\\n3) Installed g++\\n    apt-get install g++\\n\\n4) Downloaded BeautifulSoup  3.0.8.1 from [http://www.crummy.com/software/BeautifulSoup/download/](http://www.crummy.com/software/BeautifulSoup/download/). Then install was performed using the following commands:\\n    cd /home/reddit\\n    wget http://www.crummy.com/software/BeautifulSoup/download/3.x/BeautifulSoup-3.0.8.1.tar.gz\\n    tar -xzf BeautifulSoup.tar.gz\\n    cd BeautifulSoup-3.0.8.1/\\n    python setup.py install\\n\\n5) Edited the */etc/hosts* file to add the following line:\\n    127.0.0.1       reddit.local reddit pay.localhost cslowe.local\\n\\t\\n6) Downloaded [install-reddit.sh](https://gist.github.com/738525) to */home/reddit* and modified it to retrieve the 0.6.9 version of cassandra\\n    cd /home/reddit\\n    mkdir tmp\\n    cd tmp\\n    wget https://gist.github.com/gists/738525/download\\n    tar -xzf download\\n    cd gist738525-26f2a656ecf1417376210e2d172a918e53491d97\\n    sed -e \\\"s/unstable/06x/g\\\" install-reddit.sh \\u003E /home/reddit/install-reddit.sh\\n    cd /home/reddit\\n    rm -rfd tmp\\n    chmod 755 install-reddit.sh\\n    ./install-reddit.sh\\n\\n7) Go get some coffee, go to lunch, do some real work, surf Reddit, etc.\\n\\n8) Edited the *run.ini* file to change the *use_query_cache* setting to *True* (Reddit app fails to start otherwise) and other settings to get it start and run nominally.\\n    cd /home/reddit/reddit/r2\\n    rm run.ini\\n    sed -e \\\"s/use_query_cache = False/use_query_cache = True/g\\\" -e \\\"s/domain = localhost/domain = reddit.local/g\\\" -e \\\"s/media_domain = localhost/media_domain = reddit.local/g\\\" -e \\\"s/\\\\*:hc/\\\\*:hc:hc/g\\\" example.ini \\u003E development.ini\\n    vi development.ini\\n(Add the following line after after [DEFAULT])\\n    INDEXTANK_API_URL =\\n(save file and exit vi)\\n    ln -s development.ini run.ini\\n\\n9) Reset ownership of all files to \\u201creddit\\u201d under the */home/reddit* home folder\\n    cd /home/reddit\\n    chown -R reddit:reddit *\\n\\n10) Secured the root user (removed password) and rebooted the VM.\\n\\nAt this point the VM is running and is externally accessible. There are no sub-reddits defined, so I'm still working on how to post content, but I was able to register and log in, so that much is working. I'm currently researching how to load some test data to get going. If anyone has any insight on this it would be greatly appreciated.\\n\\nSpecial thanks to [spladug](http://www.reddit.com/user/spladug) for his assistance and pointing out the [install-reddit.sh](https://gist.github.com/738525) script.\\n\\nLastly, I can't say I appreciate the Reddit servers being started by way of svscan in a development environment. Since I had many issues getting it to work, having it automatically started (and restarted) became a pain since the only way I found of killing it was to reboot or switch to single-user mode. I've therefor removed the *reddit-app01* and *reddit-app02* links from the */etc/service* folder. Perhaps once things are more stable these links will be restored.\\n\\nEDIT: formatting\\n\\n**EDIT:** Not sure how I missed this originally, but I noticed yesterday that the Cassandra service wasn't starting automatically. I found it necessary to modify the \\\"*/home/reddit/reddit/srv/cassandra/run*\\\" script to get it to work. In my environment the executable for Cassandra 0.6.9 was found in the \\\"*/usr/sbin/*\\\" where the *run* script was trying to locate it in the \\\"*/usr/local/cassandra/bin*\\\" folder which doesn't exist. A simple change to the script results in Cassandra starting up normally (commented out the \\\"cd\\\" line and removed the \\\"*bin/*\\\" prefix from *exec* line).\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"f435o\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"vogon_poem_lover\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/f435o/setting_up_a_new_development_vm_followup_to/\", \"locked\": false, \"name\": \"t3_f435o\", \"created\": 1295335988.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/f435o/setting_up_a_new_development_vm_followup_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Setting Up a New Development VM (follow-up to Updating the VM)\", \"created_utc\": 1295307188.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWe\\u0026#39;ve got this bot, anarchystatsbot, that mirrors the mod chat on another page so everyone can see it.  The person who developed it has been inactive for over a year, last night the bot started banning people.  Someone has access to it\\u0026#39;s password, so we need to build another one to secure it.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso, we\\u0026#39;re interested in mirroring the deleted comments so users can see what mod deleted what comments.  And we would also like to mirror an entire sub, \\u003Ca href=\\\"/r/metanarchism\\\"\\u003E/r/metanarchism\\u003C/a\\u003E, if that\\u0026#39;s possible.  The purpose of mirroring it is so that we can keep it private but also allow unapproved submitters to see the discussions.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"We've got this bot, anarchystatsbot, that mirrors the mod chat on another page so everyone can see it.  The person who developed it has been inactive for over a year, last night the bot started banning people.  Someone has access to it's password, so we need to build another one to secure it.\\r\\n\\r\\nAlso, we're interested in mirroring the deleted comments so users can see what mod deleted what comments.  And we would also like to mirror an entire sub, /r/metanarchism, if that's possible.  The purpose of mirroring it is so that we can keep it private but also allow unapproved submitters to see the discussions.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"e61hf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"QueerCoup\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/e61hf/hey_redditdev_anyone_interested_in_helping/\", \"locked\": false, \"name\": \"t3_e61hf\", \"created\": 1289795346.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/e61hf/hey_redditdev_anyone_interested_in_helping/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Hey redditdev, anyone interested in helping /r/anarchism with a project?\", \"created_utc\": 1289766546.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo, without disclosing the full details of my project, a major aspect of the site will have users voting up/down different nodes (not too much else of what Reddit does though). Is it ok (with the license) to just take that portion  (just the sorting code)?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso, if my project takes off, is it ok to profit off of the code I use/modify?  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny other info I should use?  And advice on how to start on this?  :)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So, without disclosing the full details of my project, a major aspect of the site will have users voting up/down different nodes (not too much else of what Reddit does though). Is it ok (with the license) to just take that portion  (just the sorting code)?\\n\\nAlso, if my project takes off, is it ok to profit off of the code I use/modify?  \\n\\nAny other info I should use?  And advice on how to start on this?  :)\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"dii4s\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"antifreze\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/dii4s/wanting_to_use_a_portion_of_reddits_code/\", \"locked\": false, \"name\": \"t3_dii4s\", \"created\": 1285391033.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/dii4s/wanting_to_use_a_portion_of_reddits_code/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Wanting to use a portion of reddit's code.... questions within (license, getting started, etc).  :)\", \"created_utc\": 1285362233.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m trying to statistically compare in various ways current comments with much older comments, to try to confirm or deny the notion that reddit comments have been degrading.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EGathering comments from the current reddit frontpage is easy enough, but what is the best way to get historical data? Right now my only ideas are to go to the subreddits all time top scoring links, which will include some old links, \\u003Ca href=\\\"http://reredd.com/date/2007/12/23\\\"\\u003Ereredd.com\\u003C/a\\u003E, and \\u003Ca href=\\\"http://web.archive.org/*/http://www.reddit.com/\\\"\\u003Earchive.org\\u003C/a\\u003E. Those two sites are very poor options though because I can\\u0026#39;t really get the content directly. The first options might not provide enough data.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there any better way to get historic comment data? It would probably be too much to ask for some kind of comment dump, but I\\u0026#39;m not sure how to get enough data to be accurate.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm trying to statistically compare in various ways current comments with much older comments, to try to confirm or deny the notion that reddit comments have been degrading.\\n\\nGathering comments from the current reddit frontpage is easy enough, but what is the best way to get historical data? Right now my only ideas are to go to the subreddits all time top scoring links, which will include some old links, [reredd.com](http://reredd.com/date/2007/12/23), and [archive.org](http://web.archive.org/*/http://www.reddit.com/). Those two sites are very poor options though because I can't really get the content directly. The first options might not provide enough data.\\n\\nIs there any better way to get historic comment data? It would probably be too much to ask for some kind of comment dump, but I'm not sure how to get enough data to be accurate.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ddxi2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"LinuxFreeOrDie\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ddxi2/a_good_way_to_get_stories_comments_from_the_far/\", \"locked\": false, \"name\": \"t3_ddxi2\", \"created\": 1284536991.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ddxi2/a_good_way_to_get_stories_comments_from_the_far/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"A good way to get stories / comments from the far past?\", \"created_utc\": 1284508191.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ELet\\u0026#39;s preface this with \\u0026quot;I haven\\u0026#39;t had to upgrade or install linux in 5 years because of my cushy job\\u0026quot;.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m striving for reproducibility, so I\\u0026#39;m using VMWare Player as my platform.  I\\u0026#39;m starting with \\u003Ca href=\\\"http://www.visoracle.com/vm/ubuntu810/\\\"\\u003Ethis image\\u003C/a\\u003E; it\\u0026#39;s a full Ubuntu 8.10 install, but with Python 2.5 installed.  I installed \\u003Ca href=\\\"http://jaredforsyth.com/blog/2009/jan/20/install-python-26-ubuntu/\\\"\\u003Ethese python 2.6 debs\\u003C/a\\u003E.  However, this does not upgrade python 2.5, but instead installs a parallel python 2.6.  I suspect this is where my problem lies.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI had to make the \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/b21ri/reddit_start_to_finish_ubuntu_updates_needed/\\\"\\u003EBeautifulSoup patch\\u003C/a\\u003E from this thread.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFinally, I relinked /usr/bin/python from python2.5 to python2.6.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ll crank the VM back up and update with the current error, but I\\u0026#39;m sure there\\u0026#39;s some obvious things wrong with what I\\u0026#39;ve done so far.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Let's preface this with \\\"I haven't had to upgrade or install linux in 5 years because of my cushy job\\\".\\n\\nI'm striving for reproducibility, so I'm using VMWare Player as my platform.  I'm starting with [this image](http://www.visoracle.com/vm/ubuntu810/); it's a full Ubuntu 8.10 install, but with Python 2.5 installed.  I installed [these python 2.6 debs](http://jaredforsyth.com/blog/2009/jan/20/install-python-26-ubuntu/).  However, this does not upgrade python 2.5, but instead installs a parallel python 2.6.  I suspect this is where my problem lies.\\n\\nI had to make the [BeautifulSoup patch](http://www.reddit.com/r/redditdev/comments/b21ri/reddit_start_to_finish_ubuntu_updates_needed/) from this thread.\\n\\nFinally, I relinked /usr/bin/python from python2.5 to python2.6.\\n\\nI'll crank the VM back up and update with the current error, but I'm sure there's some obvious things wrong with what I've done so far.\\n\\nThanks.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"bt606\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 26, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/bt606/trying_and_failing_to_get_reddit_installed/\", \"locked\": false, \"name\": \"t3_bt606\", \"created\": 1271724393.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/bt606/trying_and_failing_to_get_reddit_installed/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Trying (and failing) to get Reddit installed\", \"created_utc\": 1271695593.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been setting up a local copy of Reddit for the first time today.\\nI\\u0026#39;m fairly new to Linux (Ubuntu 9.10), Python and Pylons, so I\\u0026#39;m not sure what caused this problem. But I found a work around so thought I\\u0026#39;d post it.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am following the Ubuntu instructions [\\u003Ca href=\\\"http://code.reddit.com/wiki/RedditStartToFinishIntrepid%5D(found\\\"\\u003Ehttp://code.reddit.com/wiki/RedditStartToFinishIntrepid](found\\u003C/a\\u003E here.)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe following line keeps crashing out because it can\\u0026#39;t find BeautifulSoup\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esudo python setup.py develop \\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI solved the problem by adding the correct download path.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esudo python setup.py develop --find-links http://www.crummy.com/software/BeautifulSoup/download/3.x/\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENot sure where the out of date download path is stored.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI then encountered another problem\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EBuilding lxml version 2.2.4.\\nNOTE: Trying to build without Cython, pre-generated \\u0026#39;src/lxml/lxml.etree.c\\u0026#39; needs to be available.\\nERROR: /bin/sh: xslt-config: not found\\n\\n** make sure the development packages of libxml2 and libxslt are installed **\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI solved this by installing libxslt1-dev (no idea why this was needed. Perhaps this was installed by default on the version of Ubuntu the instructions where tested on.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esudo apt-get install libxslt1-dev\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EEdit 2:\\nNow have a problem with awards , which probably did not exist when the instructions where written. I\\u0026#39;ve tried solving it with \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Epostgres$ createdb -E utf8 awards\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EBut now I am having another problem\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EFile \\u0026quot;/usr/local/lib/python2.6/dist-packages/SQLAlchemy-0.5.3-py2.6.egg/sqlalchemy/engine/base.py\\u0026quot;, line 931, in _handle_dbapi_exception\\nraise exc.DBAPIError.instance(statement, parameters, e, connection_invalidated=is_disconnect)\\nsqlalchemy.exc.ProgrammingError: (ProgrammingError) function ip_network(character varying) does not exist\\nLINE 1: ...p_network_reddit_data_award on reddit_data_award (ip_network...\\n                                                         ^\\nHINT:  No function matches the given name and argument types. You might need to add explicit type casts.\\n\\u0026quot;create index idx_ip_network_reddit_data_award on reddit_data_award (ip_network(value)) where key = \\u0026#39;ip\\u0026#39;\\u0026quot; {}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENot sure what to do about this one.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit 3:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI was also missing the authorize table\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Epostgres$ createdb -E utf8 authorize\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eand the sql functions had not run correctly due to these missing tables (I think). so I re ran\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Epostgres$ psql newreddit \\u0026lt; ../sql/functions.sql\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENow having problems with time-outs on pre-populating the data.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Econtinued below\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been setting up a local copy of Reddit for the first time today.\\nI'm fairly new to Linux (Ubuntu 9.10), Python and Pylons, so I'm not sure what caused this problem. But I found a work around so thought I'd post it.\\n\\nI am following the Ubuntu instructions [http://code.reddit.com/wiki/RedditStartToFinishIntrepid](found here.)\\n\\nThe following line keeps crashing out because it can't find BeautifulSoup\\n\\n    sudo python setup.py develop \\n\\nI solved the problem by adding the correct download path.\\n\\n    sudo python setup.py develop --find-links http://www.crummy.com/software/BeautifulSoup/download/3.x/\\n\\nNot sure where the out of date download path is stored.\\n\\nEdit: \\n\\nI then encountered another problem\\n\\n    Building lxml version 2.2.4.\\n    NOTE: Trying to build without Cython, pre-generated 'src/lxml/lxml.etree.c' needs to be available.\\n    ERROR: /bin/sh: xslt-config: not found\\n\\n    ** make sure the development packages of libxml2 and libxslt are installed **\\n\\nI solved this by installing libxslt1-dev (no idea why this was needed. Perhaps this was installed by default on the version of Ubuntu the instructions where tested on.\\n\\n    sudo apt-get install libxslt1-dev\\n\\nEdit 2:\\nNow have a problem with awards , which probably did not exist when the instructions where written. I've tried solving it with \\n\\n    postgres$ createdb -E utf8 awards\\n\\nBut now I am having another problem\\n\\n    File \\\"/usr/local/lib/python2.6/dist-packages/SQLAlchemy-0.5.3-py2.6.egg/sqlalchemy/engine/base.py\\\", line 931, in _handle_dbapi_exception\\n    raise exc.DBAPIError.instance(statement, parameters, e, connection_invalidated=is_disconnect)\\n    sqlalchemy.exc.ProgrammingError: (ProgrammingError) function ip_network(character varying) does not exist\\n    LINE 1: ...p_network_reddit_data_award on reddit_data_award (ip_network...\\n                                                             ^\\n    HINT:  No function matches the given name and argument types. You might need to add explicit type casts.\\n    \\\"create index idx_ip_network_reddit_data_award on reddit_data_award (ip_network(value)) where key = 'ip'\\\" {}\\n\\nNot sure what to do about this one.\\n\\nEdit 3:\\n\\nI was also missing the authorize table\\n\\n    postgres$ createdb -E utf8 authorize\\n\\nand the sql functions had not run correctly due to these missing tables (I think). so I re ran\\n\\n    postgres$ psql newreddit \\u003C ../sql/functions.sql\\n\\nNow having problems with time-outs on pre-populating the data.\\n\\ncontinued below\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ajk0j\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"SystemicPlural\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 19, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ajk0j/problem_with_beautifulsoup_when_setting_reddit_up/\", \"locked\": false, \"name\": \"t3_ajk0j\", \"created\": 1262128586.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ajk0j/problem_with_beautifulsoup_when_setting_reddit_up/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Problem with BeautifulSoup when setting Reddit up.\", \"created_utc\": 1262099786.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Ewhen would one justify webscraping instead of using the API?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"when would one justify webscraping instead of using the API?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4okh65\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"fantapeach1\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4okh65/webscraping_vs_api/\", \"locked\": false, \"name\": \"t3_4okh65\", \"created\": 1466214976.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4okh65/webscraping_vs_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Webscraping vs API\", \"created_utc\": 1466186176.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4ibyht\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"AllNamesTaken1234\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4ibyht/how_does_reddit_deal_with_server_location/\", \"locked\": false, \"name\": \"t3_4ibyht\", \"created\": 1462687507.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4ibyht/how_does_reddit_deal_with_server_location/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"how does reddit deal with server location? Obviously the closer webserver is to the user the faster it works. What OS do you use on your webservers?\", \"created_utc\": 1462658707.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis is an elaboration on a matter more succinctly expressed in \\u003Ca href=\\\"https://github.com/reddit/reddit/issues/1550\\\"\\u003EIssue #1550\\u003C/a\\u003E \\u2014 which is still outstanding \\u2014 but I have also implemented the solution proposed below in \\u003Ca href=\\\"https://github.com/reddit/reddit/pull/1603\\\"\\u003EPR #1603\\u003C/a\\u003E.  If this is rejected, then there is a work-around offered below also.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs indicated in that Issue and also elaborated on below, the problem is observable on production reddit, but the architecture of production reddit negates the effect of the bug.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EThe problem\\u003C/strong\\u003E (and its consequences)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhen (in development) \\u003Ccode\\u003Erun.ini\\u003C/code\\u003E option \\u003Ccode\\u003EuncompressedJS\\u003C/code\\u003E is set true, LessCSS sources are fetched directly (prefixed with the path \\u003Ccode\\u003E/static/css/\\u003C/code\\u003E) and the stylesheet is compiled clientside.  There are many \\u003Ccode\\u003Eurl()\\u003C/code\\u003E references containing parent-relative paths (eg \\u003Ccode\\u003Eurl(../sidebar-grippy-hide.png)\\u003C/code\\u003E) in these LessCSS sources because that\\u0026#39;s where the image assets live relative to the path from which the LessCSS sources are served.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor production, \\u003Ccode\\u003EuncompressedJS\\u003C/code\\u003E is set false so that precompiled stylesheets generated by \\u003Ccode\\u003Emake css\\u003C/code\\u003E are served statically (from a nice, fast CDN, cloudflare, in reddit.com\\u0026#39;s case).  Unfortunately, LessC\\u0026#39;s compilation output is stored in \\u003Ccode\\u003Epublic/static/\\u003C/code\\u003E rather than \\u003Ccode\\u003Epublic/static/css/\\u003C/code\\u003E and LessC isn\\u0026#39;t configured to modify the parent-relative paths to compensate.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe result is that the precompiled stylesheets contain many invalid paths to image assets that return 404s (or, sometimes, a 500 with an exception thrown by the reddit code; see \\u003Ca href=\\\"http://pastebin.com/Y6LDgjNA\\\"\\u003Eexample stacktrace\\u003C/a\\u003E) for any asset prefixed by \\u003Ccode\\u003E../\\u003C/code\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs explained in \\u003Ca href=\\\"https://github.com/reddit/reddit/issues/1550\\\"\\u003EIssue #1550\\u003C/a\\u003E, the same effect is observable on production reddit but everything still works there because production reddit serves its stylesheets from the \\u003Cem\\u003Eroot\\u003C/em\\u003E of a separate domain (eg \\u003Ca href=\\\"https://www.redditstatic.com/reddit.ooBJpEoO9vY.css\\\"\\u003Ewww.redditstatic.com/reddit.ooBJpEoO9vY.css\\u003C/a\\u003E), and cloudflare\\u0026#39;s servers redirect parent-relative URLs (eg \\u003Ca href=\\\"https://www.redditstatic.com/../sidebar-grippy-hide.png\\\"\\u003E/../sidebar-grippy-hide.png\\u003C/a\\u003E) to the correct URL.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt\\u0026#39;s still serving broken CSS, though, spared in reddit.com\\u0026#39;s case only by the behaviour of cloudflare\\u0026#39;s CDN servers.  For any other instance, however, it remains a problem.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003ESolution\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEdit:\\u003C/strong\\u003E target directory; solution implemented (see below)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEdit 2:\\u003C/strong\\u003E implementation of this is in \\u003Ca href=\\\"https://github.com/reddit/reddit/pull/1603\\\"\\u003EPR #1603\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt seems to me that the only way to preserve correct behaviour with both settings of \\u003Ccode\\u003EuncompressedJS\\u003C/code\\u003E is to modify the Makefile to store CSS outputs in a new subdirectory \\u003Ccode\\u003Epublic/static/compiled-css/\\u003C/code\\u003E instead of \\u003Ccode\\u003Epublic/static/\\u003C/code\\u003E as is now the case.  There are probably implications for deployment on production reddit, but obviously I can\\u0026#39;t have any insight as to what they may be.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs far as I can tell, the reddit core code locates its CSS, JS and sprite assets via \\u003Ccode\\u003Ebuild/public/static/names.json\\u003C/code\\u003E so it might be that only the Makefile need be modified.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve posted below a link to and a description of a diff that implements the above.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EWork around\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EA hackish work-around involves modifying Makefile to add the argument \\u003Ccode\\u003E--rootpath=lessc-hack/\\u003C/code\\u003E to \\u003Ccode\\u003Elessc\\u003C/code\\u003E.  After modifying the Makefile, create a symlink \\u003Ccode\\u003Ebuild/public/static/lessc-hack\\u003C/code\\u003E that points to itself, ie:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ecd build/public/static/lessc-hack\\nln -s . lessc-hack\\ncd -\\ntouch development.update\\nmake clean_css css names ini\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E(The last two lines is just to get reddit to reread the updated \\u003Ccode\\u003Enames.json\\u003C/code\\u003E.  Note that there\\u0026#39;s an underscore in \\u003Ccode\\u003Eclean_css\\u003C/code\\u003E, not a space.)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt might be that the \\u003Ccode\\u003Elessc\\u003C/code\\u003E argument \\u003Ccode\\u003E--relative-urls\\u003C/code\\u003E would generate correct URLs in the compiled output, but that doesn\\u0026#39;t work because LessCSS \\u003Ccode\\u003E@import\\u003C/code\\u003Eed files contain paths also relative to \\u003Ccode\\u003Epublic/static/css/\\u003C/code\\u003E, not relative to \\u003Ccode\\u003Epublic/static/css/components/\\u003C/code\\u003E where the imported files live (so \\u003Ccode\\u003Elessc\\u003C/code\\u003E can\\u0026#39;t find the location of the file referenced).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERegardless, even if the paths in the includes were corrected and \\u003Ccode\\u003E--relative-paths\\u003C/code\\u003E generated correct compiled output, that\\u0026#39;d break development mode  \\u003Ccode\\u003EuncompressedJS = true\\u003C/code\\u003E, so there\\u0026#39;s no help there.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This is an elaboration on a matter more succinctly expressed in [Issue #1550](https://github.com/reddit/reddit/issues/1550) \\u2014 which is still outstanding \\u2014 but I have also implemented the solution proposed below in [PR #1603](https://github.com/reddit/reddit/pull/1603).  If this is rejected, then there is a work-around offered below also.\\n\\nAs indicated in that Issue and also elaborated on below, the problem is observable on production reddit, but the architecture of production reddit negates the effect of the bug.\\n\\n**The problem** (and its consequences)\\n\\nWhen (in development) `run.ini` option `uncompressedJS` is set true, LessCSS sources are fetched directly (prefixed with the path `/static/css/`) and the stylesheet is compiled clientside.  There are many `url()` references containing parent-relative paths (eg `url(../sidebar-grippy-hide.png)`) in these LessCSS sources because that's where the image assets live relative to the path from which the LessCSS sources are served.\\n\\nFor production, `uncompressedJS` is set false so that precompiled stylesheets generated by `make css` are served statically (from a nice, fast CDN, cloudflare, in reddit.com's case).  Unfortunately, LessC's compilation output is stored in `public/static/` rather than `public/static/css/` and LessC isn't configured to modify the parent-relative paths to compensate.\\n\\nThe result is that the precompiled stylesheets contain many invalid paths to image assets that return 404s (or, sometimes, a 500 with an exception thrown by the reddit code; see [example stacktrace](http://pastebin.com/Y6LDgjNA)) for any asset prefixed by `../`.\\n\\nAs explained in [Issue #1550](https://github.com/reddit/reddit/issues/1550), the same effect is observable on production reddit but everything still works there because production reddit serves its stylesheets from the *root* of a separate domain (eg [www.redditstatic.com/reddit.ooBJpEoO9vY.css](https://www.redditstatic.com/reddit.ooBJpEoO9vY.css)), and cloudflare's servers redirect parent-relative URLs (eg [/../sidebar-grippy-hide.png](https://www.redditstatic.com/../sidebar-grippy-hide.png)) to the correct URL.\\n\\nIt's still serving broken CSS, though, spared in reddit.com's case only by the behaviour of cloudflare's CDN servers.  For any other instance, however, it remains a problem.\\n\\n**Solution**\\n\\n**Edit:** target directory; solution implemented (see below)\\n\\n**Edit 2:** implementation of this is in [PR #1603](https://github.com/reddit/reddit/pull/1603).\\n\\nIt seems to me that the only way to preserve correct behaviour with both settings of `uncompressedJS` is to modify the Makefile to store CSS outputs in a new subdirectory `public/static/compiled-css/` instead of `public/static/` as is now the case.  There are probably implications for deployment on production reddit, but obviously I can't have any insight as to what they may be.\\n\\nAs far as I can tell, the reddit core code locates its CSS, JS and sprite assets via `build/public/static/names.json` so it might be that only the Makefile need be modified.\\n\\nI've posted below a link to and a description of a diff that implements the above.\\n\\n**Work around**\\n\\nA hackish work-around involves modifying Makefile to add the argument `--rootpath=lessc-hack/` to `lessc`.  After modifying the Makefile, create a symlink `build/public/static/lessc-hack` that points to itself, ie:\\n\\n    cd build/public/static/lessc-hack\\n    ln -s . lessc-hack\\n    cd -\\n    touch development.update\\n    make clean_css css names ini\\n\\n(The last two lines is just to get reddit to reread the updated `names.json`.  Note that there's an underscore in `clean_css`, not a space.)\\n\\nIt might be that the `lessc` argument `--relative-urls` would generate correct URLs in the compiled output, but that doesn't work because LessCSS `@import`ed files contain paths also relative to `public/static/css/`, not relative to `public/static/css/components/` where the imported files live (so `lessc` can't find the location of the file referenced).\\n\\nRegardless, even if the paths in the includes were corrected and `--relative-paths` generated correct compiled output, that'd break development mode  `uncompressedJS = true`, so there's no help there.\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4i6tig\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"StrixTechnica\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1463072836.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4i6tig/problem_with_static_css_builds_uncompressedjs/\", \"locked\": false, \"name\": \"t3_4i6tig\", \"created\": 1462593556.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4i6tig/problem_with_static_css_builds_uncompressedjs/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Problem with static CSS builds (uncompressedJS = false); affects production reddit also, sort of\", \"created_utc\": 1462564756.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo, I store a list of post ID\\u0026#39;s and one of my methods I want to get the post object from that ID. In PRAW it\\u0026#39;s a pretty straight forward operation:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Er.get_info(thing_id=pID)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EHowever in C# I cannot figure this out!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo far my only solution is:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E var post = _reddit.GetPost(new Uri(createUrl(subreddit, item)));\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAnd i create the URL like this:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eprivate static string createUrl(string subreddit, string id)\\n    {\\n        return string.Format(\\u0026quot;https://www.reddit.com/r/{0}/comments/{1}\\u0026quot;, subreddit, id);\\n    }\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EWhich IMO is too sloppy and leads to me now having to also store the pid\\u0026#39;s subreddit name as this bot will manage more than one. Any suggestions?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So, I store a list of post ID's and one of my methods I want to get the post object from that ID. In PRAW it's a pretty straight forward operation:\\n\\n    r.get_info(thing_id=pID)\\n\\nHowever in C# I cannot figure this out!\\n\\nSo far my only solution is:\\n\\n     var post = _reddit.GetPost(new Uri(createUrl(subreddit, item)));\\n\\nAnd i create the URL like this:\\n\\n    private static string createUrl(string subreddit, string id)\\n        {\\n            return string.Format(\\\"https://www.reddit.com/r/{0}/comments/{1}\\\", subreddit, id);\\n        }\\n\\nWhich IMO is too sloppy and leads to me now having to also store the pid's subreddit name as this bot will manage more than one. Any suggestions?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4hj3um\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"bilago\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4hj3um/redditsharp_is_it_really_this_cumbersome_to_get_a/\", \"locked\": false, \"name\": \"t3_4hj3um\", \"created\": 1462243515.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4hj3um/redditsharp_is_it_really_this_cumbersome_to_get_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Redditsharp] Is it really this cumbersome to get a post from an ID?\", \"created_utc\": 1462214715.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m really confused how I can get a user to login to reddit from app (which is a Unity C# app). This is also my first time trying to do this so I\\u0026#39;m hoping it isn\\u0026#39;t really hard and I\\u0026#39;m just missing some detail. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve looked into api/login and using Oauth.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWith api/login I\\u0026#39;ve managed to get a modhash and cookie the likes of which I don\\u0026#39;t know what to do with. Research online I found a post saying logging in with cookies is going to be deprecated back in 2015 and that people should use oauth. So I looked into oauth and got as far as the url format for that, but the thing is the user has to login for that to work which is my initial problem to begin with. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo at this point I\\u0026#39;m lost, I\\u0026#39;m not sure if I need oauth to login or if that is just for api access or if api/login is even for actually logging into reddit. So how can I login to reddit using post/get?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm really confused how I can get a user to login to reddit from app (which is a Unity C# app). This is also my first time trying to do this so I'm hoping it isn't really hard and I'm just missing some detail. \\n\\n\\nI've looked into api/login and using Oauth.\\n\\n\\nWith api/login I've managed to get a modhash and cookie the likes of which I don't know what to do with. Research online I found a post saying logging in with cookies is going to be deprecated back in 2015 and that people should use oauth. So I looked into oauth and got as far as the url format for that, but the thing is the user has to login for that to work which is my initial problem to begin with. \\n\\n\\nSo at this point I'm lost, I'm not sure if I need oauth to login or if that is just for api access or if api/login is even for actually logging into reddit. So how can I login to reddit using post/get?\\n \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4h9qiz\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"Nyxtia\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4h9qiz/logging_in_via_post/\", \"locked\": false, \"name\": \"t3_4h9qiz\", \"created\": 1462147811.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4h9qiz/logging_in_via_post/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Logging in Via Post?\", \"created_utc\": 1462119011.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI would like to find the direct link to .gifv files without imgur overlays. Take \\u003Ca href=\\\"http://i.imgur.com/GwgbITR.gifv\\\"\\u003Ehttp://i.imgur.com/GwgbITR.gifv\\u003C/a\\u003E for example, the desktop still displays download button and imgur logo while mobile still display the full site. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I would like to find the direct link to .gifv files without imgur overlays. Take http://i.imgur.com/GwgbITR.gifv for example, the desktop still displays download button and imgur logo while mobile still display the full site. \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4h0j04\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"PoD682\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4h0j04/cant_find_gifv_direct_links_on_imgur/\", \"locked\": false, \"name\": \"t3_4h0j04\", \"created\": 1461980178.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4h0j04/cant_find_gifv_direct_links_on_imgur/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can't find .gifv direct links on Imgur\", \"created_utc\": 1461951378.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs the title states, I\\u0026#39;m having trouble retrieving comments that are hidden behind the \\u0026quot;load more comments\\u0026quot; link. I\\u0026#39;m trying to load the following link, which works in my browser:\\n\\u003Ca href=\\\"https://www.reddit.com/api/morechildren.json?%20%20%20api_type=json\\u0026amp;link_id=t3_482w7n\\u0026amp;children=d0h7gs4\\\"\\u003Ehttps://www.reddit.com/api/morechildren.json?api_type=json\\u0026amp;link_id=t3_482w7n\\u0026amp;children=d0h7gs4\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI make a call to the Reddit api using the Python requests library, as shown in my code below, but receive a \\u0026#39;403\\u0026#39; error response instead of the expected data. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere is my call to the API, which basically just wraps the above URL copied directly into the requests.get(url,headers) function:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Enew_request=requests.get(\\u0026quot;https://www.reddit.com/api/morechildren.json?   api_type=json\\u0026amp;link_id=t3_482w7n\\u0026amp;children=d0h7gs4\\u0026quot;,\\\\\\n                    headers=headers).json()\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAdditional info:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EAuthentication isn\\u0026#39;t an issue because all my other API calls work\\u003C/li\\u003E\\n\\u003Cli\\u003EI\\u0026#39;m not exceeding the 1 call per 2 seconds throttle that reddit limits users to\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EAny idea what\\u0026#39;s wrong?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello,\\n\\nAs the title states, I'm having trouble retrieving comments that are hidden behind the \\\"load more comments\\\" link. I'm trying to load the following link, which works in my browser:\\n[https://www.reddit.com/api/morechildren.json?api_type=json\\u0026link_id=t3_482w7n\\u0026children=d0h7gs4](https://www.reddit.com/api/morechildren.json?   api_type=json\\u0026link_id=t3_482w7n\\u0026children=d0h7gs4)\\n\\nI make a call to the Reddit api using the Python requests library, as shown in my code below, but receive a '403' error response instead of the expected data. \\n\\nHere is my call to the API, which basically just wraps the above URL copied directly into the requests.get(url,headers) function:\\n\\n    new_request=requests.get(\\\"https://www.reddit.com/api/morechildren.json?   api_type=json\\u0026link_id=t3_482w7n\\u0026children=d0h7gs4\\\",\\\\\\n                        headers=headers).json()\\n\\nAdditional info:\\n\\n* Authentication isn't an issue because all my other API calls work\\n* I'm not exceeding the 1 call per 2 seconds throttle that reddit limits users to\\n\\nAny idea what's wrong?\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4gji2f\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"nomos\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4gji2f/403_on_call_to_apimorechildren_python/\", \"locked\": false, \"name\": \"t3_4gji2f\", \"created\": 1461715488.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4gji2f/403_on_call_to_apimorechildren_python/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"403 on call to /api/morechildren? (Python)\", \"created_utc\": 1461686688.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi guys,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have some questions about the open-source Reddit: does the version on github also come with the same advertising functionality as the real Reddit? Are there any major functionalilties not provided in the open-source verion?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi guys,\\n\\nI have some questions about the open-source Reddit: does the version on github also come with the same advertising functionality as the real Reddit? Are there any major functionalilties not provided in the open-source verion?\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4fdgm4\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"ho4ngt\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4fdgm4/does_reddit_clone_also_come_with_advertising/\", \"locked\": false, \"name\": \"t3_4fdgm4\", \"created\": 1461035137.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4fdgm4/does_reddit_clone_also_come_with_advertising/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Does Reddit clone also come with advertising functionality?\", \"created_utc\": 1461006337.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have an Android app. I\\u0026#39;m not sure how long time ago but I guess couple weeks ago there must be some API behavior change that caused that API stopped delivering NSFW links. There was no change in my app.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI can\\u0026#39;t find any info regarding this change and possible workaround. My app is connected as Userless app, using JRAW api wrapper. I tried to set obey_over18 but with no luck.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks for any suggestion.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have an Android app. I'm not sure how long time ago but I guess couple weeks ago there must be some API behavior change that caused that API stopped delivering NSFW links. There was no change in my app.\\n\\nI can't find any info regarding this change and possible workaround. My app is connected as Userless app, using JRAW api wrapper. I tried to set obey_over18 but with no luck.\\n\\nThanks for any suggestion.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4fajiq\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"thubalek\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4fajiq/api_behaviour_changes_that_stopped_delivering/\", \"locked\": false, \"name\": \"t3_4fajiq\", \"created\": 1460989084.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4fajiq/api_behaviour_changes_that_stopped_delivering/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API behaviour changes that stopped delivering NSFW links?\", \"created_utc\": 1460960284.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhat url can I go to to logout of an account (in a browser with a working session/cookie)? Going to reddit.com/logout gives a forbidden error.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"What url can I go to to logout of an account (in a browser with a working session/cookie)? Going to reddit.com/logout gives a forbidden error.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4dzi5n\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"markasoftware\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4dzi5n/url_to_logout/\", \"locked\": false, \"name\": \"t3_4dzi5n\", \"created\": 1460195432.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4dzi5n/url_to_logout/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"URL to logout?\", \"created_utc\": 1460166632.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://redditairplane.com/\\\"\\u003Emy site\\u003C/a\\u003E is randomy \\u003Ca href=\\\"https://i.imgur.com/H0pws9N.png\\\"\\u003Egetting this error\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt happens infrequently and I don\\u0026#39;t know how to track down what\\u0026#39;s causing it. The website (\\u003Ca href=\\\"http://redditairplane.com/\\\"\\u003Emine\\u003C/a\\u003E) makes ajax request to reddit (a \\u003Cem\\u003Elot\\u003C/em\\u003E of request actually) via https://oauth* (for logged in users) and \\u003Ca href=\\\"http://www...json*\\\"\\u003Ehttp://www...json*\\u003C/a\\u003E. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs ajax just becoming an internet relic? I\\u0026#39;ve already lost the ability to support iPhone 6\\u0026#39;s (because no cross domain request are allowed for web-apps on the default browsers): Is ajax just a red flag for any page now? Should I stop working on cross domain javascript web-apps? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ethanks. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eedit: \\u003C/p\\u003E\\n\\n\\u003Cp\\u003Especific api\\u0026#39;s: \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBasically all of them. Check for mail, grabs every submitter\\u0026#39;s submission page, and a ton of image embedding.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThough I can\\u0026#39;t lock it down, I suspect this might be the culprit: if reddit offers a \\u0026quot;embed media\\u0026quot; code, I use it. The page automatically embeds the iframe reddit provides. This has never caused an issue before though and nothing has changed afaik. Did reddit get tagged by google for suspect behavior? Or is my site \\u0026quot;acting shady\\u0026quot;. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"[my site](http://redditairplane.com/) is randomy [getting this error](https://i.imgur.com/H0pws9N.png)\\n\\nIt happens infrequently and I don't know how to track down what's causing it. The website ([mine](http://redditairplane.com/)) makes ajax request to reddit (a *lot* of request actually) via https://oauth\\\\* (for logged in users) and http://www...json*. \\n\\nIs ajax just becoming an internet relic? I've already lost the ability to support iPhone 6's (because no cross domain request are allowed for web-apps on the default browsers): Is ajax just a red flag for any page now? Should I stop working on cross domain javascript web-apps? \\n\\nthanks. \\n\\nedit: \\n\\nspecific api's: \\n\\nBasically all of them. Check for mail, grabs every submitter's submission page, and a ton of image embedding.  \\n\\nThough I can't lock it down, I suspect this might be the culprit: if reddit offers a \\\"embed media\\\" code, I use it. The page automatically embeds the iframe reddit provides. This has never caused an issue before though and nothing has changed afaik. Did reddit get tagged by google for suspect behavior? Or is my site \\\"acting shady\\\". \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4d9cka\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"SamSlate\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1459750004.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4d9cka/why_am_i_getting_the_back_to_safety_warning_on_my/\", \"locked\": false, \"name\": \"t3_4d9cka\", \"created\": 1459769114.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4d9cka/why_am_i_getting_the_back_to_safety_warning_on_my/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why am I getting the \\\"back to safety\\\" warning on my site? [javascript][json]\", \"created_utc\": 1459740314.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EDoes anyone know how the rate limit works? is it per ip address?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESpecifically for installed app.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Does anyone know how the rate limit works? is it per ip address?\\n\\nSpecifically for installed app.\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4ativ1\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"rockinghouse\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1458245780.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4ativ1/rate_limit_questions/\", \"locked\": false, \"name\": \"t3_4ativ1\", \"created\": 1458256567.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4ativ1/rate_limit_questions/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Rate limit questions\", \"created_utc\": 1458227767.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"46qw6c\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"lecherous_hump\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/46qw6c/are_we_required_to_register_existing_apps_theres/\", \"locked\": false, \"name\": \"t3_46qw6c\", \"created\": 1456016456.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/46qw6c/are_we_required_to_register_existing_apps_theres/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Are we required to register existing apps? There's no link on the apps page; it doesn't appear unless you click \\\"create new app\\\".\", \"created_utc\": 1455987656.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIn my subreddit app, some users are uploading custom images of Snoo and setting it as the image of the subreddit. These photos are now appearing in the main screens of the app. Is this allowed by the licensing agreement or should we be removing these Snoos? \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"In my subreddit app, some users are uploading custom images of Snoo and setting it as the image of the subreddit. These photos are now appearing in the main screens of the app. Is this allowed by the licensing agreement or should we be removing these Snoos? \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"46huf4\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"donaldtrumpt\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/46huf4/can_the_snoo_logo_be_displayed_in_reddit_apps_if/\", \"locked\": false, \"name\": \"t3_46huf4\", \"created\": 1455869460.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/46huf4/can_the_snoo_logo_be_displayed_in_reddit_apps_if/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can the Snoo logo be displayed in reddit apps if its user submitted?\", \"created_utc\": 1455840660.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI tried to install Cassandra from the Reddit repository and it says it isn\\u0026#39;t there. I\\u0026#39;m using 14.04 for my server. Where else can I get cassandra?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I tried to install Cassandra from the Reddit repository and it says it isn't there. I'm using 14.04 for my server. Where else can I get cassandra?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"46c0l6\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"thepinkanator95\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/46c0l6/cassandra_not_in_the_reddit_repository_anymore/\", \"locked\": false, \"name\": \"t3_46c0l6\", \"created\": 1455784444.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/46c0l6/cassandra_not_in_the_reddit_repository_anymore/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Cassandra not in the Reddit repository anymore?\", \"created_utc\": 1455755644.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi. I\\u0026#39;m trying to decide whether I should host a reddit instance (\\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu\\\"\\u003Ehttps://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu\\u003C/a\\u003E) or just make a private subreddit. The latter is of course less work. However, if possible, I\\u0026#39;d like to avoid having everyone have to create an account to use this. We all already have Google Apps accounts and if users could log into the reddit instance using their Google account via OAuth, that would make it worth hosting our own. I saw this page: \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/OAuth2\\\"\\u003Ehttps://github.com/reddit/reddit/wiki/OAuth2\\u003C/a\\u003E, though it seems to be the opposite - using reddit accounts to log into other services. Has what I\\u0026#39;m looking to do been done before/would it be relatively easy? Any information you can provide would be really helpful. Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi. I'm trying to decide whether I should host a reddit instance (https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu) or just make a private subreddit. The latter is of course less work. However, if possible, I'd like to avoid having everyone have to create an account to use this. We all already have Google Apps accounts and if users could log into the reddit instance using their Google account via OAuth, that would make it worth hosting our own. I saw this page: https://github.com/reddit/reddit/wiki/OAuth2, though it seems to be the opposite - using reddit accounts to log into other services. Has what I'm looking to do been done before/would it be relatively easy? Any information you can provide would be really helpful. Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"45gdjw\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"codenrun\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/45gdjw/oauth_in_the_other_direction_logging_into_a/\", \"locked\": false, \"name\": \"t3_45gdjw\", \"created\": 1455334056.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/45gdjw/oauth_in_the_other_direction_logging_into_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OAuth in the other direction (logging into a reddit instance via Google accounts)?\", \"created_utc\": 1455305256.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EUsing PRAW\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Using PRAW\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"42w4l7\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"allthefoxes\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/42w4l7/with_the_oauthmageddon_approaching_what_is_a/\", \"locked\": false, \"name\": \"t3_42w4l7\", \"created\": 1453903739.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/42w4l7/with_the_oauthmageddon_approaching_what_is_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"With the OAuth-mageddon approaching, what is a quick and easy way to migrate already existing scripts/bots to use it?\", \"created_utc\": 1453874939.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIn my app Baconit I use a Windows WebAuthenticationBroker to do some of the Oauth login (it essentially is just a nice wrapper to show the webpage). However after this weekend for some reason it has stopped working. I can see that if I navigate to the URL it is trying to go to everything is happy.  \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehttps://www.reddit.com/api/v1/authorize.compact?client_id=\\u0026lt;AppId\\u0026gt;\\u0026amp;response_type=code\\u0026amp;state=62032516\\u0026amp;redirect_uri=http://www.quinndamerell.com/Baconit/OAuth/Auth.php\\u0026amp;duration=permanent\\u0026amp;scope=modcontributors,modconfig,subscribe,wikiread,wikiedit,vote,mysubreddits,submit,modlog,modposts,modflair,save,modothers,read,privatemessages,report,identity,livemanage,account,modtraffic,edit,modwiki,modself,history,flair\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EBut when the Windows tool tries it gets a 403. The odd thing is that this worked 100% correctly, and then all of the sudden it just stopped working. Do you have any ideas if there were any changes to this login flow that were made recently?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey,\\n\\nIn my app Baconit I use a Windows WebAuthenticationBroker to do some of the Oauth login (it essentially is just a nice wrapper to show the webpage). However after this weekend for some reason it has stopped working. I can see that if I navigate to the URL it is trying to go to everything is happy.  \\n\\n    https://www.reddit.com/api/v1/authorize.compact?client_id=\\u003CAppId\\u003E\\u0026response_type=code\\u0026state=62032516\\u0026redirect_uri=http://www.quinndamerell.com/Baconit/OAuth/Auth.php\\u0026duration=permanent\\u0026scope=modcontributors,modconfig,subscribe,wikiread,wikiedit,vote,mysubreddits,submit,modlog,modposts,modflair,save,modothers,read,privatemessages,report,identity,livemanage,account,modtraffic,edit,modwiki,modself,history,flair\\n\\n\\nBut when the Windows tool tries it gets a 403. The odd thing is that this worked 100% correctly, and then all of the sudden it just stopped working. Do you have any ideas if there were any changes to this login flow that were made recently?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"42o97f\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"quinbd\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1453762723.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/42o97f/reddit_oauth_login_changes/\", \"locked\": false, \"name\": \"t3_42o97f\", \"created\": 1453790709.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/42o97f/reddit_oauth_login_changes/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit OAuth login Changes?\", \"created_utc\": 1453761909.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI know that content is \\u003Cem\\u003Esubmitted\\u003C/em\\u003E to reddit as markdown, but is there any way (API or otherwise) to \\u003Cem\\u003Eget\\u003C/em\\u003E posts, comments, etc. as markdown?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EApologies if this is written anywhere obvious, I\\u0026#39;ve been looking for a while and decided to just ask.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I know that content is *submitted* to reddit as markdown, but is there any way (API or otherwise) to *get* posts, comments, etc. as markdown?\\n\\nApologies if this is written anywhere obvious, I've been looking for a while and decided to just ask.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"41y8w0\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"felixphew\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/41y8w0/able_to_get_reddit_posts_as_markdown/\", \"locked\": false, \"name\": \"t3_41y8w0\", \"created\": 1453378965.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/41y8w0/able_to_get_reddit_posts_as_markdown/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Able to get Reddit posts as markdown?\", \"created_utc\": 1453350165.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI was wondering if anyone knew of anywhere I could find a Postgres CREATE TABLE script for listings. I specifically want to archive some posts/listings in a database. Thanks in advanced!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I was wondering if anyone knew of anywhere I could find a Postgres CREATE TABLE script for listings. I specifically want to archive some posts/listings in a database. Thanks in advanced!\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"41b6dc\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"redkingdev\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/41b6dc/postgres_create_table_script_for_listings_etc/\", \"locked\": false, \"name\": \"t3_41b6dc\", \"created\": 1453019742.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/41b6dc/postgres_create_table_script_for_listings_etc/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Postgres CREATE TABLE script for listings, etc.?\", \"created_utc\": 1452990942.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi all, I am really new to redditdev, sorry if this question has been answered before.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI just finished my first reddit bot on python, but Im not sure how I should run it(on mac). My plan is to make the bot perform its action in every 10 seconds.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs of now I run it on terminal with an infinite loop with 10 seconds sleep in each cycle.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs this a viable solution considering I check it fairly often?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have read some other posts that recommend some third part softwares to run the bot, do you guys have good recommendations?  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: thanks for the replies!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi all, I am really new to redditdev, sorry if this question has been answered before.\\n\\nI just finished my first reddit bot on python, but Im not sure how I should run it(on mac). My plan is to make the bot perform its action in every 10 seconds.\\n\\nAs of now I run it on terminal with an infinite loop with 10 seconds sleep in each cycle.\\n\\nIs this a viable solution considering I check it fairly often?\\n\\nI have read some other posts that recommend some third part softwares to run the bot, do you guys have good recommendations?  \\n\\nThanks!\\n\\nEDIT: thanks for the replies!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3zc7m6\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"peasbean\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 27, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1451925320.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3zc7m6/running_reddit_bot/\", \"locked\": false, \"name\": \"t3_3zc7m6\", \"created\": 1451894447.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3zc7m6/running_reddit_bot/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"running reddit bot\", \"created_utc\": 1451865647.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey all,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have a few questions regarding the use of the API.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBasically, I am building a website aggregating the NSFW thumbnails of the different adult subreddits.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E1) I\\u0026#39;m using the API in a read-only fashion, only getting data from urls such as \\u003Ca href=\\\"https://api.reddit.com/r/subreddit/hot.json\\\"\\u003Ehttps://api.reddit.com/r/subreddit/hot.json\\u003C/a\\u003E. Do I still need to connect using OAuth or doing a simple curl is allowed (I stay within the limits as I connect 1x per minute)? I then store the resulting JSON in my own DB.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E2) Is displaying ads on my website allowed or does this constitute a \\u003Cem\\u003Ecommercial\\u003C/em\\u003E use of the API? If not allowed, do you know if there\\u0026#39;s some kind of paid licence? (I\\u0026#39;m an individual with no budget so I\\u0026#39;d give up the project).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey all,\\n\\nI have a few questions regarding the use of the API.\\n\\nBasically, I am building a website aggregating the NSFW thumbnails of the different adult subreddits.\\n\\n1) I'm using the API in a read-only fashion, only getting data from urls such as https://api.reddit.com/r/subreddit/hot.json. Do I still need to connect using OAuth or doing a simple curl is allowed (I stay within the limits as I connect 1x per minute)? I then store the resulting JSON in my own DB.\\n\\n2) Is displaying ads on my website allowed or does this constitute a *commercial* use of the API? If not allowed, do you know if there's some kind of paid licence? (I'm an individual with no budget so I'd give up the project).\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3yt30r\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"wtfamievenhere\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3yt30r/two_questions_regarding_what_is_allowed_with_the/\", \"locked\": false, \"name\": \"t3_3yt30r\", \"created\": 1451526642.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3yt30r/two_questions_regarding_what_is_allowed_with_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Two questions regarding what is allowed with the API\", \"created_utc\": 1451497842.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ETitle post. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERight now I am getting the front page of reddit\\u0026#39;s listings in the following format (Javascript): \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E $.getJSON(\\u0026quot;https://www.reddit.com/.json\\u0026quot;, function (data)  {\\n        var items = [];\\n        $.each(data.data.children, function (i, obj) {\\n        });\\n });\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIs there a way to modify this to pull a certain number of listings?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI understand adding ?count=100 only starts the listing at that number.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Title post. \\n\\nRight now I am getting the front page of reddit's listings in the following format (Javascript): \\n\\n     $.getJSON(\\\"https://www.reddit.com/.json\\\", function (data)  {\\n            var items = [];\\n            $.each(data.data.children, function (i, obj) {\\n            });\\n     });\\n\\nIs there a way to modify this to pull a certain number of listings?\\n\\nI understand adding ?count=100 only starts the listing at that number.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3yhgoq\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"Admsugar\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1451279081.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3yhgoq/is_there_a_url_parameter_i_can_append_json_to/\", \"locked\": false, \"name\": \"t3_3yhgoq\", \"created\": 1451307296.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3yhgoq/is_there_a_url_parameter_i_can_append_json_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a url parameter I can append .json to that would give me the front page or a subreddits listings up to a certain number?\", \"created_utc\": 1451278496.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have a question about sending many Private Messages via the API. These messages would be notifications, similar to email notifications \\u0026quot;You got a new Like\\u0026quot; etc. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EUsers on my website (authenticated in via Reddit oAuth2) opt-in to receive a notification when a certain action happens, and the website would send a PM notification to the user on Reddit. The \\u0026quot;sender\\u0026quot; would be a single Reddit user (my website\\u0026#39;s username).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy first thought was to send a PM from each Reddit user\\u0026#39;s own account to themselves. But I noticed that there is no \\u0026quot;new message\\u0026quot; notification on Reddit for messages sent to oneself. Also, I\\u0026#39;d rather not store each user\\u0026#39;s refresh_token in my database.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo, sending it from a single user to all opted-in users of my site. Question: is there a rate limit for that? Would my \\u0026quot;site\\u0026quot; user be presented with a Captcha (which it could not solve, because its an automated process)? Anything else that doesn\\u0026#39;t make sense?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have a question about sending many Private Messages via the API. These messages would be notifications, similar to email notifications \\\"You got a new Like\\\" etc. \\n\\nUsers on my website (authenticated in via Reddit oAuth2) opt-in to receive a notification when a certain action happens, and the website would send a PM notification to the user on Reddit. The \\\"sender\\\" would be a single Reddit user (my website's username).\\n\\nMy first thought was to send a PM from each Reddit user's own account to themselves. But I noticed that there is no \\\"new message\\\" notification on Reddit for messages sent to oneself. Also, I'd rather not store each user's refresh_token in my database.\\n\\nSo, sending it from a single user to all opted-in users of my site. Question: is there a rate limit for that? Would my \\\"site\\\" user be presented with a Captcha (which it could not solve, because its an automated process)? Anything else that doesn't make sense?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3wl5mq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"C14L\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3wl5mq/question_about_sending_automated_private_messages/\", \"locked\": false, \"name\": \"t3_3wl5mq\", \"created\": 1449997028.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3wl5mq/question_about_sending_automated_private_messages/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Question about sending automated Private Messages via the API\", \"created_utc\": 1449968228.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI don\\u0026#39;t know why,but it doesn\\u0026#39;t work at all. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eit redirects to \\u003Ca href=\\\"http://www.reddit.com/tb/xxxx\\\"\\u003Ewww.reddit.com/tb/xxxx\\u003C/a\\u003E and stop.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I don't know why,but it doesn't work at all. \\n\\nit redirects to www.reddit.com/tb/xxxx and stop.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3whaas\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"WhiteCat6142\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3whaas/url_shortening_reddit_doesnt_work/\", \"locked\": false, \"name\": \"t3_3whaas\", \"created\": 1449926846.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3whaas/url_shortening_reddit_doesnt_work/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"URL shortening (redd.it) doesn't work!\", \"created_utc\": 1449898046.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am confused. I thought it was \\u003Cstrong\\u003E/subreddits/where\\u003C/strong\\u003E but it\\u0026#39;s not. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003E/subreddits/mine/subscriber\\u003C/strong\\u003E - requires OAuth, I need the public endpoint. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am confused. I thought it was **/subreddits/where** but it's not. \\n\\n**/subreddits/mine/subscriber** - requires OAuth, I need the public endpoint. \\n\\nThanks\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ux9lx\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"atioxx\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ux9lx/which_public_endpoint_is_used_to_get_the/\", \"locked\": false, \"name\": \"t3_3ux9lx\", \"created\": 1448955882.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ux9lx/which_public_endpoint_is_used_to_get_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Which public endpoint is used to get the Frontpage subreddit listing?\", \"created_utc\": 1448927082.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am doing my postgrad thesis/practicum around the hot topic of containerising webapps. After some glances at the Reddit source (mostly at install-reddit.sh) I don\\u0026#39;t see why this would not possible be do with Reddit notwithstanding with some work of course. Being a newbie to the project, is there anything I should know (gotchas, etc)?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am doing my postgrad thesis/practicum around the hot topic of containerising webapps. After some glances at the Reddit source (mostly at install-reddit.sh) I don't see why this would not possible be do with Reddit notwithstanding with some work of course. Being a newbie to the project, is there anything I should know (gotchas, etc)?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3tafq0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"full-of-foo\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3tafq0/questionhelp_should_i_migrate_reddit_to_docker/\", \"locked\": false, \"name\": \"t3_3tafq0\", \"created\": 1447880538.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3tafq0/questionhelp_should_i_migrate_reddit_to_docker/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Question+help] Should I migrate Reddit to Docker?\", \"created_utc\": 1447851738.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Ewhen i do the authorization part, do i need to specify a scope if i dont actually need any permission granting user information? In non oauth mode i\\u0026#39;m currently fetching people\\u0026#39;s publicly view-able comments with PRAW:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003Ereddit_user = r.get_redditor(user)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Efor comment in reddit_user.get_comments(limit=100): \\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EMy reason for switching to oauth is that 2 seconds per request is just too slow. I dont actually need anything that requires oauth except for the benefit of being able to request every second.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso for Step 4 of the \\u003Ca href=\\\"http://praw.readthedocs.org/en/stable/pages/oauth.html\\\"\\u003EPRAW(OAuth) tutorial\\u003C/a\\u003E how does reddit know that the \\u0026quot;code\\u0026quot; from the url belongs to 1 specific user? What stops a program from re-using the code to automatically get authorization from any user?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"when i do the authorization part, do i need to specify a scope if i dont actually need any permission granting user information? In non oauth mode i'm currently fetching people's publicly view-able comments with PRAW:\\n\\n\\u003E reddit_user = r.get_redditor(user)\\n\\n\\u003E for comment in reddit_user.get_comments(limit=100): \\n\\nMy reason for switching to oauth is that 2 seconds per request is just too slow. I dont actually need anything that requires oauth except for the benefit of being able to request every second.\\n\\nAlso for Step 4 of the [PRAW(OAuth) tutorial](http://praw.readthedocs.org/en/stable/pages/oauth.html) how does reddit know that the \\\"code\\\" from the url belongs to 1 specific user? What stops a program from re-using the code to automatically get authorization from any user?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3sv72o\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Frenchiie\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1447565235.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3sv72o/oauthwith_praw_if_i_dont_actually_need_permission/\", \"locked\": false, \"name\": \"t3_3sv72o\", \"created\": 1447593095.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3sv72o/oauthwith_praw_if_i_dont_actually_need_permission/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OAuth(with PRAW) if i don't actually need permission granting information?\", \"created_utc\": 1447564295.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/blob/84632259e8af2457e555e3998a44fac25721f74a/r2/r2/controllers/front.py#L525\\\"\\u003Ehttps://github.com/reddit/reddit/blob/84632259e8af2457e555e3998a44fac25721f74a/r2/r2/controllers/front.py#L525\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m just half curious and half confused, so if anyone has the answer it would be nice. A quick code search shows that c.site is rarely manually set elsewhere; and not \\u0026quot;destaled to not poison downstream caches\\u0026quot; (which I don\\u0026#39;t understand that comment either), and when it is set it\\u0026#39;s being set to a special subreddit, like ModSR or DefaultSR. And of all things, why is it needed when simply getting the stylesheet url?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"https://github.com/reddit/reddit/blob/84632259e8af2457e555e3998a44fac25721f74a/r2/r2/controllers/front.py#L525\\n\\nI'm just half curious and half confused, so if anyone has the answer it would be nice. A quick code search shows that c.site is rarely manually set elsewhere; and not \\\"destaled to not poison downstream caches\\\" (which I don't understand that comment either), and when it is set it's being set to a special subreddit, like ModSR or DefaultSR. And of all things, why is it needed when simply getting the stylesheet url?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3roz7j\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"13steinj\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3roz7j/why_is_csite_set_and_destaled_in_get_stylesheet/\", \"locked\": false, \"name\": \"t3_3roz7j\", \"created\": 1446794747.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3roz7j/why_is_csite_set_and_destaled_in_get_stylesheet/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why is c.site set and destaled in GET_stylesheet?\", \"created_utc\": 1446765947.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Eas you may or may not know, reddit doesn\\u0026#39;t share nsfw thumbnails with non-logged in users. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs I can find no way around this, does anyone have, or use, a third party api to supply NSFW thumbnails to pseudonymous users (given a unique reddit post id)? \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"as you may or may not know, reddit doesn't share nsfw thumbnails with non-logged in users. \\n\\nAs I can find no way around this, does anyone have, or use, a third party api to supply NSFW thumbnails to pseudonymous users (given a unique reddit post id)? \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3resj2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"SamSlate\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": true, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3resj2/3rd_party_source_for_nsfw_thumbnails/\", \"locked\": false, \"name\": \"t3_3resj2\", \"created\": 1446616903.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3resj2/3rd_party_source_for_nsfw_thumbnails/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"3rd party source for nsfw thumbnails?\", \"created_utc\": 1446588103.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ll try \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m designing a simple web page app in JavaScript for retrieving Reddit comments from a specific link. I\\u0026#39;ve looked into Oauth Authentication in order to retrieve things, but I felt for something very basic as just retrieving would be too powerful to use oauth.reddit.com . After all, there are more privileges to create new things like links and comments, which is not what I want my app to do. My app just reads all the comments that a link has. It doesn\\u0026#39;t look up who\\u0026#39;s logged in or post any comments by a user.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI know I could also get comments by calling, for example... \\u003Ca href=\\\"https://www.reddit.com/r/redditdev/comments/39zje0/.json\\\"\\u003Ehttps://www.reddit.com/r/redditdev/comments/39zje0/.json\\u003C/a\\u003E to get the comments of a link and has all the information that I need. I can certainly use oauth.reddit.com... instead, but with so much unwanted privileges with oauth, why bother?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo what I\\u0026#39;m asking is: for just retrieving or reading things, is it legit to retrieve comments by using \\u003Ca href=\\\"http://www.reddit.com\\\"\\u003Ewww.reddit.com\\u003C/a\\u003E instead of using oauth.reddit.com?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'll try \\n\\nI'm designing a simple web page app in JavaScript for retrieving Reddit comments from a specific link. I've looked into Oauth Authentication in order to retrieve things, but I felt for something very basic as just retrieving would be too powerful to use oauth.reddit.com . After all, there are more privileges to create new things like links and comments, which is not what I want my app to do. My app just reads all the comments that a link has. It doesn't look up who's logged in or post any comments by a user.\\n\\nI know I could also get comments by calling, for example... https://www.reddit.com/r/redditdev/comments/39zje0/.json to get the comments of a link and has all the information that I need. I can certainly use oauth.reddit.com... instead, but with so much unwanted privileges with oauth, why bother?\\n\\nSo what I'm asking is: for just retrieving or reading things, is it legit to retrieve comments by using www.reddit.com instead of using oauth.reddit.com?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3rachq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"jprogman\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3rachq/oauth_for_just_reading_comments_overkill/\", \"locked\": false, \"name\": \"t3_3rachq\", \"created\": 1446540245.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3rachq/oauth_for_just_reading_comments_overkill/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Oauth for just reading comments... Overkill?\", \"created_utc\": 1446511445.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi. I might be what you would call a noob at using APIs.\\u003Cbr/\\u003E\\nHowever, when I decided to make a cool little script to pull random images from random subreddits, I ran into a problem. It seems that reddit really doesn\\u0026#39;t like me, and will not let me get a single page, because it will always return a \\u003Ccode\\u003E429: Too Many Requests\\u003C/code\\u003E response.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI find this odd, because I would understand it if I was spamming requests every second, however if I simply ask for 1 page it will return that response...\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026gt;\\u0026gt;\\u0026gt;import requests\\n\\u0026gt;\\u0026gt;\\u0026gt;res = requests.get(\\u0026quot;https://www.reddit.com/r/random.json\\u0026quot;)\\n\\u0026gt;\\u0026gt;\\u0026gt;print(res)\\n\\u0026lt;Response [429]\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThis is a little spooky because no matter what, it will give me a \\u003Ccode\\u003E429\\u003C/code\\u003E response. Could anyone tell me how to fix this? Am I doing something horribly wrong?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf I use the same url in a Chrome tab, it gladly gives me the json of a random page.\\u003Cbr/\\u003E\\n(For example, \\u003Ca href=\\\"https://www.reddit.com/r/random.json\\\"\\u003Ehttps://www.reddit.com/r/random.json\\u003C/a\\u003E might return \\u003Ca href=\\\"https://www.reddit.com/r/Neverwinter/.json\\\"\\u003Ehttps://www.reddit.com/r/Neverwinter/.json\\u003C/a\\u003E)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am thoroughly spooked at this... can anyone help me out?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi. I might be what you would call a noob at using APIs.  \\nHowever, when I decided to make a cool little script to pull random images from random subreddits, I ran into a problem. It seems that reddit really doesn't like me, and will not let me get a single page, because it will always return a `429: Too Many Requests` response.  \\n\\nI find this odd, because I would understand it if I was spamming requests every second, however if I simply ask for 1 page it will return that response...\\n\\n    \\u003E\\u003E\\u003Eimport requests\\n    \\u003E\\u003E\\u003Eres = requests.get(\\\"https://www.reddit.com/r/random.json\\\")\\n    \\u003E\\u003E\\u003Eprint(res)\\n    \\u003CResponse [429]\\u003E\\n\\nThis is a little spooky because no matter what, it will give me a `429` response. Could anyone tell me how to fix this? Am I doing something horribly wrong?\\n\\nIf I use the same url in a Chrome tab, it gladly gives me the json of a random page.  \\n(For example, https://www.reddit.com/r/random.json might return https://www.reddit.com/r/Neverwinter/.json)\\n\\nI am thoroughly spooked at this... can anyone help me out?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3qbll8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"RedditMattstir\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3qbll8/429_too_many_requests/\", \"locked\": false, \"name\": \"t3_3qbll8\", \"created\": 1445917717.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3qbll8/429_too_many_requests/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"429: Too Many Requests...?\", \"created_utc\": 1445888917.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have made a scraping program with nodejs that scrapes specific sub reddits for images, and that works perfectly. Though i can\\u0026#39;t scrape 18+ sub reddits. The reason for this is because you have to confirm that you want to enter. and that stores a cookie. I have looked into several ways of sending a cookie along on a request but none of them seem to be working. Here is a link to the repository: \\u003Ca href=\\\"https://github.com/MaximilianLloyd/reddit-scraping-engine\\\"\\u003Ehttps://github.com/MaximilianLloyd/reddit-scraping-engine\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have made a scraping program with nodejs that scrapes specific sub reddits for images, and that works perfectly. Though i can't scrape 18+ sub reddits. The reason for this is because you have to confirm that you want to enter. and that stores a cookie. I have looked into several ways of sending a cookie along on a request but none of them seem to be working. Here is a link to the repository: https://github.com/MaximilianLloyd/reddit-scraping-engine\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3mkxmt\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3mkxmt/im_scraping_sub_reddits_for_images_cant_scrape_18/\", \"locked\": false, \"name\": \"t3_3mkxmt\", \"created\": 1443394167.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3mkxmt/im_scraping_sub_reddits_for_images_cant_scrape_18/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I'm scraping sub reddits for images, can't scrape 18+ sub reddits.\", \"created_utc\": 1443365367.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"https://blog.pusher.com/pusher-realtime-reddit-api/\\\"\\u003EPusher\\u003C/a\\u003E is a platform that turns Reddit\\u2019s REST-based API into a smooth, realtime API using WebSockets. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHow does Pusher possibly stay within the 60 requests/minute regulation as highlighted in \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/API\\\"\\u003EReddit\\u0026#39;s API Rules\\u003C/a\\u003E? Are there any other existing free alternatives to Pusher or known techniques for converting the REST-based API in to a streaming API?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"[Pusher](https://blog.pusher.com/pusher-realtime-reddit-api/) is a platform that turns Reddit\\u2019s REST-based API into a smooth, realtime API using WebSockets. \\n\\nHow does Pusher possibly stay within the 60 requests/minute regulation as highlighted in [Reddit's API Rules](https://github.com/reddit/reddit/wiki/API)? Are there any other existing free alternatives to Pusher or known techniques for converting the REST-based API in to a streaming API?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3mi1vm\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"LowLanding\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3mi1vm/how_do_platforms_like_pusher_turn_reddits_api_in/\", \"locked\": false, \"name\": \"t3_3mi1vm\", \"created\": 1443326412.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3mi1vm/how_do_platforms_like_pusher_turn_reddits_api_in/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How do platforms like Pusher turn Reddit's API in to a streaming API?\", \"created_utc\": 1443297612.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis used to happen to me a while ago but then it stopped. Maybe upgrading to iOS9 is related?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnyway, an example.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI browse Reddit on my iPhone (5) and everything\\u0026#39;s normal:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://imgur.com/q24yVk0\\\"\\u003Ehttp://imgur.com/q24yVk0\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ethen I follow a link to imgur (but it happens with other sites as well), use the back button to return to Reddit and this is what I see:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://imgur.com/BOlFdM1\\\"\\u003Ehttp://imgur.com/BOlFdM1\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ethe font is huge all of a sudden. Anyone know why this happens, or how I can stop it?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAt the moment I just open everything in a new tab, but that\\u0026#39;s a pain.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This used to happen to me a while ago but then it stopped. Maybe upgrading to iOS9 is related?\\n\\nAnyway, an example.\\n\\nI browse Reddit on my iPhone (5) and everything's normal:\\n\\nhttp://imgur.com/q24yVk0\\n\\nthen I follow a link to imgur (but it happens with other sites as well), use the back button to return to Reddit and this is what I see:\\n\\nhttp://imgur.com/BOlFdM1\\n\\nthe font is huge all of a sudden. Anyone know why this happens, or how I can stop it?\\n\\nAt the moment I just open everything in a new tab, but that's a pain.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3lqh0e\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3lqh0e/reddit_on_mobile_safari_blows_up_font_size_when/\", \"locked\": false, \"name\": \"t3_3lqh0e\", \"created\": 1442819825.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3lqh0e/reddit_on_mobile_safari_blows_up_font_size_when/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit on mobile safari blows up font size when you navigate away and back again -- anyone else seeing this? [Pics inside]\", \"created_utc\": 1442791025.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003E\\u003Ca href=\\\"https://uforio.com\\\"\\u003EUforio\\u003C/a\\u003E\\u003C/strong\\u003E is an alternative Reddit client, which our team has been developing for the last half a year. It can organize your reddit feed by content\\u0026#39;s type (links, pictures, videos and etc.) and allows you to view content without leaving the site. Today we are ready to start a public beta-testing of its web-version (mac/ios/android versions are coming next year) and we invite you to participate in it. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYou can try it right now:\\n\\u003Ca href=\\\"http://web.uforio.com/\\\"\\u003Ehttp://web.uforio.com/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EWe will be happy to receive any feedback, so, please, feel free to leave your comments, remarks and suggestions in this thread.\\u003C/strong\\u003E We also suggest you to subscribe to the \\u003Ca href=\\\"/r/uforio\\\"\\u003E/r/uforio\\u003C/a\\u003E subreddit to keep updated about the latest news.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf you have found a bug when using our beta-version, please, inform us about it in this thread:\\n\\u003Ca href=\\\"https://www.reddit.com/r/Uforio/comments/3gyi9y/betaversion_bugs_issues_official_thread/\\\"\\u003Ehttps://www.reddit.com/r/Uforio/comments/3gyi9y/betaversion_bugs_issues_official_thread/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello!\\n\\n**[Uforio](https://uforio.com)** is an alternative Reddit client, which our team has been developing for the last half a year. It can organize your reddit feed by content's type (links, pictures, videos and etc.) and allows you to view content without leaving the site. Today we are ready to start a public beta-testing of its web-version (mac/ios/android versions are coming next year) and we invite you to participate in it. \\n\\nYou can try it right now:\\nhttp://web.uforio.com/\\n\\n**We will be happy to receive any feedback, so, please, feel free to leave your comments, remarks and suggestions in this thread.** We also suggest you to subscribe to the /r/uforio subreddit to keep updated about the latest news.\\n\\nIf you have found a bug when using our beta-version, please, inform us about it in this thread:\\nhttps://www.reddit.com/r/Uforio/comments/3gyi9y/betaversion_bugs_issues_official_thread/\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3kw89p\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"serzhb\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1442313523.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3kw89p/uforio_is_an_alternative_client_for_reddit_we_are/\", \"locked\": false, \"name\": \"t3_3kw89p\", \"created\": 1442256646.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3kw89p/uforio_is_an_alternative_client_for_reddit_we_are/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Uforio is an alternative client for Reddit. We are starting our public beta-testing. Welcome aboard!\", \"created_utc\": 1442227846.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI read through the ratelimitting section of the API documentation and I couldn\\u0026#39;t find anything about whether the API ratelimits are based on IP addresses or per registered user. I\\u0026#39;d like to run a few bots on the same host but one of them needs to use the maximum number of requests allowed per minute.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I read through the ratelimitting section of the API documentation and I couldn't find anything about whether the API ratelimits are based on IP addresses or per registered user. I'd like to run a few bots on the same host but one of them needs to use the maximum number of requests allowed per minute.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3jtv82\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Golden_Narwhal\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3jtv82/are_ratelimits_based_on_ip_addresses_or_per/\", \"locked\": false, \"name\": \"t3_3jtv82\", \"created\": 1441546755.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3jtv82/are_ratelimits_based_on_ip_addresses_or_per/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Are ratelimits based on IP addresses or per registered user?\", \"created_utc\": 1441517955.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m currently writing a reddit bot, and I want to use Heroku Scheduler with the hopes that I won\\u0026#39;t need to pay anything. I know Heroku has changed their pricing policies recently, but hopefully it\\u0026#39;s good enough for my needs.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy questions:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003EIf I schedule a job for every 10 minutes, and that job takes 3 minutes to run, does the remaining 7 minutes count as sleeping or inactivity?\\u003C/li\\u003E\\n\\u003Cli\\u003EAssuming the answer to 1 is inactivity, let\\u0026#39;s say I schedule a job for every hour, and that job also takes 3 minutes to run. So then of the 57 remaining minutes, does that mean I\\u0026#39;d only get 27 minutes of sleep time, since it sleeps after 30 minutes of inactivity?\\u003C/li\\u003E\\n\\u003Cli\\u003ECan I use something like \\u003Ca href=\\\"http://apscheduler.readthedocs.org/en/latest/userguide.html#adding-jobs\\\"\\u003EAPSchedule\\u003C/a\\u003E in replacement of Heroku Scheduler and expect the same/similar results?\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EThanks in advance!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: fixed words\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm currently writing a reddit bot, and I want to use Heroku Scheduler with the hopes that I won't need to pay anything. I know Heroku has changed their pricing policies recently, but hopefully it's good enough for my needs.\\n\\nMy questions:\\n\\n1. If I schedule a job for every 10 minutes, and that job takes 3 minutes to run, does the remaining 7 minutes count as sleeping or inactivity?\\n2. Assuming the answer to 1 is inactivity, let's say I schedule a job for every hour, and that job also takes 3 minutes to run. So then of the 57 remaining minutes, does that mean I'd only get 27 minutes of sleep time, since it sleeps after 30 minutes of inactivity?\\n3. Can I use something like [APSchedule](http://apscheduler.readthedocs.org/en/latest/userguide.html#adding-jobs) in replacement of Heroku Scheduler and expect the same/similar results?\\n\\nThanks in advance!\\n\\nEdit: fixed words\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3je21a\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"hizinfiz\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1441220754.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3je21a/noob_questions_about_scheduling_on_heroku/\", \"locked\": false, \"name\": \"t3_3je21a\", \"created\": 1441247803.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3je21a/noob_questions_about_scheduling_on_heroku/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Noob questions about scheduling on Heroku\", \"created_utc\": 1441219003.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EBeen a few posts now where I would post a Youtube link and the thumb wouldn\\u0026#39;t appear and I\\u0026#39;d use the \\u0026quot;retry thumb\\u0026quot; button and just nothing happens. In fact, when I try to submit that same video anywhere on reddit, the thumb doesn\\u0026#39;t appear (which also prevents RES from embedding a video player). \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/r/leagueoflegends/comments/3ja6j9/saintvicious_league_of_legends_patch_517_rundown/\\\"\\u003ECurrent example\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Been a few posts now where I would post a Youtube link and the thumb wouldn't appear and I'd use the \\\"retry thumb\\\" button and just nothing happens. In fact, when I try to submit that same video anywhere on reddit, the thumb doesn't appear (which also prevents RES from embedding a video player). \\n\\n[Current example](https://www.reddit.com/r/leagueoflegends/comments/3ja6j9/saintvicious_league_of_legends_patch_517_rundown/)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3jaoe9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"corylulu\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3jaoe9/retry_thumb_appears_to_be_bugged/\", \"locked\": false, \"name\": \"t3_3jaoe9\", \"created\": 1441185577.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3jaoe9/retry_thumb_appears_to_be_bugged/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"\\\"retry thumb\\\" appears to be bugged.\", \"created_utc\": 1441156777.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have an \\u003Ca href=\\\"https://github.com/SeriousBug/redditcurl\\\"\\u003Eopen-source application\\u003C/a\\u003E that depends on the Reddit API. There is one thing I don\\u0026#39;t understand about the switch to OAuth2, looking at the \\u003Ca href=\\\"https://praw.readthedocs.org/en/v3.1.0/pages/oauth.html#step-2-setting-up-praw\\\"\\u003Edocumentation for PRAW\\u003C/a\\u003E, I see that I need to keep \\u003Ccode\\u003Eclient_secret\\u003C/code\\u003E as a secret. However, how can I do that when the application is open source? Am I supposed to ask all the users to register their application?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EReading the \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/OAuth2\\\"\\u003Ereddit\\u0026#39;s documentation\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003EInstalled app: Runs on devices you don\\u0026#39;t control, such as the user\\u0026#39;s mobile phone. Cannot keep a secret, and therefore, does not receive one.\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EThe correct solution is to pick an installed app.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have an [open-source application](https://github.com/SeriousBug/redditcurl) that depends on the Reddit API. There is one thing I don't understand about the switch to OAuth2, looking at the [documentation for PRAW](https://praw.readthedocs.org/en/v3.1.0/pages/oauth.html#step-2-setting-up-praw), I see that I need to keep `client_secret` as a secret. However, how can I do that when the application is open source? Am I supposed to ask all the users to register their application?\\n\\nEdit: \\n\\nReading the [reddit's documentation](https://github.com/reddit/reddit/wiki/OAuth2)\\n\\n\\u003E Installed app: Runs on devices you don't control, such as the user's mobile phone. Cannot keep a secret, and therefore, does not receive one.\\n\\nThe correct solution is to pick an installed app.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ipjs6\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"SeriousBug\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1440877002.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ipjs6/oauth2_with_open_source_apps/\", \"locked\": false, \"name\": \"t3_3ipjs6\", \"created\": 1440784950.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ipjs6/oauth2_with_open_source_apps/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OAuth2 with open source apps\", \"created_utc\": 1440756150.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe top-level \\u003Ccode\\u003Emore\\u003C/code\\u003E object returned in \\u003Ca href=\\\"https://www.reddit.com/r/IAmA/comments/3hq15d/i_am_actor_patrick_stewart_of_yorkshire_xmen_star/\\\"\\u003EPatrick Stewart\\u0026#39;s AMA\\u003C/a\\u003E currently contains 4,530 children. Which means that the length of the request to \\u003Ca href=\\\"https://www.reddit.com/dev/api#GET_api_morechildren\\\"\\u003EGET /api/morechildren\\u003C/a\\u003E is 45,366 characters long.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAside from capping how many children are passed (for example, only including 100 id36s), is there any workaround for this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The top-level `more` object returned in [Patrick Stewart's AMA](https://www.reddit.com/r/IAmA/comments/3hq15d/i_am_actor_patrick_stewart_of_yorkshire_xmen_star/) currently contains 4,530 children. Which means that the length of the request to [GET /api/morechildren](https://www.reddit.com/dev/api#GET_api_morechildren) is 45,366 characters long.\\n\\nAside from capping how many children are passed (for example, only including 100 id36s), is there any workaround for this?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3hr3ls\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"joemtz\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1440097157.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3hr3ls/get_apimorechildren_returns_414_requesturi_too/\", \"locked\": false, \"name\": \"t3_3hr3ls\", \"created\": 1440125655.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3hr3ls/get_apimorechildren_returns_414_requesturi_too/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"GET /api/morechildren returns \\\"414 Request-URI Too Large\\\" error when there is too many children. Any workarounds?\", \"created_utc\": 1440096855.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cpre\\u003E\\u003Ccode\\u003E$.getJSON(\\u0026#39;http://www.reddit.com/r/woahdude/hot/.json\\u0026#39;, function(jsonData){\\n    children = jsonData.data.children;\\n    for (i = 0; i \\u0026lt; children.length; i++){ \\n        console.log(\\u0026quot;thumb: \\u0026quot;+children[i].data.thumbnail);\\n    }\\n});\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eajax works as expected. other subs return thumbnails no problem. anyone know what\\u0026#39;s up with \\u003Ca href=\\\"/r/woahdude\\\"\\u003E/r/woahdude\\u003C/a\\u003E?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"\\t$.getJSON('http://www.reddit.com/r/woahdude/hot/.json', function(jsonData){\\n\\t\\tchildren = jsonData.data.children;\\n        for (i = 0; i \\u003C children.length; i++){ \\n            console.log(\\\"thumb: \\\"+children[i].data.thumbnail);\\n        }\\n    });\\n\\najax works as expected. other subs return thumbnails no problem. anyone know what's up with /r/woahdude?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3hih56\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"SamSlate\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3hih56/getjson_for_rwoahdude_returns_no_thumbnails/\", \"locked\": false, \"name\": \"t3_3hih56\", \"created\": 1439969322.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3hih56/getjson_for_rwoahdude_returns_no_thumbnails/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"getJSON for /r/woahdude returns no thumbnails; anyone know why?\", \"created_utc\": 1439940522.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EFor example, \\u003Ca href=\\\"https://www.reddit.com/r/all/comments.json?limit=2\\\"\\u003Ehttps://www.reddit.com/r/all/comments.json?limit=2\\u003C/a\\u003E. I can visit that in my browser, but my server gets a 403 using several different methods, including wget.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: The inconsistent enforcement makes me think that Reddit is trying to determine if the request is coming from a non-human before responding.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"For example, https://www.reddit.com/r/all/comments.json?limit=2. I can visit that in my browser, but my server gets a 403 using several different methods, including wget.\\n\\nEdit: The inconsistent enforcement makes me think that Reddit is trying to determine if the request is coming from a non-human before responding.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3g0047\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"lecherous_hump\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1438867540.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3g0047/getting_403s_on_json_urls_but_only_when_i_fetch/\", \"locked\": false, \"name\": \"t3_3g0047\", \"created\": 1438895088.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3g0047/getting_403s_on_json_urls_but_only_when_i_fetch/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Getting 403s on .json URLs, but only when I fetch them from my server, and only sometimes. Are these considered part of the API and are now blocked for bots without authentication?\", \"created_utc\": 1438866288.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m working on an application that uses the creddit scope to gild users. Right now, each time I want to fully test the application, I have to purchase a creddit for use with it. It would be really helpful if a special-case dummy account were made that could simulate the proper gilding responses without actually spending creddits.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm working on an application that uses the creddit scope to gild users. Right now, each time I want to fully test the application, I have to purchase a creddit for use with it. It would be really helpful if a special-case dummy account were made that could simulate the proper gilding responses without actually spending creddits.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3fk4c8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"okofish\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3fk4c8/creddit_scopetesting_account/\", \"locked\": false, \"name\": \"t3_3fk4c8\", \"created\": 1438586818.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3fk4c8/creddit_scopetesting_account/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Creddit scope-testing account\", \"created_utc\": 1438558018.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m trying to set up a small VPS with reddit installed to play around with theming and stuff.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m following \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu\\\"\\u003Ethis tutorial\\u003C/a\\u003E on an Ubuntu 12.04.5 x64 VPS with 4GB RAM, as per the tutorial.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHowever, when I run the \\u003Ccode\\u003Einstall-reddit.sh\\u003C/code\\u003E script, I get the following error:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E./install-reddit.sh: line 188:  1844 Segmentation fault      (core dumped) apt-mark hold cassandra\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EAny ideas as to how I could go about fixing this?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cdel\\u003EEDIT: This can be fixed by simply running \\u003Ccode\\u003Esudo apt-mark unhold cassandra\\u003C/code\\u003E :)\\u003C/del\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT #2: So, running that command just broke my install... 0/10 would recommend\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey!\\n\\n\\nI'm trying to set up a small VPS with reddit installed to play around with theming and stuff.\\n\\nI'm following [this tutorial](https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu) on an Ubuntu 12.04.5 x64 VPS with 4GB RAM, as per the tutorial.\\n\\nHowever, when I run the `install-reddit.sh` script, I get the following error:\\n\\n\\u003E ./install-reddit.sh: line 188:  1844 Segmentation fault      (core dumped) apt-mark hold cassandra\\n\\nAny ideas as to how I could go about fixing this?\\n\\n~~EDIT: This can be fixed by simply running `sudo apt-mark unhold cassandra` :)~~\\n\\nEDIT #2: So, running that command just broke my install... 0/10 would recommend\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3d3cd7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Caschorm\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1436771634.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3d3cd7/problems_using_reddit_install_script_for_ubuntu/\", \"locked\": false, \"name\": \"t3_3d3cd7\", \"created\": 1436791702.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3d3cd7/problems_using_reddit_install_script_for_ubuntu/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Problems Using \\\"reddit install script for Ubuntu\\\"\", \"created_utc\": 1436762902.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo I have installed reddit clone on my machine and everything seems to be working perfectly as there are no errors in crons.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHowever the posts only appear in /new (\\u003Ca href=\\\"http://www.karrmic.com/new/\\\"\\u003Ehttp://www.karrmic.com/new/\\u003C/a\\u003E) not in Top(\\u003Ca href=\\\"http://www.karrmic.com/\\\"\\u003Ehttp://www.karrmic.com/\\u003C/a\\u003E) or Rising (\\u003Ca href=\\\"http://www.karrmic.com/rising/\\\"\\u003Ehttp://www.karrmic.com/rising/\\u003C/a\\u003E).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso, the system doesnt seem to be storing the upvotes or downvotes. Refreshing the page loses the votes and it looks as if I haven\\u0026#39;t upvoted it.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny help would be appreciated\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So I have installed reddit clone on my machine and everything seems to be working perfectly as there are no errors in crons.\\n\\nHowever the posts only appear in /new (http://www.karrmic.com/new/) not in Top(http://www.karrmic.com/) or Rising (http://www.karrmic.com/rising/).\\n\\nAlso, the system doesnt seem to be storing the upvotes or downvotes. Refreshing the page loses the votes and it looks as if I haven't upvoted it.\\n\\nAny help would be appreciated\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3cfkrz\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"karrmic\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3cfkrz/installing_reddit_clone/\", \"locked\": false, \"name\": \"t3_3cfkrz\", \"created\": 1436310271.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3cfkrz/installing_reddit_clone/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Installing reddit clone\", \"created_utc\": 1436281471.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m picking up Android Development and as a side project to better my skills and I would like to work with JSON parsing. I figured I would use Reddit\\u0026#39;s API to search any subreddit for images and gifs. I\\u0026#39;ve been looking through the documentation to see if there is any easy way to tell if a post is an image/gif but I wasn\\u0026#39;t able to find it. I was hoping it would be something like: \\u0026quot;type\\u0026quot;:\\u0026quot;image/gif\\u0026quot;.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there a way to know if the post included a gif/image?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks for the help! \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm picking up Android Development and as a side project to better my skills and I would like to work with JSON parsing. I figured I would use Reddit's API to search any subreddit for images and gifs. I've been looking through the documentation to see if there is any easy way to tell if a post is an image/gif but I wasn't able to find it. I was hoping it would be something like: \\\"type\\\":\\\"image/gif\\\".\\n\\nIs there a way to know if the post included a gif/image?\\n\\nThanks for the help! \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ccn2d\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"_who_lee_oh\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ccn2d/reddit_api_how_to_get_all_images_and_gifs_from/\", \"locked\": false, \"name\": \"t3_3ccn2d\", \"created\": 1436247614.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ccn2d/reddit_api_how_to_get_all_images_and_gifs_from/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit API: How to get all images and gifs from any subreddit?\", \"created_utc\": 1436218814.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIt is \\u003Ca href=\\\"https://developers.facebook.com/docs/plugins/page-plugin\\\"\\u003Every easy\\u003C/a\\u003E to make a button for your facebook page containing name, number of likes, and a link to allow people to easily like your page. I want to do the same thing for subreddit. Basically I want a button to stick on my website that people can click to subscribe  that also shows the number of subscribers.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHas anyone made something like this before? If not, would anyone else be interested in something like this? I wanna this for \\u003Ca href=\\\"http://hashmov.com/\\\"\\u003EMy website\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"It is [very easy](https://developers.facebook.com/docs/plugins/page-plugin) to make a button for your facebook page containing name, number of likes, and a link to allow people to easily like your page. I want to do the same thing for subreddit. Basically I want a button to stick on my website that people can click to subscribe  that also shows the number of subscribers.\\n\\nHas anyone made something like this before? If not, would anyone else be interested in something like this? I wanna this for [My website](http://hashmov.com/)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3bqf2o\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"TGoldOrDie1\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3bqf2o/subscribe_buttons_like_facebook_like_box/\", \"locked\": false, \"name\": \"t3_3bqf2o\", \"created\": 1435769328.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3bqf2o/subscribe_buttons_like_facebook_like_box/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"\\\"Subscribe\\\" buttons like facebook \\\"Like\\\" Box?\", \"created_utc\": 1435740528.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis has been a problem since report reasons were introduced. I\\u0026#39;ve stumbled over it a couple of times but never got to writing it down.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m not sure what it counts, but it\\u0026#39;s definitely not the number of reports in all cases. Might be the number of reports with the same text as the last report added. Could also be somehow confused by both mod_reports and user_reports being present.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/r/worldnews/comments/3abcdv/israeli_youths_arrested_after_arson_attack_on/\\\"\\u003EHere\\u003C/a\\u003E is an example where the numbers are as follows:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u0026quot;user_reports\\u0026quot;: [[\\u0026quot;some reason\\u0026quot;, 2], [\\u0026quot;another reason\\u0026quot;, 1], [\\u0026quot;a third reason\\u0026quot;, 1]]\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u0026quot;mod_reports\\u0026quot;: [[\\u0026quot;automod reason\\u0026quot;, \\u0026quot;AutoModerator\\u0026quot;]]\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u0026quot;num_reports\\u0026quot;: 1\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis is bad because some AutoModerator rules react to number of reports in order to alert mods of submissions/comments users take issue with, but in some cases it will never trigger the action despite there being numerous reports.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This has been a problem since report reasons were introduced. I've stumbled over it a couple of times but never got to writing it down.\\n\\nI'm not sure what it counts, but it's definitely not the number of reports in all cases. Might be the number of reports with the same text as the last report added. Could also be somehow confused by both mod_reports and user_reports being present.\\n\\n[Here](http://www.reddit.com/r/worldnews/comments/3abcdv/israeli_youths_arrested_after_arson_attack_on/) is an example where the numbers are as follows:\\n\\n\\\"user_reports\\\": [[\\\"some reason\\\", 2], [\\\"another reason\\\", 1], [\\\"a third reason\\\", 1]]\\n\\n\\\"mod_reports\\\": [[\\\"automod reason\\\", \\\"AutoModerator\\\"]]\\n\\n\\\"num_reports\\\": 1\\n\\nThis is bad because some AutoModerator rules react to number of reports in order to alert mods of submissions/comments users take issue with, but in some cases it will never trigger the action despite there being numerous reports.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ac7cz\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"green_flash\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ac7cz/bug_num_reports_attribute_does_not_match_number/\", \"locked\": false, \"name\": \"t3_3ac7cz\", \"created\": 1434695581.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ac7cz/bug_num_reports_attribute_does_not_match_number/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Bug: num_reports attribute does not match number of reports\", \"created_utc\": 1434666781.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"39qjyx\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"benr783\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/39qjyx/orangeoauthdead_simple_library_for_reddit_oauth/\", \"locked\": false, \"name\": \"t3_39qjyx\", \"created\": 1434254292.0, \"url\": \"https://github.com/contextapps/OrangeOAuth\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OrangeOAuth-Dead simple library for Reddit OAuth on iOS.\", \"created_utc\": 1434225492.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been having some trouble getting OAuth to work without using a browser to get the code (step 3 in the \\u003Ca href=\\\"http://praw.readthedocs.org/en/latest/pages/oauth.html\\\"\\u003Etutorial\\u003C/a\\u003E).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat I want to do get the url code (for r.get_access_information(url code)) which will then allow me to get an access and refresh token for my script through praw so that it uses OAuth instead of user/password.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have been trying to write some code using the requests library and I can get an access token, but I can\\u0026#39;t seem to get praw to work with it, and I can\\u0026#39;t get the url code with which I can get a refresh token and so hopefully let praw use the refresh token to set itself up correctly but I haven\\u0026#39;t gotten that far yet. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo in short, I want to either get a url code or a refresh token without having to open the Allow page in a browser.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been having some trouble getting OAuth to work without using a browser to get the code (step 3 in the [tutorial](http://praw.readthedocs.org/en/latest/pages/oauth.html)).\\n\\nWhat I want to do get the url code (for r.get_access_information(url code)) which will then allow me to get an access and refresh token for my script through praw so that it uses OAuth instead of user/password.\\n\\nI have been trying to write some code using the requests library and I can get an access token, but I can't seem to get praw to work with it, and I can't get the url code with which I can get a refresh token and so hopefully let praw use the refresh token to set itself up correctly but I haven't gotten that far yet. \\n\\nSo in short, I want to either get a url code or a refresh token without having to open the Allow page in a browser.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"38lo61\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"haiguise1\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/38lo61/praw_and_oauth/\", \"locked\": false, \"name\": \"t3_38lo61\", \"created\": 1433494202.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/38lo61/praw_and_oauth/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"PRAW and OAuth\", \"created_utc\": 1433465402.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi \\u003Ca href=\\\"/r/redditdev\\\"\\u003Er/redditdev\\u003C/a\\u003E,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve been having a pretty good luck transitioning to the OAuth API from the cookie API. I seem to have no issues getting a user\\u0026#39;s subreddits, subreddit things, comments, messages... everything that can be retrieved with a GET request. However, I can\\u0026#39;t get any POSTS to work... I tried to use /api/subscribe and /api/vote, but reddit always returns a 404 to me.\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EI use the standard \\u0026quot;code\\u0026quot; flow rather than the implicit flow.\\u003C/li\\u003E\\n\\u003Cli\\u003EI have set my app to be an \\u0026quot;installed type\\u0026quot; via the app\\u0026#39;s prefs on reddit.com. \\u003C/li\\u003E\\n\\u003Cli\\u003EGET requests with same OAuth code (except setting method to POST) work.\\u003C/li\\u003E\\n\\u003Cli\\u003ESame call with cookie API has worked.\\u003C/li\\u003E\\n\\u003Cli\\u003EUsed the \\u0026quot;sr\\u003Cem\\u003Ename\\u0026quot; parameter to subscribe with a name like \\u0026quot;cars\\u0026quot; in my current code. Tried passing a full thing ID as \\u0026quot;sr\\u0026quot; with a t5\\u003C/em\\u003E prefixed id and it was still 404.\\u003C/li\\u003E\\n\\u003Cli\\u003ETried re-adding modhash and api_type which I figured was not needed. Didn\\u0026#39;t work as expected.\\u003C/li\\u003E\\n\\u003Cli\\u003ENot requesting the right scopes led to access forbidden and headers indicated that I needed the subscribe or vote scopes. After requesting those, reddit responds with 404 instead.\\u003C/li\\u003E\\n\\u003Cli\\u003EUrl I copied from my debug logs: \\u003Ca href=\\\"https://oauth.reddit.com/api/subscribe\\\"\\u003Ehttps://oauth.reddit.com/api/subscribe\\u003C/a\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EI am using raw Java on Android without any libraries:\\n\\u003Ca href=\\\"https://github.com/btmura/rbb/blob/master/src/com/btmura/android/reddit/net/RedditApi2.java#L143\\\"\\u003Ehttps://github.com/btmura/rbb/blob/master/src/com/btmura/android/reddit/net/RedditApi2.java#L143\\u003C/a\\u003E\\n\\u003Ca href=\\\"https://github.com/btmura/rbb/blob/master/src/com/btmura/android/reddit/net/Urls2.java#L73\\\"\\u003Ehttps://github.com/btmura/rbb/blob/master/src/com/btmura/android/reddit/net/Urls2.java#L73\\u003C/a\\u003E\\n\\u003Ca href=\\\"https://github.com/btmura/rbb/blob/master/src/com/btmura/android/reddit/net/Urls2.java#L53\\\"\\u003Ehttps://github.com/btmura/rbb/blob/master/src/com/btmura/android/reddit/net/Urls2.java#L53\\u003C/a\\u003E\\n\\u003Ca href=\\\"https://github.com/btmura/rbb/blob/master/src/com/btmura/android/reddit/net/Urls2.java#L407\\\"\\u003Ehttps://github.com/btmura/rbb/blob/master/src/com/btmura/android/reddit/net/Urls2.java#L407\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny ideas? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks in advance!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi r/redditdev,\\n\\nI've been having a pretty good luck transitioning to the OAuth API from the cookie API. I seem to have no issues getting a user's subreddits, subreddit things, comments, messages... everything that can be retrieved with a GET request. However, I can't get any POSTS to work... I tried to use /api/subscribe and /api/vote, but reddit always returns a 404 to me.\\n\\n- I use the standard \\\"code\\\" flow rather than the implicit flow.\\n- I have set my app to be an \\\"installed type\\\" via the app's prefs on reddit.com. \\n- GET requests with same OAuth code (except setting method to POST) work.\\n- Same call with cookie API has worked.\\n- Used the \\\"sr_name\\\" parameter to subscribe with a name like \\\"cars\\\" in my current code. Tried passing a full thing ID as \\\"sr\\\" with a t5_ prefixed id and it was still 404.\\n- Tried re-adding modhash and api_type which I figured was not needed. Didn't work as expected.\\n- Not requesting the right scopes led to access forbidden and headers indicated that I needed the subscribe or vote scopes. After requesting those, reddit responds with 404 instead.\\n- Url I copied from my debug logs: https://oauth.reddit.com/api/subscribe\\n\\nI am using raw Java on Android without any libraries:\\nhttps://github.com/btmura/rbb/blob/master/src/com/btmura/android/reddit/net/RedditApi2.java#L143\\nhttps://github.com/btmura/rbb/blob/master/src/com/btmura/android/reddit/net/Urls2.java#L73\\nhttps://github.com/btmura/rbb/blob/master/src/com/btmura/android/reddit/net/Urls2.java#L53\\nhttps://github.com/btmura/rbb/blob/master/src/com/btmura/android/reddit/net/Urls2.java#L407\\n\\nAny ideas? \\n\\nThanks in advance!\\n\\n\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"381w1q\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"btmura\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/381w1q/getting_404_from_oauth_post_apis_like_apisubscribe/\", \"locked\": false, \"name\": \"t3_381w1q\", \"created\": 1433169748.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/381w1q/getting_404_from_oauth_post_apis_like_apisubscribe/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Getting 404 from OAuth POST APIs like /api/subscribe\", \"created_utc\": 1433140948.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve mentioned this before but it seems to still be broken.  When requesting a list of objects by their id where one doesn\\u0026#39;t exist, the API returns an empty children array within the data field.  It should make a best effort and not error out completely when it encounters a missing object. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETest Case:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESuccessful Call:  \\u003Ca href=\\\"http://www.reddit.com/api/info.json?id=t3_37wvvr\\\"\\u003Ehttp://www.reddit.com/api/info.json?id=t3_37wvvr\\u003C/a\\u003E (this object exists)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFailed Call: \\u003Ca href=\\\"http://www.reddit.com/api/info.json?id=t3_37wvvr,t3_999999\\\"\\u003Ehttp://www.reddit.com/api/info.json?id=t3_37wvvr,t3_999999\\u003C/a\\u003E (one object exists, the other does not)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EObviously, submission t3_999999 hasn\\u0026#39;t occurred yet, but the API should still return an array with the found objects and not return a null children array.  Either it should return the same response as the successful call above or indicate that the second object is missing (easier to just do the former and assume missing id\\u0026#39;s aren\\u0026#39;t available within the application logic).  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI would imagine this should be an easy fix in the code by simply not having the loop error out when it encounters a missing object id.  I haven\\u0026#39;t looked at the source but that\\u0026#39;s my guess as to what is happening.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've mentioned this before but it seems to still be broken.  When requesting a list of objects by their id where one doesn't exist, the API returns an empty children array within the data field.  It should make a best effort and not error out completely when it encounters a missing object. \\n\\nTest Case:\\n\\nSuccessful Call:  http://www.reddit.com/api/info.json?id=t3_37wvvr (this object exists)\\n\\nFailed Call: http://www.reddit.com/api/info.json?id=t3_37wvvr,t3_999999 (one object exists, the other does not)\\n\\nObviously, submission t3_999999 hasn't occurred yet, but the API should still return an array with the found objects and not return a null children array.  Either it should return the same response as the successful call above or indicate that the second object is missing (easier to just do the former and assume missing id's aren't available within the application logic).  \\n\\nI would imagine this should be an easy fix in the code by simply not having the loop error out when it encounters a missing object id.  I haven't looked at the source but that's my guess as to what is happening.\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"37wxca\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1433038721.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/37wxca/bug_with_api_endpoint_apiinfo/\", \"locked\": false, \"name\": \"t3_37wxca\", \"created\": 1433067130.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/37wxca/bug_with_api_endpoint_apiinfo/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Bug with API endpoint /api/info\", \"created_utc\": 1433038330.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"355r7d\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Steak_59\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 41, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/355r7d/is_there_a_setting_for_you_to_be_notified_by_your/\", \"locked\": false, \"name\": \"t3_355r7d\", \"created\": 1431021306.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/355r7d/is_there_a_setting_for_you_to_be_notified_by_your/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a setting for you to be notified by your linked email account when you receive a PM or comment notification in your Reddit mail box? If not WHY is this not an option?\", \"created_utc\": 1430992506.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EEditing ini configuration for my reddit clone doesn\\u0026#39;t appear to have any effect.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve edited \\u003Ccode\\u003Edevelopment.update\\u003C/code\\u003E to include \\u003Ccode\\u003Eshort_description = Something else\\u003C/code\\u003E and run \\u003Ccode\\u003Emake ini\\u003C/code\\u003E and \\u003Ccode\\u003Esudo reddit-restart\\u003C/code\\u003E, but the browser tab still shows \\u0026quot;open source is awesome\\u0026quot;. I also tried putting that configuration in a different \\u003Ccode\\u003Esomething.update\\u003C/code\\u003E file, but that too had no effect.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAm I missing something?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Editing ini configuration for my reddit clone doesn't appear to have any effect.\\n\\nI've edited `development.update` to include `short_description = Something else` and run `make ini` and `sudo reddit-restart`, but the browser tab still shows \\\"open source is awesome\\\". I also tried putting that configuration in a different `something.update` file, but that too had no effect.\\n\\nAm I missing something?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"33xvz7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"philalether\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/33xvz7/reddit_clone_update_config_files/\", \"locked\": false, \"name\": \"t3_33xvz7\", \"created\": 1430100700.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/33xvz7/reddit_clone_update_config_files/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit clone .update config files\", \"created_utc\": 1430071900.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m working on a pull request for PRAW that adds multireddits, or at least the basics. However, all of my DELETE requests are giving me 403 \\u0026quot;please sign in\\u0026quot; errors.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://i.imgur.com/pp4tg9B.png\\\"\\u003Ehttp://i.imgur.com/pp4tg9B.png\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://i.imgur.com/bGPrgrb.png\\\"\\u003Ehttp://i.imgur.com/bGPrgrb.png\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/dev/api/oauth#DELETE_api_multi_%7Bmultipath%7D_r_%7Bsrname%7D\\\"\\u003Ehttp://www.reddit.com/dev/api/oauth#DELETE_api_multi_{multipath}_r_{srname}\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EClearly, post and put both work just fine, but delete fails even though it has the same modhash as the others. I tried watching what Chrome does when deleting a subreddit, but it doesn\\u0026#39;t seem to have any special headers.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOverall, I\\u0026#39;ve found the Multireddit api to be kinda inconsistent and disjointed from everything else. Multireddits and /api/v1/me/friends/ are the only things to use a Delete method, everything else uses a Post to a specific deletion url. Is there any reason for this, and is it related to my problem?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny help is appreciated.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: \\u003Ca href=\\\"https://github.com/praw-dev/praw/pull/404\\\"\\u003EPull request Part 1\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm working on a pull request for PRAW that adds multireddits, or at least the basics. However, all of my DELETE requests are giving me 403 \\\"please sign in\\\" errors.\\n\\nhttp://i.imgur.com/pp4tg9B.png\\n\\nhttp://i.imgur.com/bGPrgrb.png\\n\\nhttp://www.reddit.com/dev/api/oauth#DELETE_api_multi_{multipath}_r_{srname}\\n\\nClearly, post and put both work just fine, but delete fails even though it has the same modhash as the others. I tried watching what Chrome does when deleting a subreddit, but it doesn't seem to have any special headers.\\n\\nOverall, I've found the Multireddit api to be kinda inconsistent and disjointed from everything else. Multireddits and /api/v1/me/friends/ are the only things to use a Delete method, everything else uses a Post to a specific deletion url. Is there any reason for this, and is it related to my problem?\\n\\n\\nAny help is appreciated.\\n\\nEdit: [Pull request Part 1](https://github.com/praw-dev/praw/pull/404).\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"33rz9o\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"GoldenSights\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1429940278.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/33rz9o/all_delete_requests_are_returning_403_please_sign/\", \"locked\": false, \"name\": \"t3_33rz9o\", \"created\": 1429954283.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/33rz9o/all_delete_requests_are_returning_403_please_sign/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"All DELETE requests are returning 403 \\\"Please sign in to do that\\\"\", \"created_utc\": 1429925483.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey everybody!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m part of a research team studying Reddit. We\\u0026#39;ve been collecting and analyzing data for the past 2 years using PRAW. We have a server that is constantly collecting data from 2013, and it\\u0026#39;s been hitting server errors for the past week or so. The error is \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Erequests.exceptions.HTTPError: 500 Server Error: Server Error\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EWe\\u0026#39;ve tried using a VPN to see if this was caused by our IP, but it seems that it\\u0026#39;s not IP related. I tried using PRAW in the console to manually pull just one submission to see if it\\u0026#39;s a problem with our script, and even that one pull didn\\u0026#39;t work. The code I used was:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport praw\\nr = praw.Reddit(user_agent=\\u0026#39;MrFanzyPanz miscellanious data collector for research\\u0026#39;)\\nr.get_info(thing_id=\\u0026#39;t3_1km4e2\\u0026#39;)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI have the latest version of PRAW and am operating in Python 2.7.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECan anybody help me figure this out? It seems like this is an internal problem with PRAW.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey everybody!\\n\\nI'm part of a research team studying Reddit. We've been collecting and analyzing data for the past 2 years using PRAW. We have a server that is constantly collecting data from 2013, and it's been hitting server errors for the past week or so. The error is \\n\\n    requests.exceptions.HTTPError: 500 Server Error: Server Error\\n\\nWe've tried using a VPN to see if this was caused by our IP, but it seems that it's not IP related. I tried using PRAW in the console to manually pull just one submission to see if it's a problem with our script, and even that one pull didn't work. The code I used was:\\n\\n    import praw\\n    r = praw.Reddit(user_agent='MrFanzyPanz miscellanious data collector for research')\\n    r.get_info(thing_id='t3_1km4e2')\\n\\nI have the latest version of PRAW and am operating in Python 2.7.\\n\\nCan anybody help me figure this out? It seems like this is an internal problem with PRAW.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"31wpmp\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MrFanzyPanz\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/31wpmp/praw_suddenly_returning_endless_http_errors/\", \"locked\": false, \"name\": \"t3_31wpmp\", \"created\": 1428549693.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/31wpmp/praw_suddenly_returning_endless_http_errors/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"PRAW suddenly returning endless HTTP errors\", \"created_utc\": 1428520893.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been using reddit\\u0026#39;s OAuth2 to allow users to log into my website, but one thing I\\u0026#39;ve noticed is that I frequently get http 503 errors on the very first API call with a fresh access token. Last night, I tried to log in 5 times in a row, and the reddit \\u0026quot;allow access\\u0026quot; page worked just fine, and so did the \\u003Ccode\\u003Eaccess_token\\u003C/code\\u003E endpoint, but I got a 503 error on \\u003Ccode\\u003Ehttps://oauth.reddit.com/api/v1/me.json\\u003C/code\\u003E every single time. I\\u0026#39;m not asking for much, I just want the username!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis is seriously degrading the experience for my users when it happens. If it was reddit returning the 503 on the allow access page at least it wouldn\\u0026#39;t look like a problem with my site. And why does \\u003Ccode\\u003Eaccess_token\\u003C/code\\u003E work every time but not \\u003Ccode\\u003Eme.json\\u003C/code\\u003E?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI don\\u0026#39;t believe this is a rate limiting issue because I\\u0026#39;m not making many requests, plus I would expect an http 429 \\u0026quot;Too Many Requests\\u0026quot; error in that case, not a 503 \\u0026quot;Service Unavailable\\u0026quot;.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been using reddit's OAuth2 to allow users to log into my website, but one thing I've noticed is that I frequently get http 503 errors on the very first API call with a fresh access token. Last night, I tried to log in 5 times in a row, and the reddit \\\"allow access\\\" page worked just fine, and so did the `access_token` endpoint, but I got a 503 error on `https://oauth.reddit.com/api/v1/me.json` every single time. I'm not asking for much, I just want the username!\\n\\nThis is seriously degrading the experience for my users when it happens. If it was reddit returning the 503 on the allow access page at least it wouldn't look like a problem with my site. And why does `access_token` work every time but not `me.json`?\\n\\nI don't believe this is a rate limiting issue because I'm not making many requests, plus I would expect an http 429 \\\"Too Many Requests\\\" error in that case, not a 503 \\\"Service Unavailable\\\".\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"31ub2d\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"sub_surfer\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1428470712.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/31ub2d/frequent_503_errors_from_reddit_api_with_fresh/\", \"locked\": false, \"name\": \"t3_31ub2d\", \"created\": 1428498827.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/31ub2d/frequent_503_errors_from_reddit_api_with_fresh/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"frequent 503 errors from reddit API with fresh access token\", \"created_utc\": 1428470027.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi, i\\u0026#39;m currently working on creating an app to browse reddit on android for a school project.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI can request a token without any problem but then when I try to use it I get shutdown by reddit because it thinks i\\u0026#39;m a bot. I tried making my user agent as descriptive as possible and not making too many requests per minute but even making 2 requests seems to be too much. It says that I should not make more than 1 request per 2 seconds and I complied but somehow I get blocked anyway. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDoes anybody what i\\u0026#39;m doing wrong? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThank you\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eedit:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks everyone for taking the time to help me out. Turns out I just had to add the User-Agent to all my requests as a header. I was wrongly under the impression that it was no longer necessary after getting my access token.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EhttpGet.addHeader(\\u0026quot;User-Agent\\u0026quot;, userAgent);\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ethis is the full response I get:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026lt;title\\u0026gt;Too Many Requests\\u0026lt;/title\\u0026gt;    \\u0026lt;style\\u0026gt;      body {          font: small verdana, arial, helvetica, sans-serif;  \\nwidth: 600px;          margin: 0 auto;      }      h1 {          height: 40px;       \\nbackground: transparent url(//www.redditstatic.com/reddit.com.header.png) no-repeat scroll top right;      }  \\n\\u0026lt;/style\\u0026gt;  \\u0026lt;/head\\u0026gt;  \\u0026lt;body\\u0026gt;    \\u0026lt;h1\\u0026gt;whoa there, pardner!\\u0026lt;/h1\\u0026gt;  \\n\\u0026lt;p\\u0026gt;we\\u0026#39;re sorry, but you appear to be a bot and we\\u0026#39;ve seen too many requestsfrom you lately. \\nwe enforce a hard speed limit on requests that appear to comefrom bots to prevent abuse.\\u0026lt;/p\\u0026gt;\\n\\u0026lt;p\\u0026gt;if you are not a bot but are spoofing one via your browser\\u0026#39;s user agentstring: please change your user agent string to avoid seeing this messageagain.\\u0026lt;/p\\u0026gt;\\n\\u0026lt;p\\u0026gt;please wait 3 second(s) and try again.\\u0026lt;/p\\u0026gt;    \\u0026lt;p\\u0026gt;as a reminder to developers, we recommend that clients make no  \\nmore than \\u0026lt;a href=\\u0026quot;http://github.com/reddit/reddit/wiki/API\\u0026quot;\\u0026gt;one    request every two seconds\\u0026lt;/a\\u0026gt; to avoid seeing this message.\\u0026lt;/p\\u0026gt;  \\u0026lt;/body\\u0026gt;\\u0026lt;/html\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi, i'm currently working on creating an app to browse reddit on android for a school project.\\n\\nI can request a token without any problem but then when I try to use it I get shutdown by reddit because it thinks i'm a bot. I tried making my user agent as descriptive as possible and not making too many requests per minute but even making 2 requests seems to be too much. It says that I should not make more than 1 request per 2 seconds and I complied but somehow I get blocked anyway. \\n\\nDoes anybody what i'm doing wrong? \\n\\nThank you\\n\\nedit:\\n\\nThanks everyone for taking the time to help me out. Turns out I just had to add the User-Agent to all my requests as a header. I was wrongly under the impression that it was no longer necessary after getting my access token.\\n\\nhttpGet.addHeader(\\\"User-Agent\\\", userAgent);\\n\\nthis is the full response I get:\\n\\n\\t\\u003Ctitle\\u003EToo Many Requests\\u003C/title\\u003E    \\u003Cstyle\\u003E      body {          font: small verdana, arial, helvetica, sans-serif;  \\n\\twidth: 600px;          margin: 0 auto;      }      h1 {          height: 40px;       \\n\\tbackground: transparent url(//www.redditstatic.com/reddit.com.header.png) no-repeat scroll top right;      }  \\n\\t\\u003C/style\\u003E  \\u003C/head\\u003E  \\u003Cbody\\u003E    \\u003Ch1\\u003Ewhoa there, pardner!\\u003C/h1\\u003E  \\n\\t\\u003Cp\\u003Ewe're sorry, but you appear to be a bot and we've seen too many requestsfrom you lately. \\n\\twe enforce a hard speed limit on requests that appear to comefrom bots to prevent abuse.\\u003C/p\\u003E\\n\\t\\u003Cp\\u003Eif you are not a bot but are spoofing one via your browser's user agentstring: please change your user agent string to avoid seeing this messageagain.\\u003C/p\\u003E\\n\\t\\u003Cp\\u003Eplease wait 3 second(s) and try again.\\u003C/p\\u003E    \\u003Cp\\u003Eas a reminder to developers, we recommend that clients make no  \\n\\tmore than \\u003Ca href=\\\"http://github.com/reddit/reddit/wiki/API\\\"\\u003Eone    request every two seconds\\u003C/a\\u003E to avoid seeing this message.\\u003C/p\\u003E  \\u003C/body\\u003E\\u003C/html\\u003E\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"31g31k\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Megaminx1900\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1428202401.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/31g31k/making_request_to_reddit_api_without_being_blocked/\", \"locked\": false, \"name\": \"t3_31g31k\", \"created\": 1428204148.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/31g31k/making_request_to_reddit_api_without_being_blocked/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Making request to reddit API without being blocked\", \"created_utc\": 1428175348.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EFyi, the votes api now throws an error when you pass in things that you can\\u0026#39;t vote on instead of happily returning 200 no matter what.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Fyi, the votes api now throws an error when you pass in things that you can't vote on instead of happily returning 200 no matter what.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2ziwiy\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"thorarakis\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2ziwiy/votes_api_now_returns_400s_on_error/\", \"locked\": false, \"name\": \"t3_2ziwiy\", \"created\": 1426749004.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2ziwiy/votes_api_now_returns_400s_on_error/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Votes api now returns 400's on error\", \"created_utc\": 1426720204.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EE.g., if I use PRAW to fetch a user\\u0026#39;s inbox, is that data passed over the wire encrypted or is it in plaintext?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"E.g., if I use PRAW to fetch a user's inbox, is that data passed over the wire encrypted or is it in plaintext?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2yhhm3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"cyanoge\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2yhhm3/prawpythonare_requests_encrypted/\", \"locked\": false, \"name\": \"t3_2yhhm3\", \"created\": 1425964936.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2yhhm3/prawpythonare_requests_encrypted/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[PRAW][python]Are requests encrypted?\", \"created_utc\": 1425936136.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m not sure if this is the right place, so mods feel free to delete.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have a question about bots. Here\\u0026#39;s my predicament:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EI\\u0026#39;m having a sub where users vote on posts\\u003C/li\\u003E\\n\\u003Cli\\u003EI would like to know when \\u0026gt;50% of \\u0026quot;members\\u0026quot; have upvoted a post\\u003C/li\\u003E\\n\\u003Cli\\u003EI was originally going to only let subscribers vote, and then just see when \\u0026gt;50% of subscribers had upvoted\\u003C/li\\u003E\\n\\u003Cli\\u003EThat\\u0026#39;s impossible.\\u003C/li\\u003E\\n\\u003Cli\\u003ESo I thought, and I came up with this:\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EUse CSS to:\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003ESet the background image for \\u003Ccode\\u003Eupvote:click\\u003C/code\\u003E to a pixel (different pixels for subscribers and otherwise)\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003EThat pixel would then be hosted by a bot.\\u003C/li\\u003E\\n\\u003Cli\\u003EThe bot would then tabulate the upvotes based on the number of pixels requested. (And verify this with the post\\u0026#39;s karma)\\u003C/li\\u003E\\n\\u003Cli\\u003ECompute the number of upvotes needed as \\u003Ccode\\u003E(.5 * (%not_subscribers / %subscribers + 1) * total_subscribers) + 1\\u003C/code\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003EIf that number of upvotes has been reached, do stuff (give the award, comment, change flair, send a PM).\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m curious, If I put somewhere in the sidebar a privacy policy (We don\\u0026#39;t store your data. We only store if you are a subscriber, and no identifying information), is that against the rules of reddit (The pixel thing)?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\nAri\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EP.S. I also accept reasons why this is a terrible idea.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi!\\n\\nI'm not sure if this is the right place, so mods feel free to delete.\\n\\nI have a question about bots. Here's my predicament:\\n\\n* I'm having a sub where users vote on posts\\n* I would like to know when \\u003E50% of \\\"members\\\" have upvoted a post\\n* I was originally going to only let subscribers vote, and then just see when \\u003E50% of subscribers had upvoted\\n* That's impossible.\\n* So I thought, and I came up with this:\\n    * Use CSS to:\\n       * Set the background image for `upvote:click` to a pixel (different pixels for subscribers and otherwise)\\n    * That pixel would then be hosted by a bot.\\n    * The bot would then tabulate the upvotes based on the number of pixels requested. (And verify this with the post's karma)\\n    * Compute the number of upvotes needed as `(.5 * (%not_subscribers / %subscribers + 1) * total_subscribers) + 1`\\n    * If that number of upvotes has been reached, do stuff (give the award, comment, change flair, send a PM).\\n\\nI'm curious, If I put somewhere in the sidebar a privacy policy (We don't store your data. We only store if you are a subscriber, and no identifying information), is that against the rules of reddit (The pixel thing)?\\n\\nThanks!\\nAri\\n\\nP.S. I also accept reasons why this is a terrible idea.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2ya2ix\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ariporad\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2ya2ix/question_about_bots_pixels/\", \"locked\": false, \"name\": \"t3_2ya2ix\", \"created\": 1425794548.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2ya2ix/question_about_bots_pixels/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Question about bots \\u0026 pixels.\", \"created_utc\": 1425765748.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EDoes the dev team keep any kind of list of low-hanging fruit that might be possible for outsiders who want to contribute to attack?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Does the dev team keep any kind of list of low-hanging fruit that might be possible for outsiders who want to contribute to attack?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2wxwut\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"evman182\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2wxwut/list_of_lowhanging_fruit/\", \"locked\": false, \"name\": \"t3_2wxwut\", \"created\": 1424771951.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2wxwut/list_of_lowhanging_fruit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"List of low-hanging fruit?\", \"created_utc\": 1424743151.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2vwj9j\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"jzelinskie\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2vwj9j/comments_api_returns_an_empty_string_instead_of/\", \"locked\": false, \"name\": \"t3_2vwj9j\", \"created\": 1423973826.0, \"url\": \"https://github.com/reddit/reddit/issues/662\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Comments API returns an empty string instead of None for the field \\\"replies\\\" when there are no replies\", \"created_utc\": 1423945026.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}], \"after\": \"t3_2vwj9j\", \"before\": null}}"
        },
        "headers": {
          "CF-RAY": "2b59b9d087b620c6-LAX",
          "Connection": "keep-alive",
          "Content-Type": "application/json; charset=UTF-8",
          "Date": "Sun, 19 Jun 2016 20:28:10 GMT",
          "Server": "cloudflare-nginx",
          "Strict-Transport-Security": "max-age=15552000; includeSubDomains; preload",
          "Transfer-Encoding": "chunked",
          "Vary": "accept-encoding",
          "X-Moose": "majestic",
          "cache-control": "private, s-maxage=0, max-age=0, must-revalidate",
          "expires": "-1",
          "x-content-type-options": "nosniff",
          "x-frame-options": "SAMEORIGIN",
          "x-ratelimit-remaining": "579.0",
          "x-ratelimit-reset": "111",
          "x-ratelimit-used": "21",
          "x-reddit-tracking": "https://pixel.redditmedia.com/pixel/of_destiny.png?v=nrV7l%2BueAfnvbuZ0uckHKMqwk1xYIu8d6K%2FpSY872AEDoKrvJeD8CUgribaek%2BIf1G5o9qwCa5%2B86sVcBDMQC3AtJy%2BhWhhE",
          "x-ua-compatible": "IE=edge",
          "x-xss-protection": "1; mode=block"
        },
        "status": {
          "code": 200,
          "message": "OK"
        },
        "url": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_11yttx&raw_json=1"
      }
    },
    {
      "recorded_at": "2016-06-19T20:28:11",
      "request": {
        "body": {
          "encoding": "utf-8",
          "string": ""
        },
        "headers": {
          "Accept": "*/*",
          "Accept-Encoding": "identity",
          "Authorization": "bearer YE4ESXZ9lTjIP0ukcopwXM6361g",
          "Connection": "keep-alive",
          "Cookie": "loid=SdAZIdDQoUDzjNqfRk; loidcreated=2016-06-19T20%3A27%3A59.507Z; __cfduid=da3dbedd6bfa050f4f12fea14395b830f1466368079",
          "User-Agent": "prawtools/0.19 PRAW/4.0.0b4 prawcore/0.0.8"
        },
        "method": "GET",
        "uri": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_2vwj9j&raw_json=1"
      },
      "response": {
        "body": {
          "encoding": "UTF-8",
          "string": "{\"kind\": \"Listing\", \"data\": {\"modhash\": null, \"children\": [{\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am a survey researcher by trade (think Gallup political polling) and the mod over at \\u003Ca href=\\\"/r/surveyresearch\\\"\\u003E/r/surveyresearch\\u003C/a\\u003E. I\\u0026#39;ve noticed that the current state of surveys conducted on reddit are a pretty unscientific and questionable at best. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOne of the thing I\\u0026#39;ve been kicking around is, figuring out how to survey subreddits without the fear of a self selection bias or trolls flooding the survey biasing the results. In the survey world, we like to send unique links out to people so we can control who does and does not get links, ensuring only our random sample of responses are untampered with.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy current idea is, with the permissions of the mods in a given subreddit, I would like to have my bot to send +500 PM\\u0026#39;s over the course of a day inviting users to complete a survey. But I obviously do not want to do anything to get banned. Are there any limits or considerations I need to take into account? For example, I do know my bot will need a minimum level of karma to bypass the captcha. Is there anything else?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe API documentation just says not to make more than 30 request per minute. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThoughts? Suggestions? Concerns?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENote: All surveys I would conduct would be for research purposes only, I would not be messaging anyone to sell or promote a product. Most it would be for producing accurate demographic and opinion based information of the active user base, either to entertain the subreddit or inform the moderators.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am a survey researcher by trade (think Gallup political polling) and the mod over at /r/surveyresearch. I've noticed that the current state of surveys conducted on reddit are a pretty unscientific and questionable at best. \\n\\nOne of the thing I've been kicking around is, figuring out how to survey subreddits without the fear of a self selection bias or trolls flooding the survey biasing the results. In the survey world, we like to send unique links out to people so we can control who does and does not get links, ensuring only our random sample of responses are untampered with.\\n\\nMy current idea is, with the permissions of the mods in a given subreddit, I would like to have my bot to send +500 PM's over the course of a day inviting users to complete a survey. But I obviously do not want to do anything to get banned. Are there any limits or considerations I need to take into account? For example, I do know my bot will need a minimum level of karma to bypass the captcha. Is there anything else?\\n\\nThe API documentation just says not to make more than 30 request per minute. \\n\\nThoughts? Suggestions? Concerns?\\n\\nNote: All surveys I would conduct would be for research purposes only, I would not be messaging anyone to sell or promote a product. Most it would be for producing accurate demographic and opinion based information of the active user base, either to entertain the subreddit or inform the moderators.\\n\\n\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2ugd78\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Adamworks\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1422885133.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2ugd78/is_there_limit_to_the_number_of_pms_a_bot_can/\", \"locked\": false, \"name\": \"t3_2ugd78\", \"created\": 1422860000.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2ugd78/is_there_limit_to_the_number_of_pms_a_bot_can/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there limit to the number of PM's a bot can send out before it gets flagged as spam?\", \"created_utc\": 1422831200.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAs in I don\\u0026#39;t want to get into trouble for appearing to pretend I\\u0026#39;m affiliated with reddit or anything.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EJust want to ensure what I\\u0026#39;m doing is allowed and everything that needs to be stated on the site is stated.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"As in I don't want to get into trouble for appearing to pretend I'm affiliated with reddit or anything.\\n\\nJust want to ensure what I'm doing is allowed and everything that needs to be stated on the site is stated.\\n\\nThanks\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2s388m\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bustyLaserCannon\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1421007853.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2s388m/made_a_webapp_that_uses_reddit_api_to_display/\", \"locked\": false, \"name\": \"t3_2s388m\", \"created\": 1421036219.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2s388m/made_a_webapp_that_uses_reddit_api_to_display/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Made a webapp that uses reddit API to display some images do I need to state anything regarding non-affiliation with reddit?\", \"created_utc\": 1421007419.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EVisit \\u003Ca href=\\\"http://www.reddit.com/register.compact\\\"\\u003Ewww.reddit.com/register.compact\\u003C/a\\u003E on an iPhone safari and fill in the password field.  The password field fills in but it\\u0026#39;s invisible and the user can\\u0026#39;t read it. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe confirmation password field works (shows black text). \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI was able to reproduce this issue on an android phone as well. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny help is appreciated. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Visit www.reddit.com/register.compact on an iPhone safari and fill in the password field.  The password field fills in but it's invisible and the user can't read it. \\n\\nThe confirmation password field works (shows black text). \\n\\nI was able to reproduce this issue on an android phone as well. \\n\\nAny help is appreciated. \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2qdkk4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"sunny001\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2qdkk4/registercompact_has_an_invisible_password_text/\", \"locked\": false, \"name\": \"t3_2qdkk4\", \"created\": 1419558487.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2qdkk4/registercompact_has_an_invisible_password_text/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"register.compact has an invisible password text entry.\", \"created_utc\": 1419529687.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi all. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAs you might know at \\u003Ca href=\\\"/r/millionairemakers\\\"\\u003E/r/millionairemakers\\u003C/a\\u003E we are trying to run a drawing using the comments retrieved from reddit API. Our picking method is described in detail \\u003Ca href=\\\"http://www.reddit.com/r/millionairemakers/comments/2ournt/explanation_of_our_new_drawing_system_inspired_by/\\\"\\u003Ehere\\u003C/a\\u003E. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have wrote a python script to grab the comments and generate the winner using PRAW, but the issue is that apparently 110K comments is too much for the reddit API to handle. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe code is available here: \\n\\u003Ca href=\\\"https://github.com/millionairemakers/millionairemakers\\\"\\u003Ehttps://github.com/millionairemakers/millionairemakers\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMore specifically, the issue rises in \\u003Ca href=\\\"https://github.com/millionairemakers/millionairemakers/blob/master/webserver.py\\\"\\u003Ethis\\u003C/a\\u003E file at line 152, where the system tries to replace the \\u0026quot;more\\u0026quot; comments.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esubmission.replace_more_comments(limit=None, threshold=0)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI get the following error:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EException in thread Thread-1:\\nTraceback (most recent call last):\\n  File \\u0026quot;/usr/lib64/python2.7/threading.py\\u0026quot;, line 811, in __bootstrap_inner\\n    self.run()\\n  File \\u0026quot;webserver.py\\u0026quot;, line 150, in run\\n    submission.replace_more_comments(limit=None, threshold=0)\\n  File \\u0026quot;/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/objects.py\\u0026quot;, line 1029, in replace_more_comments\\n    new_comments = item.comments(update=False)\\n  File \\u0026quot;/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/objects.py\\u0026quot;, line 638, in comments\\n    response = self.reddit_session.request_json(url, data=data)\\n  File \\u0026quot;/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/decorators.py\\u0026quot;, line 161, in wrapped\\n    return_value = function(reddit_session, *args, **kwargs)\\n  File \\u0026quot;/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/__init__.py\\u0026quot;, line 519, in request_json\\n    response = self._request(url, params, data)\\n  File \\u0026quot;/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/__init__.py\\u0026quot;, line 383, in _request\\n    _raise_response_exceptions(response)\\n  File \\u0026quot;/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/internal.py\\u0026quot;, line 172, in _raise_response_exceptions\\n    response.raise_for_status()\\n  File \\u0026quot;/usr/lib/python2.7/site-packages/requests-2.5.0-py2.7.egg/requests/models.py\\u0026quot;, line 829, in raise_for_status\\n    raise HTTPError(http_error_msg, response=self)\\nHTTPError: 413 Client Error: Too Big\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI know this is coming from reddit and not from the client, but is there anyway to fix this?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi all. \\n\\nAs you might know at /r/millionairemakers we are trying to run a drawing using the comments retrieved from reddit API. Our picking method is described in detail [here](http://www.reddit.com/r/millionairemakers/comments/2ournt/explanation_of_our_new_drawing_system_inspired_by/). \\n\\nI have wrote a python script to grab the comments and generate the winner using PRAW, but the issue is that apparently 110K comments is too much for the reddit API to handle. \\n\\nThe code is available here: \\nhttps://github.com/millionairemakers/millionairemakers\\n\\nMore specifically, the issue rises in [this](https://github.com/millionairemakers/millionairemakers/blob/master/webserver.py) file at line 152, where the system tries to replace the \\\"more\\\" comments.\\n\\n    submission.replace_more_comments(limit=None, threshold=0)\\n\\nI get the following error:\\n\\n    Exception in thread Thread-1:\\n    Traceback (most recent call last):\\n      File \\\"/usr/lib64/python2.7/threading.py\\\", line 811, in __bootstrap_inner\\n        self.run()\\n      File \\\"webserver.py\\\", line 150, in run\\n        submission.replace_more_comments(limit=None, threshold=0)\\n      File \\\"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/objects.py\\\", line 1029, in replace_more_comments\\n        new_comments = item.comments(update=False)\\n      File \\\"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/objects.py\\\", line 638, in comments\\n        response = self.reddit_session.request_json(url, data=data)\\n      File \\\"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/decorators.py\\\", line 161, in wrapped\\n        return_value = function(reddit_session, *args, **kwargs)\\n      File \\\"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/__init__.py\\\", line 519, in request_json\\n        response = self._request(url, params, data)\\n      File \\\"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/__init__.py\\\", line 383, in _request\\n        _raise_response_exceptions(response)\\n      File \\\"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/internal.py\\\", line 172, in _raise_response_exceptions\\n        response.raise_for_status()\\n      File \\\"/usr/lib/python2.7/site-packages/requests-2.5.0-py2.7.egg/requests/models.py\\\", line 829, in raise_for_status\\n        raise HTTPError(http_error_msg, response=self)\\n    HTTPError: 413 Client Error: Too Big\\n\\nI know this is coming from reddit and not from the client, but is there anyway to fix this?\\n\\nThanks.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2q5zly\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"minlite\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 25, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/\", \"locked\": false, \"name\": \"t3_2q5zly\", \"created\": 1419362013.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Error 413 : Too Big with PRAW\", \"created_utc\": 1419333213.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ELet\\u0026#39;s say we have a particular subreddit ordered by \\u0026quot;top\\u0026quot;. Resulting list would change over time, depending on votes. Is this list cached for any period of time? It looks like if it were recalculated for every request, there would be no way \\u0026quot;before/after\\u0026quot; would work, because posts would travel from one page to another very often.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Let's say we have a particular subreddit ordered by \\\"top\\\". Resulting list would change over time, depending on votes. Is this list cached for any period of time? It looks like if it were recalculated for every request, there would be no way \\\"before/after\\\" would work, because posts would travel from one page to another very often.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2nydlp\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"supergnawer\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2nydlp/does_reddit_cache_post_order/\", \"locked\": false, \"name\": \"t3_2nydlp\", \"created\": 1417482576.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2nydlp/does_reddit_cache_post_order/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Does Reddit cache post order?\", \"created_utc\": 1417453776.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI would strongly prefer a way to do this using JavaScript or Node.js. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI want to bypass the limited number of comments that reddit shows on a page (before requiring me to click \\u0026#39;more comments\\u0026#39;) for a secret project of mine. I may not be able to get all the comments, but I want more than whatever reddit\\u0026#39;s maximum limit is for a single page. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMaintaining tree structure is not necessary.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I would strongly prefer a way to do this using JavaScript or Node.js. \\n\\nI want to bypass the limited number of comments that reddit shows on a page (before requiring me to click 'more comments') for a secret project of mine. I may not be able to get all the comments, but I want more than whatever reddit's maximum limit is for a single page. \\n\\nMaintaining tree structure is not necessary.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2nnaso\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Antrikshy\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2nnaso/how_can_i_get_all_or_most_reddit_comments_from_a/\", \"locked\": false, \"name\": \"t3_2nnaso\", \"created\": 1417185330.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2nnaso/how_can_i_get_all_or_most_reddit_comments_from_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How can I get all (or most) reddit comments from a post?\", \"created_utc\": 1417156530.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EEvening all,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve been playing with OAuth tonight and seem to be getting a strange error.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf I setup a \\u0026quot;web app\\u0026quot; I have no problem grabbing the access_token and using it for future OAuth calls.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHowever if I set up an \\u0026quot;installed app\\u0026quot; and set the grant_type as \\u0026quot;\\u003Ca href=\\\"https://oauth.reddit.com/grants/installed_client\\\"\\u003Ehttps://oauth.reddit.com/grants/installed_client\\u003C/a\\u003E\\u0026quot; I get an access_token (when calling \\u003Ca href=\\\"https://ssl.reddit.com/api/v1/access_token\\\"\\u003Ehttps://ssl.reddit.com/api/v1/access_token\\u003C/a\\u003E) but every OAuth call returns:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E{\\nexplanation: \\u0026quot;please login to do that\\u0026quot;\\nreason: \\u0026quot;USER_REQUIRED\\u0026quot;\\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAny suggestions? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECheers\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Evening all,\\n\\nI've been playing with OAuth tonight and seem to be getting a strange error.\\n\\nIf I setup a \\\"web app\\\" I have no problem grabbing the access_token and using it for future OAuth calls.\\n\\nHowever if I set up an \\\"installed app\\\" and set the grant_type as \\\"https://oauth.reddit.com/grants/installed_client\\\" I get an access_token (when calling https://ssl.reddit.com/api/v1/access_token) but every OAuth call returns:\\n\\n    {\\n    explanation: \\\"please login to do that\\\"\\n    reason: \\\"USER_REQUIRED\\\"\\n    }\\n\\nAny suggestions? \\n\\nCheers\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2nfdur\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ljdawson\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2nfdur/oauth_access_token_token_invalid_when_using_the/\", \"locked\": false, \"name\": \"t3_2nfdur\", \"created\": 1416989409.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2nfdur/oauth_access_token_token_invalid_when_using_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OAuth access_token token invalid when using the grant_type installed_client\", \"created_utc\": 1416960609.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI put together an extremely light-weight bot using \\u003Ca href=\\\"https://github.com/jcleblanc/reddit-php-sdk\\\"\\u003EReddit PHP SDK\\u003C/a\\u003E. I made a new account for the bot, got the required karma to not be forced to enter a Captcha and have made 2 successful posts with it (in my own test sub). The problem is, every time I open the URL to my bot in my browser to execute it\\u0026#39;s logic, it asks me to authenticate the app with my Reddit account. As soon as I click \\u0026#39;Allow It\\u0026#39; then the post is made and it runs great; until I try to do it again.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECan somebody please help me either extend my authentication or somehow prevent it from asking me every time to allow the app? Thanks in advance!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I put together an extremely light-weight bot using [Reddit PHP SDK](https://github.com/jcleblanc/reddit-php-sdk). I made a new account for the bot, got the required karma to not be forced to enter a Captcha and have made 2 successful posts with it (in my own test sub). The problem is, every time I open the URL to my bot in my browser to execute it's logic, it asks me to authenticate the app with my Reddit account. As soon as I click 'Allow It' then the post is made and it runs great; until I try to do it again.\\n\\nCan somebody please help me either extend my authentication or somehow prevent it from asking me every time to allow the app? Thanks in advance!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2emi95\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"WDKevin\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2emi95/bot_works_but_makes_me_authenticate_my_app_every/\", \"locked\": false, \"name\": \"t3_2emi95\", \"created\": 1409088686.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2emi95/bot_works_but_makes_me_authenticate_my_app_every/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Bot works, but makes me authenticate my app every time it tries to post\", \"created_utc\": 1409059886.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello, I\\u0026#39;m not getting very far in trying out to connect to the live threads via websockets in python.\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Efrom websocket import create_connection\\nws = create_connection(url)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EWhere \\u003Cem\\u003Eurl\\u003C/em\\u003E is \\u003Cem\\u003Ewebsocket_url\\u003C/em\\u003E from  \\u003Ca href=\\\"http://www.reddit.com/live/3rgnbke2rai6hen7ciytwcxadi/about.json\\\"\\u003Ehere\\u003C/a\\u003E and this is all i get:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003ETraceback (most recent call last):\\n  File \\u0026quot;\\u0026lt;stdin\\u0026gt;\\u0026quot;, line 1, in \\u0026lt;module\\u0026gt;\\n  File \\u0026quot;/usr/local/lib/python2.7/dist-packages/websocket/_core.py\\u0026quot;, line 217, in create_connection\\n    websock.connect(url, **options)\\n  File \\u0026quot;/usr/local/lib/python2.7/dist-packages/websocket/_core.py\\u0026quot;, line 466, in connect\\n    self._handshake(hostname, port, resource, **options)\\n  File \\u0026quot;/usr/local/lib/python2.7/dist-packages/websocket/_core.py\\u0026quot;, line 527, in _handshake\\n    resp_headers = self._get_resp_headers()\\n  File \\u0026quot;/usr/local/lib/python2.7/dist-packages/websocket/_core.py\\u0026quot;, line 484, in _get_resp_headers\\n    raise WebSocketException(\\u0026quot;Handshake status %d\\u0026quot; % status)\\nwebsocket._exceptions.WebSocketException: Handshake status 403\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ESo I\\u0026#39;m kinda dead in my tracks. What am I missing?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: \\u003Ca href=\\\"/u/largenocream\\\"\\u003E/u/largenocream\\u003C/a\\u003E got me back on track. So a follow up question, I take it that e=1408904748 from about.json is the expiration time of the websocket?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello, I'm not getting very far in trying out to connect to the live threads via websockets in python.\\n\\n    from websocket import create_connection\\n    ws = create_connection(url)\\n\\nWhere *url* is *websocket_url* from  [here](http://www.reddit.com/live/3rgnbke2rai6hen7ciytwcxadi/about.json) and this is all i get:\\n\\n    Traceback (most recent call last):\\n      File \\\"\\u003Cstdin\\u003E\\\", line 1, in \\u003Cmodule\\u003E\\n      File \\\"/usr/local/lib/python2.7/dist-packages/websocket/_core.py\\\", line 217, in create_connection\\n        websock.connect(url, **options)\\n      File \\\"/usr/local/lib/python2.7/dist-packages/websocket/_core.py\\\", line 466, in connect\\n        self._handshake(hostname, port, resource, **options)\\n      File \\\"/usr/local/lib/python2.7/dist-packages/websocket/_core.py\\\", line 527, in _handshake\\n        resp_headers = self._get_resp_headers()\\n      File \\\"/usr/local/lib/python2.7/dist-packages/websocket/_core.py\\\", line 484, in _get_resp_headers\\n        raise WebSocketException(\\\"Handshake status %d\\\" % status)\\n    websocket._exceptions.WebSocketException: Handshake status 403\\n\\nSo I'm kinda dead in my tracks. What am I missing?\\n\\nEdit: /u/largenocream got me back on track. So a follow up question, I take it that e=1408904748 from about.json is the expiration time of the websocket?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2edh3b\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"lynxlynxlynx-\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1408818395.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2edh3b/live_threads_and_websockets/\", \"locked\": false, \"name\": \"t3_2edh3b\", \"created\": 1408839740.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2edh3b/live_threads_and_websockets/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"live threads and websockets?\", \"created_utc\": 1408810940.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EFor example, i.reddit.com, pay.reddit.com, np.reddit.com (and the rest of the language ones)... Where/how can I get a list of all of them?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"For example, i.reddit.com, pay.reddit.com, np.reddit.com (and the rest of the language ones)... Where/how can I get a list of all of them?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2dh18g\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"eigenheid\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2dh18g/is_there_a_list_of_redditcom_subdomains_not/\", \"locked\": false, \"name\": \"t3_2dh18g\", \"created\": 1407991578.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2dh18g/is_there_a_list_of_redditcom_subdomains_not/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a list of reddit.com subdomains (not subreddits)? If not, how would I create one?\", \"created_utc\": 1407962778.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cpre\\u003E\\u003Ccode\\u003EUser user = new User(\\u0026quot;username\\u0026quot;, \\u0026quot;password\\u0026quot;);\\nuser.connect();\\nboolean gold = user.isGold();\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m trying to do a simple task check if the user has gold but eclipse tells me:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EException in thread \\u0026quot;main\\u0026quot; java.lang.Error: Unresolved compilation problems: \\nThe type org.json.simple.parser.ParseException cannot be resolved. It is indirectly referenced from required .class files\\nThe method isGold() from the type User refers to the missing type ParseException\\nat reddit.reddittest.main(reddittest.java:9)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eanyone here that could help?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"\\tUser user = new User(\\\"username\\\", \\\"password\\\");\\n\\tuser.connect();\\n\\tboolean gold = user.isGold();\\n\\nI'm trying to do a simple task check if the user has gold but eclipse tells me:\\n\\nException in thread \\\"main\\\" java.lang.Error: Unresolved compilation problems: \\nThe type org.json.simple.parser.ParseException cannot be resolved. It is indirectly referenced from required .class files\\nThe method isGold() from the type User refers to the missing type ParseException\\nat reddit.reddittest.main(reddittest.java:9)\\n\\nanyone here that could help?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2col0f\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"algae12\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1407250158.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2col0f/problems_with_jreddit/\", \"locked\": false, \"name\": \"t3_2col0f\", \"created\": 1407270472.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2col0f/problems_with_jreddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"problems with jreddit.\", \"created_utc\": 1407241672.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey, \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI was having major trouble trying to work this out. I was attempting to use \\u003Ca href=\\\"http://guzzle.readthedocs.org/en/latest/\\\"\\u003EGuzzle\\u003C/a\\u003E to fetch the stylesheet using the Reddit API (oAuth) and I kept getting a 400 error, it turns out because the API reroutes you to Amazon AWS to fetch the stylesheet the bearer access token gets pushed with it causing an auth error with Amazon AWS but attempting to remove the bearer will obviously give you an oAuth error with reddit.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI know you can just fetch the stylesheet without using oAuth but it\\u0026#39;s meant having to alter my API class to accommodate for it, it doesn\\u0026#39;t break the API I just thought I\\u0026#39;d make the devs aware of it\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EeNzy\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey, \\n\\nI was having major trouble trying to work this out. I was attempting to use [Guzzle](http://guzzle.readthedocs.org/en/latest/) to fetch the stylesheet using the Reddit API (oAuth) and I kept getting a 400 error, it turns out because the API reroutes you to Amazon AWS to fetch the stylesheet the bearer access token gets pushed with it causing an auth error with Amazon AWS but attempting to remove the bearer will obviously give you an oAuth error with reddit.\\n\\nI know you can just fetch the stylesheet without using oAuth but it's meant having to alter my API class to accommodate for it, it doesn't break the API I just thought I'd make the devs aware of it\\n\\neNzy\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2c9pzb\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"eNzyy\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2c9pzb/possible_api_bug/\", \"locked\": false, \"name\": \"t3_2c9pzb\", \"created\": 1406865341.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2c9pzb/possible_api_bug/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Possible API bug?\", \"created_utc\": 1406836541.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI wrote a very quick bit of code with PRAW that will let you nuke all of your comments to a predefined message\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport praw\\n\\nuser = input(\\u0026#39;Enter username: \\u0026#39;)\\npass = input(\\u0026#39;Enter password: \\u0026#39;)\\nmsg = input(\\u0026#39;Enter replacement message: \\u0026#39;)\\n\\nr = praw.Reddit(user_agent=\\u0026#39;Comment Nuker\\u0026#39;)\\nr.login(user, pass)\\n\\nuser_object = r.get_redditor(user)\\ncomments = user_object.get_comments(limit = None)\\n\\nfor comment in comments:\\n  comment.edit(msg)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIt was mostly just as a way to see if it was doable, but it\\u0026#39;s a way to wipe your comment history.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: fixed a couple syntax typos.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I wrote a very quick bit of code with PRAW that will let you nuke all of your comments to a predefined message\\n\\n    import praw\\n    \\n    user = input('Enter username: ')\\n    pass = input('Enter password: ')\\n    msg = input('Enter replacement message: ')\\n\\n    r = praw.Reddit(user_agent='Comment Nuker')\\n    r.login(user, pass)\\n\\n    user_object = r.get_redditor(user)\\n    comments = user_object.get_comments(limit = None)\\n\\n    for comment in comments:\\n      comment.edit(msg)\\n\\nIt was mostly just as a way to see if it was doable, but it's a way to wipe your comment history.\\n      \\nEdit: fixed a couple syntax typos.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2byjzp\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 65, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1406568582.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2byjzp/comment_nuker/\", \"locked\": false, \"name\": \"t3_2byjzp\", \"created\": 1406595872.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2byjzp/comment_nuker/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Comment Nuker\", \"created_utc\": 1406567072.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWe are developing a small web app similar to downforeveryoneorjustme.com, and we want to see if it would work on reddit when reddit goes down. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHowever we want to inspect their downtime page to see what HTTP status codes are given; but I can\\u0026#39;t think of a way to view that page without waiting for the site to go down!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDoes anyone know what status codes it gives off? Or even better somewhere that I can just go and inspect the page myself?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks a lot!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"We are developing a small web app similar to downforeveryoneorjustme.com, and we want to see if it would work on reddit when reddit goes down. \\n\\nHowever we want to inspect their downtime page to see what HTTP status codes are given; but I can't think of a way to view that page without waiting for the site to go down!\\n\\nDoes anyone know what status codes it gives off? Or even better somewhere that I can just go and inspect the page myself?\\n\\nThanks a lot!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"2bsg4d\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Midasx\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/2bsg4d/is_there_anyway_i_could_view_the_reddit_is_down/\", \"locked\": false, \"name\": \"t3_2bsg4d\", \"created\": 1406422730.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/2bsg4d/is_there_anyway_i_could_view_the_reddit_is_down/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there anyway I could view the \\\"reddit is down\\\" page that is used during maintenance?\", \"created_utc\": 1406393930.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi guys. I\\u0026#39;m trying to make a little program that needs to get the .json for a specific user name. So, for example, if I use my user name I would request \\u003Ca href=\\\"http://www.reddit.com/user/ELFAHBEHT_SOOP/.json\\\"\\u003Ehttp://www.reddit.com/user/ELFAHBEHT_SOOP/.json\\u003C/a\\u003E However, the scores for the comments seem to stop changing after I request the file a few times, yet it updates on the site still. I even made the amount of requests 2 per minute so it wouldn\\u0026#39;t think my program is a malicious bot or something. What gives? Am I missing something?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: I just changed my user-agent and it seems to have worked. It still takes a bit to update, but it\\u0026#39;s working like a charm!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit2: I further changed it to use \\u003Ca href=\\\"https://pay.reddit.com\\\"\\u003Ehttps://pay.reddit.com\\u003C/a\\u003E which you\\u0026#39;re \\u003Cem\\u003Esupposed\\u003C/em\\u003E to use for API requests. I think it works, but there looks like there\\u0026#39;s vote fuzzing going on here.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi guys. I'm trying to make a little program that needs to get the .json for a specific user name. So, for example, if I use my user name I would request http://www.reddit.com/user/ELFAHBEHT_SOOP/.json However, the scores for the comments seem to stop changing after I request the file a few times, yet it updates on the site still. I even made the amount of requests 2 per minute so it wouldn't think my program is a malicious bot or something. What gives? Am I missing something?\\n\\nEdit: I just changed my user-agent and it seems to have worked. It still takes a bit to update, but it's working like a charm!\\n\\nEdit2: I further changed it to use https://pay.reddit.com which you're *supposed* to use for API requests. I think it works, but there looks like there's vote fuzzing going on here.\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"28qlw5\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ELFAHBEHT_SOOP\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1403798455.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/28qlw5/my_requests_for_the_json_on_accounts_seem_to_be/\", \"locked\": false, \"name\": \"t3_28qlw5\", \"created\": 1403405162.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/28qlw5/my_requests_for_the_json_on_accounts_seem_to_be/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"My requests for the json on accounts seem to be outdated. Why?\", \"created_utc\": 1403376362.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EKind of like pushing Tweets to the sidebar. This would be very useful for gaming subreddits.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E[EDIT] Twitter doesn\\u0026#39;t even have RSS anymore, so I\\u0026#39;m at a loss here.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Kind of like pushing Tweets to the sidebar. This would be very useful for gaming subreddits.\\n\\n[EDIT] Twitter doesn't even have RSS anymore, so I'm at a loss here.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"27ej93\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"reseph\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1401999890.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/27ej93/is_there_a_bot_that_pushes_rsscontent_to_the/\", \"locked\": false, \"name\": \"t3_27ej93\", \"created\": 1402022275.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/27ej93/is_there_a_bot_that_pushes_rsscontent_to_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a bot that pushes RSS/content to the sidebar?\", \"created_utc\": 1401993475.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHow do we add other languages to reddit, and where do we get the localization files?\\nthanks\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"How do we add other languages to reddit, and where do we get the localization files?\\nthanks\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"26jdup\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/26jdup/adding_other_languages_to_reddit/\", \"locked\": false, \"name\": \"t3_26jdup\", \"created\": 1401156925.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/26jdup/adding_other_languages_to_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"adding other languages to Reddit\", \"created_utc\": 1401128125.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI posted something interactive over on \\u003Ca href=\\\"/r/dataisbeautiful\\\"\\u003Er/dataisbeautiful\\u003C/a\\u003E, and used an og:image metatag with a thumbnail image of the viz in the hopes reddit would use that as the thumbnail. It works on facebook, but not here. How does reddit scan the page to decide what thumbnail to use? I\\u0026#39;d like to know how to make my thumnail pop up here, as I don\\u0026#39;t plan on this being the last interactive viz I post :) Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I posted something interactive over on r/dataisbeautiful, and used an og:image metatag with a thumbnail image of the viz in the hopes reddit would use that as the thumbnail. It works on facebook, but not here. How does reddit scan the page to decide what thumbnail to use? I'd like to know how to make my thumnail pop up here, as I don't plan on this being the last interactive viz I post :) Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"26c0ox\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Ian2400\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/26c0ox/how_does_reddit_scan_for_thumbnail_images/\", \"locked\": false, \"name\": \"t3_26c0ox\", \"created\": 1400912061.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/26c0ox/how_does_reddit_scan_for_thumbnail_images/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How does reddit scan for thumbnail images?\", \"created_utc\": 1400883261.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"25mvj7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"IndexPlusPlus\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/25mvj7/im_a_new_bot_dev_ive_created_dogehelpbot_a_bot/\", \"locked\": false, \"name\": \"t3_25mvj7\", \"created\": 1400199228.0, \"url\": \"https://github.com/fourohfour/dogehelpbot\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I'm a new bot dev. I've created DogeHelpBot - a bot that spews up help pages for dogecoin on request. Can you check over the code and tell me any awful problems?\", \"created_utc\": 1400170428.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAs far as I can see, reddit only does two things when a post is deleted by its author:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E1) Changes the \\u0026quot;author\\u0026quot; property to \\u0026quot;[deleted]\\u0026quot;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E2) Removes the post from the listings in its subreddit\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENote that the post\\u0026#39;s JSON is still available though. For example, here is the JSON for a test post that I made and deleted:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/by_id/t3_25k8fy.json\\\"\\u003Ehttp://www.reddit.com/by_id/t3_25k8fy.json\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHow long is it until the data there goes away? Or does it stay forever?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"As far as I can see, reddit only does two things when a post is deleted by its author:\\n\\n1) Changes the \\\"author\\\" property to \\\"[deleted]\\\"\\n\\n2) Removes the post from the listings in its subreddit\\n\\nNote that the post's JSON is still available though. For example, here is the JSON for a test post that I made and deleted:\\n\\nhttp://www.reddit.com/by_id/t3_25k8fy.json\\n\\nHow long is it until the data there goes away? Or does it stay forever?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"25kdts\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"aheckler\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1400098520.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/25kdts/how_long_is_the_json_data_for_deleted_posts_kept/\", \"locked\": false, \"name\": \"t3_25kdts\", \"created\": 1400126983.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/25kdts/how_long_is_the_json_data_for_deleted_posts_kept/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How long is the JSON data for deleted posts kept?\", \"created_utc\": 1400098183.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI was thinking of adding an account age check to the bot that helps me mod a subreddit since we\\u0026#39;re occasionally spammed by 0 day old accounts. It wasn\\u0026#39;t going to remove the posts, but rather send a modmail asking the mods to check to make sure it\\u0026#39;s a legit post.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnyway, I can\\u0026#39;t seem to find anything about account ages in either the PRAW code overview or the reddit API documentation. Am I just missing it, or does it not exist?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThank you.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I was thinking of adding an account age check to the bot that helps me mod a subreddit since we're occasionally spammed by 0 day old accounts. It wasn't going to remove the posts, but rather send a modmail asking the mods to check to make sure it's a legit post.\\n\\nAnyway, I can't seem to find anything about account ages in either the PRAW code overview or the reddit API documentation. Am I just missing it, or does it not exist?\\n\\nThank you.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"24vkho\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"JBHUTT09\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/\", \"locked\": false, \"name\": \"t3_24vkho\", \"created\": 1399424192.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a way to get the age of an account?\", \"created_utc\": 1399395392.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI want to make a reddit bot, for myself to send me a message when it finds certain words on a subreddit, I have the program, but whenever I have the reddit bot\\u0026#39;s account send a message, it wants me to type in the image at a URL, this means that I can not automate it completely and I need to be there for every message it sends. I have verified my email, and it still does it.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there some way to get an account specific for a personal use reddit bot? I don\\u0026#39;t care if it can upvote/downvote stuff, I just need it to send me messages (and maybe post replies to very specific questions, on a subreddit that I will start/own/make myself anyway if that is the case), is there a quick way to do this, or do I need to use the bot\\u0026#39;s account for a while and post useful stuff so that it gets enough karma?  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFYI: I want it to be an account that\\u0026#39;s not mine so that I see that I have \\u0026quot;new messages!\\u0026quot;, but it doesn\\u0026#39;t do that when I \\u0026quot;send myself a message\\u0026quot;.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I want to make a reddit bot, for myself to send me a message when it finds certain words on a subreddit, I have the program, but whenever I have the reddit bot's account send a message, it wants me to type in the image at a URL, this means that I can not automate it completely and I need to be there for every message it sends. I have verified my email, and it still does it.  \\n  \\nIs there some way to get an account specific for a personal use reddit bot? I don't care if it can upvote/downvote stuff, I just need it to send me messages (and maybe post replies to very specific questions, on a subreddit that I will start/own/make myself anyway if that is the case), is there a quick way to do this, or do I need to use the bot's account for a while and post useful stuff so that it gets enough karma?  \\n  \\nFYI: I want it to be an account that's not mine so that I see that I have \\\"new messages!\\\", but it doesn't do that when I \\\"send myself a message\\\".\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"249d2o\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"HellFireKoder\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 21, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1398755729.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/249d2o/can_i_create_an_new_account_for_my_reddit_bot_and/\", \"locked\": false, \"name\": \"t3_249d2o\", \"created\": 1398784345.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/249d2o/can_i_create_an_new_account_for_my_reddit_bot_and/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can I Create An New Account For My Reddit Bot, and Have It Send Me Messages Right Away?\", \"created_utc\": 1398755545.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have a script that manages flair on one of my subreddits, and it has started failing as of a day ago.  I looked at the output, and the POST to \\u003Ca href=\\\"/r/subreddit/api/flaircsv\\\"\\u003E/r/subreddit/api/flaircsv\\u003C/a\\u003E.json now fails.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnyone else having this issue? Has something changed?  I don\\u0026#39;t see anything here in \\u003Ca href=\\\"/r/redditdev\\\"\\u003E/r/redditdev\\u003C/a\\u003E or on \\u003Ca href=\\\"/r/changelog\\\"\\u003E/r/changelog\\u003C/a\\u003E ... \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have a script that manages flair on one of my subreddits, and it has started failing as of a day ago.  I looked at the output, and the POST to /r/subreddit/api/flaircsv.json now fails.\\n\\nAnyone else having this issue? Has something changed?  I don't see anything here in /r/redditdev or on /r/changelog ... \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"23svce\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"honestbleeps\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/23svce/flaircsvjson_now_a_404_has_something_changed_or/\", \"locked\": false, \"name\": \"t3_23svce\", \"created\": 1398316515.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/23svce/flaircsvjson_now_a_404_has_something_changed_or/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"flaircsv.json now a 404? has something changed, or is this broken?\", \"created_utc\": 1398287715.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve seen at least one bot on GitHub importing the login details from somewhere. How do I do this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've seen at least one bot on GitHub importing the login details from somewhere. How do I do this?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"22u2rq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Antrikshy\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/22u2rq/nooblet_question_i_made_a_simple_reddit_bot_that/\", \"locked\": false, \"name\": \"t3_22u2rq\", \"created\": 1397306471.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/22u2rq/nooblet_question_i_made_a_simple_reddit_bot_that/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[Nooblet question] I made a simple reddit bot that I want to open-source using PRAW, but it has its password in the code. How do I hide it?\", \"created_utc\": 1397277671.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIs this possible? I have been looking everywhere for the answer.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESorry about all the posts :)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Is this possible? I have been looking everywhere for the answer.\\n\\nSorry about all the posts :)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"22dspy\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"calebkeith\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/22dspy/api_question_get_user_by_fullname_t2_xxxxx/\", \"locked\": false, \"name\": \"t3_22dspy\", \"created\": 1396857071.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/22dspy/api_question_get_user_by_fullname_t2_xxxxx/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API Question: Get user by fullname? (t2_xxxxx)\", \"created_utc\": 1396828271.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIs there a way to get a list of all subreddits in json format or any format for that matter?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Is there a way to get a list of all subreddits in json format or any format for that matter?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"21v30m\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"thebeardwilleatyou\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/21v30m/getting_a_list_of_all_subreddits/\", \"locked\": false, \"name\": \"t3_21v30m\", \"created\": 1396330105.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/21v30m/getting_a_list_of_all_subreddits/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Getting a list of all subreddits\", \"created_utc\": 1396301305.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have a reddit instance setup and I would like to set up the search features on it. I read the post  \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/wqx7o/does_reddit_using_amazon_cloud_search/\\\"\\u003Ehere\\u003C/a\\u003E. Which is a great start. Could some one point me to more detailed documentation or be able to provide some help?\\nThanks\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have a reddit instance setup and I would like to set up the search features on it. I read the post  [here](http://www.reddit.com/r/redditdev/comments/wqx7o/does_reddit_using_amazon_cloud_search/). Which is a great start. Could some one point me to more detailed documentation or be able to provide some help?\\nThanks\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"20qudv\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"slcclimber1\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/20qudv/reddit_with_amazon_cloud_search/\", \"locked\": false, \"name\": \"t3_20qudv\", \"created\": 1395201592.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/20qudv/reddit_with_amazon_cloud_search/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit with Amazon Cloud Search\", \"created_utc\": 1395172792.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello \\u003Ca href=\\\"/r/redditdev\\\"\\u003E/r/redditdev\\u003C/a\\u003E \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m trying to write a simple reddit bot but can\\u0026#39;t figure out why I\\u0026#39;m not able to access the API. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEvery attempt is responded with an HTTP 429 (too many requests).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI didn\\u0026#39;t sent to many requests and also tried to wait an entire night - the next morning the response is still 429. The user agent is changed to an unique string.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am probably overlooking something very simple but can\\u0026#39;t figure it out.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy code (PHP) is \\u003Ca href=\\\"https://gist.github.com/Nijin22/61b3327a599ccd04b41b\\\"\\u003Ethis\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks for your help!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello /r/redditdev \\n\\nI'm trying to write a simple reddit bot but can't figure out why I'm not able to access the API. \\n\\nEvery attempt is responded with an HTTP 429 (too many requests).\\n\\nI didn't sent to many requests and also tried to wait an entire night - the next morning the response is still 429. The user agent is changed to an unique string.\\n\\nI am probably overlooking something very simple but can't figure it out.\\n\\nMy code (PHP) is [this](https://gist.github.com/Nijin22/61b3327a599ccd04b41b).\\n\\nThanks for your help!\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"20bl0u\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"TimezoneSimplifier\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/20bl0u/http_429_and_no_idea_why/\", \"locked\": false, \"name\": \"t3_20bl0u\", \"created\": 1394751981.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/20bl0u/http_429_and_no_idea_why/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"HTTP 429 and no idea why\", \"created_utc\": 1394723181.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1zxw9m\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Stuck_In_the_Matrix\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1zxw9m/after_how_many_days_does_a_reddit_post_go_into/\", \"locked\": false, \"name\": \"t3_1zxw9m\", \"created\": 1394364910.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1zxw9m/after_how_many_days_does_a_reddit_post_go_into/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"After how many days does a Reddit Post go into archives (no more voting, commenting, etc.) ?\", \"created_utc\": 1394336110.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWould anyone be interested in writing a bot for \\u003Ca href=\\\"/r/KnowYourShit\\\"\\u003E/r/KnowYourShit\\u003C/a\\u003E? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EA bot that would post the top post from four different subs every 12 hours or so. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBasically making a collection of the best of \\u003Ca href=\\\"/r/howto\\\"\\u003E/r/howto\\u003C/a\\u003E, \\u003Ca href=\\\"/r/lifeprotips\\\"\\u003E/r/lifeprotips\\u003C/a\\u003E, \\u003Ca href=\\\"/r/todayilearned\\\"\\u003E/r/todayilearned\\u003C/a\\u003E and \\u003Ca href=\\\"/r/youshouldknow\\\"\\u003E/r/youshouldknow\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Would anyone be interested in writing a bot for /r/KnowYourShit? \\n\\nA bot that would post the top post from four different subs every 12 hours or so. \\n\\nBasically making a collection of the best of /r/howto, /r/lifeprotips, /r/todayilearned and /r/youshouldknow\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1wzwcg\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"hero0fwar\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1wzwcg/would_anyone_be_interested_in_writing_a_bot_for/\", \"locked\": false, \"name\": \"t3_1wzwcg\", \"created\": 1391561388.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1wzwcg/would_anyone_be_interested_in_writing_a_bot_for/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Would anyone be interested in writing a bot for /r/KnowYourShit?\", \"created_utc\": 1391532588.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m writing a bot that finds and removes posts/comments that violate my subreddit\\u0026#39;s rules. When a comment is deleted, the bot leaves a comment for the user explaining the removal. The problem is, I can\\u0026#39;t find a way to distinguish normal comments from ones that have already been removed, so every time the bot runs, it adds another explanation comment. Any ideas?\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT:\\u003C/strong\\u003E I have a pseudo-solution, I found out about praw.helpers.comment_stream() and I found get_spam(), so I\\u0026#39;m basically checking that each comment is \\u003Cem\\u003Enot in\\u003C/em\\u003E the spam list yet, and only then will the bot comment on it. The problem is, I\\u0026#39;m not seeing what the limit is on returned spam comments/posts. Is there a chance I could end up checking a comment from comment_stream() that wasn\\u0026#39;t returned by get_spam()? Ideally I\\u0026#39;d like something like \\u003Cem\\u003Ecomment.is_removed\\u003C/em\\u003E or \\u003Cem\\u003Ecomment.is_spam\\u003C/em\\u003E but I\\u0026#39;m not finding anything like that... \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EQuestion part 2:\\u003C/strong\\u003E Using get_spam(), is there a way to return comments only (instead of comments and posts)?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm writing a bot that finds and removes posts/comments that violate my subreddit's rules. When a comment is deleted, the bot leaves a comment for the user explaining the removal. The problem is, I can't find a way to distinguish normal comments from ones that have already been removed, so every time the bot runs, it adds another explanation comment. Any ideas?\\n***\\n**EDIT:** I have a pseudo-solution, I found out about praw.helpers.comment_stream() and I found get_spam(), so I'm basically checking that each comment is *not in* the spam list yet, and only then will the bot comment on it. The problem is, I'm not seeing what the limit is on returned spam comments/posts. Is there a chance I could end up checking a comment from comment_stream() that wasn't returned by get_spam()? Ideally I'd like something like *comment.is_removed* or *comment.is_spam* but I'm not finding anything like that... \\n\\n**Question part 2:** Using get_spam(), is there a way to return comments only (instead of comments and posts)?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1vrfu3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Jyrroe\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1390317498.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1vrfu3/praw_how_can_i_tell_if_a_postcomment_has_been/\", \"locked\": false, \"name\": \"t3_1vrfu3\", \"created\": 1390342669.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1vrfu3/praw_how_can_i_tell_if_a_postcomment_has_been/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[PRAW] How can I tell if a post/comment has been removed?\", \"created_utc\": 1390313869.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo, I\\u0026#39;ve been following the PRAW tutorials (wonderfully written btw) to write my own reddit bot, and I saw many times in the published code the login line is just r.login() or r.login(USERNAME, PASSWORD). I was wondering if this was just so that the code can be published without revealing the credentials, or is there some way that the fields are being dynamically filled in?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So, I've been following the PRAW tutorials (wonderfully written btw) to write my own reddit bot, and I saw many times in the published code the login line is just r.login() or r.login(USERNAME, PASSWORD). I was wondering if this was just so that the code can be published without revealing the credentials, or is there some way that the fields are being dynamically filled in?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1u1pwy\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"pachufir\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/\", \"locked\": false, \"name\": \"t3_1u1pwy\", \"created\": 1388470098.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"open sourcing code with login credentials\", \"created_utc\": 1388441298.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi, \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI noticed that spam comments/posts are not returned by the API, while the search in the Reddit interface returns them. I might be wrong, but wanted to be sure.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs it because of these lines? (\\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/models/builder.py#L602-L603\\\"\\u003Ehttps://github.com/reddit/reddit/blob/master/r2/r2/models/builder.py#L602-L603\\u003C/a\\u003E)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOr, can I provide another parameter to the search API in order to return also spam comments?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi, \\n\\nI noticed that spam comments/posts are not returned by the API, while the search in the Reddit interface returns them. I might be wrong, but wanted to be sure.\\n\\nIs it because of these lines? (https://github.com/reddit/reddit/blob/master/r2/r2/models/builder.py#L602-L603)\\n\\nOr, can I provide another parameter to the search API in order to return also spam comments?\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1tiwu3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"redditmasterr\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1tiwu3/reddit_search_api_not_returning_spam_while_search/\", \"locked\": false, \"name\": \"t3_1tiwu3\", \"created\": 1387827792.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1tiwu3/reddit_search_api_not_returning_spam_while_search/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit search API not returning spam, while search in the interface returns it\", \"created_utc\": 1387798992.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ll try and explain my best, I mod at \\u003Ca href=\\\"/r/firefighting\\\"\\u003E/r/firefighting\\u003C/a\\u003E and I wanted to have a live feed of LODDs (Line of Duty Deaths) incorporated somewhere on the sub. If it is possible anyways. Firefighterclosecalls.com is a possible website that would provide the information. I want it in a memorial kind of format, but where you can click for more information. I know this kind of does not make much sense, so I hope someone can help.  \\u003Ca href=\\\"/r/csshelp\\\"\\u003E/r/csshelp\\u003C/a\\u003E told me to post this here.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'll try and explain my best, I mod at /r/firefighting and I wanted to have a live feed of LODDs (Line of Duty Deaths) incorporated somewhere on the sub. If it is possible anyways. Firefighterclosecalls.com is a possible website that would provide the information. I want it in a memorial kind of format, but where you can click for more information. I know this kind of does not make much sense, so I hope someone can help.  /r/csshelp told me to post this here.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1t1dyn\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"karazykid\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1t1dyn/i_mod_over_at_rfirefighting_and_i_am_curious_on/\", \"locked\": false, \"name\": \"t3_1t1dyn\", \"created\": 1387259710.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1t1dyn/i_mod_over_at_rfirefighting_and_i_am_curious_on/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I mod over at /r/firefighting and I am curious on how to, if there is a way, to add a live feed to a site?\", \"created_utc\": 1387230910.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHow do create apps like Reddit News, Baconreader, etc. even function when the API limit is just 30/minute? That seems incredibly low. Or is it 30/minute/user?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAm I missing something?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"How do create apps like Reddit News, Baconreader, etc. even function when the API limit is just 30/minute? That seems incredibly low. Or is it 30/minute/user?\\n\\nAm I missing something?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1rozim\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"curtainlikeobstacles\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1rozim/how_is_it_possible_to_create_a_reddit_client_with/\", \"locked\": false, \"name\": \"t3_1rozim\", \"created\": 1385733165.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1rozim/how_is_it_possible_to_create_a_reddit_client_with/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How is it possible to create a Reddit client with a 30 request per minute API limit?\", \"created_utc\": 1385704365.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWith reddit gold it is possible to save comments, but if I request the comments of a specific submission there seems to be no way to tell if a comment was saved or not? I would like to distinguish and display a save / unsave button correctly - is there any way I can do that?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"With reddit gold it is possible to save comments, but if I request the comments of a specific submission there seems to be no way to tell if a comment was saved or not? I would like to distinguish and display a save / unsave button correctly - is there any way I can do that?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1rhmut\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"myell0w\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1rhmut/how_to_determine_if_a_comment_was_saved_gold/\", \"locked\": false, \"name\": \"t3_1rhmut\", \"created\": 1385494380.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1rhmut/how_to_determine_if_a_comment_was_saved_gold/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to determine if a comment was saved (gold)?\", \"created_utc\": 1385465580.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI wanted to make a simple bot that did the following:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EWatches a specific subreddit\\u003C/li\\u003E\\n\\u003Cli\\u003EWhen a thread is deleted by the user, it posts a new topic in a subreddit owned by the bot with a link to the deleted thread\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EI feel like this is probably very simple, but the basics are challenging me since I am not familiar with this.  The reddit APi seems simple, but how do I actually get my bot to run?  \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I wanted to make a simple bot that did the following:\\n\\n* Watches a specific subreddit\\n* When a thread is deleted by the user, it posts a new topic in a subreddit owned by the bot with a link to the deleted thread\\n\\nI feel like this is probably very simple, but the basics are challenging me since I am not familiar with this.  The reddit APi seems simple, but how do I actually get my bot to run?  \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1resz4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"deten\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1385379233.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1resz4/new_to_bot_making_some_basic_questions_i_have/\", \"locked\": false, \"name\": \"t3_1resz4\", \"created\": 1385405609.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1resz4/new_to_bot_making_some_basic_questions_i_have/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New to Bot Making: Some basic questions I have\", \"created_utc\": 1385376809.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/r/modhelp/comments/1oh08r/subscribe_buttons_like_twitters_follow_buttons/\\\"\\u003EAs best I can tell\\u003C/a\\u003E, nobody has made this before. So I did. \\u003Ca href=\\\"http://jdscheff.github.io/subreddit-buttons/\\\"\\u003E\\u003Cstrong\\u003ECheck it out!\\u003C/strong\\u003E\\u003C/a\\u003E The obvious use case is if you have a blog/website/whatever and you want an attractive link to a subreddit you own.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"[As best I can tell](http://www.reddit.com/r/modhelp/comments/1oh08r/subscribe_buttons_like_twitters_follow_buttons/), nobody has made this before. So I did. [**Check it out!**](http://jdscheff.github.io/subreddit-buttons/) The obvious use case is if you have a blog/website/whatever and you want an attractive link to a subreddit you own.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1qedkf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"dumbmatter\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1qedkf/subreddit_button_showing_subscriber_count_like/\", \"locked\": false, \"name\": \"t3_1qedkf\", \"created\": 1384229229.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1qedkf/subreddit_button_showing_subscriber_count_like/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Subreddit button showing subscriber count, like Twitter's \\\"Follow\\\" button\", \"created_utc\": 1384200429.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m attempting to create a new post on a subreddit \\u0026quot;punkband2\\u0026quot; after authenticating using OAuth2.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe authentication goes fine, and I\\u0026#39;m able to GET the data I need off of reddit, but am racking my brain trying to POST to reddit. You can see my code below, where I\\u0026#39;ve added a comment detailing where the suspect code is.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECan anyone please shed light on what I\\u0026#39;m doing incorrectly?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe response I get back is:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Earray(3) { [\\u0026quot;result\\u0026quot;]=\\u0026gt; array(1) { [\\u0026quot;error\\u0026quot;]=\\u0026gt; int(403) } [\\u0026quot;code\\u0026quot;]=\\u0026gt; int(403) [\\u0026quot;content_type\\u0026quot;]=\\u0026gt; string(31) \\u0026quot;application/json; charset=UTF-8\\u0026quot; }\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAnd my code is:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026lt;?php session_start(); ?\\u0026gt;\\n\\u0026lt;?php\\nif(@$_SERVER[\\u0026#39;PATH_INFO\\u0026#39;] == \\u0026quot;/logout\\u0026quot;) {\\n    session_destroy();\\n    header(\\u0026#39;Location: ../index.php\\u0026#39;);\\n    die;\\n}\\n?\\u0026gt;\\n\\n\\u0026lt;?php\\nif (isset($_GET[\\u0026quot;error\\u0026quot;]))\\n{\\necho(\\u0026quot;\\u0026lt;pre\\u0026gt;OAuth Error: \\u0026quot; . $_GET[\\u0026quot;error\\u0026quot;].\\u0026quot;\\\\n\\u0026quot;);\\necho(\\u0026#39;\\u0026lt;a href=\\u0026quot;index.php\\u0026quot;\\u0026gt;Retry\\u0026lt;/a\\u0026gt;\\u0026lt;/pre\\u0026gt;\\u0026#39;);\\ndie;\\n}\\n\\n$authorizeUrl = \\u0026#39;https://ssl.reddit.com/api/v1/authorize\\u0026#39;;\\n$accessTokenUrl = \\u0026#39;https://ssl.reddit.com/api/v1/access_token\\u0026#39;;\\n$clientId = \\u0026#39;xxxxxxxxxxxxxxxxxxxxx\\u0026#39;;\\n$clientSecret = \\u0026#39;xxxxxxxxxxxxxxxxxxxxxxxxxxx\\u0026#39;;\\n\\n$redirectUrl = \\u0026quot;http://gameofbands.co/login.php\\u0026quot;;\\n\\nrequire(\\u0026quot;Client.php\\u0026quot;);\\nrequire(\\u0026quot;GrantType/IGrantType.php\\u0026quot;);\\nrequire(\\u0026quot;GrantType/AuthorizationCode.php\\u0026quot;);\\n\\n$client = new OAuth2\\\\Client($clientId, $clientSecret, OAuth2\\\\Client::AUTH_TYPE_AUTHORIZATION_BASIC);\\n\\nif (isset($_GET[\\u0026quot;code\\u0026quot;]))\\n{\\n$_SESSION[\\u0026#39;GOB\\u0026#39;][\\u0026#39;code\\u0026#39;]=$_GET[\\u0026quot;code\\u0026quot;];\\n$_SESSION[\\u0026#39;GOB\\u0026#39;][\\u0026#39;loggedin\\u0026#39;]=true;\\n}\\n\\nif (!isset($_SESSION[\\u0026#39;GOB\\u0026#39;][\\u0026#39;loggedin\\u0026#39;]))\\n{\\n$_SESSION[\\u0026#39;GOB\\u0026#39;][\\u0026#39;loggedin\\u0026#39;]=false;\\n}\\n\\nif ($_SESSION[\\u0026#39;GOB\\u0026#39;][\\u0026#39;loggedin\\u0026#39;])\\n{\\n$params = array(\\u0026quot;code\\u0026quot; =\\u0026gt; $_SESSION[\\u0026#39;GOB\\u0026#39;][\\u0026#39;code\\u0026#39;], \\u0026quot;redirect_uri\\u0026quot; =\\u0026gt; $redirectUrl);\\n\\n$response = $client-\\u0026gt;getAccessToken($accessTokenUrl, \\u0026quot;authorization_code\\u0026quot;, $params);\\n\\n$accessTokenResult = $response[\\u0026quot;result\\u0026quot;];\\n\\nif (isset($_SESSION[\\u0026#39;GOB\\u0026#39;][\\u0026#39;token\\u0026#39;])) {\\n    $client-\\u0026gt;setAccessToken($_SESSION[\\u0026#39;GOB\\u0026#39;][\\u0026#39;token\\u0026#39;]);\\n} else {\\n    $client-\\u0026gt;setAccessToken($accessTokenResult[\\u0026quot;access_token\\u0026quot;]);\\n    $_SESSION[\\u0026#39;GOB\\u0026#39;][\\u0026#39;token\\u0026#39;] = $accessTokenResult[\\u0026quot;access_token\\u0026quot;];\\n}\\n\\n$client-\\u0026gt;setAccessTokenType(OAuth2\\\\Client::ACCESS_TOKEN_BEARER);\\n\\n$response = $client-\\u0026gt;fetch(\\u0026quot;https://oauth.reddit.com/api/v1/me.json\\u0026quot;);\\n$_SESSION[\\u0026#39;GOB\\u0026#39;][\\u0026#39;name\\u0026#39;] = $response[\\u0026quot;result\\u0026quot;][\\u0026quot;name\\u0026quot;];\\n$_SESSION[\\u0026#39;GOB\\u0026#39;][\\u0026#39;karma\\u0026#39;] = $response[\\u0026quot;result\\u0026quot;][\\u0026quot;link_karma\\u0026quot;];\\n\\n\\n$response = $client-\\u0026gt;fetch(\\u0026quot;https://oauth.reddit.com/r/gameofbands/about.json\\u0026quot;);\\n$_SESSION[\\u0026#39;GOB\\u0026#39;][\\u0026#39;ismod\\u0026#39;] = $response[\\u0026quot;result\\u0026quot;][\\u0026#39;data\\u0026#39;][\\u0026#39;user_is_moderator\\u0026#39;];\\n\\n//Problem Starts here\\n$args = array(\\n    \\u0026#39;kind\\u0026#39;=\\u0026gt;\\u0026#39;self\\u0026#39;,\\n    \\u0026#39;sr\\u0026#39;=\\u0026gt;\\u0026#39;punkband2\\u0026#39;,\\n    \\u0026#39;title\\u0026#39;=\\u0026gt;\\u0026#39;test test\\u0026#39;,\\n    \\u0026#39;text\\u0026#39;=\\u0026gt;\\u0026#39;This is a test post\\u0026#39;,\\n    \\u0026#39;api_type\\u0026#39;=\\u0026gt;\\u0026#39;json\\u0026#39;\\n);\\n\\n$client-\\u0026gt;setClientAuthType(\\u0026quot;AUTH_TYPE_FORM\\u0026quot;); \\n$response = $client-\\u0026gt;fetch(\\u0026quot;http://www.reddit.com/api/submit\\u0026quot;,$args);\\n\\n$_SESSION[\\u0026#39;GOB\\u0026#39;][\\u0026#39;submit\\u0026#39;] = $response;\\n\\nheader(\\u0026#39;Location: index.php\\u0026#39;);\\n\\n} else {\\n$authUrl = $client-\\u0026gt;getAuthenticationUrl($authorizeUrl, $redirectUrl, array(\\u0026quot;scope\\u0026quot; =\\u0026gt; \\u0026quot;identity,read,submit\\u0026quot;, \\u0026quot;state\\u0026quot; =\\u0026gt; \\u0026quot;SomeUnguessableValue\\u0026quot;));\\nheader(\\u0026#39;Location: \\u0026#39;.$authUrl);\\n}\\n?\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm attempting to create a new post on a subreddit \\\"punkband2\\\" after authenticating using OAuth2.\\n\\nThe authentication goes fine, and I'm able to GET the data I need off of reddit, but am racking my brain trying to POST to reddit. You can see my code below, where I've added a comment detailing where the suspect code is.\\n\\nCan anyone please shed light on what I'm doing incorrectly?\\n\\nThe response I get back is:\\n\\n    array(3) { [\\\"result\\\"]=\\u003E array(1) { [\\\"error\\\"]=\\u003E int(403) } [\\\"code\\\"]=\\u003E int(403) [\\\"content_type\\\"]=\\u003E string(31) \\\"application/json; charset=UTF-8\\\" }\\n\\nAnd my code is:\\n\\n    \\u003C?php session_start(); ?\\u003E\\n    \\u003C?php\\n\\tif(@$_SERVER['PATH_INFO'] == \\\"/logout\\\") {\\n\\t\\tsession_destroy();\\n\\t\\theader('Location: ../index.php');\\n\\t\\tdie;\\n\\t}\\n    ?\\u003E\\n \\n    \\u003C?php\\n    if (isset($_GET[\\\"error\\\"]))\\n    {\\n    echo(\\\"\\u003Cpre\\u003EOAuth Error: \\\" . $_GET[\\\"error\\\"].\\\"\\\\n\\\");\\n    echo('\\u003Ca href=\\\"index.php\\\"\\u003ERetry\\u003C/a\\u003E\\u003C/pre\\u003E');\\n    die;\\n    }\\n\\n    $authorizeUrl = 'https://ssl.reddit.com/api/v1/authorize';\\n    $accessTokenUrl = 'https://ssl.reddit.com/api/v1/access_token';\\n    $clientId = 'xxxxxxxxxxxxxxxxxxxxx';\\n    $clientSecret = 'xxxxxxxxxxxxxxxxxxxxxxxxxxx';\\n\\n    $redirectUrl = \\\"http://gameofbands.co/login.php\\\";\\n\\n    require(\\\"Client.php\\\");\\n    require(\\\"GrantType/IGrantType.php\\\");\\n    require(\\\"GrantType/AuthorizationCode.php\\\");\\n\\n    $client = new OAuth2\\\\Client($clientId, $clientSecret, OAuth2\\\\Client::AUTH_TYPE_AUTHORIZATION_BASIC);\\n\\n    if (isset($_GET[\\\"code\\\"]))\\n    {\\n\\t$_SESSION['GOB']['code']=$_GET[\\\"code\\\"];\\n\\t$_SESSION['GOB']['loggedin']=true;\\n    }\\n\\n    if (!isset($_SESSION['GOB']['loggedin']))\\n    {\\n\\t$_SESSION['GOB']['loggedin']=false;\\n    }\\n\\t\\n    if ($_SESSION['GOB']['loggedin'])\\n    {\\n    $params = array(\\\"code\\\" =\\u003E $_SESSION['GOB']['code'], \\\"redirect_uri\\\" =\\u003E $redirectUrl);\\n\\n    $response = $client-\\u003EgetAccessToken($accessTokenUrl, \\\"authorization_code\\\", $params);\\n\\t\\n    $accessTokenResult = $response[\\\"result\\\"];\\n\\t\\n\\tif (isset($_SESSION['GOB']['token'])) {\\n\\t\\t$client-\\u003EsetAccessToken($_SESSION['GOB']['token']);\\n\\t} else {\\n\\t\\t$client-\\u003EsetAccessToken($accessTokenResult[\\\"access_token\\\"]);\\n\\t\\t$_SESSION['GOB']['token'] = $accessTokenResult[\\\"access_token\\\"];\\n\\t}\\n\\t\\n    $client-\\u003EsetAccessTokenType(OAuth2\\\\Client::ACCESS_TOKEN_BEARER);\\n\\n    $response = $client-\\u003Efetch(\\\"https://oauth.reddit.com/api/v1/me.json\\\");\\n\\t$_SESSION['GOB']['name'] = $response[\\\"result\\\"][\\\"name\\\"];\\n\\t$_SESSION['GOB']['karma'] = $response[\\\"result\\\"][\\\"link_karma\\\"];\\n\\t\\n\\t\\n\\t$response = $client-\\u003Efetch(\\\"https://oauth.reddit.com/r/gameofbands/about.json\\\");\\n\\t$_SESSION['GOB']['ismod'] = $response[\\\"result\\\"]['data']['user_is_moderator'];\\n\\n\\t//Problem Starts here\\n\\t$args = array(\\n\\t\\t'kind'=\\u003E'self',\\n\\t\\t'sr'=\\u003E'punkband2',\\n\\t\\t'title'=\\u003E'test test',\\n\\t\\t'text'=\\u003E'This is a test post',\\n\\t\\t'api_type'=\\u003E'json'\\n\\t);\\n\\t\\n\\t$client-\\u003EsetClientAuthType(\\\"AUTH_TYPE_FORM\\\"); \\n\\t$response = $client-\\u003Efetch(\\\"http://www.reddit.com/api/submit\\\",$args);\\n\\n\\t$_SESSION['GOB']['submit'] = $response;\\n\\t\\n\\theader('Location: index.php');\\n\\t\\n    } else {\\n\\t$authUrl = $client-\\u003EgetAuthenticationUrl($authorizeUrl, $redirectUrl, array(\\\"scope\\\" =\\u003E \\\"identity,read,submit\\\", \\\"state\\\" =\\u003E \\\"SomeUnguessableValue\\\"));\\n\\theader('Location: '.$authUrl);\\n    }\\n    ?\\u003E\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1q562g\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"tgpo\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1q562g/using_php_oauth2_authentication_i_cant_post_back/\", \"locked\": false, \"name\": \"t3_1q562g\", \"created\": 1383900995.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1q562g/using_php_oauth2_authentication_i_cant_post_back/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Using PHP OAuth2 Authentication, I can't Post back to reddit\", \"created_utc\": 1383872195.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have a few hundred in game names stored in our server\\u2019s subreddit, displayed using the flair system. When name is different than Reddit\\u2019s username, it\\u2019s prepended, when it\\u2019s the same, a similar CSS is applied (to make it \\u201cverified\\u201d). It all works through user flair system (text for the former or class names for the latter). \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBecause we keep gaining new players and because we have two subreddits with majority of the community in both, I need an easy and a reproducible way to sync flair from one to another. (Yeah, to make this simpler, let\\u2019s assume I only need to copy one subreddit\\u2019s user flair to another, but it could work both ways depending on what you can suggest)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI found \\u003Ca href=\\\"https://github.com/logan/reddit-hacks/blob/master/flairsync.py\\\"\\u003Egithub.com/logan/reddit-hacks/flairsync.py\\u003C/a\\u003E but I\\u2019m not sure where to start with it or is it even a reliable way to put it in a cron job, syncing flair every 1h for example.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDo you have any suggestions? It might even be a locally stored DB, which I edit and it\\u2019s sent through API to both subreddits, I\\u2019m fine with that. :)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have a few hundred in game names stored in our server\\u2019s subreddit, displayed using the flair system. When name is different than Reddit\\u2019s username, it\\u2019s prepended, when it\\u2019s the same, a similar CSS is applied (to make it \\u201cverified\\u201d). It all works through user flair system (text for the former or class names for the latter). \\n\\nBecause we keep gaining new players and because we have two subreddits with majority of the community in both, I need an easy and a reproducible way to sync flair from one to another. (Yeah, to make this simpler, let\\u2019s assume I only need to copy one subreddit\\u2019s user flair to another, but it could work both ways depending on what you can suggest)\\n\\nI found [github.com/logan/reddit-hacks/flairsync.py](https://github.com/logan/reddit-hacks/blob/master/flairsync.py) but I\\u2019m not sure where to start with it or is it even a reliable way to put it in a cron job, syncing flair every 1h for example.\\n\\nDo you have any suggestions? It might even be a locally stored DB, which I edit and it\\u2019s sent through API to both subreddits, I\\u2019m fine with that. :)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1no2ob\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ridddle\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1no2ob/how_to_sync_user_flair_between_subreddits/\", \"locked\": false, \"name\": \"t3_1no2ob\", \"created\": 1380852217.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1no2ob/how_to_sync_user_flair_between_subreddits/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to sync user flair between subreddits?\", \"created_utc\": 1380823417.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m having a trouble deleting multireddit. Related to API \\u003Ca href=\\\"http://www.reddit.com/dev/api#DELETE_api_multi_%7Bmultipath%7D\\\"\\u003Ehttp://www.reddit.com/dev/api#DELETE_api_multi_{multipath}\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI send multipath and uh as it is described in API.\\nBut I always have following error:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E{\\u0026quot;explanation\\u0026quot;: \\u0026quot;please login to do that\\u0026quot;, \\u0026quot;reason\\u0026quot;: \\u0026quot;USER_REQUIRED\\u0026quot;}\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EWhat do I wrong?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eupdated.\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI try to delete multireddit using chrome extension:\\n\\u003Ca href=\\\"https://chrome.google.com/webstore/detail/advanced-rest-client/hgmloofddffdnphfgcellkdfbfbjeloo\\\"\\u003Ehttps://chrome.google.com/webstore/detail/advanced-rest-client/hgmloofddffdnphfgcellkdfbfbjeloo\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003ESelect DELETE\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EType in url adress: \\n\\u003Ca href=\\\"http://www.reddit.com/api/multi/user/sashatinkoff/m/ilovethisfeature\\\"\\u003Ehttp://www.reddit.com/api/multi/user/sashatinkoff/m/ilovethisfeature\\u003C/a\\u003E\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003ERaw of payload looks like following:\\nmultipath=/user/sashatinkoff/m/ilovethisfeature\\u0026amp;uh=a_valid_modhash\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EAll the same result\\n{\\u0026quot;explanation\\u0026quot;: \\u0026quot;please login to do that\\u0026quot;, \\u0026quot;reason\\u0026quot;: \\u0026quot;USER_REQUIRED\\u0026quot;}\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf I send parameters via Headers the result is still the same.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm having a trouble deleting multireddit. Related to API http://www.reddit.com/dev/api#DELETE_api_multi_{multipath}\\n\\nI send multipath and uh as it is described in API.\\nBut I always have following error:\\n\\n\\u003E{\\\"explanation\\\": \\\"please login to do that\\\", \\\"reason\\\": \\\"USER_REQUIRED\\\"}\\n\\nWhat do I wrong?\\n\\n\\n\\n\\n\\n**updated.**\\n\\nI try to delete multireddit using chrome extension:\\nhttps://chrome.google.com/webstore/detail/advanced-rest-client/hgmloofddffdnphfgcellkdfbfbjeloo\\n\\n1. Select DELETE\\n\\n2. Type in url adress: \\nhttp://www.reddit.com/api/multi/user/sashatinkoff/m/ilovethisfeature\\n\\n3. Raw of payload looks like following:\\nmultipath=/user/sashatinkoff/m/ilovethisfeature\\u0026uh=a_valid_modhash\\n\\nAll the same result\\n{\\\"explanation\\\": \\\"please login to do that\\\", \\\"reason\\\": \\\"USER_REQUIRED\\\"}\\n\\nIf I send parameters via Headers the result is still the same.\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1l40sa\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"sashatinkoff\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1377585194.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1l40sa/how_to_correctly_remove_multisubreddit_api/\", \"locked\": false, \"name\": \"t3_1l40sa\", \"created\": 1377545694.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1l40sa/how_to_correctly_remove_multisubreddit_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to correctly remove multisubreddit (API)?\", \"created_utc\": 1377516894.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have written a Greasemonkey script that will add the \\u003Ca href=\\\"http://imgur.com/bQ0PgyU\\\"\\u003Ekarma and account age to user names\\u003C/a\\u003E. I have tried to \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/1jzxqn/looking_for_permission_to_release_a_certain/\\\"\\u003Eget permission to release this script before\\u003C/a\\u003E but the response was (probably?) negative, although I can only see that on the upvotes and reddit gold that the only commenter got who told me it would be too heavy on the servers. Well, I was aware of that and that is why I had asked for permission in the first place : )\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have now taken measures to make this script much less resource hungry. The script was aimed to help my moderation tasks, and as such I have now improved this script in two ways:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EThe script is now limited to only work in subreddits that the logged in user moderates. I.e. it is now also only useful to someone who \\u003Cem\\u003Eis\\u003C/em\\u003E a moderator.\\u003C/li\\u003E\\n\\u003Cli\\u003EIt now uses the jQuery inView plugin to only load about.json for such usernames that are currently in the viewport. The delay of 2100ms between requests to about.json remains.\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003ESo what do you say now, redditdevs?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: I have now incorporated a cache in localStorage for this script and removed the ability to show karma. The script only shows user age now. The script was now \\u003Ca href=\\\"http://userscripts.org/scripts/show/175700\\\"\\u003Ereleased the script to userscripts.org\\u003C/a\\u003E. Please let me know if there are issues.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have written a Greasemonkey script that will add the [karma and account age to user names](http://imgur.com/bQ0PgyU). I have tried to [get permission to release this script before](http://www.reddit.com/r/redditdev/comments/1jzxqn/looking_for_permission_to_release_a_certain/) but the response was (probably?) negative, although I can only see that on the upvotes and reddit gold that the only commenter got who told me it would be too heavy on the servers. Well, I was aware of that and that is why I had asked for permission in the first place : )\\n\\nI have now taken measures to make this script much less resource hungry. The script was aimed to help my moderation tasks, and as such I have now improved this script in two ways:\\n\\n- The script is now limited to only work in subreddits that the logged in user moderates. I.e. it is now also only useful to someone who *is* a moderator.\\n- It now uses the jQuery inView plugin to only load about.json for such usernames that are currently in the viewport. The delay of 2100ms between requests to about.json remains.\\n\\nSo what do you say now, redditdevs?\\n\\nEDIT: I have now incorporated a cache in localStorage for this script and removed the ability to show karma. The script only shows user age now. The script was now [released the script to userscripts.org](http://userscripts.org/scripts/show/175700). Please let me know if there are issues.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1k738n\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"dub4u\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1376448931.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1k738n/show_user_karma_and_age_on_listings_take_ii/\", \"locked\": false, \"name\": \"t3_1k738n\", \"created\": 1376325732.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1k738n/show_user_karma_and_age_on_listings_take_ii/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Show User Karma and Age on Listings. Take II.\", \"created_utc\": 1376296932.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have written a Greasemonkey script that, similar to \\u003Ca href=\\\"http://userscripts.org/scripts/show/56641\\\"\\u003EReddit Uppers and Downers\\u003C/a\\u003E, will add the karma and account age to user names. \\u003Ca href=\\\"http://imgur.com/bQ0PgyU\\\"\\u003EHere is a screenshot\\u003C/a\\u003E. It does this by making calls to /user/\\u003Cem\\u003Eusername\\u003C/em\\u003E/about.json for (most) usernames visible on a page. In order to stay within the guidelines the API calls have a delay of 2100ms from each other.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe script identifies itself as User-Agent \\u003Cem\\u003E\\u0026quot;GM/show_user_karma_and_age/1.0 by dub4u\\u0026quot;\\u003C/em\\u003E.\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003ECan I release this script as is?\\u003C/li\\u003E\\n\\u003Cli\\u003EIs there a way to improve the script such that it does not have so many calls to the API? In other words, can I call about.json for multiple usernames in one single HTTP request?\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have written a Greasemonkey script that, similar to [Reddit Uppers and Downers](http://userscripts.org/scripts/show/56641), will add the karma and account age to user names. [Here is a screenshot](http://imgur.com/bQ0PgyU). It does this by making calls to /user/*username*/about.json for (most) usernames visible on a page. In order to stay within the guidelines the API calls have a delay of 2100ms from each other.\\n\\nThe script identifies itself as User-Agent *\\\"GM/show_user_karma_and_age/1.0 by dub4u\\\"*.\\n\\n1. Can I release this script as is?\\n2. Is there a way to improve the script such that it does not have so many calls to the API? In other words, can I call about.json for multiple usernames in one single HTTP request?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1jzxqn\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"dub4u\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1jzxqn/looking_for_permission_to_release_a_certain/\", \"locked\": false, \"name\": \"t3_1jzxqn\", \"created\": 1376041091.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1jzxqn/looking_for_permission_to_release_a_certain/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Looking for permission to release a certain Greasemonkey script or improve it.\", \"created_utc\": 1376012291.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EBefore I go back to my code editor, I just wanted to make sure that this didn\\u0026#39;t already exist somewhere: I find AMAs very hard to read, even with RES. When I come too late to ask a question myself, all I want to read is the answers of the OP and the related question. It would be easy to do it but if it\\u0026#39;s already out there, let\\u0026#39;s not do it again.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDo you know of somethnig similar?\\u003Cbr/\\u003E\\nWould you find it useful?  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: maybe not the best subreddit for that, but if this becomes a project it will rely on the API so... relevant-ish?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Before I go back to my code editor, I just wanted to make sure that this didn't already exist somewhere: I find AMAs very hard to read, even with RES. When I come too late to ask a question myself, all I want to read is the answers of the OP and the related question. It would be easy to do it but if it's already out there, let's not do it again.  \\n  \\nDo you know of somethnig similar?  \\nWould you find it useful?  \\n  \\nEdit: maybe not the best subreddit for that, but if this becomes a project it will rely on the API so... relevant-ish?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1jc7gy\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"cyssou\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1jc7gy/a_script_to_turn_an_ama_into_an_interview/\", \"locked\": false, \"name\": \"t3_1jc7gy\", \"created\": 1375204713.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1jc7gy/a_script_to_turn_an_ama_into_an_interview/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"A script to turn an AMA into an interview\", \"created_utc\": 1375175913.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1iofh2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"DanielGibbs\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1iofh2/ive_made_a_short_script_to_unhide_hidden_posts/\", \"locked\": false, \"name\": \"t3_1iofh2\", \"created\": 1374322910.0, \"url\": \"https://github.com/DanielGibbsNZ/reddit-unhider\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I've made a short script to unhide hidden posts. Any suggestions/critiques?\", \"created_utc\": 1374294110.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EA picture of the traceback can be found \\u003Ca href=\\\"http://imgur.com/lY8vSLd\\\"\\u003Ehere\\u003C/a\\u003E. My code simply pulls data regarding submissions from a large number of subreddits and places them in a database. I\\u0026#39;m not entirely sure what\\u0026#39;s happened here, but it seems like an internal error from praw. Maybe one of you guys can help me figure it out? Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"A picture of the traceback can be found [here](http://imgur.com/lY8vSLd). My code simply pulls data regarding submissions from a large number of subreddits and places them in a database. I'm not entirely sure what's happened here, but it seems like an internal error from praw. Maybe one of you guys can help me figure it out? Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1i51by\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MrFanzyPanz\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1i51by/my_praw_code_ran_for_3_days_then_died/\", \"locked\": false, \"name\": \"t3_1i51by\", \"created\": 1373641599.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1i51by/my_praw_code_ran_for_3_days_then_died/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"My PRAW code ran for 3 days then died.\", \"created_utc\": 1373612799.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am looking to write a API scraper for the Reddit API which will pull data for all subreddits.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI am aware of the API rules and will keep the the 30 calls per minute.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat I want to know is which endpoints would be best to call?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe process will in some way be prioritized so that more requested endpoints will be updated more frequently.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI will also want to retrieve comments for each of the \\u0026#39;Things\\u0026#39;.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYou guys got any advice or design ideas for this. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am looking to write a API scraper for the Reddit API which will pull data for all subreddits.\\n\\nI am aware of the API rules and will keep the the 30 calls per minute.\\n\\nWhat I want to know is which endpoints would be best to call?\\n\\nThe process will in some way be prioritized so that more requested endpoints will be updated more frequently.\\n\\nI will also want to retrieve comments for each of the 'Things'.\\n\\nYou guys got any advice or design ideas for this. \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1i2svy\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"jprobins\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1i2svy/batch_request_data_from_reddit_api/\", \"locked\": false, \"name\": \"t3_1i2svy\", \"created\": 1373577101.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1i2svy/batch_request_data_from_reddit_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Batch request data from Reddit API\", \"created_utc\": 1373548301.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have a web app that gets a picture from any subreddit with pics, and I just click next and cycle through pictures. Sometimes any where from 1 to 5 images will fail to load in a row, I know there is some limit to the amount of requests I can make, but even if I go slowly it fails. I was wondering if its because the image size? I have it scaling, im not sure why else the images can fail?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have a web app that gets a picture from any subreddit with pics, and I just click next and cycle through pictures. Sometimes any where from 1 to 5 images will fail to load in a row, I know there is some limit to the amount of requests I can make, but even if I go slowly it fails. I was wondering if its because the image size? I have it scaling, im not sure why else the images can fail?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1i0jtf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"droidBehavior\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1i0jtf/is_the_reddit_api_restricting_me_from_grabbing/\", \"locked\": false, \"name\": \"t3_1i0jtf\", \"created\": 1373501516.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1i0jtf/is_the_reddit_api_restricting_me_from_grabbing/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"is the reddit api restricting me from grabbing pictures?\", \"created_utc\": 1373472716.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHere is my code:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ewith open(\\u0026#39;test.csv\\u0026#39;, \\u0026#39;w\\u0026#39;) as fp:\\n    writer = csv.writer(fp)\\n    for submission in r.search(\\u0026#39;Obama OR obamacare\\u0026#39;):\\n        id = submission.id\\n        title = submission.title\\n        url = submission.short_link\\n        score = submission.score\\n        created = submission.created\\n        created_date = datetime.fromtimestamp(submission.created)\\n        domain = submission.domain\\n        num_comments = submission.num_comments\\n        ups = submission.ups\\n        downs = submission.downs\\n        edited = submission.edited\\n        edited_date = datetime.fromtimestamp(submission.edited)\\n        author = submission.author\\n        a=(id, title, url, author, ups, downs, score, created, created_date, edited, edited_date, domain, num_comments)\\n        writer.writerow(a)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EHowever, I would like to paginate through results so I get \\u003Cem\\u003Eeverything\\u003C/em\\u003E within a given timeframe, what is the best way to do that?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Here is my code:\\n\\n    with open('test.csv', 'w') as fp:\\n        writer = csv.writer(fp)\\n        for submission in r.search('Obama OR obamacare'):\\n            id = submission.id\\n            title = submission.title\\n            url = submission.short_link\\n            score = submission.score\\n            created = submission.created\\n            created_date = datetime.fromtimestamp(submission.created)\\n            domain = submission.domain\\n            num_comments = submission.num_comments\\n            ups = submission.ups\\n            downs = submission.downs\\n            edited = submission.edited\\n            edited_date = datetime.fromtimestamp(submission.edited)\\n            author = submission.author\\n            a=(id, title, url, author, ups, downs, score, created, created_date, edited, edited_date, domain, num_comments)\\n            writer.writerow(a)\\n\\n\\nHowever, I would like to paginate through results so I get *everything* within a given timeframe, what is the best way to do that?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1hhwse\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"dem358\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1hhwse/is_there_any_way_i_can_use_praw_to_search_for/\", \"locked\": false, \"name\": \"t3_1hhwse\", \"created\": 1372803074.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1hhwse/is_there_any_way_i_can_use_praw_to_search_for/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there any way I can use PRAW to search for submissions that meet a certain criteria that were posted within a given timeframe?\", \"created_utc\": 1372774274.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi all...\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI know this may be a silly question. I know reddit cache almost everything. There is a div containing user information on top right of reddit\\u0026#39;s page. It has a id of \\u0026quot;header-bottom-right\\u0026quot;. My question is, if the page I get from reddit is cached, say, when I log in, how does reddit know what is my username? There are so MANY MANY reddit users, how does reddit cache\\npages among which only user information are different but main content of these pages are the same? Or does reddit only cache content data and render page on every request? Or is that rendered by javascript? I know I can read the source code of reddit to find out how it works, but reddit\\u0026#39;s source code is way too complicated to me to read....\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESorry if this is a silly question, but I\\u0026#39;m new to web developing and learning it by myself... I really want to know how to solve this issue because I\\u0026#39;m trying to implement a basic caching mechanism.... Any tips will be appreciated. Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi all...\\n\\nI know this may be a silly question. I know reddit cache almost everything. There is a div containing user information on top right of reddit's page. It has a id of \\\"header-bottom-right\\\". My question is, if the page I get from reddit is cached, say, when I log in, how does reddit know what is my username? There are so MANY MANY reddit users, how does reddit cache\\npages among which only user information are different but main content of these pages are the same? Or does reddit only cache content data and render page on every request? Or is that rendered by javascript? I know I can read the source code of reddit to find out how it works, but reddit's source code is way too complicated to me to read....\\n\\nSorry if this is a silly question, but I'm new to web developing and learning it by myself... I really want to know how to solve this issue because I'm trying to implement a basic caching mechanism.... Any tips will be appreciated. Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1h3qir\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"xJeong\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1372249966.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1h3qir/how_does_reddit_know_who_i_am/\", \"locked\": false, \"name\": \"t3_1h3qir\", \"created\": 1372278126.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1h3qir/how_does_reddit_know_who_i_am/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How does reddit know who I am?\", \"created_utc\": 1372249326.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been using the API urls for a while now and have not seen this. After I launched my bot this morning \\u003Ca href=\\\"/u/BlackjackBot\\\"\\u003E/u/BlackjackBot\\u003C/a\\u003E, I am suddenly now noticing these 403s.  I believe I have been careful to respect all of the API usage rules.  Am I being locked out? Is there someone I can ask about this? My user agent includes my reddit username (as per the API usage rules suggestions) but no one has contacted me. What gives?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been using the API urls for a while now and have not seen this. After I launched my bot this morning /u/BlackjackBot, I am suddenly now noticing these 403s.  I believe I have been careful to respect all of the API usage rules.  Am I being locked out? Is there someone I can ask about this? My user agent includes my reddit username (as per the API usage rules suggestions) but no one has contacted me. What gives?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1gu272\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"BlackjackPitboss\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1gu272/suddenly_receiving_http_403_response_codes_from/\", \"locked\": false, \"name\": \"t3_1gu272\", \"created\": 1371896622.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1gu272/suddenly_receiving_http_403_response_codes_from/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Suddenly receiving http 403 response codes from API urls.\", \"created_utc\": 1371867822.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"mikenon.github.io\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1gfbec\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"stickytruth\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1gfbec/redditanalyticscoms_comment_stream_pubsub/\", \"locked\": false, \"name\": \"t3_1gfbec\", \"created\": 1371367537.0, \"url\": \"http://mikenon.github.io/CommentsPubSub/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"RedditAnalytics.com's comment stream -\\u003E PubSub\", \"created_utc\": 1371338737.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIt is difficult to find an example of this code anywhere, but what I came up with is:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Edef ParentObj(self, obj):\\n    assert type(obj) == praw.objects.Comment\\n    submission = self.rh.get_submission(url = obj.permalink)\\n    if obj.is_root:\\n        return submission\\n    return self.rh.get_submission(url = submission.permalink + obj.parent_id[3:])._comments[0]\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThis scares me for three reasons:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EI am using _comments, which has an underscore in front of it, which implies to me that it\\u0026#39;s dubious or marginal or internal.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EI am converting from an ID of the form \\u0026quot;t1_cag5h2j\\u0026quot; to one of the form \\u0026quot;cag5h2j\\u0026quot; by flushing the first three characters down\\nthe toilet, which seems a scary way to convert from type A to type B.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EI would think there would be something explicit somewhere that allows you to move up and down the tree, but I can\\u0026#39;t find it.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EDid I implement this in a sane fashion? Should I scrape the comment tree or something instead?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI promise that I\\u0026#39;m not an idiot, really.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"It is difficult to find an example of this code anywhere, but what I came up with is:\\n\\n    def ParentObj(self, obj):\\n        assert type(obj) == praw.objects.Comment\\n        submission = self.rh.get_submission(url = obj.permalink)\\n        if obj.is_root:\\n            return submission\\n        return self.rh.get_submission(url = submission.permalink + obj.parent_id[3:])._comments[0]\\n\\nThis scares me for three reasons:\\n\\n1. I am using _comments, which has an underscore in front of it, which implies to me that it's dubious or marginal or internal.\\n\\n2. I am converting from an ID of the form \\\"t1_cag5h2j\\\" to one of the form \\\"cag5h2j\\\" by flushing the first three characters down\\nthe toilet, which seems a scary way to convert from type A to type B.\\n\\n3. I would think there would be something explicit somewhere that allows you to move up and down the tree, but I can't find it.\\n\\nDid I implement this in a sane fashion? Should I scrape the comment tree or something instead?\\n\\nI promise that I'm not an idiot, really.\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1g308t\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"brucemo\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1g308t/in_praw_getting_the_object_that_is_the_parent_of/\", \"locked\": false, \"name\": \"t3_1g308t\", \"created\": 1370938192.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1g308t/in_praw_getting_the_object_that_is_the_parent_of/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"In PRAW, getting the object that is the parent of this comment\", \"created_utc\": 1370909392.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI just wrote an image downloader using C# / WPF and part of someones redditAPI wrapper. Unlike most imgur downloaders I\\u0026#39;ve seen this one grabs images per subreddit so you can easily get all your favorite wallpapers or images from any subreddit.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf anyone would like to try it out and make any suggestions for future development I\\u0026#39;d be really grateful as I\\u0026#39;m wanting to add some good features in upcoming versions:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere\\u0026#39;s the link to the post I made in learnprogramming: \\u003Ca href=\\\"http://www.reddit.com/r/learnprogramming/comments/1fzonv/reddit_image_downloader/\\\"\\u003Ehttp://www.reddit.com/r/learnprogramming/comments/1fzonv/reddit_image_downloader/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I just wrote an image downloader using C# / WPF and part of someones redditAPI wrapper. Unlike most imgur downloaders I've seen this one grabs images per subreddit so you can easily get all your favorite wallpapers or images from any subreddit.\\n\\nIf anyone would like to try it out and make any suggestions for future development I'd be really grateful as I'm wanting to add some good features in upcoming versions:\\n\\nHere's the link to the post I made in learnprogramming: http://www.reddit.com/r/learnprogramming/comments/1fzonv/reddit_image_downloader/\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1g1jcl\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"FloatableCat\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1g1jcl/i_just_wrote_this_reddit_imgur_image_downloader/\", \"locked\": false, \"name\": \"t3_1g1jcl\", \"created\": 1370898798.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1g1jcl/i_just_wrote_this_reddit_imgur_image_downloader/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I just wrote this Reddit / Imgur Image Downloader feedback would be welcome\", \"created_utc\": 1370869998.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhat does \\u0026quot;clicked\\u0026quot; represent? All of mine are false and I can\\u0026#39;t figure out why. My initial thought was that is turns true if the user has at any point selected a link but I guess not.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"What does \\\"clicked\\\" represent? All of mine are false and I can't figure out why. My initial thought was that is turns true if the user has at any point selected a link but I guess not.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1fyusj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"tannerQuigley\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1fyusj/what_is_clicked_in_reddits_json/\", \"locked\": false, \"name\": \"t3_1fyusj\", \"created\": 1370783602.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1fyusj/what_is_clicked_in_reddits_json/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"What is clicked in Reddit's JSON?\", \"created_utc\": 1370754802.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI used github to fork reddit/reddit a year or so ago. I don\\u0026#39;t have any local git installation; I\\u0026#39;ve used github.com only.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;d like to get back into helping fix bugs and contributing code.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDo I need to take any steps to refresh or update my fork?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m worried my fork contains year-old code, and of course I\\u0026#39;d want to start with the latest files.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf I\\u0026#39;m misunderstanding how Github works, please let me know :)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I used github to fork reddit/reddit a year or so ago. I don't have any local git installation; I've used github.com only.\\n\\nI'd like to get back into helping fix bugs and contributing code.\\n\\nDo I need to take any steps to refresh or update my fork?\\n\\nI'm worried my fork contains year-old code, and of course I'd want to start with the latest files.\\n\\nIf I'm misunderstanding how Github works, please let me know :)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1fqo32\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"crimson117\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1fqo32/forked_redditreddit_some_time_ago_how_to_keep_it/\", \"locked\": false, \"name\": \"t3_1fqo32\", \"created\": 1370489208.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1fqo32/forked_redditreddit_some_time_ago_how_to_keep_it/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Forked reddit/reddit some time ago - how to keep it up to date?\", \"created_utc\": 1370460408.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EOld submissions prevent you from leaving new comments. Does anyone know where that logic is for that cutoff occurs? Is it a hard archival date or something more complex?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAdditionally, when a submission gets archived, do all comments become un-reply-able or is the can_reply toggle based on the timestamp of the comment (not the submission)?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Old submissions prevent you from leaving new comments. Does anyone know where that logic is for that cutoff occurs? Is it a hard archival date or something more complex?\\n\\nAdditionally, when a submission gets archived, do all comments become un-reply-able or is the can_reply toggle based on the timestamp of the comment (not the submission)?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1f2fuy\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"diafygi\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1f2fuy/logic_for_when_posts_and_comments_are_archived/\", \"locked\": false, \"name\": \"t3_1f2fuy\", \"created\": 1369571998.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1f2fuy/logic_for_when_posts_and_comments_are_archived/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Logic for when posts and comments are archived and can no longer be commented on?\", \"created_utc\": 1369543198.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI tried looking, but couldn\\u0026#39;t seem to find it... is this available somewhere? \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I tried looking, but couldn't seem to find it... is this available somewhere? \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1f266q\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"teamcoltra\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1f266q/team_fortress_april_fools_code/\", \"locked\": false, \"name\": \"t3_1f266q\", \"created\": 1369561271.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1f266q/team_fortress_april_fools_code/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Team Fortress April Fools Code?\", \"created_utc\": 1369532471.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"reddit.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1e288g\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"pilooch\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1e288g/a_subreddit_recommender_with_machine_learning/\", \"locked\": false, \"name\": \"t3_1e288g\", \"created\": 1368210592.0, \"url\": \"http://www.reddit.com/r/MachineLearning/comments/1dzoim/a_subreddit_recommender_with_machine_learning/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"A subreddit recommender with Machine Learning [x-post from /r/MachineLearning]\", \"created_utc\": 1368181792.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been running a script on PythonAnywhere which logs in using PRAW. I\\u0026#39;ve run it every day, and it hasn\\u0026#39;t failed until just now, which is strange because I didn\\u0026#39;t change anything in my code or my PRAW installation. I therefore can only conclude that the Reddit API must have changed, or PythonAnywhere must have changed something. Does anyone know how I might fix this?\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003ETraceback (most recent call last):\\n  File \\u0026quot;bravery20.py\\u0026quot;, line 1314, in \\u0026lt;module\\u0026gt;\\n    r.login(username=username, password=password)\\n  File \\u0026quot;/home/person/.local/lib/python2.7/site-packages/praw/__init__.py\\u0026quot;, line 906, in login\\n    self.request_json(self.config[\\u0026#39;login\\u0026#39;], data=data)\\n  File \\u0026quot;/home/person/.local/lib/python2.7/site-packages/praw/decorators.py\\u0026quot;, line 223, in error_checked_function\\n    return_value = function(cls, *args, **kwargs)\\n  File \\u0026quot;/home/person/.local/lib/python2.7/site-packages/praw/__init__.py\\u0026quot;, line 407, in request_json\\n    response = self._request(url, params, data)\\n  File \\u0026quot;/home/person/.local/lib/python2.7/site-packages/praw/__init__.py\\u0026quot;, line 294, in _request\\n    timeout=timeout)\\n  File \\u0026quot;/home/person/.local/lib/python2.7/site-packages/praw/decorators.py\\u0026quot;, line 64, in __call__\\n    result = self.function(reddit_session, url, *args, **kwargs)\\n  File \\u0026quot;/home/person/.local/lib/python2.7/site-packages/praw/decorators.py\\u0026quot;, line 167, in __call__\\n    return self.function(*args, **kwargs)\\n  File \\u0026quot;/home/person/.local/lib/python2.7/site-packages/praw/helpers.py\\u0026quot;, line 137, in _request\\n    allow_redirects=False, auth=auth)\\n  File \\u0026quot;/usr/local/lib/python2.7/site-packages/requests/sessions.py\\u0026quot;, line 399, in post\\n    return self.request(\\u0026#39;POST\\u0026#39;, url, data=data, **kwargs)\\n  File \\u0026quot;/usr/local/lib/python2.7/site-packages/requests/sessions.py\\u0026quot;, line 354, in request\\n    resp = self.send(prep, **send_kwargs)\\n  File \\u0026quot;/usr/local/lib/python2.7/site-packages/requests/sessions.py\\u0026quot;, line 460, in send\\n    r = adapter.send(request, **kwargs)\\n  File \\u0026quot;/usr/local/lib/python2.7/site-packages/requests/adapters.py\\u0026quot;, line 246, in send\\n    raise ConnectionError(e)\\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host=\\u0026#39;proxy.server\\u0026#39;, port=3128): Max retries exceeded with url: http://www.reddit.com/api/login/.json (Caused by \\n\\u0026lt;class \\u0026#39;socket.error\\u0026#39;\\u0026gt;: [Errno 111] Connection refused)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m using PRAW 2.0.15 because the latest version doesn\\u0026#39;t work for some reason; also, I\\u0026#39;ve removed \\u003Ccode\\u003E\\u0026quot;login\\u0026quot;\\u003C/code\\u003E from the \\u003Ccode\\u003ESSL_PATHS\\u003C/code\\u003E list in \\u003Ccode\\u003E__init__.py\\u003C/code\\u003E in order to prevent PRAW from attempting to connect securely (which PythonAnywhere doesn\\u0026#39;t support).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks in advance.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been running a script on PythonAnywhere which logs in using PRAW. I've run it every day, and it hasn't failed until just now, which is strange because I didn't change anything in my code or my PRAW installation. I therefore can only conclude that the Reddit API must have changed, or PythonAnywhere must have changed something. Does anyone know how I might fix this?\\n\\n    Traceback (most recent call last):\\n      File \\\"bravery20.py\\\", line 1314, in \\u003Cmodule\\u003E\\n        r.login(username=username, password=password)\\n      File \\\"/home/person/.local/lib/python2.7/site-packages/praw/__init__.py\\\", line 906, in login\\n        self.request_json(self.config['login'], data=data)\\n      File \\\"/home/person/.local/lib/python2.7/site-packages/praw/decorators.py\\\", line 223, in error_checked_function\\n        return_value = function(cls, *args, **kwargs)\\n      File \\\"/home/person/.local/lib/python2.7/site-packages/praw/__init__.py\\\", line 407, in request_json\\n        response = self._request(url, params, data)\\n      File \\\"/home/person/.local/lib/python2.7/site-packages/praw/__init__.py\\\", line 294, in _request\\n        timeout=timeout)\\n      File \\\"/home/person/.local/lib/python2.7/site-packages/praw/decorators.py\\\", line 64, in __call__\\n        result = self.function(reddit_session, url, *args, **kwargs)\\n      File \\\"/home/person/.local/lib/python2.7/site-packages/praw/decorators.py\\\", line 167, in __call__\\n        return self.function(*args, **kwargs)\\n      File \\\"/home/person/.local/lib/python2.7/site-packages/praw/helpers.py\\\", line 137, in _request\\n        allow_redirects=False, auth=auth)\\n      File \\\"/usr/local/lib/python2.7/site-packages/requests/sessions.py\\\", line 399, in post\\n        return self.request('POST', url, data=data, **kwargs)\\n      File \\\"/usr/local/lib/python2.7/site-packages/requests/sessions.py\\\", line 354, in request\\n        resp = self.send(prep, **send_kwargs)\\n      File \\\"/usr/local/lib/python2.7/site-packages/requests/sessions.py\\\", line 460, in send\\n        r = adapter.send(request, **kwargs)\\n      File \\\"/usr/local/lib/python2.7/site-packages/requests/adapters.py\\\", line 246, in send\\n        raise ConnectionError(e)\\n    requests.exceptions.ConnectionError: HTTPConnectionPool(host='proxy.server', port=3128): Max retries exceeded with url: http://www.reddit.com/api/login/.json (Caused by \\n    \\u003Cclass 'socket.error'\\u003E: [Errno 111] Connection refused)\\n\\nI'm using PRAW 2.0.15 because the latest version doesn't work for some reason; also, I've removed `\\\"login\\\"` from the `SSL_PATHS` list in `__init__.py` in order to prevent PRAW from attempting to connect securely (which PythonAnywhere doesn't support).\\n\\nThanks in advance.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1e0ffk\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"testuser12345678\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1e0ffk/did_something_in_the_reddit_api_change_last_night/\", \"locked\": false, \"name\": \"t3_1e0ffk\", \"created\": 1368148317.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1e0ffk/did_something_in_the_reddit_api_change_last_night/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Did something in the Reddit API change last night? I can no longer log in insecurely with PRAW 2.0.15 hosted on PythonAnywhere\", \"created_utc\": 1368119517.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m very interested in any information pertaining to how someone would build a reddit app for iOS.  I was under the impression that you had to code in objective-c to work iOS, but I see that the reddit API is written in python.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m somewhat of a newb, I appreciate any assistance or relavant articles.  Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm very interested in any information pertaining to how someone would build a reddit app for iOS.  I was under the impression that you had to code in objective-c to work iOS, but I see that the reddit API is written in python.  \\n\\nI'm somewhat of a newb, I appreciate any assistance or relavant articles.  Thanks!\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1de0a6\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"tristanAG\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1de0a6/how_was_alien_blue_created/\", \"locked\": false, \"name\": \"t3_1de0a6\", \"created\": 1367321980.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1de0a6/how_was_alien_blue_created/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How was Alien Blue created?\", \"created_utc\": 1367293180.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOne thing I have not yet completely figured out is how tags work on Reddit. Are they assigned to submissions by users? Can they \\u0026quot;invent\\u0026quot; new tags or do they have to use existing ones?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there a way to crawl the tags? E.g., via PRAW?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey!\\n\\nOne thing I have not yet completely figured out is how tags work on Reddit. Are they assigned to submissions by users? Can they \\\"invent\\\" new tags or do they have to use existing ones?\\n\\nIs there a way to crawl the tags? E.g., via PRAW?\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1dbzvd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"killver\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1dbzvd/tags_on_reddit/\", \"locked\": false, \"name\": \"t3_1dbzvd\", \"created\": 1367265915.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1dbzvd/tags_on_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Tags on Reddit\", \"created_utc\": 1367237115.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u00b4m a little embarrassed asking this, but I can\\u00b4t find it anywhere.  Surely it\\u00b4s an exception of some kind, but looking over the exceptions listed in the documentation I don\\u00b4t see anything specific to this.  Should I just wrap my API calls in a try, except and assume that if they fail it\\u00b4s because Reddit is down and go to sleep for a while?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I\\u00b4m a little embarrassed asking this, but I can\\u00b4t find it anywhere.  Surely it\\u00b4s an exception of some kind, but looking over the exceptions listed in the documentation I don\\u00b4t see anything specific to this.  Should I just wrap my API calls in a try, except and assume that if they fail it\\u00b4s because Reddit is down and go to sleep for a while?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1d5adk\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1d5adk/how_can_you_tell_if_reddit_is_down_using_praw/\", \"locked\": false, \"name\": \"t3_1d5adk\", \"created\": 1367002035.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1d5adk/how_can_you_tell_if_reddit_is_down_using_praw/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How can you tell if Reddit is down using praw?\", \"created_utc\": 1366973235.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhen requesting on behalf of a user utilizing their token, is the request limit considered against that individual user (access token) or are all requests on behalf of all users by a single client rate limited?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"When requesting on behalf of a user utilizing their token, is the request limit considered against that individual user (access token) or are all requests on behalf of all users by a single client rate limited?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1cvckr\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"clexmond\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1cvckr/in_the_context_of_oauth_is_the_30_requests_minute/\", \"locked\": false, \"name\": \"t3_1cvckr\", \"created\": 1366676383.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1cvckr/in_the_context_of_oauth_is_the_30_requests_minute/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"In the context of OAuth, is the 30 requests / minute limit applied to each access token or to the client?\", \"created_utc\": 1366647583.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI know praw already cache\\u0026#39;s results from reddit for 30 seconds but I\\u0026#39;m looking for a way to add a more persistent data store so that I only hit reddit once for any \\u0026quot;thing\\u0026quot; in its API, ever.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor now, I don\\u0026#39;t care about seeing any changes due to edits or votes but if one did, I guess that would complicate things.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI can come up with something explicit that is layered on top of praw but having this store insinuated into praw so it\\u0026#39;s transparent to any existing praw-using code seems like a good design.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny thoughts on this?  Any work in this direction yet?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: I\\u0026#39;m looking for something that persists between executions.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I know praw already cache's results from reddit for 30 seconds but I'm looking for a way to add a more persistent data store so that I only hit reddit once for any \\\"thing\\\" in its API, ever.  \\n\\nFor now, I don't care about seeing any changes due to edits or votes but if one did, I guess that would complicate things.\\n\\nI can come up with something explicit that is layered on top of praw but having this store insinuated into praw so it's transparent to any existing praw-using code seems like a good design.\\n\\nAny thoughts on this?  Any work in this direction yet?\\n\\nEdit: I'm looking for something that persists between executions.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1csvar\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"frumious\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1366565763.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1csvar/persistent_store_for_praw/\", \"locked\": false, \"name\": \"t3_1csvar\", \"created\": 1366585382.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Persistent store for praw?\", \"created_utc\": 1366556582.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi guys,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI read \\u003Ca href=\\\"/u/foolblog\\\"\\u003E/u/foolblog\\u003C/a\\u003E\\u0026#39;s \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/19t48f/learning_reddits_code_journal_2_admin_status_one/\\\"\\u003Efantastic guide\\u003C/a\\u003E on how to gain admin privileges, and I noticed that he mentioned that admins have very few powers until their email address is verified. I couldn\\u0026#39;t get email to work either, so I just ran the \\u003Ca href=\\\"http://www.reddit.com/r/redditdev/comments/19t48f/learning_reddits_code_journal_2_admin_status_one/c8r2hj1\\\"\\u003Ehack that was in the comments section\\u003C/a\\u003E. Now the admin account has a verified email address. Great!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe thing is, I don\\u0026#39;t see any new tools that I couldn\\u0026#39;t access before. Notably, when I visit a user\\u0026#39;s page, the admin drop-down menu is completely empty. Is it supposed to be empty, or do you guys see a few options in there? Are there any other admin tools you\\u0026#39;ve noticed once \\u0026quot;verifying\\u0026quot; an email address?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi guys,\\n\\nI read /u/foolblog's [fantastic guide](http://www.reddit.com/r/redditdev/comments/19t48f/learning_reddits_code_journal_2_admin_status_one/) on how to gain admin privileges, and I noticed that he mentioned that admins have very few powers until their email address is verified. I couldn't get email to work either, so I just ran the [hack that was in the comments section](http://www.reddit.com/r/redditdev/comments/19t48f/learning_reddits_code_journal_2_admin_status_one/c8r2hj1). Now the admin account has a verified email address. Great!\\n\\nThe thing is, I don't see any new tools that I couldn't access before. Notably, when I visit a user's page, the admin drop-down menu is completely empty. Is it supposed to be empty, or do you guys see a few options in there? Are there any other admin tools you've noticed once \\\"verifying\\\" an email address?\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1csi2o\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"rotorcowboy\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1csi2o/admin_tools_with_email_verification/\", \"locked\": false, \"name\": \"t3_1csi2o\", \"created\": 1366561074.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1csi2o/admin_tools_with_email_verification/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Admin tools with email verification\", \"created_utc\": 1366532274.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EBy default on a successful login, user is landed to \\u003Ca href=\\\"http://reddit.com\\\"\\u003Ehttp://reddit.com\\u003C/a\\u003E. Is it possible if I would like the user to redirect to only new/hot (or other filter) posts?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor example\\n(\\u003Ca href=\\\"http://domain.com\\\"\\u003Ehttp://domain.com\\u003C/a\\u003E) -\\u0026gt; (\\u003Ca href=\\\"http://domain.com/r/all\\\"\\u003Ehttp://domain.com/r/all\\u003C/a\\u003E)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"By default on a successful login, user is landed to http://reddit.com. Is it possible if I would like the user to redirect to only new/hot (or other filter) posts?\\n\\nFor example\\n(http://domain.com) -\\u003E (http://domain.com/r/all)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1co8er\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"mrafaqi\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1co8er/how_to_change_the_default_landing_page/\", \"locked\": false, \"name\": \"t3_1co8er\", \"created\": 1366402026.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1co8er/how_to_change_the_default_landing_page/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to change the default landing page?\", \"created_utc\": 1366373226.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhen I try to invite a new moderator to a subreddit on my clone, I get the 500 error occured message. The weird this is that the user gets the actual invite message. So, I checked the logs, and found this: \\u003Ca href=\\\"http://pastebin.com/YiVvWJ6W\\\"\\u003Ehttp://pastebin.com/YiVvWJ6W\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI followed the trace quite a bit, and found that the issue is probably when the /api/friends request is made and during the validation. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis happens only when I want to add a moderator to a subreddit which has unicode characters in name, because I did that modification to enable UTF8 chars in name. For regular subreddit names, this error doesn\\u0026#39;t pop up.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo, my question is can anyone provide a piece of advice how to fix this, and avoid showing of that error?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks in advance!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: WOW IT\\u0026#39;S MY CAKEDAY!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello,\\n\\nWhen I try to invite a new moderator to a subreddit on my clone, I get the 500 error occured message. The weird this is that the user gets the actual invite message. So, I checked the logs, and found this: http://pastebin.com/YiVvWJ6W\\n\\nI followed the trace quite a bit, and found that the issue is probably when the /api/friends request is made and during the validation. \\n\\nThis happens only when I want to add a moderator to a subreddit which has unicode characters in name, because I did that modification to enable UTF8 chars in name. For regular subreddit names, this error doesn't pop up.\\n\\nSo, my question is can anyone provide a piece of advice how to fix this, and avoid showing of that error?\\n\\nThanks in advance!\\n\\nEDIT: WOW IT'S MY CAKEDAY!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1cbicr\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"elAhmo\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1cbicr/apifriend_issue/\", \"locked\": false, \"name\": \"t3_1cbicr\", \"created\": 1365962417.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1cbicr/apifriend_issue/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"/api/friend issue\", \"created_utc\": 1365933617.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve noticed that \\u003Ca href=\\\"http://www.reddit.com/new/\\\"\\u003Ewww.reddit.com/new/\\u003C/a\\u003E no longer returns anything after the few most recent thousands of submissions or so.  Is the only way to get old submissions with the API just one id at a time?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAt the rate of getting one every two seconds, it will take eons to get any real data. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhy has this undocumented change been made?  You will now force developers to make 100x more hits to your server for the same information they used to be able to get for one hit.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWorse still, there is no longer spam filtering in place. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECan a developer pay a fee to get more access?  As it stands now, our archival / search engine project is impossible now.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've noticed that www.reddit.com/new/ no longer returns anything after the few most recent thousands of submissions or so.  Is the only way to get old submissions with the API just one id at a time?\\n\\nAt the rate of getting one every two seconds, it will take eons to get any real data. \\n\\nWhy has this undocumented change been made?  You will now force developers to make 100x more hits to your server for the same information they used to be able to get for one hit.\\n\\nWorse still, there is no longer spam filtering in place. \\n\\nCan a developer pay a fee to get more access?  As it stands now, our archival / search engine project is impossible now.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1c209w\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"aphexcoil\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1c209w/reddit_has_now_made_it_impossible_to_get_old/\", \"locked\": false, \"name\": \"t3_1c209w\", \"created\": 1365619765.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1c209w/reddit_has_now_made_it_impossible_to_get_old/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit has now made it impossible to get old submissions except one at a time?\", \"created_utc\": 1365590965.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi Guys,  Does anyone have instructions for how to configure cloudsearch?  Specifically, I am not sure how to create the indexes.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks,\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi Guys,  Does anyone have instructions for how to configure cloudsearch?  Specifically, I am not sure how to create the indexes.\\n\\nThanks,\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"1b3xp1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"RealAmino\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/1b3xp1/instructions_for_configuring_aws_cloudsearch/\", \"locked\": false, \"name\": \"t3_1b3xp1\", \"created\": 1364420783.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/1b3xp1/instructions_for_configuring_aws_cloudsearch/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Instructions for configuring AWS Cloudsearch\", \"created_utc\": 1364391983.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m building an app, purely for learning purposes, but I\\u0026#39;m building it with the idea that it\\u0026#39;d be public-facing/others would use it.  As such, I imagine a user providing Reddit credentials and it gathering their saved posts.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhat I\\u0026#39;m stuck on (as a n00b) is how do I capture their credentials, securely, NOT storing them, but allowing a scheduled sync (or even a user-forced sync (while still abiding by Reddit\\u0026#39;s API rules))?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI fear I\\u0026#39;d have to store them for future syncing and I could do so with some level(s) of encryption, but then how do I unpack that later? :-/ Perhaps that question is too broad or outside the scope of \\u003Ca href=\\\"/r/redditdev\\\"\\u003E/r/redditdev\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENone the less...I thought I\\u0026#39;d ask. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks! \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm building an app, purely for learning purposes, but I'm building it with the idea that it'd be public-facing/others would use it.  As such, I imagine a user providing Reddit credentials and it gathering their saved posts.  \\n\\nWhat I'm stuck on (as a n00b) is how do I capture their credentials, securely, NOT storing them, but allowing a scheduled sync (or even a user-forced sync (while still abiding by Reddit's API rules))?\\n\\nI fear I'd have to store them for future syncing and I could do so with some level(s) of encryption, but then how do I unpack that later? :-/ Perhaps that question is too broad or outside the scope of /r/redditdev\\n\\nNone the less...I thought I'd ask. \\n\\nThanks! \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"19op8t\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"esacteksab\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/19op8t/3rdparty_app_authentication/\", \"locked\": false, \"name\": \"t3_19op8t\", \"created\": 1362484638.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/19op8t/3rdparty_app_authentication/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"3rd-party app authentication\", \"created_utc\": 1362455838.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi,\\nI don\\u0026#39;t know if it\\u0026#39;s possible to install reddit in Ubuntu 11.10 (oneiric).  If it isn\\u0026#39;t, then please ignore what follows.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI would like to install Cassandra, one of the dependencies for the reddit.  However, after adding the reddit ppa with:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E~$ sudo add-apt-repository ppa:reddit/ppa\\n[sudo] password for jack: \\nYou are about to add the following PPA to your system:\\n reddit ppa\\n\\n More info: https://launchpad.net/~reddit/+archive/ppa\\nPress [ENTER] to continue or ctrl-c to cancel adding it\\n\\ngpg: keyring `/tmp/tmp6aCndX/secring.gpg\\u0026#39; created\\ngpg: keyring `/tmp/tmp6aCndX/pubring.gpg\\u0026#39; created\\ngpg: requesting key 65506D27 from hkp server      keyserver.ubuntu.com\\ngpg: /tmp/tmp6aCndX/trustdb.gpg: trustdb created\\ngpg: key 65506D27: public key \\u0026quot;Launchpad PPA for reddit\\u0026quot;   imported\\ngpg: Total number processed: 1\\ngpg:               imported: 1  (RSA: 1)\\nOK\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E, it seems that the ppa is not available for oneiric\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EW: Failed to fetch http://ppa.launchpad.net/reddit/ppa/ubuntu/dists/oneiric/main/source/Sources  404  Not Found\\n\\nW: Failed to fetch http://ppa.launchpad.net/reddit/ppa/ubuntu/dists/oneiric/main/binary-i386/Packages  404  Not Found\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIndeed, there is no package called \\u003Cem\\u003Ecassandra\\u003C/em\\u003E:\\n    ~$ sudo apt-get install cassandra\\n    Reading package lists... Done\\n    Building dependency tree\\u003Cbr/\\u003E\\n    Reading state information... Done\\n    Package cassandra is not available, but is referred to by another package.\\n    This may mean that the package is missing, has been obsoleted, or\\nis only available from another source\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EE: Package \\u0026#39;cassandra\\u0026#39; has no installation candidate\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EWhat can I do in this case?  Use the ppa for other ubuntu releases?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks in advance for any help.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi,\\nI don't know if it's possible to install reddit in Ubuntu 11.10 (oneiric).  If it isn't, then please ignore what follows.\\n\\nI would like to install Cassandra, one of the dependencies for the reddit.  However, after adding the reddit ppa with:\\n\\n    ~$ sudo add-apt-repository ppa:reddit/ppa\\n    [sudo] password for jack: \\n    You are about to add the following PPA to your system:\\n     reddit ppa\\n \\n     More info: https://launchpad.net/~reddit/+archive/ppa\\n    Press [ENTER] to continue or ctrl-c to cancel adding it\\n\\n    gpg: keyring `/tmp/tmp6aCndX/secring.gpg' created\\n    gpg: keyring `/tmp/tmp6aCndX/pubring.gpg' created\\n    gpg: requesting key 65506D27 from hkp server      keyserver.ubuntu.com\\n    gpg: /tmp/tmp6aCndX/trustdb.gpg: trustdb created\\n    gpg: key 65506D27: public key \\\"Launchpad PPA for reddit\\\"   imported\\n    gpg: Total number processed: 1\\n    gpg:               imported: 1  (RSA: 1)\\n    OK\\n\\n, it seems that the ppa is not available for oneiric\\n    \\n    W: Failed to fetch http://ppa.launchpad.net/reddit/ppa/ubuntu/dists/oneiric/main/source/Sources  404  Not Found\\n    \\n    W: Failed to fetch http://ppa.launchpad.net/reddit/ppa/ubuntu/dists/oneiric/main/binary-i386/Packages  404  Not Found\\n\\nIndeed, there is no package called *cassandra*:\\n    ~$ sudo apt-get install cassandra\\n    Reading package lists... Done\\n    Building dependency tree       \\n    Reading state information... Done\\n    Package cassandra is not available, but is referred to by another package.\\n    This may mean that the package is missing, has been obsoleted, or\\nis only available from another source\\n    \\n    E: Package 'cassandra' has no installation candidate\\n\\nWhat can I do in this case?  Use the ppa for other ubuntu releases?\\n\\nThanks in advance for any help.\\n\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"19dzi0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"JcyU\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/19dzi0/reddit_install_problem_is_it_possible_to_install/\", \"locked\": false, \"name\": \"t3_19dzi0\", \"created\": 1362071283.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/19dzi0/reddit_install_problem_is_it_possible_to_install/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"reddit install problem:  is it possible to install cassandra as needed, in ubuntu 11.10?\", \"created_utc\": 1362042483.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello, redditdev!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m using PRAW to pull some basic information from Reddit, namely the top 500 posts from \\u003Ca href=\\\"/r/pics\\\"\\u003E/r/pics\\u003C/a\\u003E. My code is very straightforward:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport praw\\n\\nr = praw.Reddit(user_agent=\\u0026#39;MrFanzyPanz Datascraper :D\\u0026#39;)\\n\\nfor post in r.get_subreddit(\\u0026#39;pics\\u0026#39;).get_hot(limit=500, url_data={\\u0026#39;limit\\u0026#39;: 100}):\\n\\n    *post.name\\n    *post.score\\n    *post.author\\n    *post.created_utc\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m writing it out to a csv to play around with.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy problem is that, while this worked a couple weeks ago, I\\u0026#39;ve revisited the scraper, and I\\u0026#39;m now getting this error when I run it:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003ETypeError: get_content() got an unexpected keyword argument \\u0026#39;url_data\\u0026#39;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EWas url_data removed in an update? I\\u0026#39;ve update PRAW to the newest version.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!     \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello, redditdev!\\n\\nI'm using PRAW to pull some basic information from Reddit, namely the top 500 posts from /r/pics. My code is very straightforward:\\n\\n    import praw\\n\\n    r = praw.Reddit(user_agent='MrFanzyPanz Datascraper :D')\\n\\n    for post in r.get_subreddit('pics').get_hot(limit=500, url_data={'limit': 100}):\\n      \\n        *post.name\\n        *post.score\\n        *post.author\\n        *post.created_utc\\n\\nI'm writing it out to a csv to play around with.\\n\\nMy problem is that, while this worked a couple weeks ago, I've revisited the scraper, and I'm now getting this error when I run it:\\n\\t\\n    TypeError: get_content() got an unexpected keyword argument 'url_data'\\n\\nWas url_data removed in an update? I've update PRAW to the newest version.\\n\\nThanks!\\t\\t\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"197uz8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MrFanzyPanz\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/197uz8/praw_url_data_removed_from_get_content/\", \"locked\": false, \"name\": \"t3_197uz8\", \"created\": 1361859394.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/197uz8/praw_url_data_removed_from_get_content/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"PRAW: url_data removed from get_content()\", \"created_utc\": 1361830594.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIve already created an S3 bucket for the thumbnails, and fixed the scrapping part. When posting links to youtube, video player is already working good. But when placing other links, no thumbnail is show, I am sure that the scrapping part is good, and that the thumbnail was stored on S3.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT:\\u003C/strong\\u003E Okay, ive got some answers to my own question. The default preference for users is to show media(thumbnails) on subreddit. Ive edited the default value in the models/account.py, pref_media variable, to \\u0026quot;on\\u0026quot;. Now this sets media to be shown by default once a user is newly created. But on the Front page, or where the users is not logged in, no media thumbnails is still shown.  Does anyone know what file needs to be edited for this to be done?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Ive already created an S3 bucket for the thumbnails, and fixed the scrapping part. When posting links to youtube, video player is already working good. But when placing other links, no thumbnail is show, I am sure that the scrapping part is good, and that the thumbnail was stored on S3.\\n\\n**EDIT:** Okay, ive got some answers to my own question. The default preference for users is to show media(thumbnails) on subreddit. Ive edited the default value in the models/account.py, pref_media variable, to \\\"on\\\". Now this sets media to be shown by default once a user is newly created. But on the Front page, or where the users is not logged in, no media thumbnails is still shown.  Does anyone know what file needs to be edited for this to be done?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"196m6v\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1361811389.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/196m6v/how_to_enable_thumbnails/\", \"locked\": false, \"name\": \"t3_196m6v\", \"created\": 1361809794.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/196m6v/how_to_enable_thumbnails/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to enable thumbnails?\", \"created_utc\": 1361780994.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Cstrong\\u003Eupdate: justoman diagnosed and solved the problem\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003E\\u003Cstrong\\u003EWhen it does the redirect it\\u0026#39;s redirecting without the port number. Try hitting just \\u003Ca href=\\\"http://reddit.local:8081\\\"\\u003Ehttp://reddit.local:8081\\u003C/a\\u003E\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003EI think that I did the installation correctly, but trying to open \\u003Ca href=\\\"http://127.0.0.1:8081/\\\"\\u003Ehttp://127.0.0.1:8081/\\u003C/a\\u003E redirects to \\u003Ca href=\\\"http://www.reddit.local/\\\"\\u003Ehttp://www.reddit.local/\\u003C/a\\u003E and then complains: \\u0026quot;Firefox can\\u0026#39;t find the server at reddit.local.\\u0026quot;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m running Linux Mint ( Release 13 Maya, 32-bit; Kernel Linux 3.2.0-23-generic)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere\\u0026#39;s the terminal in which I start paster:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E indeed@indeedbox ~ $ cd ~/reddit/r2     \\n indeed@indeedbox ~/reddit/r2 $ paster serve --reload example.ini http_port=8081\\n Starting subprocess with file monitor\\n Overriding g.http_port to 8081\\n indeedbox:19824 started e301b34 at 20:36:55 (took 0.59s)\\n Starting server in PID 19824.\\n serving on 0.0.0.0:8081 view at http://127.0.0.1:8081\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAnd here\\u0026#39;s the last bits of syslog:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E Feb 18 18:55:37 indeedbox dbus[716]: [system] Successfully activated service \\u0026#39;com.ubuntu.SoftwareProperties\\u0026#39;\\n Feb 18 19:17:01 indeedbox CRON[17150]: (root) CMD (   cd / \\u0026amp;\\u0026amp; run-parts --report /etc/cron.hourly)\\n Feb 18 19:50:21 indeedbox sudo: pam_ecryptfs: pam_sm_authenticate: /home/indeed is already mounted\\n Feb 18 20:17:01 indeedbox CRON[19030]: (root) CMD (   cd / \\u0026amp;\\u0026amp; run-parts --report /etc/cron.hourly)\\n Feb 18 20:29:09 indeedbox sudo: pam_ecryptfs: pam_sm_authenticate: /home/indeed is already mounted\\n Feb 18 20:52:53 indeedbox sudo: pam_ecryptfs:       pam_sm_authenticate: /home/indeed is already mounted\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EWhat should I be looking at to find the problem, please?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"**update: justoman diagnosed and solved the problem**\\n\\n\\u003E **When it does the redirect it's redirecting without the port number. Try hitting just http://reddit.local:8081**\\n\\nI think that I did the installation correctly, but trying to open http://127.0.0.1:8081/ redirects to http://www.reddit.local/ and then complains: \\\"Firefox can't find the server at reddit.local.\\\"\\n\\nI'm running Linux Mint ( Release 13 Maya, 32-bit; Kernel Linux 3.2.0-23-generic)\\n\\nHere's the terminal in which I start paster:\\n\\n     indeed@indeedbox ~ $ cd ~/reddit/r2     \\n     indeed@indeedbox ~/reddit/r2 $ paster serve --reload example.ini http_port=8081\\n     Starting subprocess with file monitor\\n     Overriding g.http_port to 8081\\n     indeedbox:19824 started e301b34 at 20:36:55 (took 0.59s)\\n     Starting server in PID 19824.\\n     serving on 0.0.0.0:8081 view at http://127.0.0.1:8081\\n\\n\\nAnd here's the last bits of syslog:\\n\\n     Feb 18 18:55:37 indeedbox dbus[716]: [system] Successfully activated service 'com.ubuntu.SoftwareProperties'\\n     Feb 18 19:17:01 indeedbox CRON[17150]: (root) CMD (   cd / \\u0026\\u0026 run-parts --report /etc/cron.hourly)\\n     Feb 18 19:50:21 indeedbox sudo: pam_ecryptfs: pam_sm_authenticate: /home/indeed is already mounted\\n     Feb 18 20:17:01 indeedbox CRON[19030]: (root) CMD (   cd / \\u0026\\u0026 run-parts --report /etc/cron.hourly)\\n     Feb 18 20:29:09 indeedbox sudo: pam_ecryptfs: pam_sm_authenticate: /home/indeed is already mounted\\n     Feb 18 20:52:53 indeedbox sudo: pam_ecryptfs:       pam_sm_authenticate: /home/indeed is already mounted\\n     \\nWhat should I be looking at to find the problem, please?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"18sl6k\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"smashing_board\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1361326908.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/18sl6k/firefox_cant_find_the_server_at_redditlocal/\", \"locked\": false, \"name\": \"t3_18sl6k\", \"created\": 1361271425.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/18sl6k/firefox_cant_find_the_server_at_redditlocal/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Firefox can't find the server at reddit.local.\", \"created_utc\": 1361242625.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI don\\u0026#39;t see any likes field.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I don't see any likes field.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"17x2g0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"blakecaldwell\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/17x2g0/how_do_you_find_the_liked_status_of_comments_on/\", \"locked\": false, \"name\": \"t3_17x2g0\", \"created\": 1360071219.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/17x2g0/how_do_you_find_the_liked_status_of_comments_on/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How do you find the liked status of comments on http://www.reddit.com/message/inbox.json ?\", \"created_utc\": 1360042419.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have a bot that uses all_comments_flat to parse a submission\\u0026#39;s comments. I just set up a new development station and noticed that the newest version of praw doesn\\u0026#39;t have that method on the Submission object anymore, and the comment attribute is a tree. What\\u0026#39;s the easiest way to just get all the comments on a submission? I\\u0026#39;m suspecting that I\\u0026#39;ll need to use some kind of \\u0026quot;load more comments\\u0026quot; function to get all the available comments but I\\u0026#39;m not sure where it is. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks,\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey,\\n\\nI have a bot that uses all_comments_flat to parse a submission's comments. I just set up a new development station and noticed that the newest version of praw doesn't have that method on the Submission object anymore, and the comment attribute is a tree. What's the easiest way to just get all the comments on a submission? I'm suspecting that I'll need to use some kind of \\\"load more comments\\\" function to get all the available comments but I'm not sure where it is. \\n\\nThanks,\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"17tn7r\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"shaggorama\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/17tn7r/replacement_for_all_comments_flat/\", \"locked\": false, \"name\": \"t3_17tn7r\", \"created\": 1359951903.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/17tn7r/replacement_for_all_comments_flat/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Replacement for all_comments_flat\", \"created_utc\": 1359923103.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;d like this to be possible. It\\u0026#39;s one of my biggest issues with oauth.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBasically what I\\u0026#39;d like is a JavaScript client that never touches my server but that has full API access.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'd like this to be possible. It's one of my biggest issues with oauth.\\n\\nBasically what I'd like is a JavaScript client that never touches my server but that has full API access.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"16otwf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"damontoo\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/16otwf/client_side_auth_without_disclosure_of_private_key/\", \"locked\": false, \"name\": \"t3_16otwf\", \"created\": 1358380843.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/16otwf/client_side_auth_without_disclosure_of_private_key/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Client side auth without disclosure of private key.\", \"created_utc\": 1358352043.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe title says it all. I have some hope that a workaround is possible, particularly since I can still \\u003Cem\\u003Esee\\u003C/em\\u003E older comments (from old saved links or old submissions), they just don\\u0026#39;t seem to be tied to the user page. My instinct says that the comments still exist in the database some where, but is there any way to get to them?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EN.B. I\\u0026#39;m using PRAW.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The title says it all. I have some hope that a workaround is possible, particularly since I can still *see* older comments (from old saved links or old submissions), they just don't seem to be tied to the user page. My instinct says that the comments still exist in the database some where, but is there any way to get to them?\\n\\nN.B. I'm using PRAW.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"15v7l4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"burntsushi\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/15v7l4/retrieving_a_users_comments_seems_to_be_limited/\", \"locked\": false, \"name\": \"t3_15v7l4\", \"created\": 1357212383.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/15v7l4/retrieving_a_users_comments_seems_to_be_limited/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Retrieving a user's comments seems to be limited to 1000... is there a way around this?\", \"created_utc\": 1357183583.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESilly question here, sorry for the noobacity.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m trying to use praw\\u0026#39;s get_content() generator in order to pull data from subreddits, however I don\\u0026#39;t know how to iterate over this object. I keep receiving an error that pics.json is an unknown url type:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport praw\\nimport pprint\\n\\nr = praw.Reddit(\\u0026#39;Test Code - MrFanzyPanz\\u0026#39;)\\n\\ncontent_scrape = r.get_content(\\u0026quot;pics\\u0026quot;, limit = 10)\\n\\nfor submission in content_scrape:\\n    print submission\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eor\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E    print submission.title\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIf there is an alternative method which will allow me to simply parse \\u003Ca href=\\\"http://www.reddit.com/.json\\\"\\u003Ewww.reddit.com/.json\\u003C/a\\u003E as text, that would be also fine, although I need to be able to do it through python requests (preferably praw), rather than copy-pasting the text from the page manually.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello!\\n\\nSilly question here, sorry for the noobacity.\\n\\nI'm trying to use praw's get_content() generator in order to pull data from subreddits, however I don't know how to iterate over this object. I keep receiving an error that pics.json is an unknown url type:\\n\\n    import praw\\n    import pprint\\n\\n    r = praw.Reddit('Test Code - MrFanzyPanz')\\n\\n    content_scrape = r.get_content(\\\"pics\\\", limit = 10)\\n\\n    for submission in content_scrape:\\n        print submission\\n\\n**or**\\n\\n        print submission.title\\n\\nIf there is an alternative method which will allow me to simply parse www.reddit.com/.json as text, that would be also fine, although I need to be able to do it through python requests (preferably praw), rather than copy-pasting the text from the page manually.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"152d2w\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MrFanzyPanz\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/152d2w/praw_url_parameters/\", \"locked\": false, \"name\": \"t3_152d2w\", \"created\": 1355887409.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/152d2w/praw_url_parameters/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Praw URL parameters\", \"created_utc\": 1355858609.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIt doesn\\u0026#39;t seem that the API has an option for getting a user\\u0026#39;s saved links. I\\u0026#39;ve tried to use the \\u003Ca href=\\\"https://github.com/jcleblanc/reddit-php-sdk\\\"\\u003Ereddit PHP SDK\\u003C/a\\u003E to authenticate and then grab the json contents, but all I get is a 403 error.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECode:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Enew reddit(\\u0026quot;username\\u0026quot;, \\u0026quot;password\\u0026quot;);\\nprint_r(json_decode(file_get_contents(\\u0026#39;http://www.reddit.com/user/username/saved.json\\u0026#39;),true));\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIf I go to \\u003Ccode\\u003Ehttp://www.reddit.com/user/username/saved.json\\u003C/code\\u003E in my browser while logged in, I can see the data fine.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny ideas?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEdit: Solved.\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI added a function to the PHP SDK:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E/**\\n* Get saved posts\\n*\\n* Get the listing of a user\\u0026#39;s saved posts \\n* @param string $username The username.\\n*/\\npublic function getSaved($username){\\n    return $this-\\u0026gt;runCurl(\\u0026quot;http://www.reddit.com/user/\\u0026quot;.$username.\\u0026quot;/saved.json\\u0026quot;);\\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EUsage:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Erequire_once(\\u0026#39;reddit-sdk.php\\u0026#39;);\\n$reddit = new reddit(username, password);\\n$saved = $reddit-\\u0026gt;getSaved(username);\\nprint_r($saved);\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve submitted a pull request to jcleblanc to have this function incorporated into the PHP SDK.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"It doesn't seem that the API has an option for getting a user's saved links. I've tried to use the [reddit PHP SDK](https://github.com/jcleblanc/reddit-php-sdk) to authenticate and then grab the json contents, but all I get is a 403 error.\\n\\nCode:\\n\\n    new reddit(\\\"username\\\", \\\"password\\\");\\n    print_r(json_decode(file_get_contents('http://www.reddit.com/user/username/saved.json'),true));\\n\\nIf I go to `http://www.reddit.com/user/username/saved.json` in my browser while logged in, I can see the data fine.\\n\\nAny ideas?\\n\\n\\n\\n**Edit: Solved.**\\n\\nI added a function to the PHP SDK:\\n\\n    /**\\n    * Get saved posts\\n    *\\n    * Get the listing of a user's saved posts \\n    * @param string $username The username.\\n    */\\n    public function getSaved($username){\\n        return $this-\\u003ErunCurl(\\\"http://www.reddit.com/user/\\\".$username.\\\"/saved.json\\\");\\n    }\\n\\nUsage:\\n\\n    require_once('reddit-sdk.php');\\n    $reddit = new reddit(username, password);\\n    $saved = $reddit-\\u003EgetSaved(username);\\n    print_r($saved);\\n\\nI've submitted a pull request to jcleblanc to have this function incorporated into the PHP SDK.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"14jy6b\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1355076004.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/14jy6b/how_to_get_a_users_saved_links/\", \"locked\": false, \"name\": \"t3_14jy6b\", \"created\": 1355102740.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/14jy6b/how_to_get_a_users_saved_links/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to get a user's saved links\", \"created_utc\": 1355073940.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello \\u003Ca href=\\\"/r/redditdev\\\"\\u003E/r/redditdev\\u003C/a\\u003E,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI was wondering if there was a way to enable thumbnails for a listing request through a GET variable (or any other method). For example, if you have thumbnails disabled in preferences, is there a way to override that setting for individual requests?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks in advance\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello /r/redditdev,\\n\\nI was wondering if there was a way to enable thumbnails for a listing request through a GET variable (or any other method). For example, if you have thumbnails disabled in preferences, is there a way to override that setting for individual requests?\\n\\nThanks in advance\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"14gjdj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"kortank\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/14gjdj/enable_thumbnails_on_individual_requests/\", \"locked\": false, \"name\": \"t3_14gjdj\", \"created\": 1354937914.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/14gjdj/enable_thumbnails_on_individual_requests/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Enable thumbnails on individual requests\", \"created_utc\": 1354909114.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThe reddit API rules state \\u003Cem\\u003EChange your client\\u0026#39;s User-Agent string to something unique and descriptive, preferably referencing your reddit username.\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny idea how I would do this in html and/or JavaScript?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"The reddit API rules state *Change your client's User-Agent string to something unique and descriptive, preferably referencing your reddit username.*\\n\\nAny idea how I would do this in html and/or JavaScript?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"13rfz0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"observationalhumour\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/13rfz0/how_do_i_change_the_client_useragent_in_a/\", \"locked\": false, \"name\": \"t3_13rfz0\", \"created\": 1353887183.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/13rfz0/how_do_i_change_the_client_useragent_in_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How do I change the client UserAgent in a Phonegap App to comply with the reddit rules?\", \"created_utc\": 1353858383.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhen you fetch the JSON for your messages, you can get comment replies / post replies that look like this:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E{\\n    \\u0026quot;kind\\u0026quot;: \\u0026quot;t1\\u0026quot;,\\n    \\u0026quot;data\\u0026quot;: {\\n        \\u0026quot;body\\u0026quot;: \\u0026quot;Excellent.  :) Thanks!\\u0026quot;,\\n        \\u0026quot;was_comment\\u0026quot;: true,\\n        \\u0026quot;first_message\\u0026quot;: null,\\n        \\u0026quot;name\\u0026quot;: \\u0026quot;t1_c76bdd6\\u0026quot;,\\n        \\u0026quot;created\\u0026quot;: 1353844643.0,\\n        \\u0026quot;dest\\u0026quot;: \\u0026quot;ross456\\u0026quot;,\\n        \\u0026quot;author\\u0026quot;: \\u0026quot;GoodGuyAlex\\u0026quot;,\\n        \\u0026quot;created_utc\\u0026quot;: 1353815843.0,\\n        \\u0026quot;body_html\\u0026quot;: \\u0026quot;\\u0026amp;lt;!-- SC_OFF --\\u0026amp;gt;\\u0026amp;lt;div class=\\\\\\u0026quot;md\\\\\\u0026quot;\\u0026amp;gt;\\u0026amp;lt;p\\u0026amp;gt;Excellent.  :) Thanks!\\u0026amp;lt;/p\\u0026amp;gt;\\\\n\\u0026amp;lt;/div\\u0026amp;gt;\\u0026amp;lt;!-- SC_ON --\\u0026amp;gt;\\u0026quot;,\\n        \\u0026quot;subreddit\\u0026quot;: \\u0026quot;reddit_to_go\\u0026quot;,\\n        \\u0026quot;parent_id\\u0026quot;: \\u0026quot;t4_c76b9t4\\u0026quot;,\\n        \\u0026quot;context\\u0026quot;: \\u0026quot;/r/reddit_to_go/comments/13qfoo/feature_request_post_comment_by_hitting_ctrlenter/c76bdd6?context=3\\u0026quot;,\\n        \\u0026quot;replies\\u0026quot;: null,\\n        \\u0026quot;new\\u0026quot;: false,\\n        \\u0026quot;id\\u0026quot;: \\u0026quot;c76bdd6\\u0026quot;,\\n        \\u0026quot;subject\\u0026quot;: \\u0026quot;comment reply\\u0026quot;\\n    }\\n}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENote that there\\u0026#39;s no \\u0026quot;likes\\u0026quot; field set to true/false/null. If viewing your messages on reddit.com, you\\u0026#39;ll get voting arrows next to messages like these, but an app won\\u0026#39;t be able to initialize similar arrows to the correct state.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs it possible to get the returned data changed to properly set the vote status?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"When you fetch the JSON for your messages, you can get comment replies / post replies that look like this:\\n\\n    {\\n        \\\"kind\\\": \\\"t1\\\",\\n        \\\"data\\\": {\\n            \\\"body\\\": \\\"Excellent.  :) Thanks!\\\",\\n            \\\"was_comment\\\": true,\\n            \\\"first_message\\\": null,\\n            \\\"name\\\": \\\"t1_c76bdd6\\\",\\n            \\\"created\\\": 1353844643.0,\\n            \\\"dest\\\": \\\"ross456\\\",\\n            \\\"author\\\": \\\"GoodGuyAlex\\\",\\n            \\\"created_utc\\\": 1353815843.0,\\n            \\\"body_html\\\": \\\"\\u0026lt;!-- SC_OFF --\\u0026gt;\\u0026lt;div class=\\\\\\\"md\\\\\\\"\\u0026gt;\\u0026lt;p\\u0026gt;Excellent.  :) Thanks!\\u0026lt;/p\\u0026gt;\\\\n\\u0026lt;/div\\u0026gt;\\u0026lt;!-- SC_ON --\\u0026gt;\\\",\\n            \\\"subreddit\\\": \\\"reddit_to_go\\\",\\n            \\\"parent_id\\\": \\\"t4_c76b9t4\\\",\\n            \\\"context\\\": \\\"/r/reddit_to_go/comments/13qfoo/feature_request_post_comment_by_hitting_ctrlenter/c76bdd6?context=3\\\",\\n            \\\"replies\\\": null,\\n            \\\"new\\\": false,\\n            \\\"id\\\": \\\"c76bdd6\\\",\\n            \\\"subject\\\": \\\"comment reply\\\"\\n        }\\n    }\\n\\nNote that there's no \\\"likes\\\" field set to true/false/null. If viewing your messages on reddit.com, you'll get voting arrows next to messages like these, but an app won't be able to initialize similar arrows to the correct state.\\n\\nIs it possible to get the returned data changed to properly set the vote status?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"13r65c\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ross456\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/13r65c/likes_status_of_comment_replies_from_messageinbox/\", \"locked\": false, \"name\": \"t3_13r65c\", \"created\": 1353863322.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/13r65c/likes_status_of_comment_replies_from_messageinbox/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"\\\"Likes\\\" status of comment replies from /message/inbox?\", \"created_utc\": 1353834522.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve tried to find a way to make trac work, so far no light.\\nI thought just installing is end of work, however authentication stuff, subdomain etc,. are confusing.\\nCan anyone help me? I\\u0026#39;m using lighttpd.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've tried to find a way to make trac work, so far no light.\\nI thought just installing is end of work, however authentication stuff, subdomain etc,. are confusing.\\nCan anyone help me? I'm using lighttpd.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"11ogy1\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"freshful\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/11ogy1/how_can_i_set_up_trac_to_make_wiki_work_on_my/\", \"locked\": false, \"name\": \"t3_11ogy1\", \"created\": 1350573973.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/11ogy1/how_can_i_set_up_trac_to_make_wiki_work_on_my/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How can I set up Trac to make wiki work on my reddit clone?\", \"created_utc\": 1350545173.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m interested in validating user logins via OAuth instead of making a POST request and depending on the cookie.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEverything works great except users have to re-hit accept every ten minutes due to the expiration time on OAuth keys which Reddit returns being so short a time period.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI was wondering if Reddit had plans to increase the expiration time on these keys, or if I should stay clear of OAuth?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks,\\nDavid\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi,\\n\\nI'm interested in validating user logins via OAuth instead of making a POST request and depending on the cookie.\\n\\nEverything works great except users have to re-hit accept every ten minutes due to the expiration time on OAuth keys which Reddit returns being so short a time period.\\n\\nI was wondering if Reddit had plans to increase the expiration time on these keys, or if I should stay clear of OAuth?\\n\\nThanks,\\nDavid\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"10nfai\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Macmee\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/10nfai/request_oauth_last_longer_than_10_minutes/\", \"locked\": false, \"name\": \"t3_10nfai\", \"created\": 1348912952.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/10nfai/request_oauth_last_longer_than_10_minutes/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"request: OAuth last longer than 10 minutes\", \"created_utc\": 1348884152.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EFinally I can change my reddit clone to production stage. Thank you all of you. \\nHowever I got a problem. I backup postgresql db, then I can see details of DB. All IP related data like registration_ip are saved as 127.0.0.1.\\nDid I miss some option? \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Finally I can change my reddit clone to production stage. Thank you all of you. \\nHowever I got a problem. I backup postgresql db, then I can see details of DB. All IP related data like registration_ip are saved as 127.0.0.1.\\nDid I miss some option? \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"zpcoi\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"freshful\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/zpcoi/all_ip_address_is_saved_as_127001/\", \"locked\": false, \"name\": \"t3_zpcoi\", \"created\": 1347392754.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/zpcoi/all_ip_address_is_saved_as_127001/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"All IP address is saved as 127.0.0.1\", \"created_utc\": 1347363954.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIn reddit URL, there is \\u0026quot;5 characternumerics\\u0026quot; thing_id part (for example, \\u0026quot;wplf7\\u0026quot; from \\u0026quot;\\u003Ca href=\\\"http://redd.it/wplf7%22\\\"\\u003Ehttp://redd.it/wplf7\\u0026quot;\\u003C/a\\u003E) which is generated by base36. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ewplf7 is generated from number 54941875 =\\u0026gt; to36(54941875) =wplf7 : this is what I found so far...I\\u0026#39;m wondering how this number \\u0026quot;54941875\\u0026quot; is generated in the first place? Is it by category? or just random?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnyone who can explain this in the simple manner? Unfortunately Python is not my domain and 2000 lines of python code listed on Reddit wiki didn\\u0026#39;t help me much. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"In reddit URL, there is \\\"5 characternumerics\\\" thing_id part (for example, \\\"wplf7\\\" from \\\"http://redd.it/wplf7\\\") which is generated by base36. \\n\\nwplf7 is generated from number 54941875 =\\u003E to36(54941875) =wplf7 : this is what I found so far...I'm wondering how this number \\\"54941875\\\" is generated in the first place? Is it by category? or just random?\\n\\nAnyone who can explain this in the simple manner? Unfortunately Python is not my domain and 2000 lines of python code listed on Reddit wiki didn't help me much. \\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ws3j7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"littlemoon88\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ws3j7/5_characternumerics_thing_id_in_reddit_url_how_is/\", \"locked\": false, \"name\": \"t3_ws3j7\", \"created\": 1342673872.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ws3j7/5_characternumerics_thing_id_in_reddit_url_how_is/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"5 characternumerics thing_Id in reddit URL - how is it generated?\", \"created_utc\": 1342645072.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Elib/cssfilter.py fails to validate some recent CSS3 features, including RGBA color values. There is a pending pull request [1] regarding this issue, but I looked a little deeper.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe Python library cssutils, which is used by lib/cssfilter.py for the bulk of the validation work, seems to support RGBA colors, as well as numerous other features that are duplicated in the reddit code. For example, see this same list of color names in the Reddit code at [2] and in the library at [3].\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI note that lib/cssfilter.py has had only a handful of substantive changes in the last year, while cssutils acquired these features in the last few months. I believe the current version of the library has made most of lib/cssfilter.py obsolete.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EReddit\\u0026#39;s setup.py does specify an older version of the library that does not contain the features I mentioned, but I\\u0026#39;m not sure it\\u0026#39;s necessary. The version was pinned three years ago [4] due to \\u0026quot;API breakage\\u0026quot;. In my judgment, using a newer version of the library is worth it for the gigantic code deduplication and more-correct CSS validation.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs this a project worth pursuing, or am I missing something?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E[1] \\u003Ca href=\\\"https://github.com/reddit/reddit/pull/461\\\"\\u003Ehttps://github.com/reddit/reddit/pull/461\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E[2] \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/lib/cssfilter.py#L65\\\"\\u003Ehttps://github.com/reddit/reddit/blob/master/r2/r2/lib/cssfilter.py#L65\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E[3] \\u003Ca href=\\\"https://bitbucket.org/cthedot/cssutils/src/024c6fff92d7/src/cssutils/css/colors.py#cl-17\\\"\\u003Ehttps://bitbucket.org/cthedot/cssutils/src/024c6fff92d7/src/cssutils/css/colors.py#cl-17\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E[4] \\u003Ca href=\\\"https://github.com/reddit/reddit/commit/498c3b01fce22e1761f1dd6a93038a9dfdeda13f#r2/setup.py\\\"\\u003Ehttps://github.com/reddit/reddit/commit/498c3b01fce22e1761f1dd6a93038a9dfdeda13f#r2/setup.py\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"lib/cssfilter.py fails to validate some recent CSS3 features, including RGBA color values. There is a pending pull request [1] regarding this issue, but I looked a little deeper.\\n\\nThe Python library cssutils, which is used by lib/cssfilter.py for the bulk of the validation work, seems to support RGBA colors, as well as numerous other features that are duplicated in the reddit code. For example, see this same list of color names in the Reddit code at [2] and in the library at [3].\\n\\nI note that lib/cssfilter.py has had only a handful of substantive changes in the last year, while cssutils acquired these features in the last few months. I believe the current version of the library has made most of lib/cssfilter.py obsolete.\\n\\nReddit's setup.py does specify an older version of the library that does not contain the features I mentioned, but I'm not sure it's necessary. The version was pinned three years ago [4] due to \\\"API breakage\\\". In my judgment, using a newer version of the library is worth it for the gigantic code deduplication and more-correct CSS validation.\\n\\nIs this a project worth pursuing, or am I missing something?\\n\\n\\n[1] https://github.com/reddit/reddit/pull/461\\n\\n[2] https://github.com/reddit/reddit/blob/master/r2/r2/lib/cssfilter.py#L65\\n\\n[3] https://bitbucket.org/cthedot/cssutils/src/024c6fff92d7/src/cssutils/css/colors.py#cl-17\\n\\n[4] https://github.com/reddit/reddit/commit/498c3b01fce22e1761f1dd6a93038a9dfdeda13f#r2/setup.py\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"w5iec\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"listen2\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/w5iec/libcssfilterpy_out_of_date_library_code/\", \"locked\": false, \"name\": \"t3_w5iec\", \"created\": 1341644304.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/w5iec/libcssfilterpy_out_of_date_library_code/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"lib/cssfilter.py out of date; library code duplicated in \\napplication\", \"created_utc\": 1341615504.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ETo skip background/motivation go to the next section ;)\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EI am working on a website for an annual graduate research conference that would like to make some of their internal paper reviews public, so that the review process is more transparent to the research community.  Hopefully this will help even the playing field for researchers who are not intimate with the review process, lead to better quality paper submissions, and foster productive discussion, perhaps even innovation, regarding the process itself.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI initially thought of the reddit source engine for this purpose, but decided to go another route because of its excessive features (both user interface -- ads, gold, search, etc., and back-end enterprise design -- memcached, cassandra, etc.).  It simply had too many features and was too complicated for our needs.  I ended up prototyping a quick site from scratch using pyramid, which worked pretty well.  Unfortunately I have now been asked to add a lot more features such as comment sorting, moderation, voting, etc.  This has prompted me take a second look at the reddit source to see how hard it would be to adapt to our needs.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESo I spent this morning getting the latest version of the source up and running on a Xen Ubuntu 11.04 VM, then proceeded to dig in to how to modify it for our needs.  The \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu\\\"\\u003Eubuntu install script\\u003C/a\\u003E worked great (after I finally found it, heh), other than some problems with Cassandra crashing unexpectedly (mostly due to VM issues, but it took forever to debug :/).  I managed to figure out (mostly) how to hide / disable stuff we don\\u0026#39;t need through css / templates, but I\\u0026#39;m having a bit harder timing adding additional features we need (since that involves, you know, real coding...).  I as a bit disappointed that I couldn\\u0026#39;t find any high level descriptions of or tutorials for hacking the actual code.  I guess I should go look at the pylons layout specifically?  Is it  fairly close to a normal pylons project?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENotably I am just a lowly graduate student who was notified this had to be done \\u0026quot;urgently\\u0026quot; with less than a weeks notice for the intended rollout :/.  Obviously that\\u0026#39;s not going to happen at this point, but any expedited input would be greatly appreciated =)\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EBasically what I would like to do is force all submissions to be \\u0026quot;text\\u0026quot; and enable extra fields (such as authors). Then I would like to add an additional \\u0026quot;review\\u0026quot; section with multiple fields (such as reviewer and text) to the top of that \\u0026quot;link\\u0026quot;\\u0026#39;s page.  This review should be addable only by moderators, but should have some sort of form submission.  So for starters, \\u003Cstrong\\u003Edoes anyone have any tips on what I would have to do to enable additional submission fields in the \\u0026quot;...\\u003Ca href=\\\"/r/blah/submit\\\"\\u003E/r/blah/submit\\u003C/a\\u003E\\u0026quot; form, then have them display on the \\u0026quot;...\\u003Ca href=\\\"/r/blah/comments/xxxx/xxxxx\\\"\\u003E/r/blah/comments/xxxx/xxxxx\\u003C/a\\u003E\\u0026quot; page for that submission?\\u003C/strong\\u003E  Perhaps just a high level description of the process, including which files would have to be modified (and how), would be a \\u003Cem\\u003Ehuge\\u003C/em\\u003E help.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPresumaby I would have to modify:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E1) r2/templates/newlink.html to add the additional form fields.  (Very novice question... is the templating language Mako?  I started getting used to Chameleon with my prototype, but I\\u0026#39;m not very familiar with Python web dev.)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E2) Whatever model this form gets submitted to.  (Presumably it will still be routed correctly, right?  Maybe for the purposes of understanding, someone could mention how it gets routed there?)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E3) The persistent storage.  How is the data actually mapped back to SQL / Cassandra?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe next step will be to make the review submission form for moderators, but I think if I understand how to do this I will have a much better idea of how to tackle that.  Perhaps this post was a bit hasty, since I am still very much exploring on my own, but I figured the sooner I ask the sooner I will get feedback ;).  Regardless, I think others could benefit from a description of how to do this, as I searched for a bit and couldn\\u0026#39;t really find much (let me know if I just missed it).  Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"To skip background/motivation go to the next section ;)\\n\\n------------------------------------------------\\n\\nI am working on a website for an annual graduate research conference that would like to make some of their internal paper reviews public, so that the review process is more transparent to the research community.  Hopefully this will help even the playing field for researchers who are not intimate with the review process, lead to better quality paper submissions, and foster productive discussion, perhaps even innovation, regarding the process itself.\\n\\nI initially thought of the reddit source engine for this purpose, but decided to go another route because of its excessive features (both user interface -- ads, gold, search, etc., and back-end enterprise design -- memcached, cassandra, etc.).  It simply had too many features and was too complicated for our needs.  I ended up prototyping a quick site from scratch using pyramid, which worked pretty well.  Unfortunately I have now been asked to add a lot more features such as comment sorting, moderation, voting, etc.  This has prompted me take a second look at the reddit source to see how hard it would be to adapt to our needs.\\n\\nSo I spent this morning getting the latest version of the source up and running on a Xen Ubuntu 11.04 VM, then proceeded to dig in to how to modify it for our needs.  The [ubuntu install script](https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu) worked great (after I finally found it, heh), other than some problems with Cassandra crashing unexpectedly (mostly due to VM issues, but it took forever to debug :/).  I managed to figure out (mostly) how to hide / disable stuff we don't need through css / templates, but I'm having a bit harder timing adding additional features we need (since that involves, you know, real coding...).  I as a bit disappointed that I couldn't find any high level descriptions of or tutorials for hacking the actual code.  I guess I should go look at the pylons layout specifically?  Is it  fairly close to a normal pylons project?\\n\\nNotably I am just a lowly graduate student who was notified this had to be done \\\"urgently\\\" with less than a weeks notice for the intended rollout :/.  Obviously that's not going to happen at this point, but any expedited input would be greatly appreciated =)\\n\\n------------------------------------------------\\n\\nBasically what I would like to do is force all submissions to be \\\"text\\\" and enable extra fields (such as authors). Then I would like to add an additional \\\"review\\\" section with multiple fields (such as reviewer and text) to the top of that \\\"link\\\"'s page.  This review should be addable only by moderators, but should have some sort of form submission.  So for starters, **does anyone have any tips on what I would have to do to enable additional submission fields in the \\\".../r/blah/submit\\\" form, then have them display on the \\\".../r/blah/comments/xxxx/xxxxx\\\" page for that submission?**  Perhaps just a high level description of the process, including which files would have to be modified (and how), would be a *huge* help.\\n\\nPresumaby I would have to modify:\\n\\n   1) r2/templates/newlink.html to add the additional form fields.  (Very novice question... is the templating language Mako?  I started getting used to Chameleon with my prototype, but I'm not very familiar with Python web dev.)\\n    \\n   2) Whatever model this form gets submitted to.  (Presumably it will still be routed correctly, right?  Maybe for the purposes of understanding, someone could mention how it gets routed there?)\\n    \\n   3) The persistent storage.  How is the data actually mapped back to SQL / Cassandra?\\n\\nThe next step will be to make the review submission form for moderators, but I think if I understand how to do this I will have a much better idea of how to tackle that.  Perhaps this post was a bit hasty, since I am still very much exploring on my own, but I figured the sooner I ask the sooner I will get feedback ;).  Regardless, I think others could benefit from a description of how to do this, as I searched for a bit and couldn't really find much (let me know if I just missed it).  Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"sxcig\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"RedditSrc4Research\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/sxcig/extending_reddit_adding_more_fields_to_text_link/\", \"locked\": false, \"name\": \"t3_sxcig\", \"created\": 1335680151.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/sxcig/extending_reddit_adding_more_fields_to_text_link/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Extending Reddit: Adding more fields to text link submission?\", \"created_utc\": 1335651351.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EJust a suggestion:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECurrently, to get unread counts, I have to download the JSON that corresponds with the \\u0026quot;listing\\u0026quot; of mail - meaning, for example, /message/unread.json ... then I count the results... that\\u0026#39;s a decent bit of unnecessary data for Reddit to send, and for me to parse through.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor mod mail, there isn\\u0026#39;t (that I\\u0026#39;m aware of) even an unread page -- so I\\u0026#39;d need to download 100 messages (since that\\u0026#39;s what my prefs are set at) and parse through them individually counting up the \\u0026quot;new\\u0026quot;:\\u0026quot;true\\u0026quot; flags to determine the unread count...\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt would be great if I could just make a simple query that returned only the data I need... something like:\\n    {\\n      \\u0026quot;unread\\u0026quot;:\\u0026quot;6\\u0026quot;,\\n      \\u0026quot;last_unread_timestamp\\u0026quot;: [UTC timestamp]\\n    }\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m not even sure what other data would be needed / useful beyond that, but I\\u0026#39;m guessing it\\u0026#39;d be more efficient than downloading the JSON data that includes the text of every message, etc...\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIdeally, I\\u0026#39;d love to have this for:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003Emessages/mail\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003Emod mail  [general]\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003Emod mail [separated by subreddit, i.e. unread messages to \\u003Ca href=\\\"/r/whatever\\\"\\u003Er/whatever\\u003C/a\\u003E]\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EWhile I\\u0026#39;m at it:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003Emod queue [# of reported posts that need to be tended to] might also be good!\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Just a suggestion:\\n\\nCurrently, to get unread counts, I have to download the JSON that corresponds with the \\\"listing\\\" of mail - meaning, for example, /message/unread.json ... then I count the results... that's a decent bit of unnecessary data for Reddit to send, and for me to parse through.\\n\\nFor mod mail, there isn't (that I'm aware of) even an unread page -- so I'd need to download 100 messages (since that's what my prefs are set at) and parse through them individually counting up the \\\"new\\\":\\\"true\\\" flags to determine the unread count...\\n\\nIt would be great if I could just make a simple query that returned only the data I need... something like:\\n    {\\n      \\\"unread\\\":\\\"6\\\",\\n      \\\"last_unread_timestamp\\\": [UTC timestamp]\\n    }\\n\\nI'm not even sure what other data would be needed / useful beyond that, but I'm guessing it'd be more efficient than downloading the JSON data that includes the text of every message, etc...\\n\\nIdeally, I'd love to have this for:\\n\\n- messages/mail\\n\\n- mod mail  [general]\\n\\n- mod mail [separated by subreddit, i.e. unread messages to r/whatever]\\n\\nWhile I'm at it:\\n\\n- mod queue [# of reported posts that need to be tended to] might also be good!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"rg2we\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"honestbleeps\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/rg2we/request_unread_counts_for_mail_mod_mail_mod_mail/\", \"locked\": false, \"name\": \"t3_rg2we\", \"created\": 1332896878.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/rg2we/request_unread_counts_for_mail_mod_mail_mod_mail/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Request: unread counts for mail, mod mail, mod mail by subreddit (details inside)\", \"created_utc\": 1332868078.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m using the apigee Reddit console attempting to get a simple test submit working. All required fields are filled in. I used the api/login to obtain a uh and cookie. I copied the uh and added it to the query. I\\u0026#39;ve added the cookie returned to the header as both cookie and reddit_session. I\\u0026#39;ve set api_type to json and left that off. As a last ditch I tried adding user and passwd to the query, again failure. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBased on the API wiki in github(including talklittle) I thought this sufficient, but I\\u0026#39;m obviously missing something.  No matter what I try I always get a response that user is required and to please login. Here\\u0026#39;s an example request, I\\u0026#39;m at a loss:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EPOST /api/submit?uh=blah\\u0026amp;text=Test of API Submit on apigee\\u0026amp;kind=self\\u0026amp;sr=redditdev\\u0026amp;title=API Submit Test\\u0026amp;r=redditdev\\u0026amp;api_type=json HTTP/1.1\\n cookie: blah\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm using the apigee Reddit console attempting to get a simple test submit working. All required fields are filled in. I used the api/login to obtain a uh and cookie. I copied the uh and added it to the query. I've added the cookie returned to the header as both cookie and reddit_session. I've set api_type to json and left that off. As a last ditch I tried adding user and passwd to the query, again failure. \\n\\nBased on the API wiki in github(including talklittle) I thought this sufficient, but I'm obviously missing something.  No matter what I try I always get a response that user is required and to please login. Here's an example request, I'm at a loss:\\n\\n    POST /api/submit?uh=blah\\u0026text=Test of API Submit on apigee\\u0026kind=self\\u0026sr=redditdev\\u0026title=API Submit Test\\u0026r=redditdev\\u0026api_type=json HTTP/1.1\\n     cookie: blah\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"r81dl\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"James_GAF\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/r81dl/api_submit_cannot_seem_to_get_it_working/\", \"locked\": false, \"name\": \"t3_r81dl\", \"created\": 1332425704.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/r81dl/api_submit_cannot_seem_to_get_it_working/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"API Submit - cannot seem to get it working\", \"created_utc\": 1332396904.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi everyone.\\nI\\u0026#39;m trying to login but it gives an error saying rate limit exceeded try again in 5 hours.\\nI wasnt flooding the api. i just did it once!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi everyone.\\nI'm trying to login but it gives an error saying rate limit exceeded try again in 5 hours.\\nI wasnt flooding the api. i just did it once!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"qoyf3\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"theherok\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/qoyf3/rate_limit_exceeded_when_trying_to_login/\", \"locked\": false, \"name\": \"t3_qoyf3\", \"created\": 1331335694.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/qoyf3/rate_limit_exceeded_when_trying_to_login/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Rate limit exceeded when trying to login\", \"created_utc\": 1331306894.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Ehow do I get the top post of the week in a given subreddit?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso is there a way to do the something with comments?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"how do I get the top post of the week in a given subreddit?\\n\\nAlso is there a way to do the something with comments?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"qg8mf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"aagavin\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/qg8mf/reddit_python_api_question/\", \"locked\": false, \"name\": \"t3_qg8mf\", \"created\": 1330834134.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/qg8mf/reddit_python_api_question/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Python API question\", \"created_utc\": 1330805334.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am setting up a reddit clone and I am trying to figure out how I can completely delete a subreddit. I have banned the subreddit, removed all the posts from it, and even tried going into the database and deleting its entries but the link still appears in the top bar when you go to the site, regardless of whether or not you are logged in.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI thought it might have just been cached but it has been more than 24 hours now and it still appears. I also ran \\u0026quot;update_reddits.sh\\u0026quot; and that did not change anything. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have coding experience, but I\\u0026#39;m new to web apps and databases, so any help is greatly appreciated.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am setting up a reddit clone and I am trying to figure out how I can completely delete a subreddit. I have banned the subreddit, removed all the posts from it, and even tried going into the database and deleting its entries but the link still appears in the top bar when you go to the site, regardless of whether or not you are logged in.\\n\\nI thought it might have just been cached but it has been more than 24 hours now and it still appears. I also ran \\\"update_reddits.sh\\\" and that did not change anything. \\n\\nI have coding experience, but I'm new to web apps and databases, so any help is greatly appreciated.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"pzd8l\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ppplusplus\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/pzd8l/how_can_i_delete_a_subreddit_and_remove_it_from/\", \"locked\": false, \"name\": \"t3_pzd8l\", \"created\": 1329865670.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/pzd8l/how_can_i_delete_a_subreddit_and_remove_it_from/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How can I delete a subreddit and remove it from the top bar?\", \"created_utc\": 1329836870.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been looking through the reddit API on reddit, but I haven\\u0026#39;t found a way to get a list of \\u003Cem\\u003Eall\\u003C/em\\u003E of the posts (just title, date, etc) from a subreddit. How can I do that? Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been looking through the reddit API on reddit, but I haven't found a way to get a list of *all* of the posts (just title, date, etc) from a subreddit. How can I do that? Thanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"pz4xj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"cornmountain\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/pz4xj/how_can_i_get_a_list_of_all_posts_in_a_subreddit/\", \"locked\": false, \"name\": \"t3_pz4xj\", \"created\": 1329845008.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/pz4xj/how_can_i_get_a_list_of_all_posts_in_a_subreddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How can I get a list of all posts in a subreddit?\", \"created_utc\": 1329816208.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am trying to submit a story via the api (at /api/submit) - currently i have the reddit_session cookie, uh, title, text, sr, and kind in the request. However, when I get the response back, it is giving me a captcha error:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E{\\u0026quot;jquery\\u0026quot;: [[0, 1, \\u0026quot;call\\u0026quot;, [\\u0026quot;body\\u0026quot;]], [1, 2, \\u0026quot;attr\\u0026quot;, \\u0026quot;find\\u0026quot;], [2, 3, \\u0026quot;call\\u0026quot;, [\\u0026quot;.status\\u0026quot;]], [3, 4, \\u0026quot;attr\\u0026quot;, \\u0026quot;hide\\u0026quot;], [4, 5, \\u0026quot;call\\u0026quot;, []], [5, 6, \\u0026quot;attr\\u0026quot;, \\u0026quot;html\\u0026quot;], [6, 7, \\u0026quot;call\\u0026quot;, [\\u0026quot;\\u0026quot;]], [7, 8, \\u0026quot;attr\\u0026quot;, \\u0026quot;end\\u0026quot;], [8, 9, \\u0026quot;call\\u0026quot;, []], [1, 10, \\u0026quot;attr\\u0026quot;, \\u0026quot;captcha\\u0026quot;], [10, 11, \\u0026quot;call\\u0026quot;, [\\u0026quot;vXuRjliIHD1LsA8IexwjuYjuxT1ZZ0rv\\u0026quot;]], [1, 12, \\u0026quot;attr\\u0026quot;, \\u0026quot;find\\u0026quot;], [12, 13, \\u0026quot;call\\u0026quot;, [\\u0026quot;.error.BAD_CAPTCHA.field-captcha\\u0026quot;]], [13, 14, \\u0026quot;attr\\u0026quot;, \\u0026quot;show\\u0026quot;], [14, 15, \\u0026quot;call\\u0026quot;, []], [15, 16, \\u0026quot;attr\\u0026quot;, \\u0026quot;text\\u0026quot;], [16, 17, \\u0026quot;call\\u0026quot;, [\\u0026quot;care to try these again?\\u0026quot;]], [17, 18, \\u0026quot;attr\\u0026quot;, \\u0026quot;end\\u0026quot;], [18, 19, \\u0026quot;call\\u0026quot;, []]]}\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe api docs don\\u0026#39;t mention anything about needing a captcha for api/submit, so I went to look at the API code (\\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/controllers/api.py\\\"\\u003Ehttps://github.com/reddit/reddit/blob/master/r2/r2/controllers/api.py\\u003C/a\\u003E) - I could not find any place under Post_submit that has a captcha.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETrying to figure out what I\\u0026#39;m doing wrong, and if I do need a captcha, how would I go about implementing this.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am trying to submit a story via the api (at /api/submit) - currently i have the reddit_session cookie, uh, title, text, sr, and kind in the request. However, when I get the response back, it is giving me a captcha error:\\n\\n{\\\"jquery\\\": [[0, 1, \\\"call\\\", [\\\"body\\\"]], [1, 2, \\\"attr\\\", \\\"find\\\"], [2, 3, \\\"call\\\", [\\\".status\\\"]], [3, 4, \\\"attr\\\", \\\"hide\\\"], [4, 5, \\\"call\\\", []], [5, 6, \\\"attr\\\", \\\"html\\\"], [6, 7, \\\"call\\\", [\\\"\\\"]], [7, 8, \\\"attr\\\", \\\"end\\\"], [8, 9, \\\"call\\\", []], [1, 10, \\\"attr\\\", \\\"captcha\\\"], [10, 11, \\\"call\\\", [\\\"vXuRjliIHD1LsA8IexwjuYjuxT1ZZ0rv\\\"]], [1, 12, \\\"attr\\\", \\\"find\\\"], [12, 13, \\\"call\\\", [\\\".error.BAD_CAPTCHA.field-captcha\\\"]], [13, 14, \\\"attr\\\", \\\"show\\\"], [14, 15, \\\"call\\\", []], [15, 16, \\\"attr\\\", \\\"text\\\"], [16, 17, \\\"call\\\", [\\\"care to try these again?\\\"]], [17, 18, \\\"attr\\\", \\\"end\\\"], [18, 19, \\\"call\\\", []]]}\\n\\nThe api docs don't mention anything about needing a captcha for api/submit, so I went to look at the API code (https://github.com/reddit/reddit/blob/master/r2/r2/controllers/api.py) - I could not find any place under Post_submit that has a captcha.\\n\\nTrying to figure out what I'm doing wrong, and if I do need a captcha, how would I go about implementing this.\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"oy3ie\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"codechinchilla\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/oy3ie/captcha_error_on_apisubmit/\", \"locked\": false, \"name\": \"t3_oy3ie\", \"created\": 1327640243.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/oy3ie/captcha_error_on_apisubmit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Captcha error on api/submit\", \"created_utc\": 1327611443.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m working on a bot to help with some of the common moderation tasks, but it seems like the API data doesn\\u0026#39;t include most or all of the moderator-restricted data that can be viewed through the site itself, like:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003Enumber of reports on a comment/submission\\u003C/li\\u003E\\n\\u003Cli\\u003Ethe moderator that approved a submission (if anyone)\\u003C/li\\u003E\\n\\u003Cli\\u003Ethe moderator that removed a submission (if anyone)\\u003C/li\\u003E\\n\\u003Cli\\u003Ewhether a submission/comment on the spam page was posted by a shadow-banned user\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EIs there any way to access these?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm working on a bot to help with some of the common moderation tasks, but it seems like the API data doesn't include most or all of the moderator-restricted data that can be viewed through the site itself, like:\\n\\n* number of reports on a comment/submission\\n* the moderator that approved a submission (if anyone)\\n* the moderator that removed a submission (if anyone)\\n* whether a submission/comment on the spam page was posted by a shadow-banned user\\n\\nIs there any way to access these?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"o4op9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Deimorz\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/o4op9/accessing_moderatorrestricted_data_via_api/\", \"locked\": false, \"name\": \"t3_o4op9\", \"created\": 1325837970.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/o4op9/accessing_moderatorrestricted_data_via_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Accessing moderator-restricted data via API\", \"created_utc\": 1325809170.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ELately I\\u0026#39;ve encountered a bug in a previously reliable python script I was using to update the flair for \\u003Ca href=\\\"/r/sports\\\"\\u003E/r/sports\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI find that if I log in and then make a call to /api/me.json no data is returned.  If I log in via the browser and access \\u003Ca href=\\\"http://www.reddit.com/api/me.json\\\"\\u003Ehttp://www.reddit.com/api/me.json\\u003C/a\\u003E in the browser it loads all of the data fine.  But my python client fails.  Without getting the modhash value from me.json I\\u0026#39;m unable to make subsequent calls to the api.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs anyone else having similar problems?  Is it possible my client has been banned or something?  I\\u0026#39;m essentially using this client: \\u003Ca href=\\\"https://github.com/logan/reddit-hacks/blob/master/redditclient.py\\\"\\u003Ehttps://github.com/logan/reddit-hacks/blob/master/redditclient.py\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Lately I've encountered a bug in a previously reliable python script I was using to update the flair for /r/sports\\n\\nI find that if I log in and then make a call to /api/me.json no data is returned.  If I log in via the browser and access http://www.reddit.com/api/me.json in the browser it loads all of the data fine.  But my python client fails.  Without getting the modhash value from me.json I'm unable to make subsequent calls to the api.\\n\\nIs anyone else having similar problems?  Is it possible my client has been banned or something?  I'm essentially using this client: https://github.com/logan/reddit-hacks/blob/master/redditclient.py\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ncbok\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"theycallmemorty\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ncbok/problem_calling_apimejson/\", \"locked\": false, \"name\": \"t3_ncbok\", \"created\": 1323895169.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ncbok/problem_calling_apimejson/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Problem calling api/me.json\", \"created_utc\": 1323866369.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"groups.google.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"mev1j\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"TedFromTheFuture\", \"media\": null, \"score\": 8, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/mev1j/reddit_recommendor_google_group_to_coordinate/\", \"locked\": false, \"name\": \"t3_mev1j\", \"created\": 1321508304.0, \"url\": \"http://groups.google.com/group/rrecommender\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Recommendor: Google Group to coordinate progress\", \"created_utc\": 1321479504.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 8}}], \"after\": \"t3_mev1j\", \"before\": null}}"
        },
        "headers": {
          "CF-RAY": "2b59b9d7c8e020c6-LAX",
          "Connection": "keep-alive",
          "Content-Type": "application/json; charset=UTF-8",
          "Date": "Sun, 19 Jun 2016 20:28:11 GMT",
          "Server": "cloudflare-nginx",
          "Strict-Transport-Security": "max-age=15552000; includeSubDomains; preload",
          "Transfer-Encoding": "chunked",
          "Vary": "accept-encoding",
          "X-Moose": "majestic",
          "cache-control": "private, s-maxage=0, max-age=0, must-revalidate",
          "expires": "-1",
          "x-content-type-options": "nosniff",
          "x-frame-options": "SAMEORIGIN",
          "x-ratelimit-remaining": "578.0",
          "x-ratelimit-reset": "110",
          "x-ratelimit-used": "22",
          "x-reddit-tracking": "https://pixel.redditmedia.com/pixel/of_destiny.png?v=ERdqHYWs2KoIywLEyZLKtXPvyAXggHhUR1ccGlLe8YMZ69Fdu2PaoCzUCI5NOzBXH5wxue%2Bcu2O6vd815mF7gl4OHoF5zVRS",
          "x-ua-compatible": "IE=edge",
          "x-xss-protection": "1; mode=block"
        },
        "status": {
          "code": 200,
          "message": "OK"
        },
        "url": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_2vwj9j&raw_json=1"
      }
    },
    {
      "recorded_at": "2016-06-19T20:28:13",
      "request": {
        "body": {
          "encoding": "utf-8",
          "string": ""
        },
        "headers": {
          "Accept": "*/*",
          "Accept-Encoding": "identity",
          "Authorization": "bearer YE4ESXZ9lTjIP0ukcopwXM6361g",
          "Connection": "keep-alive",
          "Cookie": "loid=SdAZIdDQoUDzjNqfRk; loidcreated=2016-06-19T20%3A27%3A59.507Z; __cfduid=da3dbedd6bfa050f4f12fea14395b830f1466368079",
          "User-Agent": "prawtools/0.19 PRAW/4.0.0b4 prawcore/0.0.8"
        },
        "method": "GET",
        "uri": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_mev1j&raw_json=1"
      },
      "response": {
        "body": {
          "encoding": "UTF-8",
          "string": "{\"kind\": \"Listing\", \"data\": {\"modhash\": null, \"children\": [{\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EExample 1: \\u003Ca href=\\\"http://www.reddit.com/r/all/new/.json\\\"\\u003Ehttp://www.reddit.com/r/all/new/.json\\u003C/a\\u003E\\nExample 2: \\u003Ca href=\\\"http://www.reddit.com/r/all/new/.json?jsonp=callback\\\"\\u003Ehttp://www.reddit.com/r/all/new/.json?jsonp=callback\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs it a bug? How to fetch the latest posts in a jsonp format?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Example 1: http://www.reddit.com/r/all/new/.json\\nExample 2: http://www.reddit.com/r/all/new/.json?jsonp=callback\\n\\nIs it a bug? How to fetch the latest posts in a jsonp format?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"lgo4r\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"MeProtozoan\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/lgo4r/why_is_there_a_difference_between_the_posts_shown/\", \"locked\": false, \"name\": \"t3_lgo4r\", \"created\": 1318998668.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/lgo4r/why_is_there_a_difference_between_the_posts_shown/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why is there a difference between the posts shown on /r/all/new/.json and /r/all/new/.json?jsonp=callback\", \"created_utc\": 1318969868.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey guys. I\\u0026#39;ve installed reddit for a client of mine and he needs a few changes done. I\\u0026#39;ve never worked with any Python apps before (I\\u0026#39;m mostly a Rails guy), and I\\u0026#39;m just so lost in this framework. \\nThe first thing I\\u0026#39;m having a problem with is that I can\\u0026#39;t get the app to serve my modified stylesheets (reddit.css). Even after I restart the entire server (no idea how to stop/start a Python app), it still serves the old file. There must be an application cache somewhere?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThat\\u0026#39;s not really the main problem though, as I\\u0026#39;m sure I can figure that one out. He needs the app to scrub any incoming posts/comments, etc for certain URLs that we will then need to append information onto. Can anyone help me find where I should be getting this done? In Rails you would generally do this in the data model, with a callback that runs the method before the object gets saved to the database. I have no idea where to even start with this, and any help would be greatly greatly appreciated.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey guys. I've installed reddit for a client of mine and he needs a few changes done. I've never worked with any Python apps before (I'm mostly a Rails guy), and I'm just so lost in this framework. \\nThe first thing I'm having a problem with is that I can't get the app to serve my modified stylesheets (reddit.css). Even after I restart the entire server (no idea how to stop/start a Python app), it still serves the old file. There must be an application cache somewhere?\\n    \\nThat's not really the main problem though, as I'm sure I can figure that one out. He needs the app to scrub any incoming posts/comments, etc for certain URLs that we will then need to append information onto. Can anyone help me find where I should be getting this done? In Rails you would generally do this in the data model, with a callback that runs the method before the object gets saved to the database. I have no idea where to even start with this, and any help would be greatly greatly appreciated.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"la2vf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Chairmonkey\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/la2vf/never_worked_with_redditpython_apps_before_could/\", \"locked\": false, \"name\": \"t3_la2vf\", \"created\": 1318486530.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/la2vf/never_worked_with_redditpython_apps_before_could/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Never worked with reddit/Python apps before, could use some guidance from anyone who's willing\", \"created_utc\": 1318457730.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EBelow is my stunning summary of how I access youtube links, since just clicking on them won\\u0026#39;t work:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://i.imgur.com/m02ra.png\\\"\\u003EPart 1\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://imgur.com/cTIfV.png\\\"\\u003EPart 2\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESorry for the horrible visual representation. Anyone have a clue what\\u0026#39;s wrong? Is this the correct subreddit for this by the way?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Below is my stunning summary of how I access youtube links, since just clicking on them won't work:\\n\\n[Part 1](http://i.imgur.com/m02ra.png)\\n\\n[Part 2](http://imgur.com/cTIfV.png)\\n\\nSorry for the horrible visual representation. Anyone have a clue what's wrong? Is this the correct subreddit for this by the way?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"l3z1o\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Yggdrazzil\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/l3z1o/i_have_a_problem_watching_youtube_links_via_reddit/\", \"locked\": false, \"name\": \"t3_l3z1o\", \"created\": 1318007400.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/l3z1o/i_have_a_problem_watching_youtube_links_via_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I have a problem watching youtube links via reddit.\", \"created_utc\": 1317978600.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have been programming in python for a while now and I am familiar with some web frameworks (django specifically).  I want to get involved in an open source project and Reddit seemed like an amazing fit (I wanted to give back to it too!) I have the system up and running and I am starting to look into the code, but I was wondering what might be a good first problem to tackle and get my toes wet.  I didn\\u0026#39;t see anything under the issue tracker that stood out in particular.  Any suggests on a good bug/feature to work on?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI also have some experience with Markdown.  If there\\u0026#39;s anything that would be good to document on the wiki I would be down for doing that too.  Cheers!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have been programming in python for a while now and I am familiar with some web frameworks (django specifically).  I want to get involved in an open source project and Reddit seemed like an amazing fit (I wanted to give back to it too!) I have the system up and running and I am starting to look into the code, but I was wondering what might be a good first problem to tackle and get my toes wet.  I didn't see anything under the issue tracker that stood out in particular.  Any suggests on a good bug/feature to work on?\\n\\nI also have some experience with Markdown.  If there's anything that would be good to document on the wiki I would be down for doing that too.  Cheers!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"knkpl\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"ciferkey\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/knkpl/getting_started_with_contributing_good_first/\", \"locked\": false, \"name\": \"t3_knkpl\", \"created\": 1316694849.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/knkpl/getting_started_with_contributing_good_first/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Getting started with contributing - Good first problem to tackle.\", \"created_utc\": 1316666049.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI just setup a new local install of reddit but I have no idea how to create an admin account. How do I do this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I just setup a new local install of reddit but I have no idea how to create an admin account. How do I do this?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"kagvo\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"mozillalives\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/kagvo/new_install_how_do_i_get_an_admin_account/\", \"locked\": false, \"name\": \"t3_kagvo\", \"created\": 1315631477.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/kagvo/new_install_how_do_i_get_an_admin_account/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"New install - how do I get an admin account?\", \"created_utc\": 1315602677.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have a weird problem when trying to install reddit. It all began with the \\u0026quot;keyspace reddit not found\\u0026quot; error message (more details \\u003Ca href=\\\"https://github.com/reddit/reddit/issues/182#issuecomment-1755430\\\"\\u003Ehere\\u003C/a\\u003E). I was advised to create a new keyspace using cassandra-cli. However when I\\u0026#39;ve tried to connect using the \\u0026quot;connect localhost/9160;\\u0026quot; I was greeted by this:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cem\\u003ELogin failure. Did you specify \\u0026#39;keyspace\\u0026#39;, \\u0026#39;username\\u0026#39; and \\u0026#39;password\\u0026#39;?\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECould anyone lend me a hand with this? I\\u0026#39;m not really knowledgeable about Cassandra so I have no clue about the problems I might encounter and their solution (I\\u0026#39;m a PHP type of guy :P).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso every time I start the \\u0026quot;whole shebang\\u0026quot; (i.e. Cassandra and everything that goes with it) I\\u0026#39;m unable to shut it down. The program doesn\\u0026#39;t seem to react to commands such as \\u0026quot;service cassandra stop\\u0026quot; or \\u0026quot;/etc/init.d/cassandra stop\\u0026quot; at all. Therefore first I had to rename /usr/bin/svscanboot, shut it down, then proceed with shutting down the rest of the stuff required by reddit that doesn\\u0026#39;t shut down by shutting down svscanboot. It goes witthout saying that PostgreSQL isn\\u0026#39;t affected by this at all. Any ideas about this and/or suggestions for a possible fix?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi!\\n\\nI have a weird problem when trying to install reddit. It all began with the \\\"keyspace reddit not found\\\" error message (more details [here](https://github.com/reddit/reddit/issues/182#issuecomment-1755430)). I was advised to create a new keyspace using cassandra-cli. However when I've tried to connect using the \\\"connect localhost/9160;\\\" I was greeted by this:\\n\\n*Login failure. Did you specify 'keyspace', 'username' and 'password'?*\\n\\nCould anyone lend me a hand with this? I'm not really knowledgeable about Cassandra so I have no clue about the problems I might encounter and their solution (I'm a PHP type of guy :P).\\n\\nAlso every time I start the \\\"whole shebang\\\" (i.e. Cassandra and everything that goes with it) I'm unable to shut it down. The program doesn't seem to react to commands such as \\\"service cassandra stop\\\" or \\\"/etc/init.d/cassandra stop\\\" at all. Therefore first I had to rename /usr/bin/svscanboot, shut it down, then proceed with shutting down the rest of the stuff required by reddit that doesn't shut down by shutting down svscanboot. It goes witthout saying that PostgreSQL isn't affected by this at all. Any ideas about this and/or suggestions for a possible fix?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"jdztk\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"CoolKoon\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/jdztk/weird_issues_when_trying_to_install_reddit/\", \"locked\": false, \"name\": \"t3_jdztk\", \"created\": 1312959777.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/jdztk/weird_issues_when_trying_to_install_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Weird issues when trying to install reddit\", \"created_utc\": 1312930977.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EA little over a month ago, \\u003Ca href=\\\"http://www.reddit.com/r/gaming/comments/hzp98/official_whats_rgaming_playing_statistics/\\\"\\u003EI set up a bot in /r/gaming that uses the API and processes PMs that people send to it\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENow I want to start \\u0026quot;expiring\\u0026quot; the data from particular users if they haven\\u0026#39;t sent in an update for a while, but I\\u0026#39;d like to send the user a PM when their submission expires. Sending a PM from the bot\\u0026#39;s account forces a captcha, so that doesn\\u0026#39;t seem workable.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWould it be acceptable to send the PMs from my own account, or will a certain volume of them get me flagged somehow, or start requiring a captcha after some point? The bot currently checks for new messages once every 5 minutes, so I\\u0026#39;ll probably just process a maximum of one expiration on each run, which means the fastest I\\u0026#39;ll ever be sending these PMs out is one every 5 minutes.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs that a reasonable approach, or is there some other way I could make it work through the bot account?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"A little over a month ago, [I set up a bot in /r/gaming that uses the API and processes PMs that people send to it](http://www.reddit.com/r/gaming/comments/hzp98/official_whats_rgaming_playing_statistics/).\\n\\nNow I want to start \\\"expiring\\\" the data from particular users if they haven't sent in an update for a while, but I'd like to send the user a PM when their submission expires. Sending a PM from the bot's account forces a captcha, so that doesn't seem workable.\\n\\nWould it be acceptable to send the PMs from my own account, or will a certain volume of them get me flagged somehow, or start requiring a captcha after some point? The bot currently checks for new messages once every 5 minutes, so I'll probably just process a maximum of one expiration on each run, which means the fastest I'll ever be sending these PMs out is one every 5 minutes.\\n\\nIs that a reasonable approach, or is there some other way I could make it work through the bot account?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"istty\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Deimorz\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/istty/sending_large_numbers_of_pms_through_api/\", \"locked\": false, \"name\": \"t3_istty\", \"created\": 1311027746.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/istty/sending_large_numbers_of_pms_through_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Sending large numbers of PMs through API\", \"created_utc\": 1310998946.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ENot 100% sure where to post this..\\u003Cbr/\\u003E\\nWhen you create a subreddit you are allowed to create a CNAME and link to it from your own domain name. I did this and it seems that the only thing that happens is that reddit\\u0026#39;s servers serve an iframe linking to a trivially modified version of the subreddit itself. One click and you are off the domain entirely. Am I missing something or is that working as intended?\\u003Cbr/\\u003E\\n\\u003Ca href=\\\"http://reddit.sixcorners.info/\\\"\\u003Ehttp://reddit.sixcorners.info/\\u003C/a\\u003E  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: I\\u0026#39;m stupid.. I should have said frameset and frame instead of iframe..\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Not 100% sure where to post this..  \\nWhen you create a subreddit you are allowed to create a CNAME and link to it from your own domain name. I did this and it seems that the only thing that happens is that reddit's servers serve an iframe linking to a trivially modified version of the subreddit itself. One click and you are off the domain entirely. Am I missing something or is that working as intended?  \\nhttp://reddit.sixcorners.info/  \\n\\nEdit: I'm stupid.. I should have said frameset and frame instead of iframe..\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"i4wkk\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/i4wkk/reddits_custom_domain_name_thing_its_just_an/\", \"locked\": false, \"name\": \"t3_i4wkk\", \"created\": 1308653634.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/i4wkk/reddits_custom_domain_name_thing_its_just_an/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit's custom domain name thing: it's just an iframe?\", \"created_utc\": 1308624834.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI was installing the prerequisites, and after I installed cassandra, my mouse started slowing down, and then my computer froze.  So I hard-restarted it.  The same thing happened again, but this time after I logged in from the login screen for Ubuntu.  I kept trying, and sometimes it happened during the login screen, sometimes after I login but before things loaded, but at no point was my system usable.  I couldn\\u0026#39;t even get to a virtual terminal.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI had to reboot with my Ubuntu CD and go to /media/whatever/etc/init.d/cassandra and delete it.  After that, it boots just fine.  And yeah, I installed Ubuntu \\u003Cem\\u003Eyesterday\\u003C/em\\u003E.  This was a fresh system.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs this normal?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: I guess it was sorta like a forkbomb...like all my memory was used up.  I couldn\\u0026#39;t do anything.  Also, I should note that I\\u0026#39;m kinda a linux noob-intermediate, and not at all a programmer.  I just wanted to see how reddit worked, really.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I was installing the prerequisites, and after I installed cassandra, my mouse started slowing down, and then my computer froze.  So I hard-restarted it.  The same thing happened again, but this time after I logged in from the login screen for Ubuntu.  I kept trying, and sometimes it happened during the login screen, sometimes after I login but before things loaded, but at no point was my system usable.  I couldn't even get to a virtual terminal.\\n\\nI had to reboot with my Ubuntu CD and go to /media/whatever/etc/init.d/cassandra and delete it.  After that, it boots just fine.  And yeah, I installed Ubuntu *yesterday*.  This was a fresh system.\\n\\nIs this normal?\\n\\nEDIT: I guess it was sorta like a forkbomb...like all my memory was used up.  I couldn't do anything.  Also, I should note that I'm kinda a linux noob-intermediate, and not at all a programmer.  I just wanted to see how reddit worked, really.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"hmad8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"sje46\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/hmad8/i_tried_installing_reddit_on_my_laptopbut_after/\", \"locked\": false, \"name\": \"t3_hmad8\", \"created\": 1306627376.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/hmad8/i_tried_installing_reddit_on_my_laptopbut_after/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I tried installing Reddit on my laptop...but after installing Cassandra, I couldn't use linux at all.  Is this normal?\", \"created_utc\": 1306598576.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIs there any way to get a user\\u0026#39;s total upvote/downvote count through the api? \\u003Ca href=\\\"http://www.reddit.com/user/adotout.api\\\"\\u003Ehttp://www.reddit.com/user/adotout.api\\u003C/a\\u003E just seems to return my posts, which isn\\u0026#39;t what I\\u0026#39;m looking for. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Is there any way to get a user's total upvote/downvote count through the api? http://www.reddit.com/user/adotout.api just seems to return my posts, which isn't what I'm looking for. \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"fjr2p\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"adotout\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/fjr2p/users_total_upvotedownvote_api/\", \"locked\": false, \"name\": \"t3_fjr2p\", \"created\": 1297493351.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/fjr2p/users_total_upvotedownvote_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"User's Total upvote/downvote API\", \"created_utc\": 1297464551.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo I thought it would be neat to determine some trends on Reddit through the use of concept extraction from titles and comments, comment volume, and up/down votes.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOf course, in order to do that, I would need access to data. I could crawl say, the politics or worldnews subreddits, but perhaps there is a sample database I could use instead?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf there isn\\u0026#39;t a sample database available, are there specific rules (other than respecting robots.txt) to crawling reddit?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So I thought it would be neat to determine some trends on Reddit through the use of concept extraction from titles and comments, comment volume, and up/down votes.\\n\\nOf course, in order to do that, I would need access to data. I could crawl say, the politics or worldnews subreddits, but perhaps there is a sample database I could use instead?\\n\\nIf there isn't a sample database available, are there specific rules (other than respecting robots.txt) to crawling reddit?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"fjgtj\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"i_ate_god\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/fjgtj/trending_on_reddit/\", \"locked\": false, \"name\": \"t3_fjgtj\", \"created\": 1297465747.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/fjgtj/trending_on_reddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Trending on Reddit?\", \"created_utc\": 1297436947.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Cstrong\\u003ETL;DR\\u003C/strong\\u003E \\u2013 Getting the following error when attempting to run the application after rebuilding the application from latest source in the VM:\\n    ...\\n      File \\u0026quot;/usr/local/lib/python2.6/dist-packages/pycassa-0.3.0-py2.6.egg/pycassa/columnfamily.py\\u0026quot;, line 6, in \\u0026lt;module\\u0026gt;\\n        from cassandra.ttypes import Column, ColumnOrSuperColumn, ColumnParent, \\\\\\n    ImportError: cannot import name Mutation\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAppears to be related to the pycassa and/or possibly the thrift python packages, but I don\\u0026#39;t know where to go from here.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EBefore I get into the details I feel it necessary to point out that while I\\u0026#39;m an experienced programmer, I\\u0026#39;ve done very little in the way of Python development (basically, I\\u0026#39;m pretty much a newb in that regard). Also, I\\u0026#39;m only slightly more experienced with regard to Linux. I\\u0026#39;m reasonably capable on the command-line but it\\u0026#39;s not my primary OS and I\\u0026#39;ll be the first to admit that I\\u0026#39;m no guru. Please forgive me if I erred in some way in attempting to get this to work. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m trying to setup a local Reddit dev environment partly as an exercise and partly to see if I might in some small way contribute to the community. To this end I reasoned that the best place to start would be to download the VM.  Despite having some minor issues with getting the image start VM (VT wasn\\u0026#39;t enabled in the BIOS and the HP BIOS had dumbed down setting labels which didn\\u0026#39;t allow me to quickly identify which settings were in need of toggling) eventually I got it running and everything came up fine. Once the VM booted up I happily found that all the services started up and I was able to access the server from a browser in the host.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOf course, anytime you want to begin coding, or even just looking at the code, you will want to work off the latest version. So I attempted to update the sources under the ~/reddit folder. This is where things went awry.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAt first I tried updating it by pulling in the latest source and did manage to get it to build, but I ran into the error described above and then tried to completely replace that folder by removing it and then following the instructions in the \\n\\u003Ca href=\\\"http://code.reddit.com/wiki/RedditStartToFinish#CheckingoutGIT\\\"\\u003EStart to Finish\\u003C/a\\u003E guide to pull down the latest code.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAfter many false starts, I finally managed to get it to build, but every attempt to get it to run has failed with the above error. I\\u0026#39;m guessing that there is some basic incompatibility with the pycassa and thrift packages installed, but I\\u0026#39;ve not determined how to resolve this and Google isn\\u0026#39;t providing many clues on this one. I did try installing updated versions of pycassa (modified the ~/reddit/r2/setup.py to reference a slightly newer version) but I determining that was a dead end and restored to 0.3.0 version. I then followed \\u003Ca href=\\\"http://pycassa.github.com/pycassa/installation.html\\\"\\u003Ethese instructions\\u003C/a\\u003E for updating the Thrift python package. Updating thrift python packages also had no effect.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf anyone has any ideas as to what has gone wrong or if you require more information please let me know.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFYI: I did run into issues in trying to build latest sources whereby it was necessary to install the Cython package and update the BeautifulSoup package. I\\u0026#39;m currently trying to verify these and any other steps which were taken prior to getting the error above and I will post those once the refreshed VM decides to cooperate (PostgreSQL didn\\u0026#39;t start properly for some reason and it\\u0026#39;s taking forever to shutdown). I had every intention of posting a step-by-step guide of updating the VM when I could not find one, but until I can get it to work I can\\u0026#39;t guarantee the steps I\\u0026#39;ve taken are valid. Of course if a guide to getting the VM up-to-date exists, please feel free to point it out!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: formatting\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT:\\u003C/strong\\u003E Given up on using the existing VM. Please read my comment below regarding getting a fresh install to work.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"**TL;DR** \\u2013 Getting the following error when attempting to run the application after rebuilding the application from latest source in the VM:\\n    ...\\n      File \\\"/usr/local/lib/python2.6/dist-packages/pycassa-0.3.0-py2.6.egg/pycassa/columnfamily.py\\\", line 6, in \\u003Cmodule\\u003E\\n        from cassandra.ttypes import Column, ColumnOrSuperColumn, ColumnParent, \\\\\\n    ImportError: cannot import name Mutation\\n\\nAppears to be related to the pycassa and/or possibly the thrift python packages, but I don't know where to go from here.\\n\\n-----------------------------------------\\n\\nBefore I get into the details I feel it necessary to point out that while I'm an experienced programmer, I've done very little in the way of Python development (basically, I'm pretty much a newb in that regard). Also, I'm only slightly more experienced with regard to Linux. I'm reasonably capable on the command-line but it's not my primary OS and I'll be the first to admit that I'm no guru. Please forgive me if I erred in some way in attempting to get this to work. \\n\\nI'm trying to setup a local Reddit dev environment partly as an exercise and partly to see if I might in some small way contribute to the community. To this end I reasoned that the best place to start would be to download the VM.  Despite having some minor issues with getting the image start VM (VT wasn't enabled in the BIOS and the HP BIOS had dumbed down setting labels which didn't allow me to quickly identify which settings were in need of toggling) eventually I got it running and everything came up fine. Once the VM booted up I happily found that all the services started up and I was able to access the server from a browser in the host.\\n\\nOf course, anytime you want to begin coding, or even just looking at the code, you will want to work off the latest version. So I attempted to update the sources under the ~/reddit folder. This is where things went awry.\\n\\nAt first I tried updating it by pulling in the latest source and did manage to get it to build, but I ran into the error described above and then tried to completely replace that folder by removing it and then following the instructions in the \\n[Start to Finish](http://code.reddit.com/wiki/RedditStartToFinish#CheckingoutGIT) guide to pull down the latest code.\\n\\nAfter many false starts, I finally managed to get it to build, but every attempt to get it to run has failed with the above error. I'm guessing that there is some basic incompatibility with the pycassa and thrift packages installed, but I've not determined how to resolve this and Google isn't providing many clues on this one. I did try installing updated versions of pycassa (modified the ~/reddit/r2/setup.py to reference a slightly newer version) but I determining that was a dead end and restored to 0.3.0 version. I then followed [these instructions](http://pycassa.github.com/pycassa/installation.html) for updating the Thrift python package. Updating thrift python packages also had no effect.\\n\\nIf anyone has any ideas as to what has gone wrong or if you require more information please let me know.\\n\\nFYI: I did run into issues in trying to build latest sources whereby it was necessary to install the Cython package and update the BeautifulSoup package. I'm currently trying to verify these and any other steps which were taken prior to getting the error above and I will post those once the refreshed VM decides to cooperate (PostgreSQL didn't start properly for some reason and it's taking forever to shutdown). I had every intention of posting a step-by-step guide of updating the VM when I could not find one, but until I can get it to work I can't guarantee the steps I've taken are valid. Of course if a guide to getting the VM up-to-date exists, please feel free to point it out!\\n\\nEDIT: formatting\\n\\n**EDIT:** Given up on using the existing VM. Please read my comment below regarding getting a fresh install to work.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"f36q5\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"vogon_poem_lover\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/f36q5/updating_the_vm/\", \"locked\": false, \"name\": \"t3_f36q5\", \"created\": 1295192473.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/f36q5/updating_the_vm/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Updating the VM\", \"created_utc\": 1295163673.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have been playing around with the reddit VMWare image and have been trying to replace the \\u003Ca href=\\\"http://static.reddit.com/reddit.com.header.png\\\"\\u003Ereddit header in the upper left hand corner\\u003C/a\\u003E with my own image. By default it is set to \\u003Ca href=\\\"http://static.reddit.com/reddit.com.header.png\\\"\\u003Ehttp://static.reddit.com/reddit.com.header.png\\u003C/a\\u003E. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve searched for help and found instructions telling me to change the string referencing this location that is found in \\u003Ca href=\\\"http://code.reddit.com/browser/r2/r2/models/subreddit.py\\\"\\u003E/r2/r2/models/subreddit.py\\u003C/a\\u003E on line 721 to be the new location. I can no longer remember where I found the documentation telling me how to do this and I can\\u0026#39;t seem to find it in any searches. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI made this change, and restarted my reddit. Afterward, my reddit would no longer load. It times out every time. Even after undoing the changes I made, the reddit still times out. I have repeated this process once already, so I am fairly certain that this change is what is causing my problem. \\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EFirstly, I believe that there has got to be a better way to apply changes to .py files than rebooting reddit. I would be highly appreciative of anyone who can let me know how to do this. \\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003ESecond, does anyone have any idea how I can recover my reddit from this, ideally without having to start it from scratch? \\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EThird, does anyone know of a working way to change the reddit header image?  \\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EThanks everyone!\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EEDIT: I managed to solve this problem. It turns out that the documentation I had read before was correct \\nand modifying the subreddit.py file will change your header image. It would have helped if I had gotten the \\nURL I was using for the replacement image correct. (I had been copying from a file that had http;//)\\n\\nI would still appreciate anybody would could instruct me as to how to apply the changes I make without \\nhaving to reboot. Thanks again!\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have been playing around with the reddit VMWare image and have been trying to replace the [reddit header in the upper left hand corner](http://static.reddit.com/reddit.com.header.png) with my own image. By default it is set to [http://static.reddit.com/reddit.com.header.png](http://static.reddit.com/reddit.com.header.png). \\n \\nI've searched for help and found instructions telling me to change the string referencing this location that is found in [/r2/r2/models/subreddit.py](http://code.reddit.com/browser/r2/r2/models/subreddit.py) on line 721 to be the new location. I can no longer remember where I found the documentation telling me how to do this and I can't seem to find it in any searches. \\n \\nI made this change, and restarted my reddit. Afterward, my reddit would no longer load. It times out every time. Even after undoing the changes I made, the reddit still times out. I have repeated this process once already, so I am fairly certain that this change is what is causing my problem. \\n \\n* Firstly, I believe that there has got to be a better way to apply changes to .py files than rebooting reddit. I would be highly appreciative of anyone who can let me know how to do this. \\n\\n* Second, does anyone have any idea how I can recover my reddit from this, ideally without having to start it from scratch? \\n\\n* Third, does anyone know of a working way to change the reddit header image?  \\n\\nThanks everyone!\\n\\n    EDIT: I managed to solve this problem. It turns out that the documentation I had read before was correct \\n    and modifying the subreddit.py file will change your header image. It would have helped if I had gotten the \\n    URL I was using for the replacement image correct. (I had been copying from a file that had http;//)\\n\\n    I would still appreciate anybody would could instruct me as to how to apply the changes I make without \\n    having to reboot. Thanks again!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ej9az\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"nubsauce111\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ej9az/reddit_timing_out/\", \"locked\": false, \"name\": \"t3_ej9az\", \"created\": 1291971499.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ej9az/reddit_timing_out/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Timing Out\", \"created_utc\": 1291942699.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI basically want to do the following query:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eselect count(*), avg(votes), avg(comments) from submitted_links\\nwhere user = \\u0026lt;me\\u0026gt;\\ngroup by subreddit\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThen I can display stuff using fancy graphs etc.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI started to implement this by downloading my data to a local db (props on the awesome API, btw) and working on it there, but I realized that this was kind of stupid if Reddit would allow me to build this functionality into the site. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf I made this, would it be something that I could donate to reddit? Otherwise, it\\u0026#39;s probably easier for me to just do it locally.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: I realize reddit doesn\\u0026#39;t have that db layout, I was using it as pseudocode. A more realistic query might be something like:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eselect avg(Ups - Downs) from Thing\\nwhere Thing.Type=\\u0026quot;Link\\u0026quot; AND Thing.ID in (\\n    select distinct Thing_ID from Data\\n    where Data.Key = \\u0026quot;User\\u0026quot; AND Data.Value = \\u0026lt;me\\u0026gt;\\n)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m still downloading the VM, so that might not be exactly right, but you get the gist.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I basically want to do the following query:\\n\\n    select count(*), avg(votes), avg(comments) from submitted_links\\n    where user = \\u003Cme\\u003E\\n    group by subreddit\\n\\nThen I can display stuff using fancy graphs etc.\\n\\nI started to implement this by downloading my data to a local db (props on the awesome API, btw) and working on it there, but I realized that this was kind of stupid if Reddit would allow me to build this functionality into the site. \\n\\nIf I made this, would it be something that I could donate to reddit? Otherwise, it's probably easier for me to just do it locally.\\n\\nEDIT: I realize reddit doesn't have that db layout, I was using it as pseudocode. A more realistic query might be something like:\\n\\n    select avg(Ups - Downs) from Thing\\n    where Thing.Type=\\\"Link\\\" AND Thing.ID in (\\n        select distinct Thing_ID from Data\\n        where Data.Key = \\\"User\\\" AND Data.Value = \\u003Cme\\u003E\\n    )\\n\\nI'm still downloading the VM, so that might not be exactly right, but you get the gist.\\n    \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"edi0g\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Xodarap\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/edi0g/aggregate_submission_data/\", \"locked\": false, \"name\": \"t3_edi0g\", \"created\": 1291090845.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/edi0g/aggregate_submission_data/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Aggregate Submission Data\", \"created_utc\": 1291062045.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m going to be interacting with the Reddit API for a little personal project of my own. To help facilitate this project I\\u0026#39;m looking to add some functionality to the \\u003Ca href=\\\"https://github.com/jamescook/RubyRedditAPI\\\"\\u003ERuby Reddit API\\u003C/a\\u003E.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAdding the ability to get my list of unread messages was simple enough, and marking them as read and unread works just fine. That said, I\\u0026#39;d love to be able to mark the messages as read when I fetch the unread list to avoid calling the API for each message I\\u0026#39;ve read.  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI can\\u0026#39;t identify any way to do this in the API. Is there any way that any of you know to set all unread messages as read at the same time as the GET unread messages? Either that or some way to mass mark messages as read. I\\u0026#39;d like to cut back on HTTP traffic.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm going to be interacting with the Reddit API for a little personal project of my own. To help facilitate this project I'm looking to add some functionality to the [Ruby Reddit API](https://github.com/jamescook/RubyRedditAPI).  \\n\\nAdding the ability to get my list of unread messages was simple enough, and marking them as read and unread works just fine. That said, I'd love to be able to mark the messages as read when I fetch the unread list to avoid calling the API for each message I've read.  \\n\\nI can't identify any way to do this in the API. Is there any way that any of you know to set all unread messages as read at the same time as the GET unread messages? Either that or some way to mass mark messages as read. I'd like to cut back on HTTP traffic.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"e6592\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"coderjoe\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/e6592/reddit_api_set_all_unread_messages_as_read/\", \"locked\": false, \"name\": \"t3_e6592\", \"created\": 1289814322.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/e6592/reddit_api_set_all_unread_messages_as_read/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit API: Set all unread messages as read \", \"created_utc\": 1289785522.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"dcrws\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"rmccue\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/dcrws/ireddit_with_better_voting_feedback/\", \"locked\": false, \"name\": \"t3_dcrws\", \"created\": 1284316026.0, \"url\": \"http://github.com/rmccue/iReddit\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"iReddit with better voting feedback\", \"created_utc\": 1284287226.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAt the moment I can get onto the page 192.168.xx.xx as can anyone else on the network. However, the page then redirects to reddit.local. Which isn\\u0026#39;t on our dns. How can I change the redirect to continue to be 192.168.xx.xx/url?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"At the moment I can get onto the page 192.168.xx.xx as can anyone else on the network. However, the page then redirects to reddit.local. Which isn't on our dns. How can I change the redirect to continue to be 192.168.xx.xx/url?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ctv5p\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"slavishmuffin\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ctv5p/ive_got_reddit_set_up_on_virtualbox_but_need_to/\", \"locked\": false, \"name\": \"t3_ctv5p\", \"created\": 1280191482.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ctv5p/ive_got_reddit_set_up_on_virtualbox_but_need_to/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"I've got reddit set up on virtualbox, but need to make it available to my network. How can I get it to redirect to my servers IP?\", \"created_utc\": 1280162682.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ECan this be done, please? This came from ideasforadmins. raldi told me to ask here.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Can this be done, please? This came from ideasforadmins. raldi told me to ask here.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"ctlzs\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"skookybird\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/ctlzs/when_you_click_reply_load_new_comments_to_the/\", \"locked\": false, \"name\": \"t3_ctlzs\", \"created\": 1280129864.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/ctlzs/when_you_click_reply_load_new_comments_to_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"When you click \\u201creply\\u201d, load new comments to the comment you are replying to\", \"created_utc\": 1280101064.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThis has been requested more than a few times... e.g.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/comments/9uicm/could_we_possibly_get_a_way_to_export_all_of_our/\\\"\\u003Ehttp://www.reddit.com/comments/9uicm/could_we_possibly_get_a_way_to_export_all_of_our/\\u003C/a\\u003E\\n\\u003Ca href=\\\"http://www.reddit.com/comments/9z4e1/can_we_make_the_saved_page_searchable/\\\"\\u003Ehttp://www.reddit.com/comments/9z4e1/can_we_make_the_saved_page_searchable/\\u003C/a\\u003E\\n\\u003Ca href=\\\"http://www.reddit.com/comments/a93wu/can_we_please_have_a_way_of_sorting_through_our/\\\"\\u003Ehttp://www.reddit.com/comments/a93wu/can_we_please_have_a_way_of_sorting_through_our/\\u003C/a\\u003E\\n\\u003Ca href=\\\"http://www.reddit.com/comments/ackbz/saving_all_ones_reddit_posts_to_a_html_file/\\\"\\u003Ehttp://www.reddit.com/comments/ackbz/saving_all_ones_reddit_posts_to_a_html_file/\\u003C/a\\u003E\\n\\u003Ca href=\\\"http://www.reddit.com/comments/ak3lp/you_should_be_able_to_arrange_the_order_of_your/\\\"\\u003Ehttp://www.reddit.com/comments/ak3lp/you_should_be_able_to_arrange_the_order_of_your/\\u003C/a\\u003E\\n\\u003Ca href=\\\"http://www.reddit.com/comments/amxb6/a_way_to_categorize_saved_links/\\\"\\u003Ehttp://www.reddit.com/comments/amxb6/a_way_to_categorize_saved_links/\\u003C/a\\u003E\\n\\u003Ca href=\\\"http://www.reddit.com/comments/ca1l0/option_to_organize_saved_links_by_category/\\\"\\u003Ehttp://www.reddit.com/comments/ca1l0/option_to_organize_saved_links_by_category/\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESource is \\u003Ca href=\\\"http://github.com/achea/linkhive\\\"\\u003Ehere\\u003C/a\\u003E.  Currently, you can export your stories to a SQLite or MySQL database, and then \\u0026#39;view\\u0026#39; them by typing in SQL statements into a desktop app.  \\u003Ca href=\\\"http://imgur.com/XIr7k\\\"\\u003EScreenshot\\u003C/a\\u003E.  I know, it\\u0026#39;s a little \\u0026#39;clunky\\u0026#39; at the moment, but it works. :)  It does not have any categorizing/tagging features, which is next on the TODO list, after I\\u0026#39;m satisfied with a design.  Does anyone have any suggestions?  Jimmyr has a \\u003Ca href=\\\"http://www.reddit.com/r/programming/comments/b541x/i_made_an_organizer_for_my_bookmarks_after_3/c0l0oak?context=1\\\"\\u003Eneat idea\\u003C/a\\u003E, but I\\u0026#39;m still looking for more.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso, both the fetch and search programs are untested in Windows, but I\\u0026#39;ve tried to code them to be as cross-platform as possible, so they should work.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEnjoy!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"This has been requested more than a few times... e.g.\\n\\nhttp://www.reddit.com/comments/9uicm/could_we_possibly_get_a_way_to_export_all_of_our/\\nhttp://www.reddit.com/comments/9z4e1/can_we_make_the_saved_page_searchable/\\nhttp://www.reddit.com/comments/a93wu/can_we_please_have_a_way_of_sorting_through_our/\\nhttp://www.reddit.com/comments/ackbz/saving_all_ones_reddit_posts_to_a_html_file/\\nhttp://www.reddit.com/comments/ak3lp/you_should_be_able_to_arrange_the_order_of_your/\\nhttp://www.reddit.com/comments/amxb6/a_way_to_categorize_saved_links/\\nhttp://www.reddit.com/comments/ca1l0/option_to_organize_saved_links_by_category/\\n\\nSource is [here](http://github.com/achea/linkhive).  Currently, you can export your stories to a SQLite or MySQL database, and then 'view' them by typing in SQL statements into a desktop app.  [Screenshot](http://imgur.com/XIr7k).  I know, it's a little 'clunky' at the moment, but it works. :)  It does not have any categorizing/tagging features, which is next on the TODO list, after I'm satisfied with a design.  Does anyone have any suggestions?  Jimmyr has a [neat idea](http://www.reddit.com/r/programming/comments/b541x/i_made_an_organizer_for_my_bookmarks_after_3/c0l0oak?context=1), but I'm still looking for more.\\n\\nAlso, both the fetch and search programs are untested in Windows, but I've tried to code them to be as cross-platform as possible, so they should work.\\n\\nEnjoy!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"caspu\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Gambit89\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/caspu/a_tool_to_export_savedliked_stories_to_a_database/\", \"locked\": false, \"name\": \"t3_caspu\", \"created\": 1275546265.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/caspu/a_tool_to_export_savedliked_stories_to_a_database/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"A tool to export saved/liked stories to a database\", \"created_utc\": 1275517465.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESearching via domain leaves me in a strange predicament. I\\u0026#39;m doing this from both my account and via a server authenticated (via api). I\\u0026#39;ll use imgur.com as the example: \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/domain/imgur.com\\\"\\u003Ehttp://www.reddit.com/domain/imgur.com\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENotice how everything is 2 months old? This is the same with all domains; it seems the cache is stuck for that (?) \\u003Cstrong\\u003Ehowever\\u003C/strong\\u003E it\\u0026#39;s working fine for new pages \\u003Cstrong\\u003Ewhen\\u003C/strong\\u003E set to rising:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/domain/i.imgur.com/new/?sort=rising\\\"\\u003Ehttp://www.reddit.com/domain/i.imgur.com/new/?sort=rising\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ebut if you sort by new:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://www.reddit.com/domain/i.imgur.com/new/?sort=new\\\"\\u003Ehttp://www.reddit.com/domain/i.imgur.com/new/?sort=new\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt has the same problem, stuck!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny ideas about this? If this can\\u0026#39;t be fixed it leaves me in a predicament but it\\u0026#39;s workable I guess, this is the functionality I\\u0026#39;d prefer though. Any ideas? \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Searching via domain leaves me in a strange predicament. I'm doing this from both my account and via a server authenticated (via api). I'll use imgur.com as the example: \\n\\nhttp://www.reddit.com/domain/imgur.com\\n\\nNotice how everything is 2 months old? This is the same with all domains; it seems the cache is stuck for that (?) **however** it's working fine for new pages **when** set to rising:\\n\\nhttp://www.reddit.com/domain/i.imgur.com/new/?sort=rising\\n\\nbut if you sort by new:\\n\\nhttp://www.reddit.com/domain/i.imgur.com/new/?sort=new\\n\\nIt has the same problem, stuck!\\n\\nAny ideas about this? If this can't be fixed it leaves me in a predicament but it's workable I guess, this is the functionality I'd prefer though. Any ideas? \\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"c806m\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/c806m/redditdev_help_domain_pages_are_broken_i_think/\", \"locked\": false, \"name\": \"t3_c806m\", \"created\": 1274840749.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/c806m/redditdev_help_domain_pages_are_broken_i_think/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"redditdev help: domain pages are broken... I think\", \"created_utc\": 1274811949.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EPlaying with the API and logging in via CURL (for legit things, happy to explain why if needed) however I get \\u0026quot;incorrect password\\u0026quot;, it had worked before so I switched to another of my servers and bingo, worked fine. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFrom this I can only deduct that my IP is blacklisted from logging in, I assume from too many attempts? (About 10 or so, testing things). Is it possible to be whitelisted? \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Playing with the API and logging in via CURL (for legit things, happy to explain why if needed) however I get \\\"incorrect password\\\", it had worked before so I switched to another of my servers and bingo, worked fine. \\n\\nFrom this I can only deduct that my IP is blacklisted from logging in, I assume from too many attempts? (About 10 or so, testing things). Is it possible to be whitelisted? \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"bg8wv\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/bg8wv/does_reddit_block_ips_and_if_so_possible_to_get/\", \"locked\": false, \"name\": \"t3_bg8wv\", \"created\": 1269222243.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/bg8wv/does_reddit_block_ips_and_if_so_possible_to_get/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Does reddit block IPs and if so, possible to get whitelisted?\", \"created_utc\": 1269193443.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey reddit devs. I am trying to build a little app using the reddit api, accessing the latest comments by everyone on every subreddit.\\nThe thing is that when i use the url \\u003Ca href=\\\"http://www.reddit.com/comments.json?count=100\\\"\\u003Ehttp://www.reddit.com/comments.json?count=100\\u003C/a\\u003E i always get the last 25 comments, it doesn\\u0026#39;t seem to be using the \\u0026quot;count\\u0026quot; parameters. Is this an intentional limitation or just a bug?\\nthanks for your help!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey reddit devs. I am trying to build a little app using the reddit api, accessing the latest comments by everyone on every subreddit.\\nThe thing is that when i use the url http://www.reddit.com/comments.json?count=100 i always get the last 25 comments, it doesn't seem to be using the \\\"count\\\" parameters. Is this an intentional limitation or just a bug?\\nthanks for your help!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"aruiw\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"mlambir\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/aruiw/problem_with_the_reddit_api/\", \"locked\": false, \"name\": \"t3_aruiw\", \"created\": 1264010668.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/aruiw/problem_with_the_reddit_api/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Problem with the reddit api\", \"created_utc\": 1263981868.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI have couple of points where you can make the search better. Note: I am not a troll, i have quite long experience with solr in making search and matching engines. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E1: Use lucene parser for user input. It is more natural, can use boosts on field:keyword pairs, can use wildcards and Boolean operators. In this case you can only use Dismax for \\u0026quot;must not have\\u0026quot; which can be made with lucene using AND NOT.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E2: Don\\u0026#39;t use the LowercaseFilterFactory. It doesnt work as it should. Makes the wildcards unusable.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E3: Default query operator should be AND. Internal query if i search for \\u0026quot;reddit alien\\u0026quot; should look something like this: \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E(title:(reddit+alien)^150+OR+content:(reddit+alien)^80)+AND+NOT+spam:(true)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThe fields are OR\\u0026#39;ed but user input is AND\\u0026#39;ed. That way the search will be more accurate.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E4: Add more boost to title, less to content and no boost to the other fields. Add the subreddit and other filters as facets with quotes. (more accurate).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThat\\u0026#39;s it for now. If there is any interest in this i will continue with suggestions, i\\u0026#39;ll even write some examples. I am the developer of the search and matching engine for one big German recruiting platform. I have developed 3 additional projects utilizing solr/lucene.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I have couple of points where you can make the search better. Note: I am not a troll, i have quite long experience with solr in making search and matching engines. \\n\\n \\n1: Use lucene parser for user input. It is more natural, can use boosts on field:keyword pairs, can use wildcards and Boolean operators. In this case you can only use Dismax for \\\"must not have\\\" which can be made with lucene using AND NOT.\\n\\n \\n2: Don't use the LowercaseFilterFactory. It doesnt work as it should. Makes the wildcards unusable.\\n\\n \\n\\n\\n3: Default query operator should be AND. Internal query if i search for \\\"reddit alien\\\" should look something like this: \\n\\n    (title:(reddit+alien)^150+OR+content:(reddit+alien)^80)+AND+NOT+spam:(true)\\n\\nThe fields are OR'ed but user input is AND'ed. That way the search will be more accurate.\\n\\n\\n\\n4: Add more boost to title, less to content and no boost to the other fields. Add the subreddit and other filters as facets with quotes. (more accurate).\\n\\nThat's it for now. If there is any interest in this i will continue with suggestions, i'll even write some examples. I am the developer of the search and matching engine for one big German recruiting platform. I have developed 3 additional projects utilizing solr/lucene.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"alj66\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"dekomote\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/alj66/reddit_search_improvement/\", \"locked\": false, \"name\": \"t3_alj66\", \"created\": 1262670854.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/alj66/reddit_search_improvement/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit search improvement?\", \"created_utc\": 1262642054.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI already reported this bug through the feedback section last week, but as I got no response I\\u0026#39;m sending it here.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe problem is really simple: when you\\u0026#39;re messing with your preferences, if you switch the \\u003Cstrong\\u003Econtent language\\u003C/strong\\u003E radio button from \\u003Cstrong\\u003Esome languages\\u003C/strong\\u003E to \\u003Cstrong\\u003Eall languages\\u003C/strong\\u003E, you\\u0026#39;ll end up deactivating \\u003Cem\\u003Eall\\u003C/em\\u003E checkboxes in the page (not only languages\\u0026#39;).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhere is the problem found: \\u003Ca href=\\\"http://code.reddit.com/browser/r2/r2/public/static/js/reddit.js\\\"\\u003Ereddit.js - line 1079\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EProposed solution:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E$(elem).parents(\\u0026quot;form\\u0026quot;).find(\\u0026quot;input[type=checkbox]\\u0026quot;).attr(\\u0026quot;checked\\u0026quot;, false);\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eshould be\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E$(elem).parents(\\u0026quot;td\\u0026quot;).find(\\u0026quot;input[type=checkbox]\\u0026quot;).attr(\\u0026quot;checked\\u0026quot;, false);\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I already reported this bug through the feedback section last week, but as I got no response I'm sending it here.\\n\\nThe problem is really simple: when you're messing with your preferences, if you switch the **content language** radio button from **some languages** to **all languages**, you'll end up deactivating *all* checkboxes in the page (not only languages').\\n\\nWhere is the problem found: [reddit.js - line 1079](http://code.reddit.com/browser/r2/r2/public/static/js/reddit.js)\\n\\nProposed solution:\\n\\n    $(elem).parents(\\\"form\\\").find(\\\"input[type=checkbox]\\\").attr(\\\"checked\\\", false);\\n\\nshould be\\n\\n    $(elem).parents(\\\"td\\\").find(\\\"input[type=checkbox]\\\").attr(\\\"checked\\\", false);\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"aa8k9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"celtric\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": true, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/aa8k9/small_javascript_bug_in_user_preferences_section/\", \"locked\": false, \"name\": \"t3_aa8k9\", \"created\": 1259785071.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/aa8k9/small_javascript_bug_in_user_preferences_section/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Small Javascript bug in user preferences section\", \"created_utc\": 1259756271.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI want to create a reddit site using their open source code, and I\\u0026#39;m considering posting a project on Scriptlance/Elance in order to get someone to do this for me. (I have no tech skills, but a good idea for a site)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy question is, once the site is up and running, will it use the same algorithm as reddit.com does with regards to the popularity of the content posted?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I want to create a reddit site using their open source code, and I'm considering posting a project on Scriptlance/Elance in order to get someone to do this for me. (I have no tech skills, but a good idea for a site)\\r\\n\\r\\nMy question is, once the site is up and running, will it use the same algorithm as reddit.com does with regards to the popularity of the content posted?\\r\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"9jyf7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"stiginthedumpr\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/9jyf7/question_re_reddit_algorithm/\", \"locked\": false, \"name\": \"t3_9jyf7\", \"created\": 1252822479.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/9jyf7/question_re_reddit_algorithm/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Question re reddit algorithm\", \"created_utc\": 1252793679.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EThere\\u0026#39;s a recurring problem with submitters who don\\u0026#39;t realize that text-posts are always self-posts.  People will occasionally start a submission, adding a title and a link, then click on the \\u0026#39;text\\u0026#39; tab and type what they think is going to become the \\u0026#39;first post\\u0026#39; in the submission, then submit; the result is that the text of the submission contains references to the link that has now been discarded.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis might be made more smooth if the title of the submission disappeared when switching between the \\u0026#39;link\\u0026#39; and \\u0026#39;text\\u0026#39; tabs (which might be enough of a \\u0026quot;what?\\u0026quot; moment to make the submitter understand that \\u0026#39;text\\u0026#39; and \\u0026#39;link\\u0026#39; posts are two different things).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EA better fix, and I\\u0026#39;m not sure that the DB schema supports it as-is, might be to just always allow a block of text to be submitted with every post, so that you can both link to an external site and post a block of text with the submission.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"There's a recurring problem with submitters who don't realize that text-posts are always self-posts.  People will occasionally start a submission, adding a title and a link, then click on the 'text' tab and type what they think is going to become the 'first post' in the submission, then submit; the result is that the text of the submission contains references to the link that has now been discarded.\\n\\nThis might be made more smooth if the title of the submission disappeared when switching between the 'link' and 'text' tabs (which might be enough of a \\\"what?\\\" moment to make the submitter understand that 'text' and 'link' posts are two different things).\\n\\nA better fix, and I'm not sure that the DB schema supports it as-is, might be to just always allow a block of text to be submitted with every post, so that you can both link to an external site and post a block of text with the submission.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"9fus2\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"roxm\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/9fus2/usability_problem_with_submit_page/\", \"locked\": false, \"name\": \"t3_9fus2\", \"created\": 1251755884.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/9fus2/usability_problem_with_submit_page/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Usability problem with submit page\", \"created_utc\": 1251727084.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi! I\\u0026#39;ve been testing my own reddit for a few days, and I notice that it\\u0026#39;s considerably slower than the real reddit on reddit.com, even though I have probably 4 subscribed uers and only a handful of submissions. My server is a VPS and I have plenty of resources available.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there anything I need to do to enable some caching or something that will speed things up? I searched Trac but I didn\\u0026#39;t find anything.\\nThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi! I've been testing my own reddit for a few days, and I notice that it's considerably slower than the real reddit on reddit.com, even though I have probably 4 subscribed uers and only a handful of submissions. My server is a VPS and I have plenty of resources available.\\n\\nIs there anything I need to do to enable some caching or something that will speed things up? I searched Trac but I didn't find anything.\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"8x6x7\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"siovene\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/8x6x7/do_i_need_to_do_anything_to_optimize_my/\", \"locked\": false, \"name\": \"t3_8x6x7\", \"created\": 1246454652.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/8x6x7/do_i_need_to_do_anything_to_optimize_my/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Do I need to do anything to optimize my installation?\", \"created_utc\": 1246425852.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"875lm\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Taladar\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/875lm/notification_on_reply_to_someone_elses_comment/\", \"locked\": false, \"name\": \"t3_875lm\", \"created\": 1237949404.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/875lm/notification_on_reply_to_someone_elses_comment/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Notification on reply to someone else's comment\", \"created_utc\": 1237920604.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"732jw\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"finix\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/732jw/rredditdev_as_deserted_as_it_looks_like/\", \"locked\": false, \"name\": \"t3_732jw\", \"created\": 1222220339.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/732jw/rredditdev_as_deserted_as_it_looks_like/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"/r/redditdev as deserted as it looks like?\", \"created_utc\": 1222191539.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"6oue4\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"mrpeenut24\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/6oue4/update_on_stats_server/\", \"locked\": false, \"name\": \"t3_6oue4\", \"created\": 1214391841.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/6oue4/update_on_stats_server/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Update on stats server?\", \"created_utc\": 1214363041.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHere\\u0026#39;s what he told me:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u0026quot; It\\u0026#39;s   installed fine (apparently) and you can register, but that\\u0026#39;s about it. When you try to create a subreddit, for example, it gives this nasty error:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENotImplementedError: Action u\\u0026#39;POST_submit\\u0026#39; is not implemented\\nWhich seems pretty odd. Debugging output is whargarble to me as it\\u0026#39;s just too specific to what\\u0026#39;s going on with this code, and google searches for that error seem to be about other servers/services.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlso, the Submit a new... form won\\u0026#39;t let you select Text, AND the Create button for Link posts doesn\\u0026#39;t seem to trigger / \\u0026quot;do anything\\u0026quot; either. Just discovered that attempting to comment gives a similar \\u0026quot;post not implemented\\u0026quot; error that\\u0026#39;s visible via firebug.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnyone have insight / clue / pointers / experience? If it were just Java, or just front-end / js code, or just SQL, I might have a fighting chance... but with all this fucking cassandra, rabbit, memcache stuff that I don\\u0026#39;t have experience with... whew. I don\\u0026#39;t know where to look.\\u0026quot;\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Here's what he told me:\\n\\n\\\" It's   installed fine (apparently) and you can register, but that's about it. When you try to create a subreddit, for example, it gives this nasty error:\\n\\nNotImplementedError: Action u'POST_submit' is not implemented\\nWhich seems pretty odd. Debugging output is whargarble to me as it's just too specific to what's going on with this code, and google searches for that error seem to be about other servers/services.\\n\\nAlso, the Submit a new... form won't let you select Text, AND the Create button for Link posts doesn't seem to trigger / \\\"do anything\\\" either. Just discovered that attempting to comment gives a similar \\\"post not implemented\\\" error that's visible via firebug.\\n\\nAnyone have insight / clue / pointers / experience? If it were just Java, or just front-end / js code, or just SQL, I might have a fighting chance... but with all this fucking cassandra, rabbit, memcache stuff that I don't have experience with... whew. I don't know where to look.\\\"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4nzcwh\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"Francois_Rapiste\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 28, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4nzcwh/friend_of_mine_is_trying_to_create_a_reddit_clone/\", \"locked\": false, \"name\": \"t3_4nzcwh\", \"created\": 1465901453.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4nzcwh/friend_of_mine_is_trying_to_create_a_reddit_clone/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Friend of mine is trying to create a Reddit clone. Help?\", \"created_utc\": 1465872653.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EForgive me if this is the wrong place to post this, but I need help making my own reddit hack.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI installed reddit on ubuntu 14.04 using this guide (\\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/Install-guide\\\"\\u003Ehttps://github.com/reddit/reddit/wiki/Install-guide\\u003C/a\\u003E) and everything seemed to install correctly. I even was able to populate the DB with test data. But it does not seem like it\\u0026#39;s running as I cannot access reddit.local or anything on 127.0.0.1. I\\u0026#39;ve used apache on windows, so this is slightly new to me. Did the install script install apache or nginx? Also, what are the login details for the server and Cassandra? It doesn\\u0026#39;t seem like it\\u0026#39;s running any server when I look at the process manager, but python looks like it is. Halp?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Forgive me if this is the wrong place to post this, but I need help making my own reddit hack.\\n\\nI installed reddit on ubuntu 14.04 using this guide (https://github.com/reddit/reddit/wiki/Install-guide) and everything seemed to install correctly. I even was able to populate the DB with test data. But it does not seem like it's running as I cannot access reddit.local or anything on 127.0.0.1. I've used apache on windows, so this is slightly new to me. Did the install script install apache or nginx? Also, what are the login details for the server and Cassandra? It doesn't seem like it's running any server when I look at the process manager, but python looks like it is. Halp?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4mn44c\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"sound_puppy\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4mn44c/help_configuring_fresh_installation/\", \"locked\": false, \"name\": \"t3_4mn44c\", \"created\": 1465157396.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4mn44c/help_configuring_fresh_installation/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Help configuring fresh installation\", \"created_utc\": 1465128596.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo, I have a Python script that gets all posts from a subreddit and filters them, replying to ones that match certain criteria.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EDoes the API rate limit get requests from PRAW as well as replies?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: My bot code: \\u003Ca href=\\\"http://pastebin.com/qCZp3J1A\\\"\\u003Ehttp://pastebin.com/qCZp3J1A\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So, I have a Python script that gets all posts from a subreddit and filters them, replying to ones that match certain criteria.\\n\\nDoes the API rate limit get requests from PRAW as well as replies?\\n\\nEdit: My bot code: http://pastebin.com/qCZp3J1A\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4mchkd\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"SupremeRedditBot\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1464955082.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4mchkd/praw_rate_limit/\", \"locked\": false, \"name\": \"t3_4mchkd\", \"created\": 1464983210.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4mchkd/praw_rate_limit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"PRAW Rate Limit\", \"created_utc\": 1464954410.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ESo I\\u0026#39;m not quote sure what\\u0026#39;s going on here. \\u003Ca href=\\\"http://pastebin.com/ndyDZcw4\\\"\\u003EHere\\u003C/a\\u003E is the relevant script portion. I can open the URL in my browser and it works as expected. However, when I run the script I don\\u0026#39;t download an image, I download an HTML web page saying\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EPlease enable cookies.\\nError 1010 Ray ID: 2acbbc88xxxxxx \\u2022 2016-06-02 14:53:43 UTC\\nAccess denied\\nWhat happened?\\n\\nThe owner of this website (i.redd.it) has banned your access based on your browser\\u0026#39;s signature (2acbbc88axxxxxxx-xxxx).\\n\\nCloudFlare Ray ID: 2acbbc88xxxxxxx \\u2022 Your IP: 1.2.3.4 \\u2022 Performance \\u0026amp; security by CloudFlare\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ENot sure what\\u0026#39;s going on here. It looks like CloudFlair checks my browser signature before redirecting me to the image, and if \\u003Cem\\u003Econditions\\u003C/em\\u003E aren\\u0026#39;t met I get the access denied page instead. What are \\u003Cem\\u003Econditions\\u003C/em\\u003E? I tried fucking around with the user-agent, but that didn\\u0026#39;t seem to work. Are there rules to scripting the download of images from i.redd.it that I don\\u0026#39;t know?\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003EEDIT:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere is some interesting stuff I\\u0026#39;ve found out. \\u003Ca href=\\\"http://pastebin.com/BMV49vvs\\\"\\u003EHere\\u003C/a\\u003E is the script I\\u0026#39;m working with. Findings:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003ELine 8 is necessarily. You get 403 when you don\\u0026#39;t change the user-agent. This is probably why I was getting the HTML access denied page instead of the image.\\u003C/li\\u003E\\n\\u003Cli\\u003ELine 9 \\u003Cstrong\\u003Eneeds\\u003C/strong\\u003E the replacement of HTTP with HTTPS. Without the replacement you get a 404. Apparently their redirection is broken. You can see this more easily by just doing \\u0026quot;wget \\u003Ca href=\\\"http://i.redd.it/0583lecdrv0x.jpg\\\"\\u003Ehttp://i.redd.it/0583lecdrv0x.jpg\\u003C/a\\u003E\\u0026quot; which will 404, and \\u0026quot;wget \\u003Ca href=\\\"https://i.redd.it/0583lecdrv0x.jpg\\\"\\u003Ehttps://i.redd.it/0583lecdrv0x.jpg\\u003C/a\\u003E\\u0026quot; which will download the image.\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m not sure why those two things are happening, but they are. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"So I'm not quote sure what's going on here. [Here](http://pastebin.com/ndyDZcw4) is the relevant script portion. I can open the URL in my browser and it works as expected. However, when I run the script I don't download an image, I download an HTML web page saying\\n\\n    Please enable cookies.\\n    Error 1010 Ray ID: 2acbbc88xxxxxx \\u2022 2016-06-02 14:53:43 UTC\\n    Access denied\\n    What happened?\\n\\n    The owner of this website (i.redd.it) has banned your access based on your browser's signature (2acbbc88axxxxxxx-xxxx).\\n\\n    CloudFlare Ray ID: 2acbbc88xxxxxxx \\u2022 Your IP: 1.2.3.4 \\u2022 Performance \\u0026 security by CloudFlare\\n\\nNot sure what's going on here. It looks like CloudFlair checks my browser signature before redirecting me to the image, and if *conditions* aren't met I get the access denied page instead. What are *conditions*? I tried fucking around with the user-agent, but that didn't seem to work. Are there rules to scripting the download of images from i.redd.it that I don't know?\\n\\n-------------------------------------\\n\\nEDIT:\\n\\nHere is some interesting stuff I've found out. [Here](http://pastebin.com/BMV49vvs) is the script I'm working with. Findings:\\n\\n1. Line 8 is necessarily. You get 403 when you don't change the user-agent. This is probably why I was getting the HTML access denied page instead of the image.\\n2. Line 9 **needs** the replacement of HTTP with HTTPS. Without the replacement you get a 404. Apparently their redirection is broken. You can see this more easily by just doing \\\"wget http://i.redd.it/0583lecdrv0x.jpg\\\" which will 404, and \\\"wget https://i.redd.it/0583lecdrv0x.jpg\\\" which will download the image.\\n\\nI'm not sure why those two things are happening, but they are. \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4m7kxj\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"Pandemic21\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1464886999.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4m7kxj/whats_up_with_downloading_images_from_ireddit/\", \"locked\": false, \"name\": \"t3_4m7kxj\", \"created\": 1464908610.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4m7kxj/whats_up_with_downloading_images_from_ireddit/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"What's up with downloading images from i.redd.it?\", \"created_utc\": 1464879810.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHere\\u0026#39;s the TL:DR;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI want users to send PMs to a bot, requesting changes to their user flairs\\u0026#39; text, because some users have special limited edition flairs, so they can\\u0026#39;t change the text themselves because the flairs no longer appear in the selector.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe syntax for the command is \\u003Ccode\\u003Eset_flair(subreddit, user, flair_text, flair_css_class)\\u003C/code\\u003E, so it\\u0026#39;s easy to get the username and flair text from the PM they send to the bot, but how can I get the CSS class they\\u0026#39;re currently using?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: Just to reiterate, I can\\u0026#39;t have users message the bot with the css class, because some flairs are special limited editions, so it would be easily exploitable.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Here's the TL:DR;\\n\\nI want users to send PMs to a bot, requesting changes to their user flairs' text, because some users have special limited edition flairs, so they can't change the text themselves because the flairs no longer appear in the selector.\\n\\nThe syntax for the command is `set_flair(subreddit, user, flair_text, flair_css_class)`, so it's easy to get the username and flair text from the PM they send to the bot, but how can I get the CSS class they're currently using?\\n\\nEDIT: Just to reiterate, I can't have users message the bot with the css class, because some flairs are special limited editions, so it would be easily exploitable.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4lv0xz\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"DrYoshiyahu\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1464698989.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4lv0xz/can_i_request_the_css_class_of_the_user_flair_of/\", \"locked\": false, \"name\": \"t3_4lv0xz\", \"created\": 1464727497.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4lv0xz/can_i_request_the_css_class_of_the_user_flair_of/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can I request the CSS class of the user flair of the sender of a private message?\", \"created_utc\": 1464698697.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4l7qyh\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"licktheenvelopeoff\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4l7qyh/how_can_i_use_proxies_when_using_prawpython_35/\", \"locked\": false, \"name\": \"t3_4l7qyh\", \"created\": 1464328775.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4l7qyh/how_can_i_use_proxies_when_using_prawpython_35/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How can I use proxies when using praw/python 3.5?\", \"created_utc\": 1464299975.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI use a Java app (inside Eclipse for now) to fetch the title, subreddit, author etc of each post of \\u003Ca href=\\\"/r/all\\\"\\u003E/r/all\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor that I connect to \\u003Ca href=\\\"/r/all\\\"\\u003E/r/all\\u003C/a\\u003E.json and parse the json. However it almost always returns me a 429 error (too many requests), even after 10 minutes spent waiting.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhy does it do that behavior? I am far under the 30 requests/minute.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I use a Java app (inside Eclipse for now) to fetch the title, subreddit, author etc of each post of /r/all.\\n\\nFor that I connect to /r/all.json and parse the json. However it almost always returns me a 429 error (too many requests), even after 10 minutes spent waiting.\\n\\nWhy does it do that behavior? I am far under the 30 requests/minute.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4kz0oq\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"Zezombye\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4kz0oq/too_many_requests_almost_every_time/\", \"locked\": false, \"name\": \"t3_4kz0oq\", \"created\": 1464201627.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4kz0oq/too_many_requests_almost_every_time/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"\\\"Too many requests\\\" almost every time\", \"created_utc\": 1464172827.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI don\\u0026#39;t want to resolve reddit.local locally, but actually run reddit on a different domain (experimenting with using it as a commenting engine)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve updated the config, setting domain = \\u0026lt;my_domain\\u0026gt;, but I still get redirected to reddit.local.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I don't want to resolve reddit.local locally, but actually run reddit on a different domain (experimenting with using it as a commenting engine)\\n\\nI've updated the config, setting domain = \\u003Cmy_domain\\u003E, but I still get redirected to reddit.local.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4jqh6s\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"4dd3r\", \"media\": null, \"score\": 3, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4jqh6s/how_do_i_correctly_configure_reddit_on_an_ec2/\", \"locked\": false, \"name\": \"t3_4jqh6s\", \"created\": 1463514309.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4jqh6s/how_do_i_correctly_configure_reddit_on_an_ec2/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"how do I correctly configure reddit on an EC2 instance + own domain? (I'm missing something very basic)\", \"created_utc\": 1463485509.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 3}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI get this error when I try to create a new ad, before get the ads form\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENoAuthHandlerFound: No handler was ready to authenticate. 1 handlers were checked. [\\u0026#39;HmacAuthV1Handler\\u0026#39;] Check your credentials\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E URL: http://example.com/promoted/new_promo?utm_source=advertising\\u0026amp;utm_medium=button\\u0026amp;utm_term=create+ads\\u0026amp;utm_campaign=buttons\\n File \\u0026#39;/usr/lib/python2.7/dist-packages/weberror/evalexception.py\\u0026#39;, line 431 in respond\\n   app_iter = self.application(environ, detect_start_response)\\n File \\u0026#39;/home/reddit/src/reddit/r2/r2/config/middleware.py\\u0026#39;, line 184 in __call__\\n   return self.app(environ, start_response)\\n File \\u0026#39;/home/reddit/src/reddit/r2/r2/config/middleware.py\\u0026#39;, line 293 in __call__\\n   return self.app(environ, start_response)\\n File \\u0026#39;/home/reddit/src/reddit/r2/r2/config/middleware.py\\u0026#39;, line 252 in __call__\\n   return self.app(environ, start_response)\\n  File \\u0026#39;/home/reddit/src/reddit/r2/r2/config/middleware.py\\u0026#39;, line 266 in __call__\\n   return self.app(environ, start_response)\\n File \\u0026#39;/home/reddit/src/reddit/r2/r2/config/middleware.py\\u0026#39;, line 386 in __call__\\n   return self.app(environ, start_response)\\n File \\u0026#39;/home/reddit/src/reddit/r2/r2/config/middleware.py\\u0026#39;, line 412 in __call__\\n   return self.app(environ, custom_start_response)\\n File \\u0026#39;/usr/lib/python2.7/dist-packages/routes/middleware.py\\u0026#39;, line 131 in __call__\\n   response = self.app(environ, start_response)\\n File \\u0026#39;/usr/lib/python2.7/dist-packages/pylons/wsgiapp.py\\u0026#39;, line 103 in __call__\\n   response = self.dispatch(controller, environ, start_response)\\n File \\u0026#39;/usr/lib/python2.7/dist-packages/pylons/wsgiapp.py\\u0026#39;, line 313 in dispatch\\n   return controller(environ, start_response)\\n File \\u0026#39;/home/reddit/src/reddit/r2/r2/lib/base.py\\u0026#39;, line 144 in __call__\\n   return WSGIController.__call__(self, environ, start_response)\\n File \\u0026#39;/usr/lib/python2.7/dist-packages/pylons/controllers/core.py\\u0026#39;, line 214 in __call__\\n   response = self._dispatch_call()\\n File \\u0026#39;/usr/lib/python2.7/dist-packages/pylons/controllers/core.py\\u0026#39;, line 164 in _dispatch_call\\n   response = self._inspect_call(func)\\n File \\u0026#39;/usr/lib/python2.7/dist-packages/pylons/controllers/core.py\\u0026#39;, line 107 in _inspect_call\\n   result = self._perform_call(func, args)\\n File \\u0026#39;/usr/lib/python2.7/dist-packages/pylons/controllers/core.py\\u0026#39;, line 57 in _perform_call\\n   return func(**args)\\n File \\u0026#39;/home/reddit/src/reddit/r2/r2/lib/validator/validator.py\\u0026#39;, line 209 in newfn\\n   return fn(self, *a, **kw)\\n File \\u0026#39;/home/reddit/src/reddit/r2/r2/controllers/promotecontroller.py\\u0026#39;, line 288 in GET_new_promo\\n   ads_images = _get_ads_images(c.user)\\n File \\u0026#39;/home/reddit/src/reddit/r2/r2/controllers/promotecontroller.py\\u0026#39;, line 260 in _get_ads_images\\n   keys = s3_helpers.get_keys(g.s3_client_uploads_bucket,   prefix=_get_ads_keyspace(thing), **kwargs)\\n File \\u0026#39;/home/reddit/src/reddit/r2/r2/lib/s3_helpers.py\\u0026#39;, line 136 in get_keys\\n   connection = connection or get_connection()\\n File \\u0026#39;/home/reddit/src/reddit/r2/r2/lib/s3_helpers.py\\u0026#39;, line 126 in get_connection\\n   return boto.connect_s3(g.S3KEY_ID or None, g.S3SECRET_KEY or None)\\n File \\u0026#39;/usr/lib/python2.7/dist-packages/boto/__init__.py\\u0026#39;, line 141 in connect_s3\\n   return S3Connection(aws_access_key_id, aws_secret_access_key, **kwargs)\\n File \\u0026#39;/usr/lib/python2.7/dist-packages/boto/s3/connection.py\\u0026#39;, line 190 in __init__\\n   validate_certs=validate_certs, profile_name=profile_name)\\n File \\u0026#39;/usr/lib/python2.7/dist-packages/boto/connection.py\\u0026#39;, line 569 in __init__\\n   host, config, self.provider, self._required_auth_capability())\\n File \\u0026#39;/usr/lib/python2.7/dist-packages/boto/auth.py\\u0026#39;, line 987 in get_auth_handler\\n   \\u0026#39;Check your credentials\\u0026#39; % (len(names), str(names)))\\n NoAuthHandlerFound: No handler was ready to authenticate. 1 handlers were checked. [\\u0026#39;HmacAuthV1Handler\\u0026#39;] Check your credentials\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi,\\n\\nI get this error when I try to create a new ad, before get the ads form\\n\\nNoAuthHandlerFound: No handler was ready to authenticate. 1 handlers were checked. ['HmacAuthV1Handler'] Check your credentials\\n\\n     URL: http://example.com/promoted/new_promo?utm_source=advertising\\u0026utm_medium=button\\u0026utm_term=create+ads\\u0026utm_campaign=buttons\\n     File '/usr/lib/python2.7/dist-packages/weberror/evalexception.py', line 431 in respond\\n       app_iter = self.application(environ, detect_start_response)\\n     File '/home/reddit/src/reddit/r2/r2/config/middleware.py', line 184 in __call__\\n       return self.app(environ, start_response)\\n     File '/home/reddit/src/reddit/r2/r2/config/middleware.py', line 293 in __call__\\n       return self.app(environ, start_response)\\n     File '/home/reddit/src/reddit/r2/r2/config/middleware.py', line 252 in __call__\\n       return self.app(environ, start_response)\\n      File '/home/reddit/src/reddit/r2/r2/config/middleware.py', line 266 in __call__\\n       return self.app(environ, start_response)\\n     File '/home/reddit/src/reddit/r2/r2/config/middleware.py', line 386 in __call__\\n       return self.app(environ, start_response)\\n     File '/home/reddit/src/reddit/r2/r2/config/middleware.py', line 412 in __call__\\n       return self.app(environ, custom_start_response)\\n     File '/usr/lib/python2.7/dist-packages/routes/middleware.py', line 131 in __call__\\n       response = self.app(environ, start_response)\\n     File '/usr/lib/python2.7/dist-packages/pylons/wsgiapp.py', line 103 in __call__\\n       response = self.dispatch(controller, environ, start_response)\\n     File '/usr/lib/python2.7/dist-packages/pylons/wsgiapp.py', line 313 in dispatch\\n       return controller(environ, start_response)\\n     File '/home/reddit/src/reddit/r2/r2/lib/base.py', line 144 in __call__\\n       return WSGIController.__call__(self, environ, start_response)\\n     File '/usr/lib/python2.7/dist-packages/pylons/controllers/core.py', line 214 in __call__\\n       response = self._dispatch_call()\\n     File '/usr/lib/python2.7/dist-packages/pylons/controllers/core.py', line 164 in _dispatch_call\\n       response = self._inspect_call(func)\\n     File '/usr/lib/python2.7/dist-packages/pylons/controllers/core.py', line 107 in _inspect_call\\n       result = self._perform_call(func, args)\\n     File '/usr/lib/python2.7/dist-packages/pylons/controllers/core.py', line 57 in _perform_call\\n       return func(**args)\\n     File '/home/reddit/src/reddit/r2/r2/lib/validator/validator.py', line 209 in newfn\\n       return fn(self, *a, **kw)\\n     File '/home/reddit/src/reddit/r2/r2/controllers/promotecontroller.py', line 288 in GET_new_promo\\n       ads_images = _get_ads_images(c.user)\\n     File '/home/reddit/src/reddit/r2/r2/controllers/promotecontroller.py', line 260 in _get_ads_images\\n       keys = s3_helpers.get_keys(g.s3_client_uploads_bucket,   prefix=_get_ads_keyspace(thing), **kwargs)\\n     File '/home/reddit/src/reddit/r2/r2/lib/s3_helpers.py', line 136 in get_keys\\n       connection = connection or get_connection()\\n     File '/home/reddit/src/reddit/r2/r2/lib/s3_helpers.py', line 126 in get_connection\\n       return boto.connect_s3(g.S3KEY_ID or None, g.S3SECRET_KEY or None)\\n     File '/usr/lib/python2.7/dist-packages/boto/__init__.py', line 141 in connect_s3\\n       return S3Connection(aws_access_key_id, aws_secret_access_key, **kwargs)\\n     File '/usr/lib/python2.7/dist-packages/boto/s3/connection.py', line 190 in __init__\\n       validate_certs=validate_certs, profile_name=profile_name)\\n     File '/usr/lib/python2.7/dist-packages/boto/connection.py', line 569 in __init__\\n       host, config, self.provider, self._required_auth_capability())\\n     File '/usr/lib/python2.7/dist-packages/boto/auth.py', line 987 in get_auth_handler\\n       'Check your credentials' % (len(names), str(names)))\\n     NoAuthHandlerFound: No handler was ready to authenticate. 1 handlers were checked. ['HmacAuthV1Handler'] Check your credentials\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4jjsj5\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"hhaevs\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1464028650.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4jjsj5/error_when_try_to_create_a_promoted_link/\", \"locked\": false, \"name\": \"t3_4jjsj5\", \"created\": 1463405728.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4jjsj5/error_when_try_to_create_a_promoted_link/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Error when try to create a promoted link.\", \"created_utc\": 1463376928.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m brand new to bot-making but I know python decently, and for some reason authentication is stumping me. Following \\u003Ca href=\\\"https://praw.readthedocs.io/en/stable/pages/oauth.html#oauth-scopes\\\"\\u003Ethis\\u003C/a\\u003E, I\\u0026#39;m not sure what to do once I get the access and refresh tokens. Is there a way to just make a permanent-length access token? How would I do this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm brand new to bot-making but I know python decently, and for some reason authentication is stumping me. Following [this](https://praw.readthedocs.io/en/stable/pages/oauth.html#oauth-scopes), I'm not sure what to do once I get the access and refresh tokens. Is there a way to just make a permanent-length access token? How would I do this?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4ixpa1\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"ModelNYSE\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4ixpa1/permanentlength_oauth2_tokens/\", \"locked\": false, \"name\": \"t3_4ixpa1\", \"created\": 1463036687.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4ixpa1/permanentlength_oauth2_tokens/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Permanent-length OAuth2 tokens\", \"created_utc\": 1463007887.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi guys,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI created several subreddits, then I set them to be my automatic_reddits in the .ini file. However, these subreddits still didn\\u0026#39;t automatically show up on my front page. Can someone please tell me what I should do here?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks so much for your help!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi guys,\\n\\nI created several subreddits, then I set them to be my automatic_reddits in the .ini file. However, these subreddits still didn't automatically show up on my front page. Can someone please tell me what I should do here?\\n\\nThanks so much for your help!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4itulj\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"tomhglg\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4itulj/question_about_automatic_reddits/\", \"locked\": false, \"name\": \"t3_4itulj\", \"created\": 1462981372.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4itulj/question_about_automatic_reddits/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Question about automatic_reddits\", \"created_utc\": 1462952572.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhen attempting to edit a wiki page on \\u003Ca href=\\\"/r/Rowing\\\"\\u003E/r/Rowing\\u003C/a\\u003E using PRAW or the web interface I get HTTP 413 errors. In fact when saving through the web interface I get redirected to this message\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026lt;html\\u0026gt;\\u0026lt;head\\u0026gt;\\u0026lt;script type=\\u0026#39;text/javascript\\u0026#39;\\u0026gt;parent.completedUploadImage(\\u0026#39;failed\\u0026#39;,\\u0026#39;\\u0026#39;,\\u0026#39;\\u0026#39;,[[\\u0026#39;BAD_CSS_NAME\\u0026#39;, \\u0026#39;\\u0026#39;], [\\u0026#39;IMAGE_ERROR\\u0026#39;, \\u0026#39;too big. keep it under 500 KiB\\u0026#39;]],\\u0026#39;\\u0026#39;);\\u0026lt;/script\\u0026gt;\\u0026lt;/head\\u0026gt;\\u0026lt;body\\u0026gt;you shouldn\\u0026#39;t be here\\u0026lt;/body\\u0026gt;\\u0026lt;/html\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ELooking at the reddit code, it appears to come from \\u003Ca href=\\\"https://github.com/reddit/reddit/blob/master/r2/r2/config/middleware.py#L376-L384\\\"\\u003Emiddleware.py\\u003C/a\\u003E. I\\u0026#39;m not the first person to run into this issue, someone has opened an \\u003Ca href=\\\"https://github.com/reddit/reddit/issues/1557\\\"\\u003Eissue on Github\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"When attempting to edit a wiki page on /r/Rowing using PRAW or the web interface I get HTTP 413 errors. In fact when saving through the web interface I get redirected to this message\\n\\n    \\u003Chtml\\u003E\\u003Chead\\u003E\\u003Cscript type='text/javascript'\\u003Eparent.completedUploadImage('failed','','',[['BAD_CSS_NAME', ''], ['IMAGE_ERROR', 'too big. keep it under 500 KiB']],'');\\u003C/script\\u003E\\u003C/head\\u003E\\u003Cbody\\u003Eyou shouldn't be here\\u003C/body\\u003E\\u003C/html\\u003E\\n\\nLooking at the reddit code, it appears to come from [middleware.py](https://github.com/reddit/reddit/blob/master/r2/r2/config/middleware.py#L376-L384). I'm not the first person to run into this issue, someone has opened an [issue on Github](https://github.com/reddit/reddit/issues/1557)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4ir3t1\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"Jammie1\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4ir3t1/413_error_when_attempting_to_edit_wiki_page/\", \"locked\": false, \"name\": \"t3_4ir3t1\", \"created\": 1462939175.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4ir3t1/413_error_when_attempting_to_edit_wiki_page/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"413 Error when attempting to edit wiki page\", \"created_utc\": 1462910375.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EBoth of the following kick back:\\u003C/p\\u003E\\n\\n\\u003Cblockquote\\u003E\\n\\u003Cp\\u003EstatusText: \\u0026quot;Error: The download of the specified resource has failed.\\u0026quot;\\u003C/p\\u003E\\n\\u003C/blockquote\\u003E\\n\\n\\u003Cp\\u003Ewhich has about 5 results on google -_-\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve tried\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E    $.getJSON(page,function(jsonData){\\n        callback(jsonData);\\n    }).error(function(request){\\n        console.log(\\u0026#39;  getPage failed! (un-oauth)\\u0026#39;);\\n        console.log(request);\\n        loadingStatus.innerHTML = \\u0026quot;\\u0026quot;;\\n    });             \\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eand\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E    $.ajax({\\n            url: page,\\n            method: \\u0026quot;GET\\u0026quot;,\\n            dataType: \\u0026quot;json\\u0026quot;,\\n            timeout: 6000,\\n            async: true,\\n        success: function (response) {\\n            console.log(\\u0026#39;  getPage success! (un-oauth)\\u0026#39;);\\n            console.log(response);\\n            callback(response);     \\n        },\\n        error: function (request) { \\n            console.log(\\u0026#39;  getPage failed! (un-oauth, ajax)\\u0026#39;);\\n            console.log(request);\\n        }\\n    }); \\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E(a page url would look something like \\u0026#39;\\u003Ca href=\\\"http://www.reddit.com/r/all.json\\u0026#x27;\\\"\\u003Ehttp://www.reddit.com/r/all.json\\u0026#39;\\u003C/a\\u003E)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eboth kick back this error. This code works fine on both firefox and chrome. the Internet Explorer hate is real right now. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Eedit:\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eto any future archivist/googlers: I\\u0026#39;ve had moderate success changing dataType to \\u0026quot;jsonp\\u0026quot; (\\u003Ca href=\\\"https://www.reddit.com/comments/cxh3a/we_just_added_jsonp_to_reddits_json_api/\\\"\\u003Ejust remember to include \\u0026#39;jsonp=?\\u0026#39; in your url\\u003C/a\\u003E)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Both of the following kick back:\\n\\n\\u003EstatusText: \\\"Error: The download of the specified resource has failed.\\\"\\n\\nwhich has about 5 results on google -_-\\n\\nI've tried\\n\\n\\t\\t$.getJSON(page,function(jsonData){\\n\\t\\t\\tcallback(jsonData);\\n\\t\\t}).error(function(request){\\n\\t\\t\\tconsole.log('  getPage failed! (un-oauth)');\\n\\t\\t\\tconsole.log(request);\\n\\t\\t\\tloadingStatus.innerHTML = \\\"\\\";\\n\\t\\t});\\t\\t\\t\\t\\n\\nand\\n\\n\\t\\t$.ajax({\\n\\t\\t\\t\\turl: page,\\n\\t\\t\\t\\tmethod: \\\"GET\\\",\\n\\t\\t\\t\\tdataType: \\\"json\\\",\\n\\t\\t\\t\\ttimeout: 6000,\\n\\t\\t\\t\\tasync: true,\\n\\t\\t\\tsuccess: function (response) {\\n\\t\\t\\t\\tconsole.log('  getPage success! (un-oauth)');\\n\\t\\t\\t\\tconsole.log(response);\\n\\t\\t\\t\\tcallback(response);\\t\\t\\n\\t\\t\\t},\\n\\t\\t\\terror: function (request) {\\t\\n\\t\\t\\t\\tconsole.log('  getPage failed! (un-oauth, ajax)');\\n\\t\\t\\t\\tconsole.log(request);\\n\\t\\t\\t}\\n\\t\\t});\\t\\n\\n(a page url would look something like 'http://www.reddit.com/r/all.json')\\n\\nboth kick back this error. This code works fine on both firefox and chrome. the Internet Explorer hate is real right now. \\n\\n**edit:**\\n\\nto any future archivist/googlers: I've had moderate success changing dataType to \\\"jsonp\\\" ([just remember to include 'jsonp=?' in your url](https://www.reddit.com/comments/cxh3a/we_just_added_jsonp_to_reddits_json_api/))\\n\\n\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4i3vjo\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"SamSlate\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1462654362.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4i3vjo/javascript_has_anyone_successfully_made_an_ajax/\", \"locked\": false, \"name\": \"t3_4i3vjo\", \"created\": 1462543005.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4i3vjo/javascript_has_anyone_successfully_made_an_ajax/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[javascript] Has anyone successfully made an ajax request in IE to the reddit api?\", \"created_utc\": 1462514205.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAs much for posterity as for the benefit of others, here are the results of my experimentation with using Varnish Cache instead of HAProxy.  This assumes that you\\u0026#39;ve got a functional Varnish instance and are handling SSL\\u2020 termination somewhere/somehow else (such as an instance of nginx).  In my case, these both run on another VM.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u2020 As of this writing, there is a bug in r2/lib/base.py concerning X-Forwarded-For when it has multiple IPs, as will be the case if you\\u0026#39;re using nginx in front of Varnish to terminate HTTPS requests.  I\\u0026#39;ve \\u003Ca href=\\\"https://github.com/reddit/reddit/pull/1600\\\"\\u003Esubmitted a pull request\\u003C/a\\u003E to fix this but, in the meantime, the part of the following solution marked with \\u2020 won\\u0026#39;t work.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBefore installation:\\u003C/p\\u003E\\n\\n\\u003Col\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EDo edit \\u003Ccode\\u003Einstall/install.cfg\\u003C/code\\u003E with valid, appropriate user, group and domain settings.  It does make life a lot simpler because these all get used in several places and it\\u0026#39;s much easier to get it right from the start :)\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EEdit \\u003Ccode\\u003Einstall/install_services.sh\\u003C/code\\u003E and remove haproxy from the list of packages to install (around line 39)\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EEdit \\u003Ccode\\u003Einstall/reddit.sh\\u003C/code\\u003E and comment out the lines that create \\u003Ccode\\u003E/etc/default/haproxy\\u003C/code\\u003E (approx lines 398-400) and also the line that attempts to start HAproxy (approx line 474)\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EOptionally, depending on your use case, comment out the line that enables the nginx SSL termination vhost (around line 378).\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ol\\u003E\\n\\n\\u003Cp\\u003EIn your Varnish VCL, add the following lines, making the appropriate substitution for \\u003Ccode\\u003Efoo.reddit-backend.com\\u003C/code\\u003E for the name of the machine where your reddit instance is located and prepending to any existing routines where they exist already (as \\u003Ccode\\u003Evcl_recv()\\u003C/code\\u003E almost certainly will):\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ebackend reddit {\\n        .host = \\u0026quot;foo.reddit-backend.com\\u0026quot;;\\n        .port = \\u0026quot;8001\\u0026quot;;\\n}\\n\\nbackend reddit_sutro {\\n        .host = \\u0026quot;foo.reddit-backend.com\\u0026quot;;\\n        .port = \\u0026quot;8002\\u0026quot;;\\n}\\n\\nbackend reddit_media {\\n        .host = \\u0026quot;foo.reddit-backend.com\\u0026quot;;\\n        .port = \\u0026quot;9000\\u0026quot;;\\n}\\n\\nbackend reddit_pixel {\\n        .host = \\u0026quot;foo.reddit-backend.com\\u0026quot;;\\n        .port = \\u0026quot;8082\\u0026quot;;\\n}\\n\\nsub vcl_pipe {\\n        if( req.http.upgrade ) {\\n                set bereq.http.upgrade = req.http.upgrade;\\n        }\\n}\\n\\nsub vcl_recv {\\n        # Set the X-Forwarded-For header so the backend can see the original\\n        # IP address. If one is already set by nginx on localhost, use that\\n\\n        # NB: You may already have something like this in your VCL.  If not,\\n        #     define ACL \\u0026quot;upstream_proxy\\u0026quot; and add these lines.\\n        if( req.restarts == 0 ) {\\n                if( client.ip ~ upstream_proxy \\u0026amp;\\u0026amp; req.http.X-Forwarded-For ) {\\n                        set req.http.X-Forwarded-For = req.http.X-Forwarded-For; # \\u2020\\n                } else {\\n                        set req.http.X-Forwarded-For = regsub( client.ip, \\u0026quot;:.*\\u0026quot;, \\u0026quot;\\u0026quot; );\\n                }\\n        }\\n\\n        # Route requests as HAProxy would for just the Reddit instance URL\\n        if( req.http.host ~ \\u0026quot;myredditinstance.com\\u0026quot; ) {\\n                # Route reddit production\\n                if( req.http.Upgrade  ~ \\u0026quot;(?i)websocket\\u0026quot; ) {\\n                        # Send WebSockets to Sutro\\n                        set req.backend_hint = reddit_sutro;\\n                        return( pipe );\\n                } elsif( req.url ~ \\u0026quot;^/media/\\u0026quot; ) {\\n                        # Send media requests to static file server\\n                        set req.backend_hint = reddit_media;\\n                } elsif( req.url ~ \\u0026quot;^/(pixel|click)/\\u0026quot; ) {\\n                        # Send dynamic page/click events verbatim\\n                        set req.backend_hint = reddit_pixel;\\n                        return( pipe );\\n                } else {\\n                        # Send ordinary requests to main app\\n                        set req.backend_hint = reddit;\\n                }\\n        } else {\\n            # Other backend hint logic as required\\n        }\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ERecompile the VCL with \\u003Ccode\\u003Evarnishadm\\u003C/code\\u003E, load it and then install reddit in the usual way.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf your Varnish instance is not running on the same machine as your reddit instance (separate machines recommended) or not on a machine with an IP address in the 10/8 block and you want actual client IP addresses recorded in the reddit db, you\\u0026#39;ll need to \\u003Ca href=\\\"/r/redditdev/comments/4hpjwk/configurable_trusted_proxies_varnish_and/\\\"\\u003Epatch reddit as described here\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAfter installation of reddit, you probably should also edit the \\u003Ccode\\u003Elog_format\\u003C/code\\u003E directive in \\u003Ccode\\u003E/etc/nginx/sites-available/reddit-pixel\\u003C/code\\u003E to replace \\u003Ccode\\u003E$remote_addr\\u003C/code\\u003E with \\u003Ccode\\u003E$http_x_forwarded_for\\u003C/code\\u003E, or your traffic logs will show the IP of your Varnish box rather than the true client\\u0026#39;s IP.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EVarnish works best when it can cache whole CSS, JS and images rather than less fragments (which then apparently have to be compiled into CSS client-side), so it\\u0026#39;s probably worth giving \\u003Ca href=\\\"/r/redditdev/comments/2ki4id/turn_off_debug_mode_fresh_install/cllkc1r\\\"\\u003Ethis comment a look\\u003C/a\\u003E.  It certainly sped things up for me considerably.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"As much for posterity as for the benefit of others, here are the results of my experimentation with using Varnish Cache instead of HAProxy.  This assumes that you've got a functional Varnish instance and are handling SSL\\u2020 termination somewhere/somehow else (such as an instance of nginx).  In my case, these both run on another VM.\\n\\n\\u2020 As of this writing, there is a bug in r2/lib/base.py concerning X-Forwarded-For when it has multiple IPs, as will be the case if you're using nginx in front of Varnish to terminate HTTPS requests.  I've [submitted a pull request](https://github.com/reddit/reddit/pull/1600) to fix this but, in the meantime, the part of the following solution marked with \\u2020 won't work.\\n\\nBefore installation:\\n\\n0.  Do edit `install/install.cfg` with valid, appropriate user, group and domain settings.  It does make life a lot simpler because these all get used in several places and it's much easier to get it right from the start :)\\n\\n1.  Edit `install/install_services.sh` and remove haproxy from the list of packages to install (around line 39)\\n\\n2.  Edit `install/reddit.sh` and comment out the lines that create `/etc/default/haproxy` (approx lines 398-400) and also the line that attempts to start HAproxy (approx line 474)\\n\\n3.  Optionally, depending on your use case, comment out the line that enables the nginx SSL termination vhost (around line 378).\\n\\nIn your Varnish VCL, add the following lines, making the appropriate substitution for `foo.reddit-backend.com` for the name of the machine where your reddit instance is located and prepending to any existing routines where they exist already (as `vcl_recv()` almost certainly will):\\n\\n    backend reddit {\\n            .host = \\\"foo.reddit-backend.com\\\";\\n            .port = \\\"8001\\\";\\n    }\\n\\n    backend reddit_sutro {\\n            .host = \\\"foo.reddit-backend.com\\\";\\n            .port = \\\"8002\\\";\\n    }\\n    \\n    backend reddit_media {\\n            .host = \\\"foo.reddit-backend.com\\\";\\n            .port = \\\"9000\\\";\\n    }\\n    \\n    backend reddit_pixel {\\n            .host = \\\"foo.reddit-backend.com\\\";\\n            .port = \\\"8082\\\";\\n    }\\n    \\n    sub vcl_pipe {\\n            if( req.http.upgrade ) {\\n                    set bereq.http.upgrade = req.http.upgrade;\\n            }\\n    }\\n    \\n    sub vcl_recv {\\n            # Set the X-Forwarded-For header so the backend can see the original\\n            # IP address. If one is already set by nginx on localhost, use that\\n\\n            # NB: You may already have something like this in your VCL.  If not,\\n            #     define ACL \\\"upstream_proxy\\\" and add these lines.\\n            if( req.restarts == 0 ) {\\n                    if( client.ip ~ upstream_proxy \\u0026\\u0026 req.http.X-Forwarded-For ) {\\n                            set req.http.X-Forwarded-For = req.http.X-Forwarded-For; # \\u2020\\n                    } else {\\n                            set req.http.X-Forwarded-For = regsub( client.ip, \\\":.*\\\", \\\"\\\" );\\n                    }\\n            }\\n    \\n            # Route requests as HAProxy would for just the Reddit instance URL\\n            if( req.http.host ~ \\\"myredditinstance.com\\\" ) {\\n                    # Route reddit production\\n                    if( req.http.Upgrade  ~ \\\"(?i)websocket\\\" ) {\\n                            # Send WebSockets to Sutro\\n                            set req.backend_hint = reddit_sutro;\\n                            return( pipe );\\n                    } elsif( req.url ~ \\\"^/media/\\\" ) {\\n                            # Send media requests to static file server\\n                            set req.backend_hint = reddit_media;\\n                    } elsif( req.url ~ \\\"^/(pixel|click)/\\\" ) {\\n                            # Send dynamic page/click events verbatim\\n                            set req.backend_hint = reddit_pixel;\\n                            return( pipe );\\n                    } else {\\n                            # Send ordinary requests to main app\\n                            set req.backend_hint = reddit;\\n                    }\\n            } else {\\n                # Other backend hint logic as required\\n            }\\n\\nRecompile the VCL with `varnishadm`, load it and then install reddit in the usual way.\\n\\nIf your Varnish instance is not running on the same machine as your reddit instance (separate machines recommended) or not on a machine with an IP address in the 10/8 block and you want actual client IP addresses recorded in the reddit db, you'll need to [patch reddit as described here](/r/redditdev/comments/4hpjwk/configurable_trusted_proxies_varnish_and/)\\n\\nAfter installation of reddit, you probably should also edit the `log_format` directive in `/etc/nginx/sites-available/reddit-pixel` to replace `$remote_addr` with `$http_x_forwarded_for`, or your traffic logs will show the IP of your Varnish box rather than the true client's IP.\\n\\nVarnish works best when it can cache whole CSS, JS and images rather than less fragments (which then apparently have to be compiled into CSS client-side), so it's probably worth giving [this comment a look](/r/redditdev/comments/2ki4id/turn_off_debug_mode_fresh_install/cllkc1r).  It certainly sped things up for me considerably.\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4hu4oc\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"StrixTechnica\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1462454751.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4hu4oc/howto_use_varnish_cache_in_lieu_of_haproxy/\", \"locked\": false, \"name\": \"t3_4hu4oc\", \"created\": 1462391839.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4hu4oc/howto_use_varnish_cache_in_lieu_of_haproxy/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"HOWTO: Use Varnish Cache in lieu of HAProxy\", \"created_utc\": 1462363039.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cpre\\u003E\\u003Ccode\\u003Esublist = data[\\u0026quot;userSubmissions\\u0026quot;][submission.author.name]\\nfor s in sublist:\\n    submissionlinks += s.permalink + \\u0026quot;\\\\n\\\\n\\u0026quot;\\n    s.remove(False)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThe line \\u003Ccode\\u003Es.remove(False)\\u003C/code\\u003E is giving the exception in the title - \\u003Ccode\\u003EAttributeError: \\u0026#39;Session\\u0026#39; object has no attribute \\u0026#39;validate_certs\\u0026#39;\\u003C/code\\u003E. The submission isn\\u0026#39;t already removed or deleted, and the script is logged in as a moderator using the oauth2 login.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003Edata\\u003C/code\\u003E is a \\u003Ca href=\\\"https://docs.python.org/3.5/library/shelve.html\\\"\\u003EShelve\\u003C/a\\u003E object (basically a dict but lives on disk instead of in memory. I said basically please don\\u0026#39;t kill me).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003Edata[\\u0026quot;userSubmissions\\u0026quot;]\\u003C/code\\u003E is a dict with usernames as keys and a list of Submission objects as a value. I\\u0026#39;ve checked that the submission objects in here are correct far more than is rational.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ccode\\u003Edata[\\u0026quot;userSubmissions\\u0026quot;][submission.author.name]\\u003C/code\\u003E is a list of submission objects submitted recently by a single user to a single subreddit. Again, I\\u0026#39;ve checked far more than is rational that this list is correct. I know for a fact that \\u003Ccode\\u003Es\\u003C/code\\u003E is not the problem because line 3 in my example adds the correct permalink to \\u003Ccode\\u003Esubmissionlinks\\u003C/code\\u003E, so the Submission object is fine. I\\u0026#39;ve also inspected it using Visual Studio\\u0026#39;s fantastic debugging tools, and again found no problems.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have other lines elsewhere in my script that say \\u003Ccode\\u003Esubmission.remove(False)\\u003C/code\\u003E and they work perfectly. I\\u0026#39;ve tried \\u003Ccode\\u003Es.remove()\\u003C/code\\u003E and \\u003Ccode\\u003Es.remove(True)\\u003C/code\\u003E, but it\\u0026#39;s not my line of code that\\u0026#39;s failing - it\\u0026#39;s PRAW\\u0026#39;s, and I don\\u0026#39;t know why. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EMy full stack trace is \\u003Ca href=\\\"http://i.imgur.com/2fcUwFQ.jpg\\\"\\u003Ehere\\u003C/a\\u003E. All I can conclude is that this is an issue with PRAW. I\\u0026#39;ve run the code multiple times and that one line keeps giving that same exception. I\\u0026#39;m completely stuck, and any help would be greatly appreciated.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m using python 3.5.1 on Windows 7, with the latest (non-beta) version of PRAW, as confirmed by \\u003Ccode\\u003Epip install praw --upgrade\\u003C/code\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"    sublist = data[\\\"userSubmissions\\\"][submission.author.name]\\n    for s in sublist:\\n        submissionlinks += s.permalink + \\\"\\\\n\\\\n\\\"\\n        s.remove(False)\\n\\nThe line `s.remove(False)` is giving the exception in the title - `AttributeError: 'Session' object has no attribute 'validate_certs'`. The submission isn't already removed or deleted, and the script is logged in as a moderator using the oauth2 login.\\n\\n`data` is a [Shelve](https://docs.python.org/3.5/library/shelve.html) object (basically a dict but lives on disk instead of in memory. I said basically please don't kill me).\\n\\n`data[\\\"userSubmissions\\\"]` is a dict with usernames as keys and a list of Submission objects as a value. I've checked that the submission objects in here are correct far more than is rational.\\n\\n`data[\\\"userSubmissions\\\"][submission.author.name]` is a list of submission objects submitted recently by a single user to a single subreddit. Again, I've checked far more than is rational that this list is correct. I know for a fact that `s` is not the problem because line 3 in my example adds the correct permalink to `submissionlinks`, so the Submission object is fine. I've also inspected it using Visual Studio's fantastic debugging tools, and again found no problems.\\n\\nI have other lines elsewhere in my script that say `submission.remove(False)` and they work perfectly. I've tried `s.remove()` and `s.remove(True)`, but it's not my line of code that's failing - it's PRAW's, and I don't know why. \\n\\nMy full stack trace is [here](http://i.imgur.com/2fcUwFQ.jpg). All I can conclude is that this is an issue with PRAW. I've run the code multiple times and that one line keeps giving that same exception. I'm completely stuck, and any help would be greatly appreciated.\\n\\nI'm using python 3.5.1 on Windows 7, with the latest (non-beta) version of PRAW, as confirmed by `pip install praw --upgrade`\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4fjowq\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"theonefoster\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1461101232.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4fjowq/praw_when_removing_a_submission_i_get/\", \"locked\": false, \"name\": \"t3_4fjowq\", \"created\": 1461127728.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4fjowq/praw_when_removing_a_submission_i_get/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[PRAW] - when removing a submission, I get \\\"AttributeError: 'Session' object has no attribute 'validate_certs'\\\" - Any ideas?\", \"created_utc\": 1461098928.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI know there\\u0026#39;s praw-multiprocess for multiple processes but what about multiple threads all running on one PRAW Reddit instance? Is it safe or should I make a new reddit instance per thread?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I know there's praw-multiprocess for multiple processes but what about multiple threads all running on one PRAW Reddit instance? Is it safe or should I make a new reddit instance per thread?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4ffnh7\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"Santi871\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4ffnh7/praw_is_praw_multithread_safe/\", \"locked\": false, \"name\": \"t3_4ffnh7\", \"created\": 1461064142.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4ffnh7/praw_is_praw_multithread_safe/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[PRAW] is PRAW multithread safe?\", \"created_utc\": 1461035342.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ECase in point, this name, without the underscore. I can\\u0026#39;t sign up with that name, but \\u003Ca href=\\\"/u/mckenziefriend\\\"\\u003E/u/mckenziefriend\\u003C/a\\u003E comes back \\u0026quot;page not found\\u0026quot;.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Case in point, this name, without the underscore. I can't sign up with that name, but /u/mckenziefriend comes back \\\"page not found\\\".\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4epn9q\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4epn9q/why_does_reddit_sometimes_say_a_username_is/\", \"locked\": false, \"name\": \"t3_4epn9q\", \"created\": 1460638424.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4epn9q/why_does_reddit_sometimes_say_a_username_is/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Why does Reddit sometimes say a username is \\\"already taken\\\", but /u/\\u003Cthat username\\u003E says \\\"not found\\\"?\", \"created_utc\": 1460609624.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHere\\u0026#39;s what I have so far:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport praw\\nuseragent = \\u0026#39;image downloader by /u/flame_in_darkness\\u0026#39;\\nr = praw.Reddit(useragent)\\nsubreddit_name = \\u0026#39;all\\u0026#39;\\nwhile subreddit_name != \\u0026#39; \\u0026#39;:\\n    subreddit_name = input(\\u0026quot;Enter subreddit name, enter a space to stop: \\u0026quot;)\\nsubmissions = r.get_subreddit(subreddit_name).get_hot(limit=50)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EBasically I want to display titles and images from Imgur posts. I\\u0026#39;ve been looking at the documentation and I see that I can use \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eget_content\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Ebut I don\\u0026#39;t really understand all the parameters that go with that and how to manipulate them. Basically I have this school project and it is way over my head but I\\u0026#39;m trying to get it done so I have to do this.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Here's what I have so far:\\n\\n    import praw\\n    useragent = 'image downloader by /u/flame_in_darkness'\\n    r = praw.Reddit(useragent)\\n    subreddit_name = 'all'\\n    while subreddit_name != ' ':\\n        subreddit_name = input(\\\"Enter subreddit name, enter a space to stop: \\\")\\n    submissions = r.get_subreddit(subreddit_name).get_hot(limit=50)\\n\\nBasically I want to display titles and images from Imgur posts. I've been looking at the documentation and I see that I can use \\n\\n    get_content\\n\\nbut I don't really understand all the parameters that go with that and how to manipulate them. Basically I have this school project and it is way over my head but I'm trying to get it done so I have to do this.\\n\\n    \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4ehzqi\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"flame_in_darkness\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4ehzqi/download_and_display_images/\", \"locked\": false, \"name\": \"t3_4ehzqi\", \"created\": 1460521306.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4ehzqi/download_and_display_images/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"download and display images\", \"created_utc\": 1460492506.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EPRAW technically does this (I think?), but it takes way too long, and I\\u0026#39;d at least like to know how this works.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve been trying to use\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehttps://oauth.reddit.com/api/morechildren.json?api_type=json\\u0026amp;showmore=true\\u0026amp;link_id=\\u0026lt;submission_id\\u0026gt;\\u0026amp;children=\\u0026lt;morechildrenResultId\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Ealongside\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehttps://oauth.reddit.com/api/info.json?id=\\u0026lt;full_id\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eand \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehttps://oauth.reddit.com/r/\\u0026lt;subreddit\\u0026gt;/comments/article?\\u0026amp;showmore=true\\u0026amp;article=\\u0026lt;article_id\\u0026gt;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EAnd my method is to simply pull as much as possible at first via /comments/article, then expand each thing of kind \\u0026#39;more\\u0026#39; using /morechildren and store all of those results. After I do this I do the same thing for every comment\\u0026#39;s replies, and then recurse.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there a reason this isn\\u0026#39;t getting me everything? Is there something I\\u0026#39;m missing/a simpler way to do this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"PRAW technically does this (I think?), but it takes way too long, and I'd at least like to know how this works.\\n\\n\\nI've been trying to use\\n\\n    https://oauth.reddit.com/api/morechildren.json?api_type=json\\u0026showmore=true\\u0026link_id=\\u003Csubmission_id\\u003E\\u0026children=\\u003CmorechildrenResultId\\u003E\\n\\nalongside\\n\\n    https://oauth.reddit.com/api/info.json?id=\\u003Cfull_id\\u003E\\n\\nand \\n\\n    https://oauth.reddit.com/r/\\u003Csubreddit\\u003E/comments/article?\\u0026showmore=true\\u0026article=\\u003Carticle_id\\u003E\\n\\n\\nAnd my method is to simply pull as much as possible at first via /comments/article, then expand each thing of kind 'more' using /morechildren and store all of those results. After I do this I do the same thing for every comment's replies, and then recurse.\\n\\nIs there a reason this isn't getting me everything? Is there something I'm missing/a simpler way to do this?\\n\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4d0i23\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"Phylliida\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4d0i23/how_can_you_pull_all_the_comments_from_a_single/\", \"locked\": false, \"name\": \"t3_4d0i23\", \"created\": 1459608311.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4d0i23/how_can_you_pull_all_the_comments_from_a_single/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How can you pull all the comments from a single thread?\", \"created_utc\": 1459579511.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m a complete beginner in regard to development. Only a few months of casual practice under my belt. That being said I\\u0026#39;ve got an idea I want to try with the reddit source code. I\\u0026#39;ve run the reddit source code install script on my Ubuntu machine and it seems to have been successful but I\\u0026#39;m not really sure how to open the result in the browser. The reddit github site says the default domain is \\u003Ca href=\\\"http://reddit.local/\\\"\\u003Ehttp://reddit.local/\\u003C/a\\u003E but that yields a \\u0026quot;server not found\\u0026quot; error. I didn\\u0026#39;t make any changes to the code yet so I would imagine the default location would be unchanged. Thanks in advance for any assistance. Forgive my ignorance.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm a complete beginner in regard to development. Only a few months of casual practice under my belt. That being said I've got an idea I want to try with the reddit source code. I've run the reddit source code install script on my Ubuntu machine and it seems to have been successful but I'm not really sure how to open the result in the browser. The reddit github site says the default domain is http://reddit.local/ but that yields a \\\"server not found\\\" error. I didn't make any changes to the code yet so I would imagine the default location would be unchanged. Thanks in advance for any assistance. Forgive my ignorance.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4ccjcs\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"bywithalittlehelp\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4ccjcs/wannabe_developer_here_would_somebody_help_me/\", \"locked\": false, \"name\": \"t3_4ccjcs\", \"created\": 1459232870.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4ccjcs/wannabe_developer_here_would_somebody_help_me/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Wannabe developer here. Would somebody help me open the reddit source code in my browser?\", \"created_utc\": 1459204070.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EBelow is an angular $http request that gets a response payload using reddit\\u0026#39;s \\u003Ca href=\\\"https://www.reddit.com/dev/api#GET_search\\\"\\u003Esearch api\\u003C/a\\u003E  --I get the accessToken from a successful request to         \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehttps://www.reddit.com/api/v1/access_token\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eresults in\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E{\\u0026quot;access_token\\u0026quot;: \\u0026quot;-_3SkdkoxmAitbxiykFOup1_SVaM\\u0026quot;, \\u0026quot;token_type\\u0026quot;: \\u0026quot;bearer\\u0026quot;, \\u0026quot;expires_in\\u0026quot;: 3600, \\u0026quot;scope\\u0026quot;: \\u0026quot;*\\u0026quot;}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Ebut when I try to add accessToken to the request it doesn\\u0026#39;t return a response\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Evar deferred = $q.defer();\\n\\nvar url = REDDIT_SEARCH_URL;\\n\\n$http({\\n    method: \\u0026#39;GET\\u0026#39;,\\n    url: url,\\n    headers: {\\n      \\u0026#39;Content-Type\\u0026#39;: \\u0026#39;application/json\\u0026#39;,\\n      //\\u0026#39;Authorization\\u0026#39;: \\u0026#39;bearer \\u0026#39; + accessToken, //If I uncomment this line I get an error\\n      \\u0026#39;User-Agent\\u0026#39;: \\u0026#39;rwar\\u0026#39;\\n    },\\n    timeout: 30000,\\n    cache: false\\n  })\\n  .success(function(results) {\\n    //console.log(\\u0026#39;results.data.children \\u0026#39; + JSON.stringify(results.data.children));\\n    deferred.resolve(results.data.children);\\n  })\\n  .error(function(results) {\\n    console.log(\\u0026#39;error \\u0026#39; + results);\\n    deferred.reject(results);\\n\\n  })\\n\\nreturn deferred.promise;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eif I comment out the \\u0026#39;Authorization\\u0026#39; header, the request succeeds - any ideas why this is happening?  Thanks for any suggestions.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Below is an angular $http request that gets a response payload using reddit's [search api]( https://www.reddit.com/dev/api#GET_search)  --I get the accessToken from a successful request to         \\n\\n\\n    https://www.reddit.com/api/v1/access_token\\n\\nresults in\\n\\n    {\\\"access_token\\\": \\\"-_3SkdkoxmAitbxiykFOup1_SVaM\\\", \\\"token_type\\\": \\\"bearer\\\", \\\"expires_in\\\": 3600, \\\"scope\\\": \\\"*\\\"}\\n\\nbut when I try to add accessToken to the request it doesn't return a response\\n\\n\\n    var deferred = $q.defer();\\n\\n    var url = REDDIT_SEARCH_URL;\\n\\n    $http({\\n        method: 'GET',\\n        url: url,\\n        headers: {\\n          'Content-Type': 'application/json',\\n          //'Authorization': 'bearer ' + accessToken, //If I uncomment this line I get an error\\n          'User-Agent': 'rwar'\\n        },\\n        timeout: 30000,\\n        cache: false\\n      })\\n      .success(function(results) {\\n        //console.log('results.data.children ' + JSON.stringify(results.data.children));\\n        deferred.resolve(results.data.children);\\n      })\\n      .error(function(results) {\\n        console.log('error ' + results);\\n        deferred.reject(results);\\n\\n      })\\n\\n    return deferred.promise;\\n\\n\\nif I comment out the 'Authorization' header, the request succeeds - any ideas why this is happening?  Thanks for any suggestions.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4c3vw1\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"redmond007\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1459043927.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4c3vw1/how_to_use_access_token_in_search_requests/\", \"locked\": false, \"name\": \"t3_4c3vw1\", \"created\": 1459071646.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4c3vw1/how_to_use_access_token_in_search_requests/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to use access token in search requests\", \"created_utc\": 1459042846.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m having trouble finding this. Say I\\u0026#39;m in a subreddit that supports non-moderators setting link flair on a post. On the desktop site I can post, and then click flair, and set it. But I cannot seem to find this list of available template flairs anywhere in the API. Am I missing it entirely or is it missing from the API?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm having trouble finding this. Say I'm in a subreddit that supports non-moderators setting link flair on a post. On the desktop site I can post, and then click flair, and set it. But I cannot seem to find this list of available template flairs anywhere in the API. Am I missing it entirely or is it missing from the API?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4brzh9\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"iamthatis\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1458833041.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4brzh9/is_there_a_way_to_see_subreddit_link_flair_for_a/\", \"locked\": false, \"name\": \"t3_4brzh9\", \"created\": 1458861549.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4brzh9/is_there_a_way_to_see_subreddit_link_flair_for_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there a way to see subreddit link flair for a subreddit you're not a moderator of through the API?\", \"created_utc\": 1458832749.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have a quick question regarding correct usage of the API when a user is logged out. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWith authenticated users I am using the \\u0026#39;\\u003Ca href=\\\"https://oauth.reddit.com\\\"\\u003Ehttps://oauth.reddit.com\\u003C/a\\u003E\\u0026#39; URL to make requests to reddit. However, when a user has not logged in I use the standard \\u0026#39;\\u003Ca href=\\\"https://www.reddit.com\\\"\\u003Ehttps://www.reddit.com\\u003C/a\\u003E\\u0026#39; URL and append \\u0026#39;.json\\u0026#39; to access subreddit posts and fetch comments etc.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs this the correct way to access reddit when a user hasn\\u0026#39;t logged in and does not have an OAuth token? I remember reading about an \\u0026#39;anonymous\\u0026#39; OAuth system for logged out users but cant seem to find anything about it any more.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi\\n\\nI have a quick question regarding correct usage of the API when a user is logged out. \\n\\nWith authenticated users I am using the 'https://oauth.reddit.com' URL to make requests to reddit. However, when a user has not logged in I use the standard 'https://www.reddit.com' URL and append '.json' to access subreddit posts and fetch comments etc.\\n\\nIs this the correct way to access reddit when a user hasn't logged in and does not have an OAuth token? I remember reading about an 'anonymous' OAuth system for logged out users but cant seem to find anything about it any more.\\n\\nThanks!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4b3fnx\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"JMCoo\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4b3fnx/accessing_the_api_with_logged_out_users/\", \"locked\": false, \"name\": \"t3_4b3fnx\", \"created\": 1458429329.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4b3fnx/accessing_the_api_with_logged_out_users/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Accessing the API with logged out users\", \"created_utc\": 1458400529.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIf I\\u0026#39;m logged in as me (and I can make a regular browser request to \\u003Ca href=\\\"https://reddit.com/user/SandyRegolith/saved/.json\\\"\\u003Ehttps://reddit.com/user/SandyRegolith/saved/.json\\u003C/a\\u003E just fine, shouldn\\u0026#39;t I be able to do the same with AJAX? Either in a bookmarklet or by pasting into the Console?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI get a 403 error or a TOO_MANY_REDIRECTS error depending if I use my actual name or \\u003Ccode\\u003Eme\\u003C/code\\u003E in the URL.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003EEDIT\\u003C/strong\\u003E: In the browser, \\u003Ca href=\\\"https://www.reddit.com/user/sandyregolith/saved.json?jsonp=foo\\\"\\u003Ehttps://www.reddit.com/user/sandyregolith/saved.json?jsonp=foo\\u003C/a\\u003E is a 403, althought \\u003Ca href=\\\"https://www.reddit.com/user/sandyregolith/saved.json\\\"\\u003Ehttps://www.reddit.com/user/sandyregolith/saved.json\\u003C/a\\u003E without the jsonp parameter is fine.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"If I'm logged in as me (and I can make a regular browser request to https://reddit.com/user/SandyRegolith/saved/.json just fine, shouldn't I be able to do the same with AJAX? Either in a bookmarklet or by pasting into the Console?\\n\\nI get a 403 error or a TOO_MANY_REDIRECTS error depending if I use my actual name or `me` in the URL.\\n\\n**EDIT**: In the browser, https://www.reddit.com/user/sandyregolith/saved.json?jsonp=foo is a 403, althought https://www.reddit.com/user/sandyregolith/saved.json without the jsonp parameter is fine.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4a1zdt\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"[deleted]\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 12, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1457753789.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4a1zdt/should_i_be_able_to_make_and_ajax_call_to_usermy/\", \"locked\": false, \"name\": \"t3_4a1zdt\", \"created\": 1457777676.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4a1zdt/should_i_be_able_to_make_and_ajax_call_to_usermy/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Should I be able to make and AJAX call to /user/\\u003Cmy username\\u003E/saved/.json with a logged-in browser?\", \"created_utc\": 1457748876.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been reading that Amazon cloud search is what is being used on reddit.com now, but when I look at the git source code I see a solr directory with instructions on installing it. Has anyone tried using it or have any information on it?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have found that port 8080 did not work on my installation of solr so I switched to port 8983 in \\u003Ccode\\u003E/var/lib/tomcat6/conf/server.xml\\u003C/code\\u003E and \\u003Ccode\\u003E/src/reddit/r2/example.ini\\u003C/code\\u003E followed by running the \\u003Ccode\\u003Emake ini\\u003C/code\\u003E command.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe command \\u003Ccode\\u003Esudo ln -s /path/to/reddit/solr/schema.xml /usr/share/solr/conf\\u003C/code\\u003E also told me the file already exists so I had to delete the original file and run the command again.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E*When finished editing files, run \\u0026quot;service tomcat 6 restart\\u0026quot;.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOnce I made sure solr was running on \\u003Ca href=\\\"http://YOUR.DOMAIN:8983\\\"\\u003Ehttp://YOUR.DOMAIN:8983\\u003C/a\\u003E I checked my clone to see if the search function was working and it was. However it was not indexing any new posts or comments so I added these two lines to \\u003Ccode\\u003E/etc/cron.d/reddit\\u003C/code\\u003E\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003E* * * * root cd /home/YOURUSER/src/reddit/r2; paster run run.ini -c \\u0026#39;import r2.lib.providers.search.solr as cs; cs.rebuild_subreddit_index()\\u0026#39;\\u003C/li\\u003E\\n\\u003Cli\\u003E* * * * root cd /home/YOURUSER/src/reddit/r2; paster run run.ini -c \\u0026#39;import r2.lib.providers.search.solr as cs; cs._rebuild_link_index()\\u0026#39;\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been reading that Amazon cloud search is what is being used on reddit.com now, but when I look at the git source code I see a solr directory with instructions on installing it. Has anyone tried using it or have any information on it?\\n\\n\\n\\n\\nEdit:\\n\\n\\nI have found that port 8080 did not work on my installation of solr so I switched to port 8983 in `/var/lib/tomcat6/conf/server.xml` and `/src/reddit/r2/example.ini` followed by running the `make ini` command.\\n\\n\\nThe command `sudo ln -s /path/to/reddit/solr/schema.xml /usr/share/solr/conf` also told me the file already exists so I had to delete the original file and run the command again.\\n\\n\\n*When finished editing files, run \\\"service tomcat 6 restart\\\".\\n\\n\\nOnce I made sure solr was running on http://YOUR.DOMAIN:8983 I checked my clone to see if the search function was working and it was. However it was not indexing any new posts or comments so I added these two lines to `/etc/cron.d/reddit`\\n\\n\\n* * * * * root cd /home/YOURUSER/src/reddit/r2; paster run run.ini -c 'import r2.lib.providers.search.solr as cs; cs.rebuild_subreddit_index()'\\n* * * * * root cd /home/YOURUSER/src/reddit/r2; paster run run.ini -c 'import r2.lib.providers.search.solr as cs; cs._rebuild_link_index()'\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"49ijys\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"jumpstartdash\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1457531131.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/49ijys/if_solr_is_no_longer_used_for_the_search_engine/\", \"locked\": false, \"name\": \"t3_49ijys\", \"created\": 1457473727.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/49ijys/if_solr_is_no_longer_used_for_the_search_engine/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"If Solr is no longer used for the search engine, why does it remain in the repository?\", \"created_utc\": 1457444927.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m currently trying to run the installation script from \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu\\\"\\u003Ehttps://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu\\u003C/a\\u003E on a fresh installation of Ubuntu 14.04 on a VPS.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ESteps 1 and 2 went fine (getting install-reddit.sh and making it executable). I run ./install-reddit.sh, and then I receive the following:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIt looks like you\\u0026#39;re installing without a local repo.  No problem!\\nWe\\u0026#39;re going to grab the scripts we need and show you where you can\\nedit the config to suit your environment.\\nGrabbing \\u0026#39;done.sh\\u0026#39;...\\nGrabbing \\u0026#39;install_apt.sh\\u0026#39;...\\nGrabbing \\u0026#39;install_cassandra.sh\\u0026#39;...\\nGrabbing \\u0026#39;install_services.sh\\u0026#39;...\\nGrabbing \\u0026#39;reddit.sh\\u0026#39;...\\nGrabbing \\u0026#39;setup_cassandra.sh\\u0026#39;...\\nGrabbing \\u0026#39;setup_mcrouter.sh\\u0026#39;...\\nGrabbing \\u0026#39;setup_postgres.sh\\u0026#39;...\\nGrabbing \\u0026#39;setup_rabbitmq.sh\\u0026#39;...\\nGrabbing \\u0026#39;travis.sh\\u0026#39;...\\nDone!\\u003C/p\\u003E\\n\\n\\u003Ch1\\u003EBase configuration:\\u003C/h1\\u003E\\n\\n\\u003Ch6\\u003E#################################################################\\u003C/h6\\u003E\\n\\n\\u003Cp\\u003E./install-reddit.sh: line 86: ./install/install.cfg: No such file or directory\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnd nothing else. I\\u0026#39;ve looked at google for others with similar problems but didn\\u0026#39;t find anything. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPlease help.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm currently trying to run the installation script from https://github.com/reddit/reddit/wiki/reddit-install-script-for-Ubuntu on a fresh installation of Ubuntu 14.04 on a VPS.\\n\\nSteps 1 and 2 went fine (getting install-reddit.sh and making it executable). I run ./install-reddit.sh, and then I receive the following:\\n\\nIt looks like you're installing without a local repo.  No problem!\\nWe're going to grab the scripts we need and show you where you can\\nedit the config to suit your environment.\\nGrabbing 'done.sh'...\\nGrabbing 'install_apt.sh'...\\nGrabbing 'install_cassandra.sh'...\\nGrabbing 'install_services.sh'...\\nGrabbing 'reddit.sh'...\\nGrabbing 'setup_cassandra.sh'...\\nGrabbing 'setup_mcrouter.sh'...\\nGrabbing 'setup_postgres.sh'...\\nGrabbing 'setup_rabbitmq.sh'...\\nGrabbing 'travis.sh'...\\nDone!\\n#######################################################################\\n# Base configuration:\\n#######################################################################   \\n./install-reddit.sh: line 86: ./install/install.cfg: No such file or directory\\n\\nAnd nothing else. I've looked at google for others with similar problems but didn't find anything. \\n\\nPlease help.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4964fj\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"jumpstartdash\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1457280840.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4964fj/cant_find_an_important_file_in_an_installation/\", \"locked\": false, \"name\": \"t3_4964fj\", \"created\": 1457272184.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4964fj/cant_find_an_important_file_in_an_installation/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Can't find an important file in an installation script (ubuntu 14.04)\", \"created_utc\": 1457243384.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Cstrong\\u003EUPDATE:\\u003C/strong\\u003E If I call /api/morechildren/\\u003Cstrong\\u003E.json\\u003C/strong\\u003E then I get the usual comment json.  Am I having a stroke?  There\\u0026#39;s no mention of this in the api documentation, and I don\\u0026#39;t think I needed to do this before a few days ago.  But ok, problem \\u0026#39;solved\\u0026#39;.\\u003C/p\\u003E\\n\\n\\u003Chr/\\u003E\\n\\n\\u003Cp\\u003E\\u003Cem\\u003EOriginal post for posterity\\u003C/em\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhenever I\\u0026#39;m having problems, it\\u0026#39;s usually my fault.  I know at one point I was getting response JSON back from \\u003Ccode\\u003Emorechildren\\u003C/code\\u003E that I expected.  But now I\\u0026#39;m getting JSON that I don\\u0026#39;t understand.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI ask for comments in this \\u003Ca href=\\\"/r/science\\\"\\u003E/r/science\\u003C/a\\u003E thread:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehttps://www.reddit.com/r/science/comments/489xei/science_ama_series_hi_reddit_im_noaa_scientist/.json\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/r/science/comments/489xei/science_ama_series_hi_reddit_im_noaa_scientist/.json\\\"\\u003EClick\\u003C/a\\u003E for the json from reddit.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOne of the first \\u003Ccode\\u003Emore\\u003C/code\\u003E nodes I get is this one:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E              {\\n                \\u0026quot;kind\\u0026quot;: \\u0026quot;more\\u0026quot;,\\n                \\u0026quot;data\\u0026quot;: {\\n                  \\u0026quot;count\\u0026quot;: 1,\\n                  \\u0026quot;parent_id\\u0026quot;: \\u0026quot;t1_d0hykrt\\u0026quot;,\\n                  \\u0026quot;id\\u0026quot;: \\u0026quot;d0i1jw0\\u0026quot;,\\n                  \\u0026quot;name\\u0026quot;: \\u0026quot;t1_d0i1jw0\\u0026quot;,\\n                  \\u0026quot;children\\u0026quot;: [\\n                    \\u0026quot;d0i1jw0\\u0026quot;\\n                  ]\\n                }\\n              }\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EThen I ask for \\u003Ccode\\u003Ed0i1jw0\\u003C/code\\u003E using \\u003Ccode\\u003Emorechildren\\u003C/code\\u003E:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehttps://www.reddit.com/api/morechildren?api_type=json\\u0026amp;link_id=t3_489xei\\u0026amp;sort=old\\u0026amp;children=d0i1jw0\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/api/morechildren?api_type=json\\u0026amp;link_id=t3_489xei\\u0026amp;sort=old\\u0026amp;children=d0i1jw0\\\"\\u003EClick\\u003C/a\\u003E for the json from reddit, or see a \\u003Ca href=\\\"http://pastebin.com/KedgrDza\\\"\\u003Epastebin\\u003C/a\\u003E of what I get.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBut I don\\u0026#39;t get comment JSON back, no \\u003Ccode\\u003Eauthor\\u003C/code\\u003E, no \\u003Ccode\\u003Ecreated_UTC\\u003C/code\\u003E.  There is \\u003Ccode\\u003EcontextText\\u003C/code\\u003E that looks like comment text.  But there\\u0026#39;s this markdown or HTML in a \\u003Ccode\\u003Econtent\\u003C/code\\u003E field, as if it were built for the web client.  If someone can spot a dumb mistake of mine, please tell me.  Thanks!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"**UPDATE:** If I call /api/morechildren/**.json** then I get the usual comment json.  Am I having a stroke?  There's no mention of this in the api documentation, and I don't think I needed to do this before a few days ago.  But ok, problem 'solved'.\\n\\n---\\n*Original post for posterity*\\n\\nWhenever I'm having problems, it's usually my fault.  I know at one point I was getting response JSON back from `morechildren` that I expected.  But now I'm getting JSON that I don't understand.\\n\\nI ask for comments in this /r/science thread:\\n\\n    https://www.reddit.com/r/science/comments/489xei/science_ama_series_hi_reddit_im_noaa_scientist/.json\\n\\n[Click](https://www.reddit.com/r/science/comments/489xei/science_ama_series_hi_reddit_im_noaa_scientist/.json) for the json from reddit.\\n\\nOne of the first `more` nodes I get is this one:\\n\\n                  {\\n                    \\\"kind\\\": \\\"more\\\",\\n                    \\\"data\\\": {\\n                      \\\"count\\\": 1,\\n                      \\\"parent_id\\\": \\\"t1_d0hykrt\\\",\\n                      \\\"id\\\": \\\"d0i1jw0\\\",\\n                      \\\"name\\\": \\\"t1_d0i1jw0\\\",\\n                      \\\"children\\\": [\\n                        \\\"d0i1jw0\\\"\\n                      ]\\n                    }\\n                  }\\n\\nThen I ask for `d0i1jw0` using `morechildren`:\\n\\n    https://www.reddit.com/api/morechildren?api_type=json\\u0026link_id=t3_489xei\\u0026sort=old\\u0026children=d0i1jw0\\n\\n[Click](https://www.reddit.com/api/morechildren?api_type=json\\u0026link_id=t3_489xei\\u0026sort=old\\u0026children=d0i1jw0) for the json from reddit, or see a [pastebin](http://pastebin.com/KedgrDza) of what I get.\\n\\nBut I don't get comment JSON back, no `author`, no `created_UTC`.  There is `contextText` that looks like comment text.  But there's this markdown or HTML in a `content` field, as if it were built for the web client.  If someone can spot a dumb mistake of mine, please tell me.  Thanks!\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"48etkd\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"CogitoErgoReddit\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 13, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1456854801.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/48etkd/mo_children_mo_problems_or_why_can_i_break_every/\", \"locked\": false, \"name\": \"t3_48etkd\", \"created\": 1456840281.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/48etkd/mo_children_mo_problems_or_why_can_i_break_every/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Mo' children mo' problems. Or, why can I break every endpoint I try to use? Help with /api/morechildren.\", \"created_utc\": 1456811481.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI am using /api/morechildren to load more comments , when i find objects with kind = more\\nFor example : \\u003Ca href=\\\"https://www.reddit.com/api/morechildren?api_type=json\\u0026amp;link_id=t3_482w7n\\u0026amp;children=d0h7gs4\\\"\\u003Ehttps://www.reddit.com/api/morechildren?api_type=json\\u0026amp;link_id=t3_482w7n\\u0026amp;children=d0h7gs4\\u003C/a\\u003E (This Works)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EBut sometimes i get empty reply even thought the comments exists because i test it with api/info\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor example :\\n/api/morechildren reply : \\u003Ca href=\\\"https://www.reddit.com/api/morechildren?api_type=json\\u0026amp;link_id=t3_482w7n\\u0026amp;children=d0hd5nf\\\"\\u003Ehttps://www.reddit.com/api/morechildren?api_type=json\\u0026amp;link_id=t3_482w7n\\u0026amp;children=d0hd5nf\\u003C/a\\u003E (Empty Things : [])\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E/api/info reply: \\n\\u003Ca href=\\\"https://www.reddit.com/api/info.json?id=t1_d0hd5nf\\\"\\u003Ehttps://www.reddit.com/api/info.json?id=t1_d0hd5nf\\u003C/a\\u003E ( Comment Exists )\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs this intended or i am doing something wrong ? \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny help will be appreciated\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I am using /api/morechildren to load more comments , when i find objects with kind = more\\nFor example : https://www.reddit.com/api/morechildren?api_type=json\\u0026link_id=t3_482w7n\\u0026children=d0h7gs4 (This Works)\\n\\nBut sometimes i get empty reply even thought the comments exists because i test it with api/info\\n\\nFor example :\\n/api/morechildren reply : https://www.reddit.com/api/morechildren?api_type=json\\u0026link_id=t3_482w7n\\u0026children=d0hd5nf (Empty Things : [])\\n\\n/api/info reply: \\nhttps://www.reddit.com/api/info.json?id=t1_d0hd5nf ( Comment Exists )\\n\\nIs this intended or i am doing something wrong ? \\n\\nAny help will be appreciated\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"48b2ff\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"saehon\", \"media\": null, \"score\": 3, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/48b2ff/apimorechildren_problem/\", \"locked\": false, \"name\": \"t3_48b2ff\", \"created\": 1456792819.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/48b2ff/apimorechildren_problem/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"/api/morechildren problem\", \"created_utc\": 1456764019.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 3}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHello everyone.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m developing my first bot for reddit. The main idea is that the bot can be summoned to subreddits such as \\u003Ca href=\\\"/r/london\\\"\\u003E/r/london\\u003C/a\\u003E or \\u003Ca href=\\\"/r/newyork\\\"\\u003E/r/newyork\\u003C/a\\u003E and then provide daily/weekly/monthly weather reports.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ETo do this task, the bot needs to post on subreddits but I have an issue.. captcha. I was wondering what the standard way of posting with a bot was. Obviously captach is there to stop spam/bots but surely there must be a way that I can allow my bot to post? :)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hello everyone.\\n\\nI'm developing my first bot for reddit. The main idea is that the bot can be summoned to subreddits such as /r/london or /r/newyork and then provide daily/weekly/monthly weather reports.\\n\\nTo do this task, the bot needs to post on subreddits but I have an issue.. captcha. I was wondering what the standard way of posting with a bot was. Obviously captach is there to stop spam/bots but surely there must be a way that I can allow my bot to post? :)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4848kx\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"SubredditWeatherBot\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 8, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4848kx/bot_posting_selfposts_captcha_based_question/\", \"locked\": false, \"name\": \"t3_4848kx\", \"created\": 1456710122.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4848kx/bot_posting_selfposts_captcha_based_question/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Bot posting self-posts (Captcha based question)\", \"created_utc\": 1456681322.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EAny reliable C# library for reddit botdev? I want to make a basic bot to test, but I\\u0026#39;m unsure which is the best if any to use. Please let me know\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Any reliable C# library for reddit botdev? I want to make a basic bot to test, but I'm unsure which is the best if any to use. Please let me know\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"47fq7l\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"meepcanon\", \"media\": null, \"score\": 7, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/47fq7l/reliable_c_library/\", \"locked\": false, \"name\": \"t3_47fq7l\", \"created\": 1456384009.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/47fq7l/reliable_c_library/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reliable C# library?\", \"created_utc\": 1456355209.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 7}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m working on an iOS Reddit client. Last night I was using it before going to sleep and was browsing the front page. Sure enough, something like 10 hours later I refresh the front page and it returns the same posts. I go to my computer and print out the response and the raw JSON data and sure enough it\\u0026#39;s 10 hours old there as well. Even the timestamp in the response was very old. I deleted the app off my phone and reinstalled. For some reason this fixed it. The subsequent responses were dated correctly. I\\u0026#39;m thinking that maybe this has to do with the fact that when I authorize I send the following parameter [\\u0026quot;duration\\u0026quot; : \\u0026quot;permanent\\u0026quot;] . Or maybe it\\u0026#39;s just Reddit sending me really old cached data? I\\u0026#39;m pretty sure that the whole OAuth process is fine because I was still getting feedback on the old posts that I had saved/upvoted some of them. Thanks\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere is the response from the request that is working\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EOptional(\\u0026lt;NSHTTPURLResponse: 0x137c0fdb0\\u0026gt; { URL: \\u003Ca href=\\\"https://oauth.reddit.com/.json?raw_json=1\\\"\\u003Ehttps://oauth.reddit.com/.json?raw_json=1\\u003C/a\\u003E } { status code: 200, headers {\\n    \\u0026quot;Cache-Control\\u0026quot; = \\u0026quot;private, s-maxage=0, max-age=0, must-revalidate, max-age=0, must-revalidate\\u0026quot;;\\n    \\u0026quot;Content-Encoding\\u0026quot; = gzip;\\n    \\u0026quot;Content-Length\\u0026quot; = 18823;\\n    \\u0026quot;Content-Type\\u0026quot; = \\u0026quot;application/json; charset=UTF-8\\u0026quot;;\\n    Date = \\u0026quot;Sun, 21 Feb 2016 19:38:54 GMT\\u0026quot;;\\n    Expires = \\u0026quot;-1\\u0026quot;;\\n    Server = \\u0026quot;cloudflare-nginx\\u0026quot;;\\n    \\u0026quot;Strict-Transport-Security\\u0026quot; = \\u0026quot;max-age=15552000; includeSubDomains; preload\\u0026quot;;\\n    Vary = \\u0026quot;accept-encoding\\u0026quot;;\\n    \\u0026quot;cf-ray\\u0026quot; = \\u0026quot;2784ea0582111810-MIA\\u0026quot;;\\n    \\u0026quot;x-content-type-options\\u0026quot; = nosniff;\\n    \\u0026quot;x-frame-options\\u0026quot; = SAMEORIGIN;\\n    \\u0026quot;x-moose\\u0026quot; = majestic;\\n    \\u0026quot;x-ratelimit-remaining\\u0026quot; = \\u0026quot;598.0\\u0026quot;;\\n    \\u0026quot;x-ratelimit-reset\\u0026quot; = 67;\\n    \\u0026quot;x-ratelimit-used\\u0026quot; = 2;\\n    \\u0026quot;x-reddit-tracking\\u0026quot; = \\u0026quot;\\u003Ca href=\\\"https://pixel.redditmedia.com/pixel/of_destiny.png?v=ZBYQ3kPrLv6P%2Bp6BvGi44BC0IJ5KBo5sihfN6OpRXC%2BHAaZuj2mx7HKY2zDWPagMgAk6SH8JQL6wersQavUWz4a9KN3%2FX5%2BH\\\"\\u003Ehttps://pixel.redditmedia.com/pixel/of_destiny.png?v=ZBYQ3kPrLv6P%2Bp6BvGi44BC0IJ5KBo5sihfN6OpRXC%2BHAaZuj2mx7HKY2zDWPagMgAk6SH8JQL6wersQavUWz4a9KN3%2FX5%2BH\\u003C/a\\u003E\\u0026quot;;\\n    \\u0026quot;x-ua-compatible\\u0026quot; = \\u0026quot;IE=edge\\u0026quot;;\\n    \\u0026quot;x-xss-protection\\u0026quot; = \\u0026quot;1; mode=block\\u0026quot;;\\n} })\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT:\\nAfter doing a little observing, when I ask for the currently logged-in user\\u0026#39;s front page the Reddit API sends me the same JSON object over and over with the same timestamp on the response. The upvotes reported on each post are always the same so I\\u0026#39;m pretty certain that each JSON object is identical to the last. I have been testing this for around 10 minutes. The website reports that the top post on front has 800 more upvotes than what my app is receiving. This is definitely a problem..\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT 2:\\nJust opened Alien Blue and it has exactly up to date count of upvotes for the posts on the front page. So there is a way to make this work. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT 3:\\nI have confirmed that the absolute only way to get the Reddit API to send my app new information beyond the initial JSON object is to reinstall the app. ?!?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm working on an iOS Reddit client. Last night I was using it before going to sleep and was browsing the front page. Sure enough, something like 10 hours later I refresh the front page and it returns the same posts. I go to my computer and print out the response and the raw JSON data and sure enough it's 10 hours old there as well. Even the timestamp in the response was very old. I deleted the app off my phone and reinstalled. For some reason this fixed it. The subsequent responses were dated correctly. I'm thinking that maybe this has to do with the fact that when I authorize I send the following parameter [\\\"duration\\\" : \\\"permanent\\\"] . Or maybe it's just Reddit sending me really old cached data? I'm pretty sure that the whole OAuth process is fine because I was still getting feedback on the old posts that I had saved/upvoted some of them. Thanks\\n\\nHere is the response from the request that is working\\n\\nOptional(\\u003CNSHTTPURLResponse: 0x137c0fdb0\\u003E { URL: https://oauth.reddit.com/.json?raw_json=1 } { status code: 200, headers {\\n    \\\"Cache-Control\\\" = \\\"private, s-maxage=0, max-age=0, must-revalidate, max-age=0, must-revalidate\\\";\\n    \\\"Content-Encoding\\\" = gzip;\\n    \\\"Content-Length\\\" = 18823;\\n    \\\"Content-Type\\\" = \\\"application/json; charset=UTF-8\\\";\\n    Date = \\\"Sun, 21 Feb 2016 19:38:54 GMT\\\";\\n    Expires = \\\"-1\\\";\\n    Server = \\\"cloudflare-nginx\\\";\\n    \\\"Strict-Transport-Security\\\" = \\\"max-age=15552000; includeSubDomains; preload\\\";\\n    Vary = \\\"accept-encoding\\\";\\n    \\\"cf-ray\\\" = \\\"2784ea0582111810-MIA\\\";\\n    \\\"x-content-type-options\\\" = nosniff;\\n    \\\"x-frame-options\\\" = SAMEORIGIN;\\n    \\\"x-moose\\\" = majestic;\\n    \\\"x-ratelimit-remaining\\\" = \\\"598.0\\\";\\n    \\\"x-ratelimit-reset\\\" = 67;\\n    \\\"x-ratelimit-used\\\" = 2;\\n    \\\"x-reddit-tracking\\\" = \\\"https://pixel.redditmedia.com/pixel/of_destiny.png?v=ZBYQ3kPrLv6P%2Bp6BvGi44BC0IJ5KBo5sihfN6OpRXC%2BHAaZuj2mx7HKY2zDWPagMgAk6SH8JQL6wersQavUWz4a9KN3%2FX5%2BH\\\";\\n    \\\"x-ua-compatible\\\" = \\\"IE=edge\\\";\\n    \\\"x-xss-protection\\\" = \\\"1; mode=block\\\";\\n} })\\n\\nEDIT:\\nAfter doing a little observing, when I ask for the currently logged-in user's front page the Reddit API sends me the same JSON object over and over with the same timestamp on the response. The upvotes reported on each post are always the same so I'm pretty certain that each JSON object is identical to the last. I have been testing this for around 10 minutes. The website reports that the top post on front has 800 more upvotes than what my app is receiving. This is definitely a problem..\\n\\nEDIT 2:\\nJust opened Alien Blue and it has exactly up to date count of upvotes for the posts on the front page. So there is a way to make this work. \\n\\nEDIT 3:\\nI have confirmed that the absolute only way to get the Reddit API to send my app new information beyond the initial JSON object is to reinstall the app. ?!?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"46wxk3\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"jvalldejulidev7\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1456085901.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/46wxk3/receiving_old_data_from_reddit_api_oauth/\", \"locked\": false, \"name\": \"t3_46wxk3\", \"created\": 1456113205.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/46wxk3/receiving_old_data_from_reddit_api_oauth/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Receiving old data from Reddit API (OAuth)\", \"created_utc\": 1456084405.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EWhen browsing through the inbox, seeing a random reply without context as to what comment of yours it\\u0026#39;s replying to is a lot less helpful than it could be.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERight now in my app I\\u0026#39;m doing a lot of potentially unnecessary network requests to fetch the body of the comment that the inbox comment is replying to, and it\\u0026#39;d be terrific if this could be added to the API in some fashion. Would certainly save a lot of requests at least, when scrolling through a list of replies I bump up against the 60 requests/minute rather easily. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECould even just be a parameter on the API request asking if you would like it included, so it\\u0026#39;s not brought down with every request.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"When browsing through the inbox, seeing a random reply without context as to what comment of yours it's replying to is a lot less helpful than it could be.\\n\\nRight now in my app I'm doing a lot of potentially unnecessary network requests to fetch the body of the comment that the inbox comment is replying to, and it'd be terrific if this could be added to the API in some fashion. Would certainly save a lot of requests at least, when scrolling through a list of replies I bump up against the 60 requests/minute rather easily. \\n\\nCould even just be a parameter on the API request asking if you would like it included, so it's not brought down with every request.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"450sqz\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"iamthatis\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 9, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1455076852.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/450sqz/would_it_be_possible_to_include_the_body_of_the/\", \"locked\": false, \"name\": \"t3_450sqz\", \"created\": 1455105011.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/450sqz/would_it_be_possible_to_include_the_body_of_the/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Would it be possible to include the body of the comment being replied to in the Inbox API?\", \"created_utc\": 1455076211.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cpre\\u003E\\u003Ccode\\u003E\\u0026gt;\\u0026gt;\\u0026gt; srch=r.search(\\u0026quot;[praw]\\u0026quot;)\\n\\u0026gt;\\u0026gt;\\u0026gt; srchs=list(srch)\\nTraceback (most recent call last):\\n  File \\u0026quot;\\u0026lt;stdin\\u0026gt;\\u0026quot;, line 1, in \\u0026lt;module\\u0026gt;\\n  File \\u0026quot;/usr/local/lib/python3.4/dist-packages/praw/__init__.py\\u0026quot;, line 1171, in search\\n    **kwargs):\\n  File \\u0026quot;/usr/local/lib/python3.4/dist-packages/praw/__init__.py\\u0026quot;, line 564, in get_content\\n    root = page_data.get(root_field, page_data)\\nAttributeError: \\u0026#39;list\\u0026#39; object has no attribute \\u0026#39;get\\u0026#39;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EEdit: fixed now.  Thanks all.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"    \\u003E\\u003E\\u003E srch=r.search(\\\"[praw]\\\")\\n    \\u003E\\u003E\\u003E srchs=list(srch)\\n    Traceback (most recent call last):\\n      File \\\"\\u003Cstdin\\u003E\\\", line 1, in \\u003Cmodule\\u003E\\n      File \\\"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\\\", line 1171, in search\\n        **kwargs):\\n      File \\\"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\\\", line 564, in get_content\\n        root = page_data.get(root_field, page_data)\\n    AttributeError: 'list' object has no attribute 'get'\\n\\n\\nEdit: fixed now.  Thanks all.\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"44tixp\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"boib\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1454972651.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/44tixp/praw_search_suddenly_stopped_working/\", \"locked\": false, \"name\": \"t3_44tixp\", \"created\": 1455001039.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/44tixp/praw_search_suddenly_stopped_working/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[PRAW] Search suddenly stopped working.\", \"created_utc\": 1454972239.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"twizz.co.uk\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"44a306\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"inflagrante\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 3, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/44a306/some_seo_stuff_relating_to_crawl_efficiency_meta/\", \"locked\": false, \"name\": \"t3_44a306\", \"created\": 1454692299.0, \"url\": \"http://twizz.co.uk/reddit-tech-seo-stuff/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Some SEO stuff relating to crawl efficiency \\u0026 meta descriptions (x-post /r/ideasfortheadmins)\", \"created_utc\": 1454663499.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EUnsure if this is known, but there seems to be a bug where the \\u0026quot;sign up\\u0026quot; button on the compact version of Reddit\\u0026#39;s website for registering a new account does not function. It just acts like you\\u0026#39;re selecting the form upon being tapped.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://i.imgur.com/FvzsPbU.png\\\"\\u003Ehttp://i.imgur.com/FvzsPbU.png\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENoticing this on iOS 9.2 for the record. Inability to register accounts is pretty significant. I could certainly be doing something wrong here, but I\\u0026#39;ve tested it on the iOS simulator and a live device, and if it\\u0026#39;s happening to me I\\u0026#39;m sure it could be happening to others.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT: Oddly enough I can\\u0026#39;t even seem to use the compact registration page on a desktop device, the form just never submits.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEDIT2: Through further testing, it doesn\\u0026#39;t occur every time, but it certainly does sometimes. \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Unsure if this is known, but there seems to be a bug where the \\\"sign up\\\" button on the compact version of Reddit's website for registering a new account does not function. It just acts like you're selecting the form upon being tapped.\\n\\nhttp://i.imgur.com/FvzsPbU.png\\n\\nNoticing this on iOS 9.2 for the record. Inability to register accounts is pretty significant. I could certainly be doing something wrong here, but I've tested it on the iOS simulator and a live device, and if it's happening to me I'm sure it could be happening to others.\\n\\nEDIT: Oddly enough I can't even seem to use the compact registration page on a desktop device, the form just never submits.\\n\\nEDIT2: Through further testing, it doesn't occur every time, but it certainly does sometimes. \", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"43ukwn\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"iamthatis\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1454859741.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/43ukwn/unable_to_register_on_compact_website_on_ios/\", \"locked\": false, \"name\": \"t3_43ukwn\", \"created\": 1454454489.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/43ukwn/unable_to_register_on_compact_website_on_ios/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Unable to register on compact website on iOS devices?\", \"created_utc\": 1454425689.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EUsing a valid OAuth access token and correct query parameters, this endpoint always returns \\u003Ccode\\u003E{\\u0026quot;error\\u0026quot;: 400}\\u003C/code\\u003E regardless of user requested. I\\u0026#39;m currently using \\u003Ccode\\u003E/user/username/about\\u003C/code\\u003E as a workaround, but it\\u0026#39;d be great to have this endpoint working, assuming I\\u0026#39;m not overlooking something.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Using a valid OAuth access token and correct query parameters, this endpoint always returns `{\\\"error\\\": 400}` regardless of user requested. I'm currently using `/user/username/about` as a workaround, but it'd be great to have this endpoint working, assuming I'm not overlooking something.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"43282h\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"RedBanHammer\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/43282h/apiusername_available_always_returns_400_invalid/\", \"locked\": false, \"name\": \"t3_43282h\", \"created\": 1453993434.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/43282h/apiusername_available_always_returns_400_invalid/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"/api/username_available always returns 400 (invalid request)\", \"created_utc\": 1453964634.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EOne of the users that downloaded my app had trouble logging in, so I tried figuring out with the problem was. Turns out the redirect link that contains the code displayed the \\u0026#39;=\\u0026#39; sign as \\u0026#39;-\\u0026#39; so the code I wrote didn\\u0026#39;t catch it. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EHere\\u0026#39;s the screenshots he sent me\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://imgur.com/K6zvEJe\\\"\\u003Ehttp://imgur.com/K6zvEJe\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://imgur.com/DQCTlHA\\\"\\u003Ehttp://imgur.com/DQCTlHA\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve rewritten the code to catch it but I\\u0026#39;m just wondering if this was intended? or is this a known problem?\\nThis isn\\u0026#39;t a problem on any of my devices but he says he gets the error on 2 of his, one of the devices he\\u0026#39;s using is a Galaxy Note 3 if that helps..\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"One of the users that downloaded my app had trouble logging in, so I tried figuring out with the problem was. Turns out the redirect link that contains the code displayed the '=' sign as '-' so the code I wrote didn't catch it. \\n\\nHere's the screenshots he sent me\\n\\nhttp://imgur.com/K6zvEJe\\n\\nhttp://imgur.com/DQCTlHA\\n\\nI've rewritten the code to catch it but I'm just wondering if this was intended? or is this a known problem?\\nThis isn't a problem on any of my devices but he says he gets the error on 2 of his, one of the devices he's using is a Galaxy Note 3 if that helps..\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"41q726\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"AxDal\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1453253520.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/41q726/oauth_url_parameter_equal_sign_displayed_as_hyphen/\", \"locked\": false, \"name\": \"t3_41q726\", \"created\": 1453259380.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/41q726/oauth_url_parameter_equal_sign_displayed_as_hyphen/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"OAuth URL Parameter - equal sign displayed as hyphen\", \"created_utc\": 1453230580.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi, I\\u0026#39;m working on a project and would like to collect some reddit post information. The data I would like to have would consist of the following 5 columns: post_name, num_of_comments, ups, downs, date_created. Ideally I would like to request these data in a certain date range such as from dec 10th 2015 - dec 20th 2015. I looked around in the reddit API and didn\\u0026#39;t find anything that could do this. Could anyone suggest to a newbie like me as to where I can get these data?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EFor example, I found that for hackernews there\\u0026#39;s an api that could request post info in a range of dates via the following api, maybe something similar exist for reddit?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"http://hn.algolia.com/api/v1/search_by_date?tags=story\\u0026amp;numericFilters=created_at_i%3E1449754640,created_at_i%3C1449820787\\\"\\u003Ehttp://hn.algolia.com/api/v1/search_by_date?tags=story\\u0026amp;numericFilters=created_at_i%3E1449754640,created_at_i%3C1449820787\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi, I'm working on a project and would like to collect some reddit post information. The data I would like to have would consist of the following 5 columns: post_name, num_of_comments, ups, downs, date_created. Ideally I would like to request these data in a certain date range such as from dec 10th 2015 - dec 20th 2015. I looked around in the reddit API and didn't find anything that could do this. Could anyone suggest to a newbie like me as to where I can get these data?\\n\\nFor example, I found that for hackernews there's an api that could request post info in a range of dates via the following api, maybe something similar exist for reddit?\\n\\nhttp://hn.algolia.com/api/v1/search_by_date?tags=story\\u0026numericFilters=created_at_i%3E1449754640,created_at_i%3C1449820787\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"41pd26\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"totolin\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/41pd26/how_to_get_reddit_historic_post_data/\", \"locked\": false, \"name\": \"t3_41pd26\", \"created\": 1453249355.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/41pd26/how_to_get_reddit_historic_post_data/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to get reddit historic post data?\", \"created_utc\": 1453220555.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EIf I\\u0026#39;m not doing something wrong there seems to be a bug in \\u003Ccode\\u003E/api/v1/me/prefs\\u003C/code\\u003E. Requesting specific preferences works fine, but trying to request them all (specifying no query parameters) returns a 500 Internal Server Error.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWith \\u003Ccode\\u003E?fields=over18\\u003C/code\\u003E:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E$ curl  -H \\u0026#39;Authorization: bearer \\u0026lt;token\\u0026gt;\\u0026#39; \\\\\\n    -H \\u0026#39;User-Agent: desktop:net.dean.jraw.test:v0.8.0\\u0026#39; \\\\\\n    \\u0026quot;https://oauth.reddit.com/api/v1/me/prefs?fields=over_18\\u0026quot;\\n{\\u0026quot;over_18\\u0026quot;: true}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EWithout specifying \\u003Ccode\\u003Efields\\u003C/code\\u003E:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003E$ curl  -H \\u0026#39;Authorization: bearer \\u0026lt;token\\u0026gt;\\u0026#39; \\\\\\n    -H \\u0026#39;User-Agent: desktop:net.dean.jraw.test:v0.8.0\\u0026#39; \\\\\\n    \\u0026quot;https://oauth.reddit.com/api/v1/me/prefs\\u0026quot;\\n{\\u0026quot;error\\u0026quot;: 500}\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EIs there something I\\u0026#39;m doing wrong here?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"If I'm not doing something wrong there seems to be a bug in `/api/v1/me/prefs`. Requesting specific preferences works fine, but trying to request them all (specifying no query parameters) returns a 500 Internal Server Error.\\n\\nWith `?fields=over18`:\\n\\n    $ curl  -H 'Authorization: bearer \\u003Ctoken\\u003E' \\\\\\n        -H 'User-Agent: desktop:net.dean.jraw.test:v0.8.0' \\\\\\n        \\\"https://oauth.reddit.com/api/v1/me/prefs?fields=over_18\\\"\\n    {\\\"over_18\\\": true}\\n\\nWithout specifying `fields`:\\n\\n    $ curl  -H 'Authorization: bearer \\u003Ctoken\\u003E' \\\\\\n        -H 'User-Agent: desktop:net.dean.jraw.test:v0.8.0' \\\\\\n        \\\"https://oauth.reddit.com/api/v1/me/prefs\\\"\\n    {\\\"error\\\": 500}\\n\\nIs there something I'm doing wrong here?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"4118t8\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"thatJavaNerd\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/4118t8/bug_in_apiv1meprefs/\", \"locked\": false, \"name\": \"t3_4118t8\", \"created\": 1452856979.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/4118t8/bug_in_apiv1meprefs/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Bug in /api/v1/me/prefs?\", \"created_utc\": 1452828179.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI tried searching on \\u003Ca href=\\\"/r/modhelp\\\"\\u003E/r/modhelp\\u003C/a\\u003E, here, and a few other places for something like this, but it seems most posts like \\u003Ca href=\\\"https://www.reddit.com/r/nerdfighters/comments/2tzyip/help_with_a_bot_that_posts_youtube_videos_to_a/\\\"\\u003Ethis one\\u003C/a\\u003E are about \\u003Cem\\u003Eposting\\u003C/em\\u003E videos to a subreddit, not removing posts. I\\u0026#39;m aware of \\u003Ca href=\\\"https://github.com/AndrewNeo/groompbot\\\"\\u003EGroompbot\\u003C/a\\u003E and checked that out to see if maybe it had that feature but it doesn\\u0026#39;t seem so. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf anyone has any ideas that would be great. :) Thank you!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I tried searching on /r/modhelp, here, and a few other places for something like this, but it seems most posts like [this one](https://www.reddit.com/r/nerdfighters/comments/2tzyip/help_with_a_bot_that_posts_youtube_videos_to_a/) are about *posting* videos to a subreddit, not removing posts. I'm aware of [Groompbot](https://github.com/AndrewNeo/groompbot) and checked that out to see if maybe it had that feature but it doesn't seem so. \\n\\nIf anyone has any ideas that would be great. :) Thank you!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"401tcx\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"TheFancyCollector\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/401tcx/does_a_bot_exist_that_will_autoremove_posts_from/\", \"locked\": false, \"name\": \"t3_401tcx\", \"created\": 1452299900.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/401tcx/does_a_bot_exist_that_will_autoremove_posts_from/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Does a bot exist that will auto-remove posts from Youtube from a subreddit if the Youtube video was removed or made private?\", \"created_utc\": 1452271100.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m trying to write a script that pulls down lots of reddit data (submissions, comments, and user info; basically everything), and I want to do this for entire sub-reddits for their entire history so obviously speed is an important factor.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EPRAW wants to break things up into multiple API requests because it assumes you don\\u0026#39;t want that much info, but is there a way to force it to pull more data at once?  For example, I can get PRAW to pull all the comments from a submission like so:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esub = r.get_submission(some_url)\\ncms = praw.helpers.flatten_tree(s.comments)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EBut if I want to know the author of those posts I have to do something like:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eusers = [i.author.id for i in cms]\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ESuddenly a handful of API calls (depending on the number of comments) becomes dozens, hundreds, or even thousands as PRAW fetches the authors one-by-one.  At 1-2 seconds per-call this is obviously way too slow for my purposes.  Is there a way to force PRAW to fetch all related data in a more efficient way?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAlternatively, searching this sub for the answer has lead me to a few posts recommending r.request_json as a means for pulling a lot more data at once (1000 lines) and then parsing that instead of working within PRAW\\u0026#39;s object model.  However, the documentation on r.request_json appears scant, and I\\u0026#39;m not sure how to get it to do anything useful.  It appears to simply be spitting out JSON formatted PRAW objects, which is not at all what I need.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEx:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ereddit.request_json(\\u0026#39;https://www.reddit.com/r/movies/comments/3ysj7z/leonardo_di_caprio_reveals_he_turned_down_the/\\u0026#39;)\\n\\n[{\\u0026#39;data\\u0026#39;: {\\u0026#39;after\\u0026#39;: None,\\n   \\u0026#39;before\\u0026#39;: None,\\n   \\u0026#39;children\\u0026#39;: [\\u0026lt;praw.objects.Submission at 0x7f4524716ba8\\u0026gt;],\\n   \\u0026#39;modhash\\u0026#39;: \\u0026#39;\\u0026#39;},\\n  \\u0026#39;kind\\u0026#39;: \\u0026#39;Listing\\u0026#39;},\\n {\\u0026#39;data\\u0026#39;: {\\u0026#39;after\\u0026#39;: None,\\n   \\u0026#39;before\\u0026#39;: None,\\n   \\u0026#39;children\\u0026#39;: [\\u0026lt;praw.objects.Comment at 0x7f45247054a8\\u0026gt;,\\n    \\u0026lt;praw.objects.Comment at 0x7f45246d5828\\u0026gt;,\\n    \\u0026lt;praw.objects.Comment at 0x7f45246d7c18\\u0026gt;,\\n    \\u0026lt;praw.objects.Comment at 0x7f45246d9438\\u0026gt;,\\n    \\u0026lt;praw.objects.Comment at 0x7f4524692278\\u0026gt;,\\n    \\u0026lt;praw.objects.Comment at 0x7f4524692898\\u0026gt;,\\n    \\u0026lt;praw.objects.Comment at 0x7f4524692b00\\u0026gt;,\\n    \\u0026lt;praw.objects.Comment at 0x7f45246977b8\\u0026gt;,\\n    \\u0026lt;praw.objects.Comment at 0x7f452469a0f0\\u0026gt;,\\n    \\u0026lt;praw.objects.Comment at 0x7f452469e278\\u0026gt;,\\n    \\u0026lt;praw.objects.Comment at 0x7f452469e4a8\\u0026gt;,\\n    \\u0026lt;praw.objects.Comment at 0x7f452469ee80\\u0026gt;,\\n    \\u0026lt;praw.objects.Comment at 0x7f4524636198\\u0026gt;,\\n    \\u0026lt;praw.objects.Comment at 0x7f4524636630\\u0026gt;,\\n    \\u0026lt;praw.objects.Comment at 0x7f45246369b0\\u0026gt;,\\n    \\u0026lt;praw.objects.Comment at 0x7f4524636cf8\\u0026gt;,\\n    \\u0026lt;praw.objects.Comment at 0x7f4524636eb8\\u0026gt;,\\n    \\u0026lt;praw.objects.MoreComments at 0x7f4524641d68\\u0026gt;],\\n   \\u0026#39;modhash\\u0026#39;: \\u0026#39;\\u0026#39;},\\n  \\u0026#39;kind\\u0026#39;: \\u0026#39;Listing\\u0026#39;}]\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ECan anyone give me a hint or direct me to a good tutorial on using request_json or similar tools?  Might it be easier to drop PRAW entirely and just parse /.json pages directly?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm trying to write a script that pulls down lots of reddit data (submissions, comments, and user info; basically everything), and I want to do this for entire sub-reddits for their entire history so obviously speed is an important factor.\\n\\nPRAW wants to break things up into multiple API requests because it assumes you don't want that much info, but is there a way to force it to pull more data at once?  For example, I can get PRAW to pull all the comments from a submission like so:\\n\\n    sub = r.get_submission(some_url)\\n    cms = praw.helpers.flatten_tree(s.comments)\\n\\nBut if I want to know the author of those posts I have to do something like:\\n\\n    users = [i.author.id for i in cms]\\n\\nSuddenly a handful of API calls (depending on the number of comments) becomes dozens, hundreds, or even thousands as PRAW fetches the authors one-by-one.  At 1-2 seconds per-call this is obviously way too slow for my purposes.  Is there a way to force PRAW to fetch all related data in a more efficient way?\\n\\nAlternatively, searching this sub for the answer has lead me to a few posts recommending r.request_json as a means for pulling a lot more data at once (1000 lines) and then parsing that instead of working within PRAW's object model.  However, the documentation on r.request_json appears scant, and I'm not sure how to get it to do anything useful.  It appears to simply be spitting out JSON formatted PRAW objects, which is not at all what I need.\\n\\nEx:\\n\\n    reddit.request_json('https://www.reddit.com/r/movies/comments/3ysj7z/leonardo_di_caprio_reveals_he_turned_down_the/')\\n\\n    [{'data': {'after': None,\\n       'before': None,\\n       'children': [\\u003Cpraw.objects.Submission at 0x7f4524716ba8\\u003E],\\n       'modhash': ''},\\n      'kind': 'Listing'},\\n     {'data': {'after': None,\\n       'before': None,\\n       'children': [\\u003Cpraw.objects.Comment at 0x7f45247054a8\\u003E,\\n        \\u003Cpraw.objects.Comment at 0x7f45246d5828\\u003E,\\n        \\u003Cpraw.objects.Comment at 0x7f45246d7c18\\u003E,\\n        \\u003Cpraw.objects.Comment at 0x7f45246d9438\\u003E,\\n        \\u003Cpraw.objects.Comment at 0x7f4524692278\\u003E,\\n        \\u003Cpraw.objects.Comment at 0x7f4524692898\\u003E,\\n        \\u003Cpraw.objects.Comment at 0x7f4524692b00\\u003E,\\n        \\u003Cpraw.objects.Comment at 0x7f45246977b8\\u003E,\\n        \\u003Cpraw.objects.Comment at 0x7f452469a0f0\\u003E,\\n        \\u003Cpraw.objects.Comment at 0x7f452469e278\\u003E,\\n        \\u003Cpraw.objects.Comment at 0x7f452469e4a8\\u003E,\\n        \\u003Cpraw.objects.Comment at 0x7f452469ee80\\u003E,\\n        \\u003Cpraw.objects.Comment at 0x7f4524636198\\u003E,\\n        \\u003Cpraw.objects.Comment at 0x7f4524636630\\u003E,\\n        \\u003Cpraw.objects.Comment at 0x7f45246369b0\\u003E,\\n        \\u003Cpraw.objects.Comment at 0x7f4524636cf8\\u003E,\\n        \\u003Cpraw.objects.Comment at 0x7f4524636eb8\\u003E,\\n        \\u003Cpraw.objects.MoreComments at 0x7f4524641d68\\u003E],\\n       'modhash': ''},\\n      'kind': 'Listing'}]\\n\\nCan anyone give me a hint or direct me to a good tutorial on using request_json or similar tools?  Might it be easier to drop PRAW entirely and just parse /.json pages directly?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3yu6nj\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"BearlyBreathing\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3yu6nj/praw_need_to_speed_up_big_data_collection_praw/\", \"locked\": false, \"name\": \"t3_3yu6nj\", \"created\": 1451543051.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3yu6nj/praw_need_to_speed_up_big_data_collection_praw/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[PRAW] Need to speed up big data collection. PRAW, r.request_json, or something else?\", \"created_utc\": 1451514251.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m trying to set up an instance of a reddit-based site, and I was hoping someone could give me a few hints regarding the problems I encountered so far:\\u003C/p\\u003E\\n\\n\\u003Cul\\u003E\\n\\u003Cli\\u003EI created an A record for m.\\u003Cem\\u003Emysite.com\\u003C/em\\u003E that points to the same IP as \\u003Cem\\u003Emysite.com\\u003C/em\\u003E. However, when I type this address into my browser, the bigscreen (desktop) site loads, not the mobile site. Is there anything else I need to do?\\u003C/li\\u003E\\n\\u003Cli\\u003EWhat\\u0026#39;s the difference between i.reddit.com and m.reddit.com? Which one has more features, which one is undergoing active development?\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EI tried to make reddit send e-mails through my provider\\u0026#39;s SMTP relay by adding the following lines before the session.sendmail() calls in lib/emailer.py:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esession = smtplib.SMTP(g.smtp_server,587)\\nprint \\u0026gt;\\u0026gt; sys.stderr, (\\u0026quot;Sending mail\\u0026quot;)\\nsession.set_debuglevel(1)\\nsession.starttls()\\nsession.login(\\u0026#39;user\\u0026#39;,\\u0026#39;pass\\u0026#39;)\\nsession.sendmail(from_addr, to_addr, msg.as_string())\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EHowever, I don\\u0026#39;t see \\u0026quot;Sending mail\\u0026quot; in the log file, tcpdump shows no activity on port 25 or 587, but I do see this entry in the logfile: \\u003Ccode\\u003Ereddit-paster: Generated email verification link: http://...\\u003C/code\\u003E. Where could I check next?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIn general, what would be a good way to debug issues like this? Printing to stderr is probably not the most sophisticated way to do it.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EI changed all the translation strings containing the name of the site (\\u0026quot;reddit\\u0026quot;, \\u0026quot;subreddit\\u0026quot;) in the German .po file, and then removed all other languages from the reddit_i18n directory. Now German shows up as the only available language, as well as being the default site language. Is this the proper way to do it or are there any circumstances under which users would be able to access the site in English again?\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003EIs there an easy way to remove every mention of reddit gold? My site is going to be non-commercial and I won\\u0026#39;t provide a way to sign up for reddit gold.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003Cli\\u003E\\u003Cp\\u003E(How) can I disable access external access to the API? I don\\u0026#39;t feel like dealing with all the problems that external API access to the site can generate.\\u003C/p\\u003E\\u003C/li\\u003E\\n\\u003C/ul\\u003E\\n\\n\\u003Cp\\u003EBy the way, does anyone have a (live) example of a site based on the reddit source code? All the examples I found in this sub seem to be down again.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi,\\n\\nI'm trying to set up an instance of a reddit-based site, and I was hoping someone could give me a few hints regarding the problems I encountered so far:\\n\\n* I created an A record for m.*mysite.com* that points to the same IP as *mysite.com*. However, when I type this address into my browser, the bigscreen (desktop) site loads, not the mobile site. Is there anything else I need to do?\\n* What's the difference between i.reddit.com and m.reddit.com? Which one has more features, which one is undergoing active development?\\n* I tried to make reddit send e-mails through my provider's SMTP relay by adding the following lines before the session.sendmail() calls in lib/emailer.py:\\n\\n        session = smtplib.SMTP(g.smtp_server,587)\\n        print \\u003E\\u003E sys.stderr, (\\\"Sending mail\\\")\\n        session.set_debuglevel(1)\\n        session.starttls()\\n        session.login('user','pass')\\n        session.sendmail(from_addr, to_addr, msg.as_string())\\n\\n\\n    However, I don't see \\\"Sending mail\\\" in the log file, tcpdump shows no activity on port 25 or 587, but I do see this entry in the logfile: `reddit-paster: Generated email verification link: http://...`. Where could I check next?\\n\\n    In general, what would be a good way to debug issues like this? Printing to stderr is probably not the most sophisticated way to do it.\\n\\n* I changed all the translation strings containing the name of the site (\\\"reddit\\\", \\\"subreddit\\\") in the German .po file, and then removed all other languages from the reddit_i18n directory. Now German shows up as the only available language, as well as being the default site language. Is this the proper way to do it or are there any circumstances under which users would be able to access the site in English again?\\n* Is there an easy way to remove every mention of reddit gold? My site is going to be non-commercial and I won't provide a way to sign up for reddit gold.\\n* (How) can I disable access external access to the API? I don't feel like dealing with all the problems that external API access to the site can generate.\\n\\nBy the way, does anyone have a (live) example of a site based on the reddit source code? All the examples I found in this sub seem to be down again.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ynb98\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"joheines\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1451390318.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ynb98/several_questions_regarding_reddit_setup/\", \"locked\": false, \"name\": \"t3_3ynb98\", \"created\": 1451418711.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ynb98/several_questions_regarding_reddit_setup/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Several questions regarding Reddit setup\", \"created_utc\": 1451389911.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m writing a simple reddit bot that will retrieve information of my own accounts, such as all comments or saved links and such. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThere seems to be totally open json interface when I simply replace the url ending with .json and I can access my comments and saves with simple bot that logs me in and then retrieves them. I can even browse the json version through the browser if I want to.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Ebut\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/API\\\"\\u003EAPI Rules\\u003C/a\\u003E say that I \\u003Cem\\u003Emust\\u003C/em\\u003E authenticate through \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/OAuth2\\\"\\u003Eoauth\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Cstrong\\u003Ebut\\u003C/strong\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI can browse other users\\u0026#39; full comment history through the browser interface but I cannot seem to do that through json interface.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ECan someone clarify these issues a bit for me. When should I use oauth and when am I allowed to simply retrieve a json without authentication? Also, if I want to retrieve an user\\u0026#39;s comment history, do I need to do that as the user whose history I want?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eedit. I\\u0026#39;m an idiot, I a typo in my username in the script and didn\\u0026#39;t notice it\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm writing a simple reddit bot that will retrieve information of my own accounts, such as all comments or saved links and such. \\n\\nThere seems to be totally open json interface when I simply replace the url ending with .json and I can access my comments and saves with simple bot that logs me in and then retrieves them. I can even browse the json version through the browser if I want to.\\n\\n**but**\\n\\n[API Rules](https://github.com/reddit/reddit/wiki/API) say that I *must* authenticate through [oauth](https://github.com/reddit/reddit/wiki/OAuth2)\\n\\n**but**\\n\\nI can browse other users' full comment history through the browser interface but I cannot seem to do that through json interface.\\n\\nCan someone clarify these issues a bit for me. When should I use oauth and when am I allowed to simply retrieve a json without authentication? Also, if I want to retrieve an user's comment history, do I need to do that as the user whose history I want?\\n\\nedit. I'm an idiot, I a typo in my username in the script and didn't notice it\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ye3lt\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"boarhog\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1451247226.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ye3lt/confused_by_reddit_api_guidelines/\", \"locked\": false, \"name\": \"t3_3ye3lt\", \"created\": 1451246481.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ye3lt/confused_by_reddit_api_guidelines/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Confused by reddit API guidelines\", \"created_utc\": 1451217681.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EBots such as wikipedia and xkcd bot that reply almost instantly with a content. They must be getting an instant feed of new comments on some or all subreddits. I cannot fathom how I would do that with request every 2 seconds and 100 submission per request\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Bots such as wikipedia and xkcd bot that reply almost instantly with a content. They must be getting an instant feed of new comments on some or all subreddits. I cannot fathom how I would do that with request every 2 seconds and 100 submission per request\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3xkey8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"boarhog\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3xkey8/how_does_reddit_bots_monitor_new_comments_on/\", \"locked\": false, \"name\": \"t3_3xkey8\", \"created\": 1450636881.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3xkey8/how_does_reddit_bots_monitor_new_comments_on/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How does reddit bots monitor new comments on subreddits without constantly downloading all comments?\", \"created_utc\": 1450608081.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHey guys,  \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m currently in training at a company, learning their preferred development tool. They asked me to try and make an extension that accesses an API for this tool, and I decided to try and access the Reddit API.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;m currently always getting top.json from \\u003Ca href=\\\"/r/all\\\"\\u003E/r/all\\u003C/a\\u003E. As for the parameters, the limit is based on user input, before and after are retrieved from the JSON and Count is calculated as Count + length of the posts array (not the one returned by the GET-request, but one fetched out of that JSON). \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe issue I\\u0026#39;m having is that, after having loaded new rows twice, instead of getting the next 10 rows when clicking \\u0026quot;next posts\\u0026quot;, I get the previous 10. Has anyone encountered this issue before?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThese are some example URLs my program tries to fetch:\\u003Cbr/\\u003E\\n\\u003Ca href=\\\"https://www.reddit.com/r/all/top/.json?limit=10\\u0026amp;count=30\\u0026amp;after=t3_3x30jj\\u0026amp;before=t3_3x3cw6\\\"\\u003Ehttps://www.reddit.com/r/all/top/.json?limit=10\\u0026amp;count=30\\u0026amp;after=t3_3x30jj\\u0026amp;before=t3_3x3cw6\\u003C/a\\u003E (this is an empty array, so I\\u0026#39;m guessing something is wrong with the before and after values)\\u003Cbr/\\u003E\\n  \\u003Ca href=\\\"https://www.reddit.com/r/all/top/.json?limit=10\\u0026amp;count=20\\u0026amp;after=t3_3x4sjm\\u0026amp;before=t3_3x5a6t\\\"\\u003Ehttps://www.reddit.com/r/all/top/.json?limit=10\\u0026amp;count=20\\u0026amp;after=t3_3x4sjm\\u0026amp;before=t3_3x5a6t\\u003C/a\\u003E\\u003Cbr/\\u003E\\n  \\u003Ca href=\\\"https://www.reddit.com/r/all/top/.json?limit=10\\u0026amp;count=10\\u0026amp;after=t3_3x30jj\\\"\\u003Ehttps://www.reddit.com/r/all/top/.json?limit=10\\u0026amp;count=10\\u0026amp;after=t3_3x30jj\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EYou can find my code here: \\u003Ca href=\\\"http://pastebin.com/2egaysLP\\\"\\u003Ehttp://pastebin.com/2egaysLP\\u003C/a\\u003E .\\u003Cbr/\\u003E\\nMany thanks in advance for reading and replying,\\u003Cbr/\\u003E\\nBoogy\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hey guys,  \\n\\nI'm currently in training at a company, learning their preferred development tool. They asked me to try and make an extension that accesses an API for this tool, and I decided to try and access the Reddit API.\\n\\nI'm currently always getting top.json from /r/all. As for the parameters, the limit is based on user input, before and after are retrieved from the JSON and Count is calculated as Count + length of the posts array (not the one returned by the GET-request, but one fetched out of that JSON). \\n\\nThe issue I'm having is that, after having loaded new rows twice, instead of getting the next 10 rows when clicking \\\"next posts\\\", I get the previous 10. Has anyone encountered this issue before?\\n\\nThese are some example URLs my program tries to fetch:   \\nhttps://www.reddit.com/r/all/top/.json?limit=10\\u0026count=30\\u0026after=t3_3x30jj\\u0026before=t3_3x3cw6 (this is an empty array, so I'm guessing something is wrong with the before and after values)  \\n  https://www.reddit.com/r/all/top/.json?limit=10\\u0026count=20\\u0026after=t3_3x4sjm\\u0026before=t3_3x5a6t  \\n  https://www.reddit.com/r/all/top/.json?limit=10\\u0026count=10\\u0026after=t3_3x30jj\\n\\n\\nYou can find my code here: http://pastebin.com/2egaysLP .   \\nMany thanks in advance for reading and replying,  \\nBoogy\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3x7mmd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Boogy\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1450359113.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3x7mmd/requesting_ralltopjson_always_returns_same_30_rows/\", \"locked\": false, \"name\": \"t3_3x7mmd\", \"created\": 1450387415.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3x7mmd/requesting_ralltopjson_always_returns_same_30_rows/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Requesting /r/all/top.json always returns same 30 rows\", \"created_utc\": 1450358615.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EReddit gold has a feature that lets me sort my saved posts by subreddit. Since my main list is over 1000 posts long the only way to access the older ones is to sort by subreddit. I am trying to make a script with PRAW that will backup my saved posts. Is it possible to return the list of saved posts from a specific subreddit rather than from the main list?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eedit: Problem Solved\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThank you \\u003Ca href=\\\"/u/GoldenSights\\\"\\u003E/u/GoldenSights\\u003C/a\\u003E for finding the solution:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Es=r.user.get_saved(params={\\u0026#39;sr\\u0026#39;:\\u0026#39;Askreddit\\u0026#39;})    \\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Reddit gold has a feature that lets me sort my saved posts by subreddit. Since my main list is over 1000 posts long the only way to access the older ones is to sort by subreddit. I am trying to make a script with PRAW that will backup my saved posts. Is it possible to return the list of saved posts from a specific subreddit rather than from the main list?\\n\\nedit: Problem Solved\\n\\nThank you /u/GoldenSights for finding the solution:\\n\\n    s=r.user.get_saved(params={'sr':'Askreddit'})    \\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3vqb21\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Dashing_in_the_90s\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1449459690.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3vqb21/using_praw_can_i_get_my_saved_posts_from_a/\", \"locked\": false, \"name\": \"t3_3vqb21\", \"created\": 1449479287.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3vqb21/using_praw_can_i_get_my_saved_posts_from_a/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Using PRAW can I get my saved posts from a specific subreddit rather than from the main list?\", \"created_utc\": 1449450487.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EDoes anyone use OAuth2Util? It\\u0026#39;s giving me this error:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003ETraceback (most recent call last):\\n  File \\u0026quot;update_sidebar.py\\u0026quot;, line 6, in \\u0026lt;module\\u0026gt;\\n    o = OAuth2Util.OAuth2Util(r)\\n  File \\u0026quot;/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\\u0026quot;, line 162, in __init__\\n    self.refresh()\\n  File \\u0026quot;/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\\u0026quot;, line 364, in refresh\\n    self._get_new_access_information()\\n  File \\u0026quot;/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\\u0026quot;, line 254, in _get_new_access_information\\n    self._start_webserver(url)\\n  File \\u0026quot;/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\\u0026quot;, line 229, in _start_webserver\\n    self.server = OAuth2UtilServer(server_address, OAuth2UtilRequestHandler, authorize_url)\\n  File \\u0026quot;/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\\u0026quot;, line 58, in __init__\\n    super().__init__(server_adress, handler_class, bind_and_activate)\\nTypeError: super() takes at least 1 argument (0 given)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003ECode:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Eimport praw\\nimport OAuth2Util\\n\\nuser_agent = \\u0026quot;/r/ffxiv sidebar helper by /u/reseph\\u0026quot;\\nr = praw.Reddit(user_agent=user_agent)\\no = OAuth2Util.OAuth2Util(r)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EConfig file is already set up.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Does anyone use OAuth2Util? It's giving me this error:\\n\\n    Traceback (most recent call last):\\n      File \\\"update_sidebar.py\\\", line 6, in \\u003Cmodule\\u003E\\n        o = OAuth2Util.OAuth2Util(r)\\n      File \\\"/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\\\", line 162, in __init__\\n        self.refresh()\\n      File \\\"/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\\\", line 364, in refresh\\n        self._get_new_access_information()\\n      File \\\"/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\\\", line 254, in _get_new_access_information\\n        self._start_webserver(url)\\n      File \\\"/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\\\", line 229, in _start_webserver\\n        self.server = OAuth2UtilServer(server_address, OAuth2UtilRequestHandler, authorize_url)\\n      File \\\"/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\\\", line 58, in __init__\\n        super().__init__(server_adress, handler_class, bind_and_activate)\\n    TypeError: super() takes at least 1 argument (0 given)\\n\\nCode:\\n\\n    import praw\\n    import OAuth2Util\\n\\n    user_agent = \\\"/r/ffxiv sidebar helper by /u/reseph\\\"\\n    r = praw.Reddit(user_agent=user_agent)\\n    o = OAuth2Util.OAuth2Util(r)\\n\\n\\nConfig file is already set up.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3vm3q6\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"reseph\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 24, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/\", \"locked\": false, \"name\": \"t3_3vm3q6\", \"created\": 1449396955.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[OAuth2Util] TypeError: super() takes at least 1 argument (0 given)\", \"created_utc\": 1449368155.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EG\\u0026#39;day\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EWhenever a major live event happens the reddit homepages shows a box with something along the lines of happening now. \\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there a way to get this in API form?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"G'day\\n\\nWhenever a major live event happens the reddit homepages shows a box with something along the lines of happening now. \\n\\nIs there a way to get this in API form?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3v90p9\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"stuntguy3000\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3v90p9/reddit_live_a_way_to_get_the_happening_now_thread/\", \"locked\": false, \"name\": \"t3_3v90p9\", \"created\": 1449152477.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3v90p9/reddit_live_a_way_to_get_the_happening_now_thread/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Reddit Live: A way to get the \\\"happening now\\\" thread?\", \"created_utc\": 1449123677.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m using PRAW library.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EE.g. we have this link: \\u003Ca href=\\\"https://www.reddit.com/r/DotA2/comments/3sx4su/put_camera_yaw_into_option_menu_allow_us_to/cx13thd\\\"\\u003Ehttps://www.reddit.com/r/\\u003Cstrong\\u003EDotA2\\u003C/strong\\u003E/comments/\\u003Cstrong\\u003E3sx4su\\u003C/strong\\u003E/\\u003Cstrong\\u003Eput_camera_yaw_into_option_menu_allow_us_to\\u003C/strong\\u003E/cx13thd\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI do:  \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003ESUBREDDIT = \\u0026quot;AnalyzeLast100Games+Dota2+LearnDota2\\u0026quot;\\nsubreddit = r.get_subreddit(SUBREDDIT)\\nposts = subreddit.get_comments(limit=100) \\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eand my bot finds that comment. (Let\\u0026#39;s assume the post was recently made.)\\u003Cbr/\\u003E\\nI have the post id of that comment via:  \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Efor post in posts:\\n    pid = post.id\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EHow do I get the \\u0026quot;thread id\\u0026quot;? And how do I get the thread title? How do I get the subreddit name (if relevant)?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eedit:  \\u003C/p\\u003E\\n\\n\\u003Ch1\\u003Esolution:\\u003C/h1\\u003E\\n\\n\\u003Cp\\u003Ethread title and subreddit name is irrelevant.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u0026quot;thread id\\u0026quot; or link id is retrieved via:  \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Epost.link_id\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Econstructing the link:   \\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Ehttps://www.reddit.com//comments/\\u0026lt;link id\\u0026gt;//\\u0026lt;post id\\u0026gt;\\n[Some text](/comments/\\u0026lt;link id\\u0026gt;//\\u0026lt;post id\\u0026gt;)\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm using PRAW library.\\n\\nE.g. we have this link: [https://www.reddit.com/r/**DotA2**/comments/**3sx4su**/**put_camera_yaw_into_option_menu_allow_us_to**/cx13thd](https://www.reddit.com/r/DotA2/comments/3sx4su/put_camera_yaw_into_option_menu_allow_us_to/cx13thd)\\n\\nI do:  \\n\\n    SUBREDDIT = \\\"AnalyzeLast100Games+Dota2+LearnDota2\\\"\\n    subreddit = r.get_subreddit(SUBREDDIT)\\n    posts = subreddit.get_comments(limit=100) \\n\\nand my bot finds that comment. (Let's assume the post was recently made.)  \\nI have the post id of that comment via:  \\n\\n    for post in posts:\\n        pid = post.id\\n\\nHow do I get the \\\"thread id\\\"? And how do I get the thread title? How do I get the subreddit name (if relevant)?\\n\\nedit:  \\n\\n#solution:\\n\\nthread title and subreddit name is irrelevant.\\n\\n\\\"thread id\\\" or link id is retrieved via:  \\n\\n    post.link_id\\n\\nconstructing the link:   \\n\\n    https://www.reddit.com//comments/\\u003Clink id\\u003E//\\u003Cpost id\\u003E\\n    [Some text](/comments/\\u003Clink id\\u003E//\\u003Cpost id\\u003E)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3v5muu\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"lumbdi\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1449088205.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/\", \"locked\": false, \"name\": \"t3_3v5muu\", \"created\": 1449101293.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How to generate a link to the comment?\", \"created_utc\": 1449072493.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi, I am not sure if this is the best place to post this. If not please forgive me and hopefully point me to the best place.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have created a site that has a bunch of tennis \\u003Ca href=\\\"https://www.racketlogger.com/\\\"\\u003Eracket and string specs\\u003C/a\\u003E (for \\u003Ca href=\\\"https://www.racketlogger.com/racket/babolat/pure-aero\\\"\\u003Eexample\\u003C/a\\u003E). I wanted to see if there was a way in a sub to automatically parse, say \\u0026quot;/racket/babolat/pure-aero\\u0026quot; in posts (or some other shortening) and embed some of that information.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis would would be useful to discuss strings and rackets, e.g. in \\u003Ca href=\\\"/r/tennis\\\"\\u003E/r/tennis\\u003C/a\\u003E.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThere would be two sides to this:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E1)  I will have to build the embedded version of that page (or some API that returns that info as expected). This can be done easily. And,\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E2) Something has to be embedded in the sub by the mods? I am clueless about this, though I see a lot of subs that have lots of interesting functionalities sort of \\u0026quot;embedded\\u0026quot; into them.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs this possible?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIf so, any pointers?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThanks in advance!\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi, I am not sure if this is the best place to post this. If not please forgive me and hopefully point me to the best place.\\n\\nI have created a site that has a bunch of tennis [racket and string specs](https://www.racketlogger.com/) (for [example](https://www.racketlogger.com/racket/babolat/pure-aero)). I wanted to see if there was a way in a sub to automatically parse, say \\\"/racket/babolat/pure-aero\\\" in posts (or some other shortening) and embed some of that information.\\n\\nThis would would be useful to discuss strings and rackets, e.g. in /r/tennis.\\n\\nThere would be two sides to this:\\n\\n1)  I will have to build the embedded version of that page (or some API that returns that info as expected). This can be done easily. And,\\n\\n2) Something has to be embedded in the sub by the mods? I am clueless about this, though I see a lot of subs that have lots of interesting functionalities sort of \\\"embedded\\\" into them.\\n\\nIs this possible?\\n\\nIf so, any pointers?\\n\\nThanks in advance!\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3uzsqp\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"cpg\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3uzsqp/adding_a_plugin_to_a_sub/\", \"locked\": false, \"name\": \"t3_3uzsqp\", \"created\": 1449004777.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3uzsqp/adding_a_plugin_to_a_sub/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Adding a \\\"plugin\\\" to a sub?\", \"created_utc\": 1448975977.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI cant figure out where to put the \\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eadmins = \\u0026lt;MyUsername\\u0026gt;\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eline\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I cant figure out where to put the \\n\\nadmins = \\u003CMyUsername\\u003E\\n\\nline\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3uzrqy\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"Chr1st0C0der\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3uzrqy/how_do_i_make_myself_an_admin_on_my_reddit_clone/\", \"locked\": false, \"name\": \"t3_3uzrqy\", \"created\": 1449004181.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3uzrqy/how_do_i_make_myself_an_admin_on_my_reddit_clone/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How do I make myself an admin on my reddit clone,\", \"created_utc\": 1448975381.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EBizarrely it only happens on some posts when loading more comments (ie the next page of comments rather than child comments).\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny suggestions?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Bizarrely it only happens on some posts when loading more comments (ie the next page of comments rather than child comments).\\n\\nAny suggestions?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3uoty6\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"micwallace\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3uoty6/520_origin_error_using_oauth_morechildren_endpoint/\", \"locked\": false, \"name\": \"t3_3uoty6\", \"created\": 1448811491.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3uoty6/520_origin_error_using_oauth_morechildren_endpoint/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"520 Origin Error using OAuth morechildren endpoint\", \"created_utc\": 1448782691.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been seeing many seemingly random 400 errors for requests to /api/info, with 100 comma-separated fullnames, limit as \\u003Ccode\\u003E100\\u003C/code\\u003E, and \\u003Ccode\\u003Eraw_json\\u003C/code\\u003E enabled. If I repeat those same requests exactly as they were attempted, I get a successful response.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThe response is HTML data containing \\u0026quot;400 Bad Request\\u0026quot;.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EIs there something I can do to avoid these?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EExample request details:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003EDate: 2015/11/16\\nTime 06:30:57 UTC\\nStatus code: 400\\nAuthorization: Bearer ****\\nUser-Agent: rockets:nodejs:master by u/rtheunissen\\nRequest parameters:\\n    Path: https://oauth.reddit.com/api/info\\n    Query:\\n        id: \\u0026quot;t3_3szqtn,t3_3szqto,...\\u0026quot;\\n        limit: 100\\n        raw_json: 1\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been seeing many seemingly random 400 errors for requests to /api/info, with 100 comma-separated fullnames, limit as `100`, and `raw_json` enabled. If I repeat those same requests exactly as they were attempted, I get a successful response.\\n\\nThe response is HTML data containing \\\"400 Bad Request\\\".\\n\\nIs there something I can do to avoid these?\\n\\nExample request details:\\n\\n    Date: 2015/11/16\\n    Time 06:30:57 UTC\\n    Status code: 400\\n    Authorization: Bearer ****\\n    User-Agent: rockets:nodejs:master by u/rtheunissen\\n    Request parameters:\\n        Path: https://oauth.reddit.com/api/info\\n        Query:\\n            id: \\\"t3_3szqtn,t3_3szqto,...\\\"\\n            limit: 100\\n            raw_json: 1\\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3t3hbs\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"rtheunissen\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3t3hbs/occasionally_getting_400_status_codes_for_valid/\", \"locked\": false, \"name\": \"t3_3t3hbs\", \"created\": 1447751798.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3t3hbs/occasionally_getting_400_status_codes_for_valid/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Occasionally getting 400 status codes for valid requests\", \"created_utc\": 1447722998.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EGiven just for the fact that I want to play around, I quite literally just want to make a new model just for my local install. I know, I\\u0026#39;d have to do the API setup, templates, JavaScript for the GUI to interact with such; but is there any non pure Python precedent when it comes to me needing to set up cassandra or anything of the like (when it comes to both inheriting from Thing and tdb_cassandra.Thing)?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Given just for the fact that I want to play around, I quite literally just want to make a new model just for my local install. I know, I'd have to do the API setup, templates, JavaScript for the GUI to interact with such; but is there any non pure Python precedent when it comes to me needing to set up cassandra or anything of the like (when it comes to both inheriting from Thing and tdb_cassandra.Thing)?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3s9hv0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"13steinj\", \"media\": null, \"score\": 3, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3s9hv0/is_there_any_non_python_setup_id_have_to_do_if_i/\", \"locked\": false, \"name\": \"t3_3s9hv0\", \"created\": 1447185515.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3s9hv0/is_there_any_non_python_setup_id_have_to_do_if_i/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is there any non Python setup I'd have to do if I want to make an entirely new Thing?\", \"created_utc\": 1447156715.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 3}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003Eam I crazy or is there no way to get the permalink/context-link of a comment from a user-profile?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/user/GallowBoob/.json\\\"\\u003Ehttps://www.reddit.com/user/GallowBoob/.json\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E.context gives you the post url (ie \\u0026quot;google.com\\u0026quot;)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E.link_url is the same link as context :scratches head:\\u003C/p\\u003E\\n\\n\\u003Cp\\u003E.permalink DNE\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI have the subreddit and post id and title -but I\\u0026#39;m not seriously suppose to rebuild the reddit comment permalink with that, am I? (I can never tell where the title is going to be cut off in the comment page url :-/)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ewhat am I missing? \\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"am I crazy or is there no way to get the permalink/context-link of a comment from a user-profile?\\n\\nhttps://www.reddit.com/user/GallowBoob/.json\\n\\n.context gives you the post url (ie \\\"google.com\\\")\\n\\n.link_url is the same link as context :scratches head:\\n\\n.permalink DNE\\n\\nI have the subreddit and post id and title -but I'm not seriously suppose to rebuild the reddit comment permalink with that, am I? (I can never tell where the title is going to be cut off in the comment page url :-/)\\n\\nwhat am I missing? \\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ry54o\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"SamSlate\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 10, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ry54o/get_comments_permalink_from_profile_page_json/\", \"locked\": false, \"name\": \"t3_3ry54o\", \"created\": 1446969386.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ry54o/get_comments_permalink_from_profile_page_json/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Get comment's permalink from profile page? [json]\", \"created_utc\": 1446940586.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been trying to make a bot that finds a certain domain in the submission or the body of the comment doing something with that information. What I have tried is going for submissions is\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esubmissions = r.get_subreddit(\\u0026#39;all\\u0026#39;).get_new(limit=None)\\n#iterate through\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003Eand\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Esubmissions = praw.helpers.submission_stream(r, \\u0026quot;all\\u0026quot;, limit=10)\\n#iterate through\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EI don\\u0026#39;t know which one these methods would work best for getting all submissions. I\\u0026#39;ve tried looking for a solution and looking at other bots and I haven\\u0026#39;t found a good answer yet. Does anyone have an idea?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been trying to make a bot that finds a certain domain in the submission or the body of the comment doing something with that information. What I have tried is going for submissions is\\n\\n    submissions = r.get_subreddit('all').get_new(limit=None)\\n    #iterate through\\n\\nand\\n\\n    submissions = praw.helpers.submission_stream(r, \\\"all\\\", limit=10)\\n    #iterate through\\n\\nI don't know which one these methods would work best for getting all submissions. I've tried looking for a solution and looking at other bots and I haven't found a good answer yet. Does anyone have an idea?\\n    \\n\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3rsk3k\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"VRCkid\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3rsk3k/praw_whats_the_best_way_of_checking_all/\", \"locked\": false, \"name\": \"t3_3rsk3k\", \"created\": 1446863372.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3rsk3k/praw_whats_the_best_way_of_checking_all/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[PRAW] What's the best way of checking all Submissions and all Comments?\", \"created_utc\": 1446834572.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EToday, I received reports from my users that submitting posts or comments was not working. I did some investigation and the api calls were returning html pages that had some forbidden text on them. I then noticed that /api/login is returning an empty modhash. Was this an intended breaking change or is this a regression?\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAnd yes I know I need to use oauth. I am working on the migration now, but I already have a lot of clients in the wild using /api/login\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Today, I received reports from my users that submitting posts or comments was not working. I did some investigation and the api calls were returning html pages that had some forbidden text on them. I then noticed that /api/login is returning an empty modhash. Was this an intended breaking change or is this a regression?\\n\\nAnd yes I know I need to use oauth. I am working on the migration now, but I already have a lot of clients in the wild using /api/login\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3rldxs\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"det0ur\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1446701633.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3rldxs/apilogin_not_returning_a_modhash/\", \"locked\": false, \"name\": \"t3_3rldxs\", \"created\": 1446729755.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3rldxs/apilogin_not_returning_a_modhash/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"/api/login not returning a modhash\", \"created_utc\": 1446700955.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI submitted a comment about the Random button \\u003Ca href=\\\"https://www.reddit.com/r/TheoryOfReddit/comments/3r9352/random_link_algorithm/cwlzm9x\\\"\\u003Ehere\\u003C/a\\u003E  Can anyone explain how that button works. I am interested in the algorithm that chooses the subreddit. I can look at the code, but I am sure many of you have looked into this before and have quicker response time. :-)\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I submitted a comment about the Random button [here](https://www.reddit.com/r/TheoryOfReddit/comments/3r9352/random_link_algorithm/cwlzm9x)  Can anyone explain how that button works. I am interested in the algorithm that chooses the subreddit. I can look at the code, but I am sure many of you have looked into this before and have quicker response time. :-)\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3r9fbf\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"activow\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 7, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3r9fbf/random_button_algorithm/\", \"locked\": false, \"name\": \"t3_3r9fbf\", \"created\": 1446527282.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3r9fbf/random_button_algorithm/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Random button algorithm\", \"created_utc\": 1446498482.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHere are the full logs\\n\\u003Ca href=\\\"https://gist.github.com/barmstrong/a19d198e604c28289299\\\"\\u003Ehttps://gist.github.com/barmstrong/a19d198e604c28289299\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAny ideas? Thanks!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit: I tried manually re-running the \\u003Ccode\\u003Einject_test_data.py\\u003C/code\\u003E, here is what I got\\n\\u003Ca href=\\\"https://gist.github.com/barmstrong/04a70fe7bf8e444ef37f\\\"\\u003Ehttps://gist.github.com/barmstrong/04a70fe7bf8e444ef37f\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EEdit2: Even if the test data import was unsuccessful, I figured it would still boot. Seems that things (python, cassandra, rabbitmq) are running \\u003Ca href=\\\"https://gist.github.com/barmstrong/014bdd4ec24c56dcf66d\\\"\\u003Ehttps://gist.github.com/barmstrong/014bdd4ec24c56dcf66d\\u003C/a\\u003E\\nBut reddit.local still returns \\u0026quot;503 Service Unavailable No server is available to handle this request.\\u0026quot;\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Here are the full logs\\nhttps://gist.github.com/barmstrong/a19d198e604c28289299\\n\\nAny ideas? Thanks!\\n\\nEdit: I tried manually re-running the `inject_test_data.py`, here is what I got\\nhttps://gist.github.com/barmstrong/04a70fe7bf8e444ef37f\\n\\nEdit2: Even if the test data import was unsuccessful, I figured it would still boot. Seems that things (python, cassandra, rabbitmq) are running https://gist.github.com/barmstrong/014bdd4ec24c56dcf66d\\nBut reddit.local still returns \\\"503 Service Unavailable No server is available to handle this request.\\\"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3r1d0j\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"bdarmstrong\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1446345736.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3r1d0j/vagrant_up_fails/\", \"locked\": false, \"name\": \"t3_3r1d0j\", \"created\": 1446372130.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3r1d0j/vagrant_up_fails/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Vagrant Up fails\", \"created_utc\": 1446343330.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"github.com\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3q04q8\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"spredder2016\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3q04q8/apimorechildrenjson_returns_a_flattened_comment/\", \"locked\": false, \"name\": \"t3_3q04q8\", \"created\": 1445695525.0, \"url\": \"https://github.com/reddit/reddit/issues/1436\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"/api/morechildren.json returns a flattened comment hierarchy\", \"created_utc\": 1445666725.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m very new to using \\u003Ca href=\\\"https://github.com/jcleblanc/reddit-php-sdk\\\"\\u003Ethis recommended PHP api\\u003C/a\\u003E with reddit. I\\u0026#39;ve followed instructions but I get 403 forbidden. Here is my (brief) test code:\\u003C/p\\u003E\\n\\n\\u003Cpre\\u003E\\u003Ccode\\u003Erequire_once(\\u0026#39;reddit.php\\u0026#39;);\\n$reddit = new reddit();\\n$username = strip_tags($_POST[\\u0026#39;username\\u0026#39;]);\\n$response = $reddit-\\u0026gt;getHistory($username, \\u0026quot;comments\\u0026quot;);\\necho \\u0026quot;\\u0026lt;textarea\\u0026gt;Var dump: \\u0026quot;.var_dump($response).\\u0026quot;\\u0026lt;/textarea\\u0026gt;\\u0026quot;;\\n\\u003C/code\\u003E\\u003C/pre\\u003E\\n\\n\\u003Cp\\u003EMy configuration file (minus secret and client id): \\u003Ca href=\\\"http://pastebin.com/fdLq0yeH\\\"\\u003Ehttp://pastebin.com/fdLq0yeH\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EThis outputs a textarea with nothing but \\u0026#39;Var dump:\\u0026#39; in it, and I get (in big letters) 403 Forbidden. What could I be doing wrong? Any help is appreciated.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm very new to using [this recommended PHP api](https://github.com/jcleblanc/reddit-php-sdk) with reddit. I've followed instructions but I get 403 forbidden. Here is my (brief) test code:\\n\\n    require_once('reddit.php');\\n    $reddit = new reddit();\\n    $username = strip_tags($_POST['username']);\\n    $response = $reddit-\\u003EgetHistory($username, \\\"comments\\\");\\n    echo \\\"\\u003Ctextarea\\u003EVar dump: \\\".var_dump($response).\\\"\\u003C/textarea\\u003E\\\";\\n\\nMy configuration file (minus secret and client id): http://pastebin.com/fdLq0yeH\\n\\nThis outputs a textarea with nothing but 'Var dump:' in it, and I get (in big letters) 403 Forbidden. What could I be doing wrong? Any help is appreciated.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3pod7j\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"JThistl3\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3pod7j/403_forbidden_with_reddit_php_sdkapi/\", \"locked\": false, \"name\": \"t3_3pod7j\", \"created\": 1445486092.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3pod7j/403_forbidden_with_reddit_php_sdkapi/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"403 forbidden with reddit PHP SDK/api\", \"created_utc\": 1445457292.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3pl9yw\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"notsonano\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 5, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3pl9yw/where_do_i_find_a_todo_list_for_contributions/\", \"locked\": false, \"name\": \"t3_3pl9yw\", \"created\": 1445429244.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3pl9yw/where_do_i_find_a_todo_list_for_contributions/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Where do I find a Todo list for contributions?\", \"created_utc\": 1445400444.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;m trying to find the actual commit that changed it (by it I mean \\u003Ca href=\\\"https://www.reddit.com/r/shittychangelog/comments/3oxiaz/reddit_change_voting_algorithms_updated_to_be/\\\"\\u003Ethis\\u003C/a\\u003E, yes, something \\u003Cem\\u003Ewas\\u003C/em\\u003E changed, I\\u0026#39;ve checked), and I, well, can\\u0026#39;t.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI just fastfowarded my local master branch to be even with reddit\\u0026#39;s and the vote buttons are definitely different than what is on the site right now, at a minimum, the \\u003Ccode\\u003Eonclick\\u003C/code\\u003E attribute has been removed from the live site, possibly moved elsewhere.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I'm trying to find the actual commit that changed it (by it I mean [this](https://www.reddit.com/r/shittychangelog/comments/3oxiaz/reddit_change_voting_algorithms_updated_to_be/), yes, something *was* changed, I've checked), and I, well, can't.\\n\\nI just fastfowarded my local master branch to be even with reddit's and the vote buttons are definitely different than what is on the site right now, at a minimum, the `onclick` attribute has been removed from the live site, possibly moved elsewhere.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3p8zxd\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"13steinj\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 15, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3p8zxd/was_the_change_in_the_voting_javascript_closed/\", \"locked\": false, \"name\": \"t3_3p8zxd\", \"created\": 1445219097.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3p8zxd/was_the_change_in_the_voting_javascript_closed/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Was the change in the voting javascript closed source?\", \"created_utc\": 1445190297.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi all, I\\u0026#39;m building a basic random post puller (node.js + shell script, extremely basic) and am having an issue getting a random post from a subreddit. I\\u0026#39;m sending \\u0026quot;curl -L \\u003Ca href=\\\"http://www.reddit.com/r/dadjokes/random.json?limit=1\\\"\\u003Ehttp://www.reddit.com/r/dadjokes/random.json?limit=1\\u003C/a\\u003E\\u0026quot; and am correctly parsing the data, but it takes a very long time for the resulting json to change. What do I need to do to ensure the json is different (or at least random) every time?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi all, I'm building a basic random post puller (node.js + shell script, extremely basic) and am having an issue getting a random post from a subreddit. I'm sending \\\"curl -L http://www.reddit.com/r/dadjokes/random.json?limit=1\\\" and am correctly parsing the data, but it takes a very long time for the resulting json to change. What do I need to do to ensure the json is different (or at least random) every time?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ozv4a\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"AwesomeFaic\", \"media\": null, \"score\": 6, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 6, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ozv4a/curl_request_not_receiving_random_json/\", \"locked\": false, \"name\": \"t3_3ozv4a\", \"created\": 1445035890.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ozv4a/curl_request_not_receiving_random_json/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Curl request not receiving random json\", \"created_utc\": 1445007090.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 6}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003ELong story short in the process of altering the \\u003Ccode\\u003Erun_new_comments\\u003C/code\\u003E method in \\u003Ccode\\u003Er2/lib/db/queries.py\\u003C/code\\u003E for my own purposes I made a comment to test it, and it turns out my syntax was wrong (as shown by the /var/log/upstart/reddit-consumer-newcomments_q-1.log) and I got an error thrown.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EAfter fixing the error and adding a new comment, the amount of items in the queue just didn\\u0026#39;t go down back to 0.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ERunning \\u003Ccode\\u003Esudo restart reddit-consumer-newcomments_q x=1\\u003C/code\\u003E didn\\u0026#39;t do anything for me either, the amount of items in the queue just stayed the same.\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Long story short in the process of altering the `run_new_comments` method in `r2/lib/db/queries.py` for my own purposes I made a comment to test it, and it turns out my syntax was wrong (as shown by the /var/log/upstart/reddit-consumer-newcomments_q-1.log) and I got an error thrown.\\n\\nAfter fixing the error and adding a new comment, the amount of items in the queue just didn't go down back to 0.\\n\\nRunning `sudo restart reddit-consumer-newcomments_q x=1` didn't do anything for me either, the amount of items in the queue just stayed the same.\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3oxesx\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"13steinj\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 2, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3oxesx/how_would_i_clear_out_the_newcomments_q/\", \"locked\": false, \"name\": \"t3_3oxesx\", \"created\": 1444983900.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3oxesx/how_would_i_clear_out_the_newcomments_q/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How would I clear out the newcomments_q?\", \"created_utc\": 1444955100.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EHi!\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI\\u0026#39;ve used the reddit api some time back, but it wasn\\u0026#39;t with OAuth2, it seems that this is now the required standard, alas I have but a very basic grasp of what such includes. (I have read \\u003Ca href=\\\"https://github.com/reddit/reddit/wiki/OAuth2\\\"\\u003Ethe \\u0026quot;guide\\u0026quot;\\u003C/a\\u003E)\\u003C/p\\u003E\\n\\n\\u003Cp\\u003EI pretty much just need the ability to do what I did before when I was using a cookie.. But I can\\u0026#39;t seem to figure out which method of getting the OAuth token best applies. I thought the \\u0026quot;application only\\u0026quot; might be the way to go, but apparently they are for \\u0026quot;logged out\\u0026quot; users, which I don\\u0026#39;t really understand. Trying one of the other methods requires some \\u0026quot;redirect_uri\\u0026quot;, which doesn\\u0026#39;t make sense for my use case. Can someone help me understand how to go about this?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"Hi!\\n\\nI've used the reddit api some time back, but it wasn't with OAuth2, it seems that this is now the required standard, alas I have but a very basic grasp of what such includes. (I have read [the \\\"guide\\\"](https://github.com/reddit/reddit/wiki/OAuth2))\\n\\nI pretty much just need the ability to do what I did before when I was using a cookie.. But I can't seem to figure out which method of getting the OAuth token best applies. I thought the \\\"application only\\\" might be the way to go, but apparently they are for \\\"logged out\\\" users, which I don't really understand. Trying one of the other methods requires some \\\"redirect_uri\\\", which doesn't make sense for my use case. Can someone help me understand how to go about this?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ou2eh\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"relativer\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ou2eh/need_help_with_authentication_and_using_oauth/\", \"locked\": false, \"name\": \"t3_3ou2eh\", \"created\": 1444927596.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ou2eh/need_help_with_authentication_and_using_oauth/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Need help with authentication and using OAuth.\", \"created_utc\": 1444898796.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003EI\\u0026#39;ve been dabbling around with a hobby project which is basically just another Reddit reader for Android. If and when I get it done, I\\u0026#39;d like to release it in Google Play and maybe add some ads there to get some candy money.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003ENow, reading through the \\u003Ca href=\\\"https://www.reddit.com/wiki/licensing\\\"\\u003Elicensing page\\u003C/a\\u003E, it says that commercial use requires a licensing agreement. I tried searching whether or not the license is free or not, but didn\\u0026#39;t really find anything. So... Do I have to pay for the license, or is it free?\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"I've been dabbling around with a hobby project which is basically just another Reddit reader for Android. If and when I get it done, I'd like to release it in Google Play and maybe add some ads there to get some candy money.\\n\\nNow, reading through the [licensing page](https://www.reddit.com/wiki/licensing), it says that commercial use requires a licensing agreement. I tried searching whether or not the license is free or not, but didn't really find anything. So... Do I have to pay for the license, or is it free?\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ofh43\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"-manabreak\", \"media\": null, \"score\": 4, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 4, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ofh43/is_the_licensing_agreement_free_or_paid/\", \"locked\": false, \"name\": \"t3_3ofh43\", \"created\": 1444667134.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3ofh43/is_the_licensing_agreement_free_or_paid/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Is the licensing agreement free or paid?\", \"created_utc\": 1444638334.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 4}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"redditclient.readthedocs.org\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ocnik\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"earth-tone\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 1, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3ocnik/documentation_for_the_perl_api_wrapper/\", \"locked\": false, \"name\": \"t3_3ocnik\", \"created\": 1444613436.0, \"url\": \"http://redditclient.readthedocs.org/en/latest/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Documentation for the Perl API wrapper Reddit::Client. Versions 1.0+ are Oauth only.\", \"created_utc\": 1444584636.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"ubwg.net\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": null, \"selftext\": \"\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3nksrq\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"earth-tone\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 0, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": false, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3nksrq/oauth_scripts_for_dummies/\", \"locked\": false, \"name\": \"t3_3nksrq\", \"created\": 1444083416.0, \"url\": \"http://ubwg.net/b/reddit-oauth-scripts-for-dummies\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"Oauth Scripts for Dummies\", \"created_utc\": 1444054616.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}, {\"kind\": \"t3\", \"data\": {\"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": {}, \"subreddit\": \"redditdev\", \"selftext_html\": \"\\u003C!-- SC_OFF --\\u003E\\u003Cdiv class=\\\"md\\\"\\u003E\\u003Cp\\u003E\\u003Ca href=\\\"https://www.reddit.com/r/Android/comments/3lb2ga/dev_i_have_been_experimenting_and_building_an/\\\"\\u003Ereadit\\u003C/a\\u003E app author here.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Etoday I have pushed into github a very important part of readit android app, slightly modified and integrated snudown \\u0026amp; htmltextview projects, the aim is to parse and render reddit markdown correctly on android devices.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ehere a screenshot (meh!) \\u003Ca href=\\\"http://imgur.com/0N4whz9\\\"\\u003Ehttp://imgur.com/0N4whz9\\u003C/a\\u003E\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Ethis is an initial commit, I\\u0026#39;m still trying to figure out how to implement table.\\u003C/p\\u003E\\n\\n\\u003Cp\\u003Eedit: ha ha I forgot, here the repository \\u003Ca href=\\\"https://github.com/netdur/rdown\\\"\\u003Ehttps://github.com/netdur/rdown\\u003C/a\\u003E\\u003C/p\\u003E\\n\\u003C/div\\u003E\\u003C!-- SC_ON --\\u003E\", \"selftext\": \"[readit](https://www.reddit.com/r/Android/comments/3lb2ga/dev_i_have_been_experimenting_and_building_an/) app author here.\\n\\ntoday I have pushed into github a very important part of readit android app, slightly modified and integrated snudown \\u0026 htmltextview projects, the aim is to parse and render reddit markdown correctly on android devices.\\n\\nhere a screenshot (meh!) http://imgur.com/0N4whz9\\n\\nthis is an initial commit, I'm still trying to figure out how to implement table.\\n\\nedit: ha ha I forgot, here the repository https://github.com/netdur/rdown\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3n3mv0\", \"from_kind\": null, \"gilded\": 0, \"archived\": true, \"clicked\": false, \"report_reasons\": null, \"author\": \"adel_b\", \"media\": null, \"score\": 5, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"num_comments\": 11, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"hide_score\": false, \"edited\": 1443707691.0, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"secure_media_embed\": {}, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"/r/redditdev/comments/3n3mv0/android_snudown_java_binding_now_available/\", \"locked\": false, \"name\": \"t3_3n3mv0\", \"created\": 1443735312.0, \"url\": \"https://www.reddit.com/r/redditdev/comments/3n3mv0/android_snudown_java_binding_now_available/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"[android] snudown java binding now available\", \"created_utc\": 1443706512.0, \"distinguished\": null, \"mod_reports\": [], \"visited\": false, \"num_reports\": null, \"ups\": 5}}], \"after\": \"t3_3n3mv0\", \"before\": null}}"
        },
        "headers": {
          "CF-RAY": "2b59b9df19db20c6-LAX",
          "Connection": "keep-alive",
          "Content-Type": "application/json; charset=UTF-8",
          "Date": "Sun, 19 Jun 2016 20:28:12 GMT",
          "Server": "cloudflare-nginx",
          "Strict-Transport-Security": "max-age=15552000; includeSubDomains; preload",
          "Transfer-Encoding": "chunked",
          "Vary": "accept-encoding",
          "X-Moose": "majestic",
          "cache-control": "private, s-maxage=0, max-age=0, must-revalidate",
          "expires": "-1",
          "x-content-type-options": "nosniff",
          "x-frame-options": "SAMEORIGIN",
          "x-ratelimit-remaining": "577.0",
          "x-ratelimit-reset": "108",
          "x-ratelimit-used": "23",
          "x-reddit-tracking": "https://pixel.redditmedia.com/pixel/of_destiny.png?v=oCvuxaqFRKctaXTsaVviDdoYERtq5lwjYdQD0b2180dD5ThaD%2FWx7Ot1t9%2B7lU9zTf6G4WRsTzWlCwUSximt8QqZnlipJu4A",
          "x-ua-compatible": "IE=edge",
          "x-xss-protection": "1; mode=block"
        },
        "status": {
          "code": 200,
          "message": "OK"
        },
        "url": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_mev1j&raw_json=1"
      }
    },
    {
      "recorded_at": "2016-06-19T20:28:13",
      "request": {
        "body": {
          "encoding": "utf-8",
          "string": ""
        },
        "headers": {
          "Accept": "*/*",
          "Accept-Encoding": "identity",
          "Authorization": "bearer YE4ESXZ9lTjIP0ukcopwXM6361g",
          "Connection": "keep-alive",
          "Cookie": "loid=SdAZIdDQoUDzjNqfRk; loidcreated=2016-06-19T20%3A27%3A59.507Z; __cfduid=da3dbedd6bfa050f4f12fea14395b830f1466368079",
          "User-Agent": "prawtools/0.19 PRAW/4.0.0b4 prawcore/0.0.8"
        },
        "method": "GET",
        "uri": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_3n3mv0&raw_json=1"
      },
      "response": {
        "body": {
          "encoding": "UTF-8",
          "string": "{\"kind\": \"Listing\", \"data\": {\"modhash\": null, \"children\": [], \"after\": null, \"before\": null}}"
        },
        "headers": {
          "CF-RAY": "2b59b9e6dacb20c6-LAX",
          "Connection": "keep-alive",
          "Content-Length": "93",
          "Content-Type": "application/json; charset=UTF-8",
          "Date": "Sun, 19 Jun 2016 20:28:13 GMT",
          "Server": "cloudflare-nginx",
          "Strict-Transport-Security": "max-age=15552000; includeSubDomains; preload",
          "X-Moose": "majestic",
          "cache-control": "private, s-maxage=0, max-age=0, must-revalidate",
          "expires": "-1",
          "x-content-type-options": "nosniff",
          "x-frame-options": "SAMEORIGIN",
          "x-ratelimit-remaining": "576.0",
          "x-ratelimit-reset": "107",
          "x-ratelimit-used": "24",
          "x-reddit-tracking": "https://pixel.redditmedia.com/pixel/of_destiny.png?v=iYhlMFgh7Flz1yXquRbsdKVfh8FxqTB1taZkK6e3FOtJtZ5h0W1MAWOVgKuuSHegz0LxH%2BBNiFdKhR92C35fO0yG17KbwLBQ",
          "x-ua-compatible": "IE=edge",
          "x-xss-protection": "1; mode=block"
        },
        "status": {
          "code": 200,
          "message": "OK"
        },
        "url": "https://oauth.reddit.com/r/redditdev/top?t=all&limit=1024&after=t3_3n3mv0&raw_json=1"
      }
    }
  ],
  "recorded_with": "betamax/0.5.1"
}